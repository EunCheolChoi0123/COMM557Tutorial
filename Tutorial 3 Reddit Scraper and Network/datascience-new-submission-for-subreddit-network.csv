id,author,created_utc,num_comments,permalink,subreddit,title,selftext,score,upvote_ratio,url,over_18,is_self,is_original_content
1ne37bs,ciaoshescu,1757578723.0,5,/r/datascience/comments/1ne37bs/looking_for_recent_research_on_explainable_ai_xai/,datascience,Looking for recent research on explainable AI (XAI),I'd love to get some papers on the latest advancements on explainable AI (XAI). I'm looking for papers that are at most 2-3 years old and had an impact. Thanks!,6,0.8,https://www.reddit.com/r/datascience/comments/1ne37bs/looking_for_recent_research_on_explainable_ai_xai/,False,True,False
1ne1d5t,ButtFlannel69,1757571506.0,1,/r/datascience/comments/1ne1d5t/collaborating_with_data_teams/,datascience,Collaborating with data teams,,1,1.0,/r/ProductManagement/comments/1nd6p8o/collaborating_with_data_teams/,False,False,False
1nd94cy,ThomasAger,1757494955.0,0,/r/datascience/comments/1nd94cy/smile_its_my_first_open_source_project/,datascience,(: Smile! It’s my first open source project,,1,0.52,/r/opensource/comments/1nd946a/smile_its_my_first_open_source_project/,False,False,False
1ncmcgf,Factitious_Character,1757432468.0,19,/r/datascience/comments/1ncmcgf/pytorch_lightning_vs_pytorch/,datascience,Pytorch lightning vs pytorch,"Today at work, i was criticized by a colleague for implementing my training script in pytorch instead of pytorch lightning. His rationale was that the same thing could've been done in less code using lightning, and more code means more documentation and explaining to do. I havent familiarized myself with pytorch lightning yet so im not sure if this is fair criticism, or something i should take with a grain of salt. I do intend to read the lightning docs soon but im just thinking about this for my own learning. Any thoughts?",60,0.94,https://www.reddit.com/r/datascience/comments/1ncmcgf/pytorch_lightning_vs_pytorch/,False,True,False
1nc93qq,bingbong_sempai,1757390645.0,14,/r/datascience/comments/1nc93qq/i_built_a_card_recommender_for_edh_decks/,datascience,I built a card recommender for EDH decks,"Hi guys! I built a simple card recommender system for the EDH format of Magic the Gathering. Unlike EDHREC which suggests cards based on overall popularity, this analyzes your full decklist and recommends cards based on similar decks.

Deck similarity is computed as the sum of idf weights of shared cards. It then shows the top 100 cards from similar decks that aren't already in your decklist. It's simple but will usually give more relevant suggestions for your deck.

Try it [here](https://huggingface.co/spaces/bingbong-sempai/edhrec-at-home): (Archidekt links only)

Would love to hear feedback!",19,0.91,https://www.reddit.com/r/datascience/comments/1nc93qq/i_built_a_card_recommender_for_edh_decks/,False,True,False
1nbxzs0,samushusband,1757361243.0,8,/r/datascience/comments/1nbxzs0/analysing_priority_zones_in_my_area_with/,datascience,Analysing Priority zones in my Area with unprecise home adresses,"hello, My project analyzes whether given addresses fall inside ""Quartiers Prioritaires de la Politique de la Ville ""(QPV). It uses a GeoJSON file of QPV boundaries(available on the gorvernment website) and a geocoding service (Nominatim/OSM) to convert addresses into geographic coordinates. Each address is then checked with GeoPandas + Shapely to determine if its coordinates lie within any QPV polygon. The program can process one or multiple addresses, returning results that indicate whether each is located inside or outside a QPV, along with the corresponding zone name when available. This tool can be extended to handle CSV databases, produce visualizations on maps, or integrate into larger urban policy analysis workflows. "" 

BUUUT . 

here is the ultimate problem of this project , Home addresses in my area (Martinique) are notoriously unreliable if you dont know the way and google maps or Nominatim cant pinpoint most of the places in order to be converted to coordinates to say whether or not the person who gave the adress is in a QPV or not.  when i use my python script on adresses of the main land like paris and the like it works just fine but our little island isnt as well defined in terms of urban planning. 

can someone please help me to find a way to get all the streets data into coordinates and make them match with the polygon of the QPV areas ? thank you in advance",15,0.94,https://www.reddit.com/r/datascience/comments/1nbxzs0/analysing_priority_zones_in_my_area_with/,False,True,False
1nbdtct,AutoModerator,1757304098.0,22,/r/datascience/comments/1nbdtct/weekly_entering_transitioning_thread_08_sep_2025/,datascience,"Weekly Entering & Transitioning - Thread 08 Sep, 2025 - 15 Sep, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",11,1.0,https://www.reddit.com/r/datascience/comments/1nbdtct/weekly_entering_transitioning_thread_08_sep_2025/,False,True,False
1naptuq,mutlu_simsek,1757241077.0,2,/r/datascience/comments/1naptuq/perpetual_ml_suite_now_live_on_the_snowflake/,datascience,🚀 Perpetual ML Suite: Now Live on the Snowflake Marketplace!,,1,0.6,/r/snowflake/comments/1n84pfn/perpetual_ml_suite_now_live_on_the_snowflake/,False,False,False
1nac35j,metalvendetta,1757196044.0,12,/r/datascience/comments/1nac35j/how_to_evaluate_data_transformations/,datascience,How to evaluate data transformations?,"There are several well-established benchmarks for text-to-SQL tasks like BIRD, Spider, and WikiSQL. However, I'm working on a data transformation system that handles per-row transformations with contextual understanding of the input data.

The challenge is that most existing benchmarks focus on either:

* Pure SQL generation (BIRD, Spider)
* Simple data cleaning tasks
* Basic ETL operations

But what I'm looking for are benchmarks that test:

* Complex multi-step data transformations
* Context-aware operations (where the same instruction means different things based on data context)
* Cross-column reasoning and relationships
* Domain-specific transformations that require understanding the semantic meaning of data

  
Has anyone come across benchmarks or datasets that test these more sophisticated data transformation capabilities?

",1,0.6,https://www.reddit.com/r/datascience/comments/1nac35j/how_to_evaluate_data_transformations/,False,True,False
1na6x3q,Rockingtits,1757183259.0,32,/r/datascience/comments/1na6x3q/help_me_evaluate_a_new_job_offer_stay_or_go/,datascience,Help me evaluate a new job offer - Stay or go?,"Hi all, 

I'm having a really hard time deciding whether or not to take an offer I've recently received, would really appreciate some advice and a sense check. For context I generally feel my current role is comfortable but i'm starting to plateau after the first year, i'm also in the process of buying my dream house just to complicate things.

### **Current Role**

##### The Good

- I am early 30's and have 4 years of experience as a full stack DS but am currently employed as an ML Eng for the last year. 
- My current role is effectively a senior/lead MLE in a small team (me + 3 DS) and I have loads of autonomy in how we do things and I get to lead my own  Gen AI projects with small squads as I'm the only one with experience in this domain. 
- I also get to straddle DS and MLE as much or as little as I want to in other projects, which suits my interests and background. 
- We have some interesting projects including one I'm leading. I think I have around 6 months of cool work to do where I can personally make an impact. 
- My work life balance is amazing, I'm not stressed at work at all and I can learn at my own pace. 
- Effectively remote, go into the office 1 or 2 times per month for meetings. It's 1.5 hours away but work pay for my travel. 
- Can push for a senior or principal title and will likely get it in the next ~6 months. 

##### The Bad

- The main drawbacks here are that I don't have senior technical mentors, my direct boss has good soft skills but I have nothing to learn from him technically. He's also quite chaotic, so we are always shifting priorities etc. 
- It's a brand new team so we are constantly hitting blockers in terms of processes, integration of our projects and office politics. 
- Being a legacy insurer, innovation is really hard and momentum needed to shift opinions is huge. 
- Fundamentally data quality is very poor and this won't change in my tenure. 
- Essentially in an echo chamber, I'm bringing most of the ideas and solutions to the table in the team which potentially isn't great at this stage in my career.
- It's not perfect and I'd have to leave at some point anyway. 


##### Comp
- Total comp including bonus and generous pension is £84K


### **New Job** AI Engineer

##### The Good
- Very cool AI consultancy startup, 2 years old, ~80 technical staff and growing rapidly, already profitable with a revenue of £1mill per month and partnership with Open AI.
- Lots of interesting projects with cool clients. The founders' mantra is ""cool projects, in production"" and they have some genuinely interesting case studies. 
- Some projects are genuinely cutting edge and they claim to have a nice balance between R&D and delivery. 
- Lots of technical staff to learn from, should be good for my growth. 
- Opportunity to work internationally in the future, the are opening offices in Australia now and eventually the US. 


##### The Bad

- Pigeon holing myself into AI/Agents/LLMs. No trad ML, may lose some of my very rounded skill set.
- Although it's customer facing, it sounds like the role is very delivery heavy and I'd essentially be smashing out code or researching all day with less soft skill development.
- Slightly worried about work culture and work life balance, this could end up being a meat grinder. 
- I have no experience of start ups or start up culture at all.
- Less job security as its a startup. 
- It's mostly based in London (5 hours round trip!) and I would need to travel down relatively frequently (expenses paid) for onboarding and establishing myself in the first few months, with that requirement tapering off slowly. 


##### Comp

- Total offer all in is £90K, I could try and negotiate for up to £95K based on their bandings. 
- 36000 stock units, worthless until they sell though
   


Would love to know your thoughts!


",12,0.72,https://www.reddit.com/r/datascience/comments/1na6x3q/help_me_evaluate_a_new_job_offer_stay_or_go/,False,True,False
1n9yrfy,Massive_Arm_706,1757163059.0,109,/r/datascience/comments/1n9yrfy/europe_salary_thread_2025_whats_your_role_and/,datascience,Europe Salary Thread 2025 - What's your role and salary?,"The yearly Europe-centric salary thread. You can find the last one here:

https://old.reddit.com/r/datascience/comments/1fxrmzl/europe_salary_thread_2024_whats_your_role_and/

I think it's worthwhile to learn from one another and see what different flavours of data scientists, analysts and engineers are out there in the wild. In my opinion, this is especially useful for the beginners and transitioners among us. So, do feel free to talk a bit about your work if you can and want to. 🙂

While not the focus, non-Europeans are of course welcome, too. Happy to hear from you!

**Data Science Flavour:** .

**Location:** .

**Title:** .

**Compensation (gross):** .

**Education level:** .

**Experience:** .

**Industry/vertical:** .

**Company size:** .

**Majority of time spent using (tools):** .

**Majority of time spent doing (role):** .",180,0.95,https://www.reddit.com/r/datascience/comments/1n9yrfy/europe_salary_thread_2025_whats_your_role_and/,False,True,False
1n8z37l,vtfresh,1757058618.0,145,/r/datascience/comments/1n8z37l/just_got_rejected_from_meta/,datascience,Just got rejected from meta,"Thought everything went well. Completed all questions for all interviews. Felt strong about all my SQL, A/B testing, metric/goal selection questions. No red flags during behavioral. Interviews provided 0 feedback about the rejection. I was talking through all my answers and reasoning, considering alternatives and explaining why I chose my approach over others. I led the discussions and was very proactive and always thinking 2 steps ahead and about guardrail metrics and stating my assumptions. The only ways I could think of improving was to answer more confidently and structure my thoughts more. Is it just that competitive right now? Even if I don’t make IC5 I thought for sure I’d get IC4. Anyone else interview with Meta recently? 

edit:
MS degree
3.5yoe DS
4.5yoe ChemE

edit2:
I had 2 meta referrals but didn't use them. Should I tell the recruiter or does it not matter at this point?
Meta recruiter reached out to me on LinkedIn.

edit3:
I remember now there was 1 moment I missed a beat, but recovered during a bernoulli distribution hand-calculation question. Maybe thats all it took...

edit4:
Thanks everyone for the copium, words of advice, and support.",294,0.89,https://www.reddit.com/r/datascience/comments/1n8z37l/just_got_rejected_from_meta/,False,True,False
1n8lhdx,CryoSchema,1757018259.0,56,/r/datascience/comments/1n8lhdx/mit_says_ai_isnt_replacing_you_its_just_wasting/,datascience,MIT says AI isn’t replacing you… it’s just wasting your boss’s money,,552,0.93,https://www.interviewquery.com/p/mit-ai-isnt-replacing-workers-just-wasting-money,False,False,False
1n8a1do,petburiraja,1756992274.0,105,/r/datascience/comments/1n8a1do/a_portfolio_project_for_data_scientists_looking/,datascience,"A portfolio project for Data Scientists looking to add AI Engineering skills (Pytest, Security, Docker).","Hey guys,

Like many of us, I'm comfortable in a Jupyter Notebook, but I found there's a huge gap when it comes to building and deploying a real, full-stack AI application. I created a project specifically to bridge that gap.

You build a ""GitHub Repo Analyst"" agent, but the real learning is in the production-level engineering skills that often aren't part of a data science workflow:

- Automated Testing: Writing Pytest integration tests to verify your agent's security.
- Building UIs: Creating an interactive web app with Chainlit.
- Deployment: Packaging your entire application with Docker for easy, reproducible deployment.

I've turned this into a 10-lesson guide and am looking for 10-15 beta testers. If you're a data scientist who wants to add a serious AI engineering project to your portfolio, I'll give you the complete course for free in exchange for your feedback.

Just comment below if you're interested, and I'll send you a DM.",72,0.9,https://www.reddit.com/r/datascience/comments/1n8a1do/a_portfolio_project_for_data_scientists_looking/,False,True,False
1n88v2y,ShittyLogician,1756989168.0,107,/r/datascience/comments/1n88v2y/almost_2_years_into_my_first_job_and_already/,datascience,Almost 2 years into my first job... and already disillusioned and bored with this career,"**TL;DR: I find this industry to be very unengaging, with most use cases and positions being very brainless, sluggish and just uninspiring. I am only 2 years into this job and bored and I feel like I need to shake things up a bit to keep doing this for the rest of my life.** 




Full disclosure: **this is very much a first world problem**. I get paid quite well, I have incredibly lenient work life balance, I work from home 3 days a week, etc etc. Most people would kill to be in my position at my age.


Some context: I was originally in academia doing a PhD in math, but pure math, completely unrelated to ML or anything in the real world really. ~2 years in, I was disillusioned with that (sensing a pattern here lol) so I took as many ML courses I could and jumped ship to industry. 


Regardless of all the problems I had in academia, it at least *asked* something of me. I had to think, like, *actually think*, about complex, interesting stuff. It felt like I was actually engaging my mind and growing. 


My current job is fine, basically applying LLMs for various use cases at a megacorp. On paper, I'm playing with the latest, greatest, tech, but in practice, I'm just really calling APIs on products that smarter people are building. 


I feel like I haven't actually flexed my brain muscles in years now, I'm forgetting all the stuff I've learnt at college, and the work itself is incredibly boring to me. Many many days I can barely bring myself to work as the work is so uninteresting, and the bare minimum I put in still somehow impresses my colleagues so there's no real incentive to work hard. 


I realize how privileged that sounds, I really do, but I do feel kind of unfulfilled and spiritually empty. I feel like if I keep doing this for the rest of my life I will look back with regret. 


**What I'm trying to do to fix this:** I would like to shift towards more cutting edge and harder data science. Problem here is a lack of qualifications and experience. I have a MS and a BS in Math (from T10 colleges) but no PhD and the math I studied was mostly pure/theoretical, very little to do with ML. 

I'm trying to do projects in my own time, but it's slow going on my own. I would love to aim for ML/AI research roles, but it feels like an impossible ask without a PhD, without papers, etc etc. I'm not sure that's a feasible goal. 



Another thing I've been considering is playing a DS/ML role as support in research that's *not* ML. For instance, bioinformatics or biotech, etc. This is also fairly appealing to me. The main issue is here is a complete lack of knowledge about these fields (since there can be so many fields here) and a lack of domain knowledge which I presume is required. I'm still trying, I've been applying for some bioinformatics roles, but yeah, also hard. 



**Has anyone else felt this way? What did they do about it, and what would you recommend?**",272,0.9,https://www.reddit.com/r/datascience/comments/1n88v2y/almost_2_years_into_my_first_job_and_already/,False,True,False
1n83iok,Final_Alps,1756970387.0,8,/r/datascience/comments/1n83iok/would_you_volunteer_to_join_the_team_building_ai/,datascience,Would you volunteer to join the team building AI tooling? If you have what has been your experience?,"I just learned a colleague that was part of the AI tooling team is leaving and I am considering whether to ask to be added to their old project team. 

I am a data scientist and while I have not had too many ML projects recently, I have some lined up for next quarter. 

Their team was building the tooling to build agents for use internally and customer facing. That team has obviously gotten a lot of shout out from the CEO. Their early products are well received. 

I prefer ML over AI tooling but also feel there is a new reality for my next job in that I should be above average in AI usage and development. And thus I feel that being part of the AI team would be beneficial for my career. 

So my question is. Should I ask to join the AI team? Have others done this - what has been experienced? Anything to look out for/any ways to shape the my potential journey in that team? ",0,0.25,https://www.reddit.com/r/datascience/comments/1n83iok/would_you_volunteer_to_join_the_team_building_ai/,False,True,False
1n81hrr,LilParkButt,1756962850.0,20,/r/datascience/comments/1n81hrr/how_are_you_liking_positron/,datascience,How are you liking Positron?,"I’m an undergraduate student double majoring in Data Analytics and Data Engineering and have used VSCode, Jupyter Notebook, Google Colab, and PyCharm Community Edition during my different Python courses. I haven’t used Positron yet, but it looks really appealing since I enjoy the VSCode layout and notebook style programming. Anyone with experience using Position, I’d greatly appreciate any information on how you’ve liked (or not liked) it. Thanks!",23,0.76,https://www.reddit.com/r/datascience/comments/1n81hrr/how_are_you_liking_positron/,False,True,False
1n81chu,OverratedDataScience,1756962325.0,93,/r/datascience/comments/1n81chu/whats_up_with_linkedin_posts_saying_excel_is_dead/,datascience,"What's up with LinkedIn posts saying ""Excel is dead"", ""dashboards are dead"", ""data science is dead"", ""PPTs are dead"" and so on?","Is this a trend now? I also read somewhere ""SQL is dead"" too. Ffs. What isn't dead anyway for these Linkfluencers? Only LLMs? And then you hear mangers and leadership parrtoting the same LinkedIn bullshit in team meetings... where is all this going? ",136,0.87,https://www.reddit.com/r/datascience/comments/1n81chu/whats_up_with_linkedin_posts_saying_excel_is_dead/,False,True,False
1n7zgzy,metalvendetta,1756956245.0,3,/r/datascience/comments/1n7zgzy/per_row_context_understanding_is_hard_for_sql_and/,datascience,"Per row context understanding is hard for SQL and RAG databases, here's how we solved it with LLMs","Traditional databases rely on RAG and vector databases or SQL-based transformations/analytics. But will they be able to preserve per-row contextual understanding?

We’ve released Agents as part of Datatune:

[https://github.com/vitalops/datatune](https://github.com/vitalops/datatune)

In a single prompt, you can define multiple tasks for data transformations, and Datatune performs the transformations on your data at a per-row level, with contextual understanding.

Example prompt:

""Extract categories from the product description and name. Keep only electronics products. Add a column called ProfitMargin = (Total Profit / Revenue) \* 100""

Datatune interprets the prompt and applies the right operation (map, filter, or an LLM-powered agent pipeline) on your data using OpenAI, Azure, Ollama, or other LLMs via LiteLLM.

Key Features

\- Row-level map() and filter() operations using natural language

\- Agent interface for auto-generating multi-step transformations

\- Built-in support for Dask DataFrames (for scalability)

\- Works with multiple LLM backends (OpenAI, Azure, Ollama, etc.)

\- Compatible with LiteLLM for flexibility across providers

\- Auto-token batching, metadata tracking, and smart pipeline composition

Token & Cost Optimization

\- Datatune gives you explicit control over which columns are sent to the LLM, reducing token usage and API cost:

\- Use input\_fields to send only relevant columns

\- Automatically handles batching and metadata internally

\- Supports setting tokens-per-minute and requests-per-minute limits

\- Defaults to known model limits (e.g., GPT-3.5) if not specified

\- This makes it possible to run LLM-based transformations over large datasets without incurring runaway costs.",0,0.23,https://www.reddit.com/r/datascience/comments/1n7zgzy/per_row_context_understanding_is_hard_for_sql_and/,False,True,False
1n7ops6,Gold-Artichoke-9288,1756928385.0,11,/r/datascience/comments/1n7ops6/freelance_search/,datascience,Freelance search,Any website to work as freelancer besides upwork ?,4,0.67,https://www.reddit.com/r/datascience/comments/1n7ops6/freelance_search/,False,True,False
1n70y9a,FreakedoutNeurotic98,1756860734.0,5,/r/datascience/comments/1n70y9a/diffusion_models/,datascience,Diffusion models,"What position do Diffusion models take in the spectrum of architectures to AGI like compared to jepa, auto-regressive modelling and others ? are they RL-able ?",0,0.5,https://www.reddit.com/r/datascience/comments/1n70y9a/diffusion_models/,False,True,False
1n70lcz,joshamayo7,1756859736.0,7,/r/datascience/comments/1n70lcz/ab_testing_overview/,datascience,A/B Testing Overview,"Sharing this as a guide on A/B Testing. I hope that it can help those preparing for interviews and those unfamiliar with the wide field of experimentation.

Any feedback would be appreciated as we're always on a learning journey.",38,0.9,https://medium.com/@joshamayo7/continuous-improvement-through-online-experimentation-a72406b0ee3d,False,False,False
1n6so7m,Technical-Note-4660,1756840467.0,23,/r/datascience/comments/1n6so7m/i_built_a_simulation_tool_for_students_to_learn/,datascience,I built a simulation tool for students to learn causal inference!,"\- Building a good intuition for causal inference methods requires you to play around with assumptions and data, but getting data from a paper and replicating the results takes time.   
\- **I made a simulation tool to help students quickly build an intuition for these methods (currently only difference-in-difference is available).** This tool is great for the undergraduate level (as I am still a student so the content covered isn't super advanced)

This is still a proof-of-concept, but would love your feedback and what other methods you would like to see!

Link: [https://causal-buddy.streamlit.app/](https://causal-buddy.streamlit.app/)",163,0.98,https://www.reddit.com/r/datascience/comments/1n6so7m/i_built_a_simulation_tool_for_students_to_learn/,False,True,False
1n6ez8o,jesteartyste,1756805639.0,50,/r/datascience/comments/1n6ez8o/is_it_wrong_to_be_specialized_in_specific_ds_niche/,datascience,Is it wrong to be specialized in specific DS niche?,"Hello fellows Data Scientists!
I’m coming with question/discussion about specialization in specific part of Data Science. For a long time my main duty is time series and predictive projects, mainly around finance but in retail domain. As an example, project where I predict sales per hour for month up front, later I place matrix with amount of staff needed on specific station to minimize number of employees present in the location (lot of savings in labor costs). Lately I attended few interviews, that didn’t go flawlessly from my side - most of questions were around classification problems, where most of my knowledge is in regression problems, of course I’m blaming myself on every attempt where I didn’t receive an offer because of technical interview and there is no discussion that I could prepare myself in more broad knowledge. But here comes my question, is it possible to know deeply every kind of niche knowledge when your main work spins around specific problems? I’m sure there are lot of DS which work for past 10 years or so and because of number of projects they’re familiar with a lot of specific problems, but for someone with 3 yoe is it doable? I feel like I’m very good in tackling time series problems, but as an example, my knowledge in image recognition is very limited, did you face problem like that? What are your thoughts? How did you overcome this in your career?",34,0.75,https://www.reddit.com/r/datascience/comments/1n6ez8o/is_it_wrong_to_be_specialized_in_specific_ds_niche/,False,True,False
1n6cug7,SirCasms,1756797162.0,0,/r/datascience/comments/1n6cug7/the_hidden_costs_of_naive_retrieval/,datascience,The Hidden Costs of Naive Retrieval,"We often treat Retrieval-Augmented Generation (RAG) as the default solution for knowledge-intensive tasks, but the naive 'retrieve-then-read' paradigm has significant hidden costs that can hurt, rather than help, performance. So, when is it better not to retrieve?

This series on **Adaptive RAG** starts by exploring the hidden costs of our default RAG implementations by looking at three key areas:

* **The Practical Problems:** These are the obvious unnecessary latency and compute overhead for simple or popular queries where the LLM's parametric memory would have been enough.
* **The Hidden Dangers:** There are more subtle risks to quality. Noisy or misleading context can lead to ""External Hallucinations,"" where the retriever itself induces factual errors in an otherwise correct model.
* **The Foundational Flaws:** Finally, the ""retrieval advantage"" can shrink as models scale.",0,0.5,https://blog.reachsumit.com/posts/2025/09/problems-with-naive-rag/,False,False,False
1n5eqdj,AutoModerator,1756699306.0,29,/r/datascience/comments/1n5eqdj/weekly_entering_transitioning_thread_01_sep_2025/,datascience,"Weekly Entering & Transitioning - Thread 01 Sep, 2025 - 08 Sep, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",12,0.81,https://www.reddit.com/r/datascience/comments/1n5eqdj/weekly_entering_transitioning_thread_01_sep_2025/,False,True,False
1n4rufc,Fantastic-Trouble295,1756638152.0,48,/r/datascience/comments/1n4rufc/lets_build_something_together/,datascience,Let’s Build Something Together,"Hey everyone,

After my last post about my struggles in finding a remote job, I was honestly blown away. I got over 50 messages not with job offers, but with stories, frustrations, and suggestions. The common theme? Many of us are stuck. Some are trying to break into the market, others are trying to move within it, and many just want to *make something meaningful*.

That really got me thinking: since this subreddit is literally about connecting data scientists, engineers, PMs, MLOps folks, researchers, and builders of all kinds why don’t we **actually build something together**?

It doesn’t have to be one massive project; it could be multiple smaller ones. The goal wouldn’t just be to pad CVs, but to collaborate, learn, and create something that matters. Think hackathon energy, but async and community-driven with no time limits and frustration.

I am personally interested to get involved with things i haven't been yet. Mlops,Deployment,Cloud,Azure,pytorch,Apache for example. Everyone can find their opening and what they want to improve and try and work with other experience people on this that could help them.

This would literally need

* Data scientists / analysts
* Software engineers
* MLOps / infra people
* Project managers
* Researchers / scientists
* Anyone who wants to contribute

Build something real with others (portfolio > buzzwords)

* Show initiative and collaboration on your CV/LinkedIn
* Make connections that could lead to opportunities
* Turn frustration into creation

I’d love to hear your thoughts:

* Would you be interested in joining something like this?
* What kind of projects would excite you (open-source tools, research collabs, data-for-good, etc.)?
* Should we organize a first call/Discord/Slack group to test the waters? I am waiting for connecting with you on Linkedin and here.

PS1: Yeah I am not talkig about creating a product or building the new chatgpt. Just communication and brainstorming . Working on some ideas or just simply get to know some people. ",38,0.75,https://www.reddit.com/r/datascience/comments/1n4rufc/lets_build_something_together/,False,True,False
1n4n1qz,ExcitingCommission5,1756619820.0,35,/r/datascience/comments/1n4n1qz/how_do_i_prepare_for_my_data_science_job_as_a_new/,datascience,How do I prepare for my data science job as a new grad?,"I just graduated from my bachelors in May. Recently, I’ve been fortunate enough to receive an offer as a data scientist I at a unicorn where most of the people on the ds team have PhDs. My job starts in a month and I’m having massive imposter syndrome, especially since my coding skills are kinda shit. I can barely do leetcode mediums. The job description is also super vague, only mentioning ML models and data analysis, so idk what specific things I should brush up on. What can I do in this month to make sure I do a good job?",100,0.89,https://www.reddit.com/r/datascience/comments/1n4n1qz/how_do_i_prepare_for_my_data_science_job_as_a_new/,False,True,False
1n4ecoo,NervousVictory1792,1756592609.0,0,/r/datascience/comments/1n4ecoo/career_dilemma/,datascience,Career Dilemma,,0,0.4,/r/cscareerquestionsuk/comments/1n4ec3k/career_dilemma/,False,False,False
1n4bamu,alpha_centauri9889,1756584615.0,19,/r/datascience/comments/1n4bamu/advice_for_dsasmle_interviews/,datascience,Advice for DS/AS/MLE interviews,"I am looking for data scientist (ML heavy), applied scientist or ML engineer roles in product based companies. For my interview preperation, I am unsure about which book or resources to pick so that I can cover the rigor of ML rounds in these interviews. I have background in CS and have fair knowledge of ML. Anyone who cracked such roles or have any experience that can help me? 

PS: I was considering reading Kevin Murphy's ML book but it is too heavy on math so I am not sure if that much of rigor is required for these kind of interviews. I am not looking for research roles. ",42,0.89,https://www.reddit.com/r/datascience/comments/1n4bamu/advice_for_dsasmle_interviews/,False,True,False
1n3jnpw,PathalogicalObject,1756502552.0,12,/r/datascience/comments/1n3jnpw/how_do_you_design_a_test_to_compare_two_audience/,datascience,How do you design a test to compare two audience targeting methods?,"So we have two audiences we want to test against each other. The first is one we're currently using and the second is a new audience. We want to know if a campaign using the new audience targeting method can match or exceed an otherwise identical campaign using our current targeting.

We're conducting the test on Amazon DSP and the Amazon representative recommended basically intersecting each audience with a randomized set of holdout groups. So for audience A the test cell will be all users in audience A and also in one group of randomized holdouts and similarly for audience B (with a different set of randomized holdouts)

Our team's concern is that if each campaign is getting a different set of holdout groups then we wouldn't have the same baseline. My boss is recommending we use the same set of holdout groups for both. 

My personal concern for that is if we'd have a proper isolation (e.g. if one user sees an ad from the campaign using audience A and also an ad from the campaign using audience B, then which audience targeting method gets credit). I think my boss' approach is probably the better design, but the overlap issue stands out to me as a complication.  

I'll be honest that I've never designed an A/B test before, much less on audiences, so any help at all is appreciated. I've been trying to understand how other platforms do this because Amazon does seem a bit different - as in, how (in an ideal universe) would you test two audiences against each other?",21,0.88,https://www.reddit.com/r/datascience/comments/1n3jnpw/how_do_you_design_a_test_to_compare_two_audience/,False,True,False
1n3c1ks,JayBong2k,1756484777.0,11,/r/datascience/comments/1n3c1ks/choice_of_ai_tool_for_personal_projects_and/,datascience,Choice of AI tool for personal projects and learning,"Hello,

I am DS with \~4 YoE and now looking to upskill and start my job hunt. Due to the nature of my work, which is primarily model maintenance and automation, I don't have a wealth of development and deployment projects on my resume. I do, but very sparsely. 

One of my major problems is a form of ""**I don't know what I don't know**"". Basically, I keep doing the same stuff with public datasets and I don't know what new stuff to do. So, as a trial I used ChatGPT to suggest projects after giving it a sample dataset and I got overwhelmed with its suggestions. I have so many questions that I know I will run out of tokens.

So, I was thinking of getting the premium version of ChatGPT or Claude or Perplexity to help me in this endeavor. I want to execute personal projects with its help and learn concepts that I can deep-dive on my own.

So, if you can suggest which one would be best for the 20$ everyone is charging, it would be very helpful!

Thanks a lot!!",3,0.56,https://www.reddit.com/r/datascience/comments/1n3c1ks/choice_of_ai_tool_for_personal_projects_and/,False,True,False
1n2s33v,sg6128,1756424786.0,0,/r/datascience/comments/1n2s33v/shopify_applied_machine_learning_engineer_pair/,datascience,Shopify Applied Machine Learning Engineer Pair Programming Interview,"Has anyone done the pair programming interview with Shopify? 

Currently interviewing for a Machine Learning Engineer position and the description is really vague.  
All I know is that I can use AI tools and that they don't like Leetcode.  
It will be pair programming and bring your own IDE, but beyond this I really have no idea what to expect and how to prepare.

  
My interview is in a week - I'd really appreciate any guidance and help, thank you!

(also based in Canada, flair says US only for some reason)",11,0.78,https://www.reddit.com/r/datascience/comments/1n2s33v/shopify_applied_machine_learning_engineer_pair/,False,True,False
1n2s1v1,sg6128,1756424691.0,2,/r/datascience/comments/1n2s1v1/shopify_applied_machine_learning_engineer_pair/,datascience,Shopify Applied Machine Learning Engineer Pair Programming Interview,"Has anyone done the pair programming interview with Shopify? 

Currently interviewing for a Machine Learning Engineer position and the description is really vague.  
All I know is that I can use AI tools and that they don't like Leetcode.  
It will be pair programming and bring your own IDE, but beyond this I really have no idea what to expect and how to prepare.

  
My interview is in a week - I'd really appreciate any guidance and help, thank you!

(also based in Canada, flair says US only for some reason)",17,0.8,https://www.reddit.com/r/datascience/comments/1n2s1v1/shopify_applied_machine_learning_engineer_pair/,False,True,False
1n2o7c1,Ok_Post_149,1756414984.0,6,/r/datascience/comments/1n2o7c1/free_1000_cpu_100_gpu_hours_for_testers/,datascience,"Free 1,000 CPU + 100 GPU hours for testers","I believe it should be dead simple for data scientists, analysts, and researchers to scale their code in the cloud without relying on DevOps. At my last company, whenever the data team needed to scale workloads, we handed it off to DevOps. They wired it up in Airflow DAGs, managed the infrastructure, and quickly became the bottleneck. When they tried teaching the entire data team how to deploy DAGs, it fell apart and we ended up back to queuing work for DevOps.

That experience pushed me to build cluster compute software that makes scaling dead simple for any Python developer. With a single function you can deploy to massive clusters (10k vCPUs, 1k GPUs). You can bring your own Docker image, define hardware requirements, run jobs as background tasks you can fire and forget, and kick off a million simple functions in seconds.

It’s [open source](https://github.com/Burla-Cloud/burla) and I’m still making install easier, but I also have a few managed versions.

Right now I’m looking for test users running embarrassingly parallel workloads like data prep, hyperparameter tuning, batch inference, or Monte Carlo simulations. If you’re interested, email me at [**joe@burla.dev**]() and I’ll set you up with a managed cluster that includes 1,000 CPU hours and 100 GPU hours.

Here’s an example of it in action: I spun up 4k vCPUs to screenshot 30k arXiv PDFs and push them to GCS in just a couple minutes: [https://x.com/infra\_scale\_5/status/1938024103744835961](https://x.com/infra_scale_5/status/1938024103744835961?utm_source=chatgpt.com)

Would love testers.",5,0.73,https://www.reddit.com/r/datascience/comments/1n2o7c1/free_1000_cpu_100_gpu_hours_for_testers/,False,True,False
1n2fmqs,nullstillstands,1756395468.0,54,/r/datascience/comments/1n2fmqs/stanford_study_finds_that_ai_has_already_started/,datascience,Stanford study finds that AI has already started wiping out new grad jobs,,262,0.94,https://www.interviewquery.com/p/ai-killing-entry-level-jobs,False,False,False
1n28ukj,Sudden_Beginning_597,1756377856.0,3,/r/datascience/comments/1n28ukj/i_built_runcell_an_ai_agent_for_jupyter_that/,datascience,I built Runcell - an AI agent for Jupyter that actually understands your notebook context,"I've been working on something called Runcell that I think fills a gap I was frustrated with in existing AI coding tools.

**What it is:** Runcell is an AI agent that lives inside JupyterLab (can be used as an extension) and can understand the full context of your notebook - your data, charts, previous code, kernel state, etc. Instead of just generating code, it can actually edit and execute specific cells, read/write files, and take actions on its own.

**Why I built it:** I tried Cursor and Claude Code, but they mostly just generate a bunch of cells at once without really understanding what happened in previous steps. When I'm doing data science work, I usually need to look at the results from one cell before deciding what to write next. That's exactly what Runcell does - it analyzes your previous results and decides what code to run next based on that context.

**How it's different:**

* vs AI IDEs like Cursor: Runcell focuses specifically on building context for Jupyter environments instead of treating notebooks like static files
* vs Jupyter AI: Runcell is more of an autonomous agent rather than just a chatbot - it has tools to actually work and take actions

You can try it with just `pip install runcell`.

I'm looking for feedback from the community. Has anyone else felt this frustration with existing tools? Does this approach make sense for your workflow?",0,0.39,https://www.reddit.com/r/datascience/comments/1n28ukj/i_built_runcell_an_ai_agent_for_jupyter_that/,False,True,False
1n1zo5y,Illustrious-Pound266,1756345706.0,38,/r/datascience/comments/1n1zo5y/why_is_typescript_starting_to_gain_adoption_in_ai/,datascience,Why is Typescript starting to gain adoption in AI?,"I've noticed that, increasingly, using TypeScript has become more common for AI tools. For example, Langgraph has Langgraph.js for Typescript developers. Same with OpenAI's Agents SDK.

I've also seen some AI engineer job openings for roles that use both Python and Typescript.

Python still seems to be dominant, but it seems like Typescript is definitely starting to gain traction in the field. So why is this? Why the appeal of building AI apps in Typescript? It wasn't originally like this with more traditional ML / deep learning, where Python was so dominant.

Why is it gaining increasing adoption and what's the appeal?

",22,0.83,https://www.reddit.com/r/datascience/comments/1n1zo5y/why_is_typescript_starting_to_gain_adoption_in_ai/,False,True,False
1n1tk23,1234okie1234,1756329708.0,66,/r/datascience/comments/1n1tk23/rejected_after_3rd_round_live_coding_oa_round/,datascience,Rejected after 3rd round live coding OA round,"As the title says, I made it to the 3rd round interview for a Staff DS role. Thought I was doing well, but I bombed the coding portion, I only managed to outline my approach instead of producing actual code. That’s on me, mostly because I’ve gotten used to relying on GPT to crank out code for me over the last two years. Most of what I do is build POCs, check hypotheses, then have GPT generate small snippets that I review for logic before applying it. I honestly haven’t done “live coding” in a while.

Before the interview, I prepped with DataLemur for the pandas related questions and brushed up on building simple NNs and GNNs from scratch to cover the conceptual/simple DS side. A little bit on the transformer module as well to have my bases cover if they ask for it. I didn’t expect a LeetCode-style live coding question. I ended up pseudo-coding it, then stumbling hard when I tried to actually implement it.

Got the rejection email today. Super heartbreaking to see. Do I go back to live-coding and memorizing syntax and practicing leetcodes for upcoming future DS interview?",93,0.83,https://www.reddit.com/r/datascience/comments/1n1tk23/rejected_after_3rd_round_live_coding_oa_round/,False,True,False
1n191lg,Technical-Love-8479,1756274072.0,7,/r/datascience/comments/1n191lg/nvidia_ai_released_jetnemotron_53x_faster/,datascience,NVIDIA AI Released Jet-Nemotron: 53x Faster Hybrid-Architecture Language Model Series,"NVIDIA Jet-Nemotron is a new LLM series which is about 50x faster for inferencing. The model introduces 3 main concept :

* **PostNAS**: a new search method that tweaks only attention blocks on top of pretrained models, cutting massive retraining costs.
* **JetBlock**: a dynamic linear attention design that filters value tokens smartly, beating older linear methods like Mamba2 and GLA.
* **Hybrid Attention**: keeps a few full-attention layers for reasoning, replaces the rest with JetBlocks, slashing memory use while boosting throughput.

Video explanation : [https://youtu.be/hu\_JfJSqljo](https://youtu.be/hu_JfJSqljo)

Paper : [https://arxiv.org/html/2508.15884v1](https://arxiv.org/html/2508.15884v1)",11,0.87,https://www.reddit.com/r/datascience/comments/1n191lg/nvidia_ai_released_jetnemotron_53x_faster/,False,True,False
1n17500,IronManFolgore,1756267419.0,56,/r/datascience/comments/1n17500/what_exactly_is_prompt_engineering_in_data_science/,datascience,"What exactly is ""prompt engineering"" in data science?","I keep seeing people talk about prompt engineering, but I'm not sure I understand what that actually means in practice.

Is it just writing one-off prompts to get a model to do something specific? Or is it more like setting up a whole system/workflow (e.g. using LangChain, agents, RAG, etc.) where prompts are just one part of the stack in developing an application?

For those of you working as data scientists:
- Are you actively building internal end-to-end agents with RAG and tool integrations (either external like MCP or creating your own internal files to serve as tools)?

- Is prompt engineering part of your daily work, or is it more of an experimental/prototyping thing?",68,0.88,https://www.reddit.com/r/datascience/comments/1n17500/what_exactly_is_prompt_engineering_in_data_science/,False,True,False
1n105of,jason-airroi,1756247804.0,38,/r/datascience/comments/1n105of/airbnb_data/,datascience,Airbnb Data,"Hey everyone,

I work on the data team at [AirROI](https://www.airroi.com). For a while, we offered free datasets for about **250** cities, but we always wanted to do more for the community. Recently, we just expanded our free public dataset from \~250 to nearly **1000** global Airbnb markets on **properties** and **pricing data**. As far as we know, this makes it the single **largest free Airbnb dataset** ever released on the internet.

You can browse the collection and download here, no sign-up required: [Airbnb Data](http://www.airroi.com/data-portal)



**What’s in the data?**

For each market (cities, regions, etc.), the CSV dumps include:

Property Listings: Details like room type, amenities, number of bedrooms/bathrooms, guest capacity, etc.

Pricing Data: This is the cool part. We include historical rates, future calendar rates (for investment modeling), and minimum/maximum stay requirements.

Host Data: Host ID, superhost status, and other host-level metrics.



**What can you use it for?**

This is a treasure trove for:

Trend Analysis: Track pricing and occupancy trends across the globe.

Investment & Rental Arbitrage Analysis: Model potential ROI for properties in new markets.

Academic Research: Perfect for papers on the sharing economy, urban development, or tourism.

Portfolio Projects: Build a killer dashboard or predictive model for your GitHub.

General Data Wrangling Practice: It's real, messy, world-class data.



**A quick transparent note**: If you need hyper-specific or real-time data for a region not in the free set, we do have a ridiculously cheap [Airbnb API](https://www.airroi.com/api) to get more customized data. Alternatively, if you are a researcher who wants a larger customized data just reach out to us, we'll try our best to support!

  
If you require something that's not currently in the free dataset please comment below, we'll try to accommodate within reason.

Happy analyzing and go building something cool!



[Airbnb Data](https://preview.redd.it/vi9bjqphxflf1.png?width=3038&format=png&auto=webp&s=6953d029e8bc9aa21280b411df543d3b5bbc3d66)

[Download Airbnb Data](https://preview.redd.it/ydtx5oqjxflf1.png?width=1920&format=png&auto=webp&s=bb4f4dfc361d83734a1c088750d8167e1327bdae)

",319,0.97,https://www.reddit.com/r/datascience/comments/1n105of/airbnb_data/,False,True,False
1n0ke01,Technical-Love-8479,1756211056.0,6,/r/datascience/comments/1n0ke01/internvl_35_released_best_multimodal_llm_ranks_3/,datascience,"InternVL 3.5 released : Best MultiModal LLM, ranks 3 overall","InternVL 3.5 has been released, and given the benchmark, the model looks to be the best multi-model LLM, ranking 3 overall just behind Gemini 2.5 Pro and GPT-5.  Multiple variants released ranging from 1B to 241B

*Processing img 5v5hfeg9wclf1...*

The team has introduced a number of new technical inventions, including *Cascade RL, Visual Resolution Router,  Decoupled Vision-Language Deployment.*  

Model weights : [https://huggingface.co/OpenGVLab/InternVL3\_5-8B](https://huggingface.co/OpenGVLab/InternVL3_5-8B)

Tech report : [https://arxiv.org/abs/2508.18265](https://arxiv.org/abs/2508.18265)

Video summary : [https://www.youtube.com/watch?v=hYrdHfLS6e0](https://www.youtube.com/watch?v=hYrdHfLS6e0)",9,0.69,https://www.reddit.com/r/datascience/comments/1n0ke01/internvl_35_released_best_multimodal_llm_ranks_3/,False,True,False
1n0ep0g,ChubbyFruit,1756190673.0,17,/r/datascience/comments/1n0ep0g/how_do_i_make_the_most_of_this_opportunity/,datascience,How do I make the most of this opportunity,"Hello everyone, I’m a senior studying data science at a large state school. Recently, through some networking, I got to interview with a small real estate and financial data aggregator company with around \~100 employees.

I met with the CEO for my interview. As far as I know, they haven’t had an engineering or science intern before, mainly marketing and business interns. The firm has been primarily a more traditional real estate company for the last 150 years. Many tasks are done through SQL queries and Excel. Much of the product team at the company has been there for over 20 years and is resistant to change.

The ceo wants to make the company more efficient and modern, and implement some statistical and ML models and automated workflows with their large amounts of data. He has given me some of the ideas that he and others at the company have considered. I will list those at the end. But I am starting to feel that I’m a bit in over my head here as he hinted towards using my work as a proof of concept to show the board that these new technologies and techniques r what the company needs to stay relevant and competitive. As someone who is just wrapping up their undergrad, some of it feels beyond my abilities if I’m mainly going to be implementing a lot of these things solo.

  
These are some of the possible projects I would work on:



#  Chatbot Knowledge Base Enhancement

**Background**: The Company is deploying AI-powered chatbots (HubSpot/CoPilot) for customer engagement and internal knowledge access. Current limitations include incomplete coverage of FAQs and inconsistent performance tracking.

**Objective**: Enhance chatbot functionality through improved training, monitoring, and analytics.

**Scope**:

* Automate FAQ training using internal documentation.
* Log and classify failed responses for continuous improvement.
* Develop a performance dashboard.

**Deliverables**:

* Enhanced training process.
* Error classification system.
* Prototype dashboard.

**Value**: Improves customer engagement, reduces staff workload, and provides analytics on chatbot usage.



# Automated Data Quality Scoring

**Background**: Clients demand AI-ready datasets, and the company must ensure high data quality standards.

**Objective**: Prototype an automated scoring system for dataset quality.

**Scope**:

* Metrics: completeness, duplicates, anomalies, missing metadata.
* Script to evaluate any dataset.

**Intern Fit**: Candidate has strong Python/Pandas skills and experience with data cleaning.

**Deliverables**:

* Reusable script for scoring.
* Sample reports for selected datasets.

**Value**: Positions the company as a provider of AI-ready data, improving client trust.

  
Entity Resolution Prototype

**Background**: The company datasets are siloed (deeds, foreclosures, liens, rentals) with no shared key.

**Objective**: Prototype entity resolution methods for cross-dataset linking.

**Scope**:

* Fuzzy matching, probabilistic record linkage, ML-based classifiers.
* Apply to limited dataset subset.

**Intern Fit**: Candidate has ML and data cleaning experience but limited production-scale exposure.

**Deliverables**:

* Prototype matching algorithms.
* Confidence scoring for matches.
* Report on results.

**Value**: Foundation for the company's long-term, unique master identifier initiative.

  
Predictive Micro-Models

**Background**: Predictive analytics represents an untapped revenue stream for the company.

**Objective**: Build small predictive models to demonstrate product potential.

**Scope**:

* Predict foreclosure or lien filing risk.
* Predict churn risk for subscriptions.

**Intern Fit**: Candidate has built credit risk models using XGBoost and regression.

**Deliverables**:

* Trained models with evaluation metrics.
* Prototype reports showcasing predictions.

**Value**: Validates feasibility of predictive analytics as a company product.



# Generative Summaries for Court/Legal Documents

**Background**: Processing court filings is time-intensive, requiring manual metadata extraction.

**Objective**: Automate structured metadata extraction and summary generation using NLP/LLM.

**Scope**:

* Extract entities (names, dates, amounts).
* Generate human-readable summaries.

**Intern Fit**: Candidate has NLP and ML experience through research work.

**Deliverables**:

* Prototype NLP pipeline.
* Example structured outputs.
* Evaluation of accuracy.

**Value**: Reduces operational costs and increases throughput.

  
Automation of Customer Revenue Analysis

**Background**: The company currently runs revenue analysis scripts manually, limiting scale.

**Objective**: Automate revenue forecasting and anomaly detection.

**Scope**:

* Extend existing forecasting models.
* Build anomaly detection.
* Dashboard for finance/sales.

**Intern Fit**: Candidate’s statistical background aligns with forecasting work.

**Deliverables**:

* Automated pipeline.
* Interactive dashboard.

**Value**: Improves financial planning and forecasting accuracy.

  
Data Product Usage Tracking

**Background**: Customer usage patterns are not fully tracked, limiting upsell opportunities.

**Objective**: Prototype a product usage analytics system.

**Scope**:

* Track downloads, API calls, subscriptions.
* Apply clustering/churn prediction models.

**Intern Fit**: Candidate’s experience in clustering and predictive modeling fits well.

**Deliverables**:

* Usage tracking prototype.
* Predictive churn model.

**Value**: Informs sales strategies and identifies upsell/cross-sell opportunities.

  
AI Policy Monitoring Tool

**Background**: The company has implemented an AI Use Policy, requiring compliance monitoring.

**Objective**: Build a prototype tool that flags non-compliant AI usage.

**Scope**:

* Detect unapproved file types or sensitive data.
* Produce compliance dashboards.

**Intern Fit**: Candidate has built automation pipelines before, relevant experience.

**Deliverables**:

* Monitoring scripts.
* Dashboard with flagged activity.

**Value**: Protects the company against compliance and cybersecurity risks.",6,0.64,https://www.reddit.com/r/datascience/comments/1n0ep0g/how_do_i_make_the_most_of_this_opportunity/,False,True,False
1n0biew,Technical-Love-8479,1756179497.0,0,/r/datascience/comments/1n0biew/microsoft_released_vibevoice_tts/,datascience,Microsoft released VibeVoice TTS,"Microsoft just dropped VibeVoice, an Open-sourced TTS model in 2 variants (1.5B and 7B) which can support audio generation upto 90 mins and also supports multiple speaker audio for podcast generation. 

Demo Video : https://youtu.be/uIvx_nhPjl0?si=_pzMrAG2VcE5F7qJ

GitHub : https://github.com/microsoft/VibeVoice",9,0.81,https://www.reddit.com/r/datascience/comments/1n0biew/microsoft_released_vibevoice_tts/,False,True,False
1n035we,Fantastic-Trouble295,1756157064.0,132,/r/datascience/comments/1n035we/is_the_market_really_like_this_the_reality_for_a/,datascience,Is the market really like this? The reality for a recent graduate looking for opportunities.,"Hello . I’m a recent Master of Science in Analytics graduate from Georgia Tech (GPA 3.91, top 5% of my class). I completed a practicum with Sandia Labs and I’m currently in discussions about further research with GT and SANDIA. I’m originally from Greece and I’ve built a strong portfolio of projects, ranging from classic data analysis and machine learning to a Resume AI chatbot.

I entered the job market feeling confident, but I’ve been surprised and disappointed by how tough things are here. The Greek market is crazy: I’ve seen openings that attract 100 applicants and still offer very low pay while expecting a lot. I’m applying to junior roles and have gone as far as seven interview rounds that tested pandas, PyTorch, Python, LeetCode-style problems, SQL, and a lot of behavioral and technical assessments.

Remote opportunities seem rare on EUROPE or US. I may be missing something, but I can’t find many remote openings.

This isn’t a complaint so much as an expression of frustration. It’s disheartening that a master’s from a top university, solid skills, hands-on projects, and a real practicum can still make landing a junior role so difficult. I’ve also noticed many job listings now list deep learning and PyTorch as mandatory, or rebrand positions as “AI engineer,” even when it doesn’t seem necessary.

On a positive note, I’ve had strong contacts reach out via LinkedIn  though most ask for relocation, which I can’t manage due to family reasons.

I’m staying proactive: building new projects, refining my interviewing skills, and growing my network. I’d welcome any advice, referrals, or remote-friendly opportunities. Thank you!

PS. If you comment your job experience state your country to get a picture of the worldwide problem.

PS2. Started as an attempt for networking and opportunities, came down to an interesting realistic discussion. Still sad to read, what's the future of this job? What will happen next? What recent grads and on university juniors should be doing? 

Ps3. If anyone wants to connect send me a message ",203,0.92,https://www.reddit.com/r/datascience/comments/1n035we/is_the_market_really_like_this_the_reality_for_a/,False,True,False
1mzzzu7,fark13,1756149836.0,20,/r/datascience/comments/1mzzzu7/we_are_back_with_many_data_science_jobs_in_soccer/,datascience,"We are back with many Data science jobs in Soccer, NFL, NHL, Formula1 and more sports! 2025-08","Hey guys,

I've been silent here lately but many opportunities keep appearing and being posted.

These are a few from the last 10 days or so

* [Quantitative Analyst Associate (Spring/Summer 2026) - Philadelphia Phillies](http://www.sportsjobs.online/jobs/9015-quantitative-analyst-associate-spring-summer-2026?utm_source=sportsjobs-online.beehiiv.com&utm_medium=newsletter&utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&_bhlid=24b4748bef795f9a693e2911693d223c99632356)
* [Senior Sports Data Scientist - ESPN](http://www.sportsjobs.online/jobs/9018-senior-sports-data-scientist?utm_source=sportsjobs-online.beehiiv.com&utm_medium=newsletter&utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&_bhlid=58166f06c2cb14a5f60c555a80e63eff791ece6a)
* [Baseball Analyst/Data Scientist - Miami Marlins](http://www.sportsjobs.online/jobs/9014-baseball-analyst-data-scientist?utm_source=sportsjobs-online.beehiiv.com&utm_medium=newsletter&utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&_bhlid=7d5181c9bd523683761c79ffcd23fafab8877728)
* [Data Engineer, Athletics - University of Pittsburgh](http://www.sportsjobs.online/jobs/8992-data-engineer-athletics?utm_source=sportsjobs-online.beehiiv.com&utm_medium=newsletter&utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&_bhlid=90aede97e283411c5e9a31b34a982299320cc5e6)
* [Senior Data Scientist - Tottenham Hotspur](http://www.sportsjobs.online/jobs/8997-senior-data-scientist?utm_source=sportsjobs-online.beehiiv.com&utm_medium=newsletter&utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&_bhlid=e35ef1aeb7939cd356689d46e49afdff95535e1a)
* [Sports Scientist - Human Data Science - McLaren Racing](http://www.sportsjobs.online/jobs/8996-sports-scientist-human-data-science?utm_source=sportsjobs-online.beehiiv.com&utm_medium=newsletter&utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&_bhlid=e40bd45b1f6178064b5c7cf165f65e5821c8ad0d)
* [Lead Engineer - Phoenix Suns](http://www.sportsjobs.online/jobs/8981-lead-engineer?utm_source=sportsjobs-online.beehiiv.com&utm_medium=newsletter&utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&_bhlid=841248c85b1774cec3812e308b803fbcaa9b570e)
* [Business Intelligence Intern - Houston Texans](http://www.sportsjobs.online/jobs/8967-business-intelligence-intern?utm_source=sportsjobs-online.beehiiv.com&utm_medium=newsletter&utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&_bhlid=35c476cde3ddf380fbd3d5f4beccd3424bdcb356)
* [Technical Data Analyst - Portland Timbers](http://www.sportsjobs.online/jobs/8953-technical-staff-data-analyst-mls?utm_source=sportsjobs-online.beehiiv.com&utm_medium=newsletter&utm_campaign=new-jobs-in-sports-analytics-how-to-learn-analytics-quickly-from-a-youtube-sports-channel&_bhlid=14f0d07bcd9a80d670a7cc018bb6d16d6e2e9c2b)

I run www.sportsjobs(.)online, a job board in that niche. In the last month I added around 300 jobs.

For the ones that already saw my posts before, I've added more sources of jobs lately. I'm open to suggestions to prioritize the next batch.

It's a niche, there aren't thousands of jobs as in Software in general but my commitment is to **keep improving a simple metric, jobs per month.** We always need some metric in DS..

I run also a newsletter to receive emails with jobs and interesting content on sports analytics (next edition tomorrow!)  
[https://sportsjobs-online.beehiiv.com/subscribe](https://sportsjobs-online.beehiiv.com/subscribe)

Finally, I've created also a [reddit community](https://www.reddit.com/r/sports_jobs/) where I post recurrently the openings if that's easier to check for you.

I hope this helps someone!",113,0.94,https://www.reddit.com/r/datascience/comments/1mzzzu7/we_are_back_with_many_data_science_jobs_in_soccer/,False,True,False
1mzxmx3,SmartPizza,1756144536.0,10,/r/datascience/comments/1mzxmx3/looking_to_transition_to_experimentation/,datascience,Looking to transition to experimentation,"Hi all, I am looking to transition from ml analytics generalized roles to more experimentation focused roles. Where to start looking for experimentation heavy roles. I know the market is trash right now, but are there any specific portals that can help find such roles. Also usually faang is very popular for such roles, but are there any other companies which would be a good step to make a transition to. ",13,0.85,https://www.reddit.com/r/datascience/comments/1mzxmx3/looking_to_transition_to_experimentation/,False,True,False
1mzwdws,ElectrikMetriks,1756141796.0,15,/r/datascience/comments/1mzwdws/the_vibes_are_off_server_logs_filling_with_errors/,datascience,"""The Vibes are Off..."" *server logs filling with errors*",,62,0.82,https://i.redd.it/dcz0qwbk67lf1.png,False,False,False
1mzlzsp,Bus-cape,1756114791.0,10,/r/datascience/comments/1mzlzsp/first_time_writing_a_technical_article_would_love/,datascience,"First time writing a technical article, would love constructive feedback","Hi everyone,

I recently wrote my first blog post where I share a method I’ve been using to get good results on a fine-grained classification benchmark. This is something I’ve worked on for a while and wanted to put my thoughts together in an article.

I’m sharing it here **not as a promo** but because I’m genuinely looking to improve my writing and make sure my explanations are clear and useful. If you have a few minutes to read and share your thoughts (on structure, clarity, tone, level of detail, or anything else), I’d really appreciate it.

Here’s the link: [https://towardsdatascience.com/a-refined-training-recipe-for-fine-grained-visual-classification/](https://towardsdatascience.com/a-refined-training-recipe-for-fine-grained-visual-classification/)

Thanks a lot for your time and feedback!",9,0.68,https://www.reddit.com/r/datascience/comments/1mzlzsp/first_time_writing_a_technical_article_would_love/,False,True,False
1mzgkc7,AutoModerator,1756094498.0,25,/r/datascience/comments/1mzgkc7/weekly_entering_transitioning_thread_25_aug_2025/,datascience,"Weekly Entering & Transitioning - Thread 25 Aug, 2025 - 01 Sep, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",7,0.82,https://www.reddit.com/r/datascience/comments/1mzgkc7/weekly_entering_transitioning_thread_25_aug_2025/,False,True,False
1mz2jgn,sourabharsh,1756058206.0,23,/r/datascience/comments/1mz2jgn/day_to_day_work_at_leadprincipal_data_scientist/,datascience,Day to day work at lead/principal data scientist,"Hi, 

I have 9 years of experience in ml/dl. I have been looking for a role in lead/principal ds. Can you tell me what expectations do you guys face at the role.

Data science knowledge? 
Ml ops knowledge? 
Team management? 
",66,0.9,https://www.reddit.com/r/datascience/comments/1mz2jgn/day_to_day_work_at_leadprincipal_data_scientist/,False,True,False
1mymb21,Technical-Love-8479,1756009971.0,13,/r/datascience/comments/1mymb21/googles_new_research_measuring_the_environmental/,datascience,Google's new Research : Measuring the environmental impact of delivering AI at Google Scale,"Google has dropped in a very important research paper measuring the impact of AI on the environment, suggesting how much carbon emission, water, and energy consumption is done for running a prompt on Gemini. Surprisingly, the numbers have been quite low compared to the previously reported numbers by other studies, suggesting that the evaluation framework is flawed. 

Google measured the environmental impact of **a single Gemini prompt** and here’s what they found:

* **0.24 Wh of energy**
* **0.03 grams of CO₂**
* **0.26 mL of water**

Paper : [https://services.google.com/fh/files/misc/measuring\_the\_environmental\_impact\_of\_delivering\_ai\_at\_google\_scale.pdf](https://services.google.com/fh/files/misc/measuring_the_environmental_impact_of_delivering_ai_at_google_scale.pdf)

Video : [https://www.youtube.com/watch?v=q07kf-UmjQo](https://www.youtube.com/watch?v=q07kf-UmjQo)",56,0.85,https://www.reddit.com/r/datascience/comments/1mymb21/googles_new_research_measuring_the_environmental/,False,True,False
1mxyprj,posiela,1755947665.0,14,/r/datascience/comments/1mxyprj/anyone_using_search_apis_as_a_data_source/,datascience,Anyone Using Search APIs as a Data Source?,"I've been working on a research project recently and have encountered a frustrating issue: the amount of time spent cleaning scraped web results is insane. 

Half of the pages I collect are:  

*  Ads disguised as content  
* Keyword-stuffed SEO blogs  
* Dead or outdated links  

While it's possible to write filters and regex pipelines, it often feels like I spend more time cleaning the data than actually analyzing it. This got me thinking: instead of scraping, has anyone here tried using structured search APIs as a data acquisition step? 

In theory, the benefits could be significant:  

* Fewer junk pages since the API does some filtering already  
* Results delivered in structured JSON format instead of raw HTML  
* Built-in citations and metadata, which could save hours of wrangling  

However, I haven't seen many researchers discuss this yet. I'm curious if APIs like these are actually good enough to replace scraping or if they come with their own issues (such as coverage, rate limits, cost, etc.). 

If you've used a search API in your pipeline, how did it compare to scraping in terms of:

* Data quality  
* Preprocessing time  
* Flexibility for different research domains  

I would love to hear if this is a viable shortcut or just wishful thinking on my part.",50,0.96,https://www.reddit.com/r/datascience/comments/1mxyprj/anyone_using_search_apis_as_a_data_source/,False,True,False
1mxrbck,Technical-Love-8479,1755921166.0,21,/r/datascience/comments/1mxrbck/nvidia_new_paper_small_language_models_are_the/,datascience,NVIDIA new paper : Small Language Models are the Future of Agentic AI,"NVIDIA have just published a paper claiming SLMs (small language models) are the future of agentic AI. They provide a number of claims as to why they think so, some important ones being they are cheap. Agentic AI requires just a tiny slice of LLM capabilities, SLMs are more flexible and other points. The paper is quite interesting and short as well to read. 

Paper : [https://arxiv.org/pdf/2506.02153](https://arxiv.org/pdf/2506.02153)

Video Explanation : [https://www.youtube.com/watch?v=6kFcjtHQk74](https://www.youtube.com/watch?v=6kFcjtHQk74)",254,0.97,https://www.reddit.com/r/datascience/comments/1mxrbck/nvidia_new_paper_small_language_models_are_the/,False,True,False
1mxpyef,Rich-Effect2152,1755916893.0,17,/r/datascience/comments/1mxpyef/when_do_we_really_need_an_agent_instead_of_just/,datascience,When do we really need an Agent instead of just ChatGPT?,"I’ve been diving into the whole “Agent” space lately, and I keep asking myself a simple question: *when does it actually make sense to use an Agent, rather than just a ChatGPT-like interface?*

Here’s my current thinking:

* Many user needs are **low-frequency, one-off, low-risk**. For those, opening a ChatGPT window is usually enough. You ask a question, get an answer, maybe copy a piece of code or text, and you’re done. No Agent required.
* Agents start to make sense only when certain conditions are met:
   1. **High-frequency or high-value tasks** → worth automating.
   2. **Horizontal complexity** → need to pull in information from multiple external sources/tools.
   3. **Vertical complexity** → decisions/actions today depend on context or state from previous interactions.
   4. **Feedback loops** → the system needs to check results and retry/adjust automatically.

In other words, if you don’t have multi-step reasoning + tool orchestration + memory + feedback, an “Agent” is often just a chatbot with extra overhead.

I feel like a lot of “Agent products” right now haven’t really thought through what incremental value they add compared to a plain ChatGPT dialog.

Curious what others think:

* Do you agree that most low-frequency needs are fine with just ChatGPT?
* What’s your personal checklist for deciding when an Agent is *actually* worth building?
* Any concrete examples from your work where Agents clearly beat a plain chatbot?

Would love to hear how this community thinks about it.",54,0.76,https://www.reddit.com/r/datascience/comments/1mxpyef/when_do_we_really_need_an_agent_instead_of_just/,False,True,False
1mxhji7,DataAnalystWanabe,1755894603.0,25,/r/datascience/comments/1mxhji7/dsda_recruiters_do_you_approve_of_my_plan/,datascience,"DS/DA Recruiters, do you approve of my plan","Pivoting away from lab research after I finish my PhD, I'm thinking of taking this approach to landing a DS/DA job:

- Spot an ideal job and study it's requirements.

- Develop all (or most of) the skills associated with that job.

- Compensate for wet-lab-heavy experiences by undertaking projects (even if hypothetical) in said job domain and learn to think like an analyst.

I want to read from recruiters to know what they look for so I can.... Be that 😅 ",5,0.55,https://www.reddit.com/r/datascience/comments/1mxhji7/dsda_recruiters_do_you_approve_of_my_plan/,False,True,False
1mwdbr8,Due-Duty961,1755788311.0,25,/r/datascience/comments/1mwdbr8/where_to_reference_personal_projects_on_my_cv/,datascience,Where to reference personal projects on my CV?,"I havn t work as a data scientist in a long time and I want to get back to the field. I had mostly data analysis missions. I recently did a data science personal project. do I put it in professional experiences in the top of the cv for visibility, or lower in the cv with projects? thanks.",23,0.85,https://www.reddit.com/r/datascience/comments/1mwdbr8/where_to_reference_personal_projects_on_my_cv/,False,True,False
1mwchp8,AnalyticsDepot--CEO,1755786515.0,12,/r/datascience/comments/1mwchp8/hiring_mle_position_enterprisegrade_llm_solutions/,datascience,[Hiring] MLE Position - Enterprise-Grade LLM Solutions,"Hey all,  
  
I'm the founder of Analytics Depot, and we're looking for a talented Machine Learning Engineer to join our team. We have a premium brand name and are positioned to deliver a product to match. The Home depot of Analytics if you will.  
  
We've built a solid platform that combines LLMs, LangChain, and custom ML pipelines to help enterprises actually understand their data. Our stack is modern (FastAPI, Next.js), our approach is practical, and we're focused on delivering real value, not chasing buzzwords.   
  
We need someone who knows their way around production ML systems and can help us push our current LLM capabilities further. You'll be working directly with me and our core team on everything from prompt engineering to scaling our document processing pipeline. If you have experience with Python, LangChain, and NLP, and want to build something that actually matters in the enterprise space, let's talk.   
  
We offer competitive compensation, equity, and a remote-first environment. DM me if you're interested in learning more about what we're building.  
",25,0.7,https://www.reddit.com/r/datascience/comments/1mwchp8/hiring_mle_position_enterprisegrade_llm_solutions/,False,True,False
1mv5ojf,idan_huji,1755666095.0,10,/r/datascience/comments/1mv5ojf/asking_for_feedback_on_databases_course_content/,datascience,Asking for feedback on databases course content,,1,0.57,/r/Database/comments/1mth4ru/asking_for_feedback_on_databases_course_content/,False,False,False
1mumd4y,save_the_panda_bears,1755618775.0,20,/r/datascience/comments/1mumd4y/causal_inference_tech_screen_structure/,datascience,Causal Inference Tech Screen Structure,"This will be my first time administering a tech screen for this type of role.

The HM and I are thinking about formatting this round as more of a verbal case study on DoE within our domain since LC questions and take homes are stupid. The overarching prompt would be something along the lines of ""marketing thinks they need to spend more in XYZ channel, how would we go about determining whether they're right or not?"", with a series of broad, guided questions diving into DoE specifics, pitfalls, assumptions, and touching on high level domain knowledge.

I'm sure a few of you out there have either conducted or gone through these sort of interviews, are there any specific things we should watch out for when structuring a round this way? If this approach is wrong, do you have any suggestions for better ways to format the tech screen for this sort of role? My biggest concern is having an objective grading scale since there are so many different ways this sort of interview can unfold.",35,0.95,https://www.reddit.com/r/datascience/comments/1mumd4y/causal_inference_tech_screen_structure/,False,True,False
1mu3c6j,CanYouPleaseChill,1755562768.0,149,/r/datascience/comments/1mu3c6j/mit_report_95_of_generative_ai_pilots_at/,datascience,MIT report: 95% of generative AI pilots at companies are failing,,2272,0.99,https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/,False,False,False
1mtmvuc,NervousVictory1792,1755525653.0,33,/r/datascience/comments/1mtmvuc/scared_of_ai/,datascience,Scared of AI,I have been working with a principal data scientist on a project. Although I am the sole data scientist working on this project  and discussing stuff with him but I am so impressed at his articulate way of thinking. Literally putting his suggestions in chatgpt gives me the code I need. Honestly I am a little scare about AI now. Am I falling behind ?? Just to beat my own drum. I am probably asking the right questions. ,0,0.39,https://www.reddit.com/r/datascience/comments/1mtmvuc/scared_of_ai/,False,True,False
1mtehzk,explorer_seeker,1755499075.0,37,/r/datascience/comments/1mtehzk/curious_to_know_about_people_who_switched_from_ds/,datascience,Curious to know about people who switched from DS to DE or SWE or Solutions Architect,"Hello, I was just curious to know about people who have switched from DS to DE or SWE or Solutions Architect. If you have done it, what was your rationale behind doing it, what pushed or motivated you for it and how has been your experience after you did it?",48,0.89,https://www.reddit.com/r/datascience/comments/1mtehzk/curious_to_know_about_people_who_switched_from_ds/,False,True,False
1mtbra1,AutoModerator,1755489698.0,27,/r/datascience/comments/1mtbra1/weekly_entering_transitioning_thread_18_aug_2025/,datascience,"Weekly Entering & Transitioning - Thread 18 Aug, 2025 - 25 Aug, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",4,0.75,https://www.reddit.com/r/datascience/comments/1mtbra1/weekly_entering_transitioning_thread_18_aug_2025/,False,True,False
1msw56a,Technical-Love-8479,1755449815.0,32,/r/datascience/comments/1msw56a/dijkstra_defeated_new_shortest_path_algorithm/,datascience,Dijkstra defeated: New Shortest Path Algorithm revealed,"Dijkstra, the goto shortest path algorithm (time complexity nlogn) has now been outperformed by a new algorithm by top Chinese University which looks like a hybrid of bellman ford+ dijsktra algorithm.

Paper : https://arxiv.org/abs/2504.17033

Algorithm explained with example : https://youtu.be/rXFtoXzZTF8?si=OiB6luMslndUbTrz",454,0.89,https://www.reddit.com/r/datascience/comments/1msw56a/dijkstra_defeated_new_shortest_path_algorithm/,False,True,False
1mr8nwu,empirical-sadboy,1755288153.0,56,/r/datascience/comments/1mr8nwu/how_different_is_senior_data_analyst_from_data/,datascience,"How different is ""Senior Data Analyst"" from ""Data Scientist""?","I often see Senior DA roles that seem focused on using R/Python for analysis (vs. Excel and Power BI), but don't have any insight into the day-to-day of theese roles. 

At the senior level, how different is Data Analyst from Data Scientist? ",111,0.86,https://www.reddit.com/r/datascience/comments/1mr8nwu/how_different_is_senior_data_analyst_from_data/,False,True,False
1mqlp7d,CorpusculantCortex,1755226919.0,9,/r/datascience/comments/1mqlp7d/suspicious_ad/,datascience,Suspicious ad,"Describe the results you want and then have ai manufacture those results for you... who's going to tell them that's not how science works 🤣

Disclosure: I did not read about their tool at all,I just that the advert sounded terribly bad.",76,0.9,https://i.redd.it/7xhknwjsm3jf1.jpeg,False,False,False
1mqfubv,big_data_mike,1755211482.0,19,/r/datascience/comments/1mqfubv/time_series_with_value_dependent_lag/,datascience,Time series with value dependent lag,"I build models of factories that process liquids. Liquid flows through the factory in various steps and sits in tanks. A tank will have a flow rate in and a flow rate out, a level, and a volume so I can calculate the residence time. It takes ~3 days for liquid to get from the start of the process to the end and it goes through various temperatures, separations, and various other things get added to it along the way. 

If the factory is in a steady state the residence times and lags are relatively easy to calculate. The problem is I am looking at 6 months worth of data and during that time the  rate of the whole facility varies and therefore the residence times vary. If the flow rate goes up residence time goes down. 

How would you adjust the lags based on the flow rates? Chunk the data into months and calculate the lags for each month then concaténate everything? Vary the lags and just drop the overlaps and gaps?",16,1.0,https://www.reddit.com/r/datascience/comments/1mqfubv/time_series_with_value_dependent_lag/,False,True,False
1mq78jd,Helloiamwhoiam,1755192625.0,17,/r/datascience/comments/1mq78jd/getting_masters_worth_it_with_t5_bachelors/,datascience,Getting Master's worth it with T5 Bachelor's?,"As a bit of background, I have 2 years of work experience as a Data Scientist, and I have a Bachelor's Degree in Mathematics from a 'top' University: think MIT/Harvard/Princeton.

I'm currently employed. Making about $105k in total comp. I have a feeling I could be doing better compensation wise and even task wise so I've been considering applying to more jobs. 

I've noticed a lot of job postings seem to have a minimum requirement of at least a Master's degree, but I'm sort of hesitant to pursue this route right now for a few reasons. For one, master's are expensive, and I don't want to quit my job and go into debt. Secondly, if I were to pursue an online Master's degree, I'm not sure the available options would increase my signal. For example, does a MIT Math Bachelor's -> Texas AM Master's Data Science really boost the resume?

The only reason I'd get a Master's is for my love of learning, and I'd pursue something theoretical ML oriented and maybe transition into a more research-heavy or even quant role. But I'm not feeling this is an imminent or necessary next step for me.

I'm not trying to be cocky; I'm just trying to get insight from more seasoned people in the field who might be closer to hiring expectations.",0,0.41,https://www.reddit.com/r/datascience/comments/1mq78jd/getting_masters_worth_it_with_t5_bachelors/,False,True,False
1mq737g,Its_lit_in_here_huh,1755192309.0,37,/r/datascience/comments/1mq737g/overfitting_on_training_data_time_series/,datascience,"Overfitting on training data time series forecasting on commodity price, test set fine. XGBclassifier. Looking for feedback","Good morning nerds, I’m looking for some feedback I’m sure is rather obvious but I seem to be missing. 

I’m using XGBclassifier to predict the direction of commodity x price movement one month the  the future. 

~60 engineered features and 3500 rows. 
Target = one month return > 0.001
 
Class balance is 0.52/0.48. Backtesting shows an average accuracy of 60% on the test with a lot of variance through testing periods which I’m going to accept given the stochastic nature of financial markets. 

I know my back test isn’t leaking, but my training performance is too high, sitting at >90% accuracy. 

Not particularly relevant, but hyperparameters were selected with Optuna.

Does anything jump out as the obvious cause for the training over performance?  


",102,0.94,https://www.reddit.com/r/datascience/comments/1mq737g/overfitting_on_training_data_time_series/,False,True,False
1mq4sfp,Affectionate_Use9936,1755187361.0,19,/r/datascience/comments/1mq4sfp/copypasting_jupyter_notebooks_is_memory_heavy_on/,datascience,Copy-pasting jupyter notebooks is memory heavy on VSCode,"Currently for most of my work, I found out that copy-pasting jupyter notebooks and slightly modifying them is the most effective way to do my work. So basically I have a ipynb for every project I do every day.

However, some issues is that they can sometimes get a pretty big memory footprint especially when I have a lot of plots. Like around 1GB per notebook. So sometimes it takes several seconds to a minute to open some files on vscode. I was wondering if there's a way to optimize this?

  
I saw there's marimo and stuff. Wondering what you guys do.",40,0.96,https://www.reddit.com/r/datascience/comments/1mq4sfp/copypasting_jupyter_notebooks_is_memory_heavy_on/,False,True,False
1mq4ai4,tits_mcgee_92,1755186275.0,42,/r/datascience/comments/1mq4ai4/would_you_jump_jobs_if_youre_in_fear_of_a_layoff/,datascience,Would you jump jobs if you're in fear of a layoff?,"EDIT: Just looked and this new company has 2.5 stars out of 600 reviews on Glassdoor. Oof.

Currently based in the U.S., working remote, medium cost of living area. I make 90k a year and I'm the lead (and only) data scientist / frontend software dev for our area in the company. On top of data science/analyst stuff, I maintain/build our training website for around 500 employees (solo dev as well using React).

The down side? I work for Medicaid, and if you know what's going on in the United States you know Medicaid is having major cuts, and especially for 2026. We have laid off 300 people this year (so far). I was told ""You have nothing to worry about because your role is so niche"" but I still feel worried.

New job:

- Pay raise to 115k a year

- Still remote

- I would be working under my current boss who is transitioning to this new company (I have worked with him for 8 years, and the fact that my boss left this current job says something).

- 401k is comparable (3% match), health insurance is better and less cost, PTO is comparable.

- What I'm worried about: He is starting this new department from the ground up. I would be the only data/front-end website guy basically doing what I do in my current role. I'm worried the workload will be too much, or I'm not good enough to start from scratch. Feeling some imposter syndrome here.

Thanks for any insight here! This job I am currently at is fun, productive, and I love my team. But I am scared to death of layoffs. The company I am going to now has been around for 25 years, is growing a lot, and has much more ""lasting power"" in my opinion.",98,0.95,https://www.reddit.com/r/datascience/comments/1mq4ai4/would_you_jump_jobs_if_youre_in_fear_of_a_layoff/,False,True,False
1mpp8sv,Tyrannosaurus_Secks,1755141389.0,10,/r/datascience/comments/1mpp8sv/what_should_my_job_title_be/,datascience,What should my job title be,"I’ve been in my current role for ~5 months after finishing up my masters in geospatial data science. My official title is Energy Analyst, so essentially a data analyst role in the energy industry. 

I feel like the work I do is potentially beyond what is meant for the position (though I’m happy to be told otherwise if that’s not true) and am planning on asking for a title change and raise in the next few months. 

We have a weird set-up where we have a central IT team that supports ~12 implementation contractor teams that work with various utilities. The central IT team owns all of our data and does not allow any sort of read access or api to access data, and only exposes anything through SSRS reports. In theory, the IT team is meant to support a lot of our analytics, but historically they’ve done a pretty bad job at that so I was hired into one of the distributed teams to run their analytics and build out an internal IT capacity. So far that has included the following:

- Recreating a database from the SSRS extracts. So far this is only a few tables in a sqlite3 db so nothing crazy. 
- Developing optimization models in pyomo to inform program design.
- Lots of ad hoc analysis and reporting. Most of this can be done with some filtering and group-bys but has also included some iterative proportional fitting and other kind of ‘medium difficulty’ methods. 
- creating power bi dashboards as well as a couple java script maplibre-gl-js maps with complex symbology.
- we accept applications to our program via an online intake, where applicants fill out forms one by one. Most of these applicants submit tens to hundreds of these applications at once. I am working in parallel on a few different potential solutions to this: templates for batch uploading is the easy one, and a potential api integration to pull applications directly from applicant systems is another.
- looking into creating some llm-agents to automate very simply data extraction. I have already tried automating these processes via dom ids and such but haven’t gotten it to work reliably enough yet. My manager specifically asked for me to try agentic approaches to appease higher ups that we are implementing AI.

I’m not entirely sure where I fall in the landscape of data titles and would appreciate input. I mostly use python with a bit of power query and vanilla excel as well. Very little Java script (just for certain visualizations). Power bi. 

Edit to add- I also manage an intern-turned-part-time-employee that supports me in the above tasks basically at my own discretion ",10,0.75,https://www.reddit.com/r/datascience/comments/1mpp8sv/what_should_my_job_title_be/,False,True,False
1mpkk2n,BB_147,1755128505.0,79,/r/datascience/comments/1mpkk2n/job_market_getting_any_better_or_nah/,datascience,Job market getting any better or nah?,I’ve been staying in my role and refusing to leave for the last several years. I’m wondering if there’s any signs yet the job market is coming back yet or if we’re still stuck in the slog,89,0.89,https://www.reddit.com/r/datascience/comments/1mpkk2n/job_market_getting_any_better_or_nah/,False,True,False
1mpei2b,Odd_Artist4319,1755114298.0,52,/r/datascience/comments/1mpei2b/how_can_i_gain_business_acumen_as_a_data_scientist/,datascience,How can I gain business acumen as a data scientist?,"I can build models, but can I build profits? That’s the gap I’m trying to close.

I’m doing my Master’s in Data Science with a BSc in Computer Science. My technical skills are strong, but I lack business acumen. In interviews, I’ve noticed many questions aren’t just about models or algorithms, but about how those translate into profits or measurable business value.

Senior data scientists seem to connect their work to revenue, retention, or strategy with ease, while I still default to thinking in terms of accuracy and technical metrics. How did you learn to bridge that gap? Did you focus on general business knowledge, industry-specific skills, or hands-on projects?

I want to speak the “language of the business” so my work is not just technically solid but strategically impactful.",105,0.93,https://www.reddit.com/r/datascience/comments/1mpei2b/how_can_i_gain_business_acumen_as_a_data_scientist/,False,True,False
1mpa610,jambery,1755104702.0,33,/r/datascience/comments/1mpa610/research_data_scientists_without_heavy_coding/,datascience,"Research Data Scientists without heavy coding backgrounds (stats, econ, etc), has LLM's improved your workflow?","I remember for a while there were many CS folks saying that Data Science has become software engineering, and that if you aren't fluent in software engineering fundamentals then you're going to fall behind. It became enough of a popular rhetoric that people said they preferred to hire a coder with some math knowledge than a math person with some coding knowledge. 

As a Statistician that works in Research Data Science with an average level of coding experience, enough to write my own code in notebooks, but translating it into a fully fleshed Python module with classes and functions was much more difficult for me. For a while I thought my lack of advanced software engineering knowledge would become a crutch in my career and as someone with a busy personal life I didn't want to spend that much time learning these fundamentals. Then, my company rolled out LLM's integrated into the software we use, like Visual Studio. Suddenly I'm able to create fully fleshed out modules from my notebooks in a flash. I can ask the LLM to write unit tests to test out how my code processes data or test its various subfunctions. I can use it to code up various types of models quickly to compare results. Handing off my code to engineering in the form of a Python package wasn't such a pain anymore. 

Sure the LLM produces some weird results sometimes, and I do have to spend time making sure I ask it the correct things and/or cleaning up the code so that it works properly. But now I feel like that crutch I had is no longer present.",142,0.93,https://www.reddit.com/r/datascience/comments/1mpa610/research_data_scientists_without_heavy_coding/,False,True,False
1mo6ofm,Clicketrie,1754999802.0,6,/r/datascience/comments/1mo6ofm/using_experiment_tracking_for_backtests/,datascience,Using Experiment Tracking For Backtests,"I’ve used MLFlow as a data scientist, but here it’s being used for managing algo trading backtests and I thought this was an awesome use case. (And these aren’t ML runs, this is testing a momentum strategy).",4,0.65,https://www.reddit.com/r/datascience/comments/1mo6ofm/using_experiment_tracking_for_backtests/,False,True,False
1mniapg,tinkinc,1754931018.0,3,/r/datascience/comments/1mniapg/databricks_freea_course_recs/,datascience,Databricks Freea course Recs,"Can anyone recommend a great free databricks catalog or otherwise course to level up as a DS using databricks itself? 

",6,1.0,https://www.reddit.com/r/datascience/comments/1mniapg/databricks_freea_course_recs/,False,True,False
1mnhsx7,ElectrikMetriks,1754929932.0,9,/r/datascience/comments/1mnhsx7/when_you_edit_the_massive_query_someone_sent_you/,datascience,"When you edit the massive query someone sent you, forgot where you deleted something, and left a comma behind...",,138,0.9,https://i.redd.it/9sri1xwz2fif1.png,False,False,False
1mnggxa,DataAnalystWanabe,1754927012.0,10,/r/datascience/comments/1mnggxa/catch22_followup/,datascience,Catch-22 followup,"I'm following up on my post about ""Catch-22: learning R with projects""

Thank you to all those who responded. The replies were very reassuring.

After reading through the replies and reflecting on it, I realised the core of my struggle came from a specific fear that I would have to go through a rigorous coding interview, similar to what software engineers face.

I was picturing a scenario where I'd be given a problem and have to write perfect, memorised R code on the spot without any help. That pressure is what made me feel like I had to absorb every cheat sheet and learn all the syntax before I could even start a project. It created the syntax vs. projects Catch-22 that my original post was about.

For those who pivoted to data science or data analytics, did you have to go through some sort of coding interview or was it just like any other interview?",19,0.88,https://www.reddit.com/r/datascience/comments/1mnggxa/catch22_followup/,False,True,False
1mn3338,AutoModerator,1754884893.0,39,/r/datascience/comments/1mn3338/weekly_entering_transitioning_thread_11_aug_2025/,datascience,"Weekly Entering & Transitioning - Thread 11 Aug, 2025 - 18 Aug, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",7,0.89,https://www.reddit.com/r/datascience/comments/1mn3338/weekly_entering_transitioning_thread_11_aug_2025/,False,True,False
1mmzk4s,DataAnalystWanabe,1754874402.0,33,/r/datascience/comments/1mmzk4s/catch22_learning_r_through_hands_on_projects/,datascience,"Catch-22: Learning R through ""hands on"" Projects","

I often get told ""learn data science by doing hands-on projects"" and then I get all fired up and motivated to learn, and then I open up R.... And then I stare at a blank screen because I don't know the syntax from memory. 

And then I tell myself I'm going to learn the syntax so that I can do projects, but then I get caught up creating folders for each function of dplyr and the subfunctions of that and cheat sheets for this.

And then I come across the advice that I shouldn't learn syntax for the sake of learning syntax - I should do hands on projects.

I need projects to learn syntax and I need syntax to start doing projects.

________


Edit - Thank you so much to all of you who have replied and I would respond to each one of you but I don't want to sound like a parrot.

The reassurance that you don't have to have absorbed every R cheat sheet before being a professional Data Scientist/Analyst is very much appreciated. 

My assumption was these data analyst/scientist roles had coding-exams as part of the interview process, which is what stressed me out. Seeing some of you here as experienced analysts who still Google code is very relieving. I am very grateful for each response, and I read each one carefully.",47,0.76,https://www.reddit.com/r/datascience/comments/1mmzk4s/catch22_learning_r_through_hands_on_projects/,False,True,False
1mly9hm,DataAnalystWanabe,1754768416.0,31,/r/datascience/comments/1mly9hm/business_focused_data_science/,datascience,Business focused data science,"As a microbiology researcher, I'm far away from the business world. I do more -omics and growth curves and molecular techniques, but I want to move away from biology.

I believe the bridge that can help me do that is data. I have got experience with R and excel. I'm looking at learning SQL and PowerBI.

But I want to do it away from biology. The problem is, if I was to go from the UK, as a PhD microbiologist, and approach GCC consulting/business analyst recruiters, I get the sense that they'd scoff at me for thinking too highly of my ""transferrable skills"" and tell me that I don't have experience in the world of business.

How would I get myself job-ready for GCC business-focused data science roles. Is there anyone out there that has made the switch that can share some advice?

Thanks in advance",40,0.93,https://www.reddit.com/r/datascience/comments/1mly9hm/business_focused_data_science/,False,True,False
1mluc12,RookFlame4882,1754758611.0,44,/r/datascience/comments/1mluc12/burnout_disillusionment_and_imposter_syndrome/,datascience,"Burnout, disillusionment, and imposter syndrome after 1 year in DS. Am I just an API monkey? Reality check needed.","Hey folks,

I am about a year into my first data science job. It took roughly a year and more than 400 applications to land it, so the idea of another long search is scary.

Early on I worked with an internally built causal AI model that captures relationships for further analysis. I did not build the model. I ran experiments to make it more explainable and easier for others to use. I also built data orchestration pipelines using third party tools that are common in industry and cloud providers like AWS and GCP.

The last six months have shifted to LLM and NLP work. A lot of API calls, large text analysis. The next six months look even more LLM heavy since I am leading an internal tool build.

On paper there are wins:
- I have led projects and designed tools from scratch.
- My communication and client skills have improved.


My concerns:

- I am not doing much classical DS or rigorous modeling.
- LLM work often feels like API wrangling rather than technical depth.
- Work life balance is rough with frequent weekends.
- Even with a possible 5 to 10 percent raise (possibly within the next 6 months), the work likely stays the same.

I feel imposter syndrome and worry I am behind my peers on fundamentals and interview depth. I’m so burned out and honestly can’t tell if I’m just being a negative Nancy or if my concerns are legit. Am I shortchanging myself by thinking that I'm just not skilled enough? Idk


What I would love input on:

Am I building valuable skills for the DS market, or am I narrowing myself too much?

What types of companies or industries might value this mix of causal modeling, LLM work, and consulting style analysis?

If I want to keep doors open for more traditional DS or ML roles, what should I focus on learning now?

Portfolio ideas I can ship from my current work that would impress a hiring manager?

Would you ride out six months to finish the tool and try for a promotion, or start looking sooner?

Honest takes are very welcome.

",115,0.9,https://www.reddit.com/r/datascience/comments/1mluc12/burnout_disillusionment_and_imposter_syndrome/,False,True,False
1mlmwk0,takenorinvalid,1754738009.0,189,/r/datascience/comments/1mlmwk0/ai_isnt_taking_your_job_executives_are/,datascience,AI isn't taking your job. Executives are.,"If AI is ready to replace developers, why aren't developers replacing themselves with AI and just taking it easy at work?

I'm a Director at my company. I'm in the meetings and helping set up the tools that cost people their jobs. Here's how they work:

1. Claude AI writes some code

2. The code gets passed to a developer for validation

3. Since the developer's ""just validating"", he can be replaced with an overseas contractor that'll work for a fraction of the pay

We've tracked the tools, and we haven't seen any evidence that having Claude take a crack at the code saves anybody any time - but it does let us justify replacing expensive employees with cheap overseas contractors.

You're not getting replaced by AI.

Your job's being outsourced overseas.",1820,0.95,https://www.reddit.com/r/datascience/comments/1mlmwk0/ai_isnt_taking_your_job_executives_are/,False,True,False
1ml6fxs,gonna_get_tossed,1754685652.0,59,/r/datascience/comments/1ml6fxs/just_bombed_a_technical_interview_any_advice/,datascience,Just bombed a technical interview. Any advice?,"I've been looking for a new job because my current employer is re-structuring and I'm just not a big fan of the new org chart or my reporting line. It's not the best market, so I've been struggling to get interviews. 

But I finally got an interview recently. The first round interview was a chat with the hiring manager that went well. Today, I had a technical interview (concept based, not coding) and I really flubbed it. I think I generally/eventually got to what they were asking, but my responses weren't sharp.* It just sort of felt like I studied for the wrong test. 

How do you guys rebound in situations like this? How do you go about practicing/preparing for interviews? And do I acknowledge my poor performance in a thank you follow up email?

*Example (paraphrasing): They built a model that indicated that logging into a system was predictive of some outcome and management wanted to know how they might incorporate that result into their business processes to drive the outcome. I initially thought they were asking about the effect of requiring/encouraging engagement with this system, so I talked about the effect of drift and self selection on would have on model performance. Then they rephrased the question and it became clear they were talking about causation/correlation, so I talked about controlling for confounding variables and natural experiments.",77,0.89,https://www.reddit.com/r/datascience/comments/1ml6fxs/just_bombed_a_technical_interview_any_advice/,False,True,False
1mkzhvp,redditisthenewblak,1754669660.0,7,/r/datascience/comments/1mkzhvp/resourcestips_for_someone_brand_new_to_model/,datascience,Resources/tips for someone brand new to model building and deployment in Azure?,"Context: my current company is VERY (VERY) far behind, technologically. Our data isn't that big and currently resides in SQL Server databases, which I query directly via SSMS.

Whenever a project requires me to build models, my workflow would generally look like:

1. Query the data I need, make features, etc. from SQL Server.
2. Once I have the data, use Jupyter Notebooks to train/build models. 
3. Use best model to score dataset.
4. Send dataset/results to stakeholder as a file.

My company doesn't have a dedicated Dev team (on-shore, at least) nor a DE team. And this workflow works to make ends meet. 

Now my company has opened up Azure accounts for me and my manager, but neither one of us have developed anything in it before.

Microsoft has PLENTY of documentation, but the more I read, the more questions I have, and I feel like my time will be spent reading articles rather than getting anything done.

It seems like quite a shift from doing everything ""locally"" like what we have been doing to actually using cloud resources. So does anyone have any tips/guides that are beginner-friendly where I can do my entire workflow in the cloud?",21,0.9,https://www.reddit.com/r/datascience/comments/1mkzhvp/resourcestips_for_someone_brand_new_to_model/,False,True,False
1mkov0u,Damp_Out,1754638355.0,6,/r/datascience/comments/1mkov0u/semiauto_fully_automated_machine_learning/,datascience,"""SemiAuto"" Fully Automated Machine Learning Lifecycle by Just API Calling","So for the last 4 months I have been working on this project which was first supposed to be a upgrade of AutoML, but I later recognised it's potential.

This project could be one of the best things in ML reasearch, This project is just that good.

For context, I have the knowledge around ML for about 1.5 years now and thanks to the tools available, I have been able to build a grand project like this,

The Project's or you can say the Tool name is 'SemiAuto', A full fledged ML lifecycle Automation tool. It has 3 microservice, Regression, Classification, and Clustering.

I have completely build the Version 1 of this project.


It has 6 parts, First ingest the Data.csv file and the target column.

Second choose whatever preprocessing you want to and apply them.

Third use feature tools to build new features and then SHAP to select the amount of features you want.

Fourth choose any algorithm you want with the hyper params and build the model.

Fifth choose the optimization technique and get an optimised model.

At last, get the report, model.pkl, and processor.pkl and use them wherever you want.


As of why this project would be extremely good in research as researchers needs to test with different techniques and different models to get the best thing out and this tool provides that,

This tool will in a semiautomatic way can fully do each and everything by itself, no coding required.

The version 2 of this project is in production and I are introducing much more than the previous version,
For example, Parallel model building, Simple Ensemble design and Staged Ensemble design.

And also the thing that no one as of today has ever implemented in their ML automation tool, Meta-Heuristics Algorithms for feature selection.


Version 2 will be one of the most mind blowingly incredible release of the SemiAuto",0,0.36,https://www.reddit.com/r/datascience/comments/1mkov0u/semiauto_fully_automated_machine_learning/,False,True,False
1mkmjje,Proof_Wrap_2150,1754629942.0,11,/r/datascience/comments/1mkmjje/how_would_you_visualize_or_analyze_movements/,datascience,How would you visualize or analyze movements across a categorical grid over time?,"I’m working with a dataset where each entity is assigned to one of N categories that form a NxN grid. Over time, entities move between positions (e.g., from “N1” to “N2”).

Has anyone tackled this kind of problem before? I’m curious how you’ve visualized or even clustered trajectory types when working with time-series data on a discrete 2D space.",14,0.94,https://www.reddit.com/r/datascience/comments/1mkmjje/how_would_you_visualize_or_analyze_movements/,False,True,False
1mkdy7a,Starktony11,1754605281.0,27,/r/datascience/comments/1mkdy7a/how_do_you_analyse_unbalanced_data_you_get_in_ab/,datascience,How do you analyse unbalanced data you get in A/B testing?,"Hi 
I have two questions related unbalanced data in A/B testing. Would appreciate resources or thoughts. 

1. Usually when we perform A/B testing, we have 5-10% in treatment, after doing power analysis we get the sample size needed, we run tge experiment, by the time we get required sample size for treatment we get way more control samples, so now when we analyse, which samples do we keep in control group? For example by the time we collect 10k samples from treatment we might get 100k samples of control. So what to do now before performing t-test or any kinds of test? 
 (In ML we can downsample or over sample but what to do in causal side) 

2. Again similar question Lets say we are performing test on 50/50 but if one variant get way more samples as more ppl come through that channel and common for users, hiw do we segment users such as way? And again which samples we keep once we get way more sample than needed? 

I want to know how it is tackeled in day to day, and this thing happen frequently right? Or am i wrong? 

Also, what if you get sample size before expected time? (Like was thinking to run them for 2 weeks but got the required size in 10 days) Do you stop the experiment and start analyzing? 

Sorry for this dumb question but i could not find good answers and honestly don’t trust chat gpt much as many time it hallucinates in this topic. 

Thanks!",32,0.91,https://www.reddit.com/r/datascience/comments/1mkdy7a/how_do_you_analyse_unbalanced_data_you_get_in_ab/,False,True,False
1mk7lpa,Pristine-Item680,1754590523.0,19,/r/datascience/comments/1mk7lpa/what_elective_course_should_i_take/,datascience,What elective course should I take,"Hey all,

About to start my last semester for my masters in computer science, with a concentration in AI. I’m a veteran data scientist, this is more of a vanity degree and an ability to say “yes I do have a masters degree” on a job application, but I have enjoyed the studying overall. 

I have room for one elective class, and I’m trying to decide what I should take. None of them that fit my schedule seem particularly appealing:

- data analysis: hyper redundant given my background
- computer networks: possibly useful, but I’d much rather learn something like distributed systems
- intro to cybersecurity: maybe good, but seems like it would be mostly terminology and not so much a deep dive on anything 
- object oriented design: could be nice for refining my actual design choices, but programming seems like the least valuable skill to upskill on in computer science now (as compared to, say, cloud computing, which is and will continue to be good to know). 

It’s not exactly the most pressing choice, but I thought I’d throw it to Reddit, and see if anyone has a strong opinion on what’s good to learn to augment my ML/AI background

Edit: okay I think you people convinced me. Object oriented design it is! Which sounds a whole lot better than computer networks, that’s for sure. ",7,0.82,https://www.reddit.com/r/datascience/comments/1mk7lpa/what_elective_course_should_i_take/,False,True,False
1mizivg,Astherol,1754470591.0,9,/r/datascience/comments/1mizivg/seeking_meaningful_nonprofit_data_volunteering/,datascience,"Seeking Meaningful, Non-Profit Data Volunteering Projects",,28,0.94,/r/dataengineering/comments/1mix8g7/seeking_meaningful_nonprofit_data_volunteering/,False,False,False
1miresg,vishal-vora,1754443300.0,9,/r/datascience/comments/1miresg/share_your_thought_on_open_source_alternative_for/,datascience,Share your thought on open source alternative for data robot,Data robot is the market leader when it comes to enterprises data science project life cycle management. But there is no open source alternative available in the market right now. What are the chances of getting a good adoption if I can build the open source alternative of data robot?,0,0.31,https://www.reddit.com/r/datascience/comments/1miresg/share_your_thought_on_open_source_alternative_for/,False,True,False
1miccmb,techlatest_net,1754407816.0,4,/r/datascience/comments/1miccmb/how_i_built_and_deployed_a_genai_app_in_minutes/,datascience,How I built and deployed a GenAI app in minutes using open‑source tools + Azure,"Hey everyone building AI apps always felt like a massive undertaking. So much code, setup, server stuff. I recently tried something different and launched a working GenAI app in just under 15 minutes. I used Dify AI (an open‑source platform) to design the app and Microsoft Azure to deploy it.

What I learned:
• No heavy DevOps or managing servers
• Very user‑friendly interface—just plug in your AI logic
• Scales automatically via Azure cloud resources

Would love to hear if anyone’s tried Dify AI or other open‑source builders for AI—and what challenges you faced!

Full details in this write‑up:
https://medium.com/@techlatest.net/launch-genai-apps-in-minutes-with-techlatest-dify-ai-on-azure-cloud-platform-8307bccf4aed

Happy to answer questions or breakdown steps if interested 😊",0,0.29,https://www.reddit.com/r/datascience/comments/1miccmb/how_i_built_and_deployed_a_genai_app_in_minutes/,False,True,False
1mhikh4,ProbaDude,1754325757.0,40,/r/datascience/comments/1mhikh4/how_can_i_give_a_good_data_sciencemachine/,datascience,How can I *give* a good data science/machine learning interview?,"I'm around 6 months into my first non intern job and am the only data scientist/MLE in my company. My company has decided they want to bring on some much needed help (thank god) and want me to do ""the more technical side"" of the interview (with others taking care of the behavioral etc)

I do have some questions in mind specific to my job for what I want in a colleague but I still feel a bit underprepared. My plan is to ask the 'basic' questions that I got asked in every interview (classification vs clustering, what is r^2, etc) before asking them how they would solve some of the problems I'm actually working on

But like that's all I have in the pipeline at the moment, and I'd really like to avoid this becoming the blind interviewing the blind moment.  

Does anyone have any good tips on how to do the interviews, what to look for or what to include? Thank you!!!!

EDIT: In reply to the DMs, we are not accepting any new applicants at this time 😅",171,0.96,https://www.reddit.com/r/datascience/comments/1mhikh4/how_can_i_give_a_good_data_sciencemachine/,False,True,False
1mh9tvo,SharePlayful1851,1754303514.0,19,/r/datascience/comments/1mh9tvo/what_would_be_a_better_job_position_data/,datascience,What would be a better job Position ? Data Scientist or AI/ML Engineer.,,0,0.33,/r/careerguidance/comments/1mh97i5/what_would_be_a_better_job_position_data/,False,False,False
1mh3i7n,AutoModerator,1754280102.0,57,/r/datascience/comments/1mh3i7n/weekly_entering_transitioning_thread_04_aug_2025/,datascience,"Weekly Entering & Transitioning - Thread 04 Aug, 2025 - 11 Aug, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",7,1.0,https://www.reddit.com/r/datascience/comments/1mh3i7n/weekly_entering_transitioning_thread_04_aug_2025/,False,True,False
1mgxgpl,CleanDataDirtyMind,1754262535.0,4,/r/datascience/comments/1mgxgpl/is_there_a_term_for_internal_processing_vs_data/,datascience,Is there a term for internal processing vs data that needs to be stakeholding/customer facing?,"For example I had my physical credit card stolen. I was trying to get information from the CC company about when the card was used so that the local PD could check security cameras. (We thought it was particular person so they made a little bit more effort). When I called the credit card company, the customer service person started telling me these random times that made no sense and I realized he was reading the wrong column which were basically the time the charge was converted from “?” to an actual money transfer. I assume to him it gave insight into how to refund each charge so “relvant” just not “relvant” information I would ever need to know.

Two years later, I am setting up a model with my team and we batting around terms to differentiate between data like these dates & times that are relvant but are not relvant un-manipulated or laid bare for the stakeholder to see visualized or be discussed outside of our team.

You can hear the inevitable pause from a team member every time the concept comes up as they attempt a new word.  While it was amusing it’s starting to eat at me. Any ideas?",3,0.71,https://www.reddit.com/r/datascience/comments/1mgxgpl/is_there_a_term_for_internal_processing_vs_data/,False,True,False
1mgxbio,NervousVictory1792,1754262146.0,18,/r/datascience/comments/1mgxbio/algorithm_idea/,datascience,Algorithm Idea,This sudden project has fallen on my lap where I have a lot of survey results and I have to identify how many of those are actually done by bots. I haven’t see what kind of data the survey holds but I was wondering how can I accomplish this task. A quick search points me towards anomaly detections algorithms like isolation forest and dbscan clusters. Just wanted to know if I am headed in the right direction or can I use any LLM tools. TIA :) ,0,0.5,https://www.reddit.com/r/datascience/comments/1mgxbio/algorithm_idea/,False,True,False
1mgsshu,indie-devops,1754250970.0,11,/r/datascience/comments/1mgsshu/personal_projects_and_skill_set/,datascience,Personal projects and skill set,"Hi everyone,
I was just wondering how do you guys specify personal acquired skills from your personal projects in your CV.
I’m in the midst of a pretty large project - end to end pipeline for predicting real time probabilities of winning chances in a game. This includes a lot of tools, from scraping, database management (mostly tables creations, indexing, nothing DBA-like), scheduling, training, prediction and data drift pipelines, cloud hosting, etc. and I was wondering how I can specify those skills after I finish my project, because I do learn tons from this project. To say I’m using some of those tools in my current job is not entirely right so…

What would you say?
Cheers.",23,0.96,https://www.reddit.com/r/datascience/comments/1mgsshu/personal_projects_and_skill_set/,False,True,False
1mgrvsh,1_plate_parcel,1754248765.0,5,/r/datascience/comments/1mgrvsh/hi_i_am_a_junior_dev_need_advice_regarding/,datascience,Hi! i am a junior dev need advice regarding fraud/risk scoring (not credit) on my rules based fraud detection system.,"so i our team has developed a rules based fraud detecton system....now we have received a new requirement that we have to score every transaction as how much risky or if flagged as fraud how much fraud it is.

i did some research and i found out its easier if it is a supervisied operation but in my case i wont be able to access prod transaction data due to policy.

now i have 2 problems data which i guess i have to make a fake one.

2nd how to score i was thinking of going witb regression if i keep my target value bete 0 and 1 but realised that the model can predict above that
then thought of classification and use predict_proba() to get prediction probability.

or isolation forest

till now thats what i bave you thought what else shoudl i consider any advices or guidance to set me in the right path so i dont get any rework
",0,0.43,https://www.reddit.com/r/datascience/comments/1mgrvsh/hi_i_am_a_junior_dev_need_advice_regarding/,False,True,False
1mgfcke,Anu_Rag9704,1754214617.0,10,/r/datascience/comments/1mgfcke/built_this_out_of_pure_laziness_for_all_my/,datascience,Built this out of pure laziness for all my Feature engineering/model training jobs,"Built this out of pure laziness 
A lightweight Telegram bot that lets me: 
- Get Databricks job alerts
- Check today’s status
- Repair failed runs
- Pause/reschedule ,
All from my phone.
No laptop. No dashboard. Just / Commands.",58,0.93,https://i.redd.it/jh3mhg0p0sgf1.jpeg,False,False,False
1mf44ek,pokelord13,1754072573.0,17,/r/datascience/comments/1mf44ek/using_a_hybrid_role_in_job_title_data_science_and/,datascience,Using a hybrid role in job title (Data Science and Engineer),"I have an BS and MS in data science and got hired as a data analyst for a small ish scale company for about a year now as my first job. I'm the only data person in the entire company and I've been wanting to transition into a data science focused role for awhile, so I have been using DS and DE principles at every opportunity to boost my resume. This has ended up extending far beyond the typical DA responsibilities as I have been utilizing a lot of stats modeling and predictive analytics over company data/KPIs, using MLOps occasionally, as well as building ETL pipelines, managing the internal DBMS and streamlining data acquisition through RESTful APIs with contracted third parties. I still do excel monkey work/tableau dashboards along with this.

Management ended up taking notice and since nobody in the building has any familiarity with data science/tech, they have asked me to rewrite my job description including my job title as a semi promotion. Since I have been working as a bit of a hybrid between DS and DE I am wondering if I should put the new contracted job title as a hybrid role (e.g. Data Science Engineer) or just pick one? My department head has suggested the title of Data Architect but I don't really think that aligns with my job responsibilities and it's also a senior sounding position which feels strange to take on considering I've only been in the industry for a year.",55,0.9,https://www.reddit.com/r/datascience/comments/1mf44ek/using_a_hybrid_role_in_job_title_data_science_and/,False,True,False
1mets4m,JumbleGuide,1754047160.0,6,/r/datascience/comments/1mets4m/how_to_convert_data_to_conceptual_models/,datascience,How to convert data to conceptual models,"I am not sure if I am in the right subreddit, so please by patient with me.

I am working on a tool to reverse-engineer conceptual models from existing data. The idea is you take a legacy system, collect sample data (for example JSON messages communicated by the system), and get a precise model from them. The conceptual model can be then used to develop new parts of the system, component replacements, build documentation, tests, etc...

One of the open issues I struggle with is the fully-automated conversion from 'packaging' model to conceptual model.

When some data is uploaded, it's model reflects the packaging mechanism, rather than the concepts itself. For example. if I upload JSON-formatted data, the model initially consists of objects, arrays, and values. For XML, it is elements and attributes. And so on.

[JSON messages consist of objects, arrays, and values](https://preview.redd.it/rq6k13ej2egf1.png?width=737&format=png&auto=webp&s=415800ea39e0b408f91124f5d03fab02b631e75e)

I can convert the keys, levels, paths to detect concepts and their relationships.  It can look something like this:

[Data structures converted to concepts](https://preview.redd.it/r1d2ti683egf1.png?width=695&format=png&auto=webp&s=0927e6222a90412d7dd5b722fdb43ad07b49e027)

  
The issue I am struggling with is that this conversion is not straightforward. Sometimes, it helps to use keys, other times it is better to use paths. For some YAML files, I need to treat the keys as values (typically package.yaml samples).

Did anyone tried to convert data to conceptual models before? Any real-word use cases?

Is there any theory at least about the reverse direction - use conceptual model and map it into XML schema / JSON schema / YAML ... ?

Thanks in advance.",10,0.92,https://www.reddit.com/r/datascience/comments/1mets4m/how_to_convert_data_to_conceptual_models/,False,True,False
1mem3t5,Minotaar_Pheonix,1754019466.0,9,/r/datascience/comments/1mem3t5/generative_ai_shell_interface_for_browsing_and/,datascience,Generative AI shell interface for browsing and processing data?,"So vibe coding is a thing, and I'm not super into it.

However, I often need to write little scripts and parsers and things to collect and analyze data in a shell environment for various code that I've written.  It might be for debugging, or just collecting production science data.  Writing that shit is a real pain, because you need to be careful about exceptions and errors and folder names and such.

Is there a way to do ""vibe data gathering"" where I can ask some LLM to write me a script that does a number of things like open up a couple thousand files that fit various properties in various folders, parse them for specific information, then draw say a graph?  ChatGPT can of course do that, but it needs to know the folder structure and examine the files to see what issues there are in collecting this information.  Any way I can do this without having to roll my sleeves up?",2,0.6,https://www.reddit.com/r/datascience/comments/1mem3t5/generative_ai_shell_interface_for_browsing_and/,False,True,False
1me934o,giantwaterwithice,1753985789.0,42,/r/datascience/comments/1me934o/why_is_there_no_cursorwindsurf_for_notebooks_or/,datascience,Why is there no Cursor/Windsurf for Notebooks or Google Collab?," Last week, I tried Windsurf to build a web application and OMG my world was changed. I have used AI tools before but having an agent that implements the code for you is a game changer, my productivity probably went up x5 or x10 times. 

This made me think why is there nothing like this for a data scientist workflow? I know you can do notebook markdown but it is still not the same because Cursor cannot see outputs of your graphs. Also, this tool wouldn’t work on Google Collab where I have access to powerful GPUs. 

Now, imagine if you have a tool that goes from a prompt “make the predictive model to predict customer churn” and instead of something like Chatgpt giving you one slob of generic BS that will definitely give out an error, an agent goes and executes each cell one by one: making plots, studying the data, modifying the outliers etc. and adjusting the plan as it goes before finally making a few models and testing them. Basically, the standard data science workflow. 

I would like to build something this (I have no idea how yet lol) if there is interest in this community. What do you guys think? Those of you who are working in the field, would you actually use it? 

Also, if someone wants to build it with me, DM me. ",9,0.6,https://www.reddit.com/r/datascience/comments/1me934o/why_is_there_no_cursorwindsurf_for_notebooks_or/,False,True,False
1me91rq,Aristoteles1988,1753985705.0,6,/r/datascience/comments/1me91rq/figma_is_the_tech_industry_back/,datascience,FIGMA? Is the tech industry back?,"Have you guys heard of this IPO? Stock tripled on debut. What does this company do? 

I feel like you tech bros might have a come back soon fyi ",0,0.24,https://www.reddit.com/r/datascience/comments/1me91rq/figma_is_the_tech_industry_back/,False,True,False
1mdf6fn,FinalRide7181,1753901845.0,22,/r/datascience/comments/1mdf6fn/my_take_on_the_microsoft_paper/,datascience,My take on the Microsoft paper,"I read the paper myself (albeit pretty quickly) and tried to analyze the situation for us Data Scientists.

The jobs on the list, as you can intuitively see (and it is also explicitly mentioned in the paper), are mostly jobs that require writing reports and gathering information because, as the paper claims, AI is good at it.

If you check the chart present in the paper (which I linked in this post), you can see that the clear winner in terms of activities done by AI is “Gathering Information”, while “Analyzing Data” instead is much less impacted and also most of it is people asking AI to help with analysis, not AI doing them as an agent (red bar represents the former, blue bar the latter).

It seems that our beloved occupation is in the list mainly because it involves gathering information and writing reports. However, the data analysis part is much less affected and that’s just data analysis, let alone the more advanced tasks that separate a Data Scientist from a Data Analyst.

So, from what I understand, Data Scientists are not at risk. The things that AI does do not represent the actual core of the job at all, and are possibly even activities that a Data Scientist wants to get rid of.

If you’ve read the paper too, I’d appreciate your feedback. Thanks!",169,0.94,https://imgur.com/a/Ba5m1Po,False,False,False
1mdan3p,-phototrope,1753891640.0,13,/r/datascience/comments/1mdan3p/model_governance_requests_what_is_normal/,datascience,Model Governance Requests - what is normal?,"I’m looking for some advice. I work at a company that provides inference as a service to other customers, specifically we have model outputs in an API. This is used across industries, but specifically when working with Banks, the amount of information they request through model governance is staggering.

I am trying to understand if my privacy team is keeping things too close to the chest, because I find that what is in our standard governance docs, vs the details we are asked, is hugely lacking. It ends up being this ridiculous back and forth and is a huge burn on time and resources. 


Here are some example questions:

* specific features used in the model 

* specific data sources we use

* detailed explanations of how we arrived at our modeling methodology, what other models we considered, the results of those other models, and the rationale for our decision with a comparative analysis

* a list of all metrics used to evaluate model performance, and why we chose those metrics

* time frame for train/test/val sets, to the day

I really want to understand if this is normal, and if my org needs to improve how we report these out to customers that are very concerned about these kinds of things (banks). Are there any resources out there showing what is industry standard? How does your org do it?

Thanks",6,1.0,https://www.reddit.com/r/datascience/comments/1mdan3p/model_governance_requests_what_is_normal/,False,True,False
1mdaa40,itssdgm,1753890832.0,40,/r/datascience/comments/1mdaa40/working_remote/,datascience,Working remote,"hey all 
i’ve been a data scientist for a while now, and i’ve noticed my social anxiety has gotten worse since going fully remote since covid. i love the work itself - building models, finding insights etc, but when it comes to presenting those insights, i get really anxious. it’s easily the part of the job i dread most.

i think being remote makes it harder. less day-to-day interaction, fewer casual chats - and it just feels like the pressure is higher when you do have to speak. imposter syndrome also sneaks in at time. tech is constantly evolving, and sometimes i feel like i’m barely keeping up, even though i’m doing the work.

i guess i’m wondering:
	•	does anyone else feel this way?
	•	have you found ways to make communications feel less overwhelming?

would honestly just be nice to hear from others in the same boat. thanks for reading.",118,0.92,https://www.reddit.com/r/datascience/comments/1mdaa40/working_remote/,False,True,False
1md5gvk,timusw,1753879067.0,163,/r/datascience/comments/1md5gvk/microsoft_just_dropped_a_study_showing_the_40/,datascience,Microsoft just dropped a study showing the 40 jobs most affected by Al and the 40 that Al can't touch (yet).,,410,0.86,https://www.reddit.com/gallery/1mcup6s,False,False,False
1mcngiy,askdatadawn,1753822927.0,25,/r/datascience/comments/1mcngiy/python_summer_party_free_15day_coding_challenge/,datascience,Python Summer Party (free!): 15-day coding challenge for Data folks,"I’ve been cooking up something fun for the summer.. A Python-themed challenge to help Data Scientists & Data Analysts practice and level up their Python skills. Totally free to play!

It’s called **Python Summer Party**, and it runs for 15 days, starting August 1.

Here’s what to expect:

* One Python challenge + 3 parts per day
* Focused on Data skills using *NumPy*, *Pandas*, and regular Python
* All questions based on real companies, so you can practice working with real problems
* Beginner to intermediate to advanced questions
* AI chat to help you if you get stuck
* Discord community (if you still need more help)
* A chance to win 5 free annual Data Camp subscriptions if you complete the challenges
* Totally free

I built this because I know how hard it can be to stay consistent when you’re learning alone. Plus, when I was learning Python I couldn't find questions that allowed me to apply Python to realistic business problems.

So this is meant to be a light, motivating way to practice and have fun with others. *I even tried to design it such that it's cute & fun.*

Would love to have you join us (and hear your feedback if you have any!) 

[www.interviewmaster.ai/python-party](http://www.interviewmaster.ai/python-party)",82,0.93,https://www.reddit.com/r/datascience/comments/1mcngiy/python_summer_party_free_15day_coding_challenge/,False,True,False
1mcd3n5,Lamp_Shade_Head,1753799717.0,55,/r/datascience/comments/1mcd3n5/since_when_did_meets_expectations_become_a_bad/,datascience,Since when did “meets” expectations become a bad thing in this industry?,"I work at a pretty big named company on west coast. It is pretty shocking to see that in my company anyone who gets “meets” expectations have not been getting any salary increments, not even a dollar each year. I’d think if you are meeting expectations, it means you are holding up your end of the deal and it shouldn’t be a bad thing. But now, you actually have to exceeds expectations to get measly 1% salary raises and sometimes to just keep your job.

Did this used to happen pre covid as well?",224,0.98,https://www.reddit.com/r/datascience/comments/1mcd3n5/since_when_did_meets_expectations_become_a_bad/,False,True,False
1mc8w7g,bandaian,1753788531.0,12,/r/datascience/comments/1mc8w7g/how_to_use_ai_effectively_and_efficiently_to_code/,datascience,How to use AI effectively and efficiently to code,Any tips on how to teach beginners on how to use AI effectively and efficiently to code?,0,0.25,https://www.reddit.com/r/datascience/comments/1mc8w7g/how_to_use_ai_effectively_and_efficiently_to_code/,False,True,False
1mc2zaz,CableInevitable6840,1753766333.0,181,/r/datascience/comments/1mc2zaz/does_a_data_scientist_need_to_learn_all_these/,datascience,Does a Data Scientist need to learn all these skills?,"* Strong knowledge of Machine Learning, Deep Learning, NLP, and LLMs.
* Experience with Python, PyTorch, TensorFlow.
* Familiarity with Generative AI frameworks: Hugging Face, LangChain, MLFlow, LangGraph, LangFlow.
* Cloud platforms: AWS (SageMaker, Bedrock), Azure AI, and GCP
* Databases: MongoDB, PostgreSQL, Pinecone, ChromaDB.
* MLOps tools, Kubernetes, Docker, MLflow.

I have been browsing many jobs and noticed they all are asking for all these skills.. is it the new norm? Looks like I need to download everything and subscribe to a platform that teaches all these lol (cries in pain).",356,0.97,https://www.reddit.com/r/datascience/comments/1mc2zaz/does_a_data_scientist_need_to_learn_all_these/,False,True,False
1mby05c,bass581,1753751156.0,106,/r/datascience/comments/1mby05c/any_phds_having_trouble_in_the_job_market/,datascience,Any PhDs having trouble in the job market,"I am a Math Bio PhD who is currently working for a pharma company. I am trying to look for new positions outside the industry, as it seems most data science work at my current employer and previous employers has been making simple listings for use across the company. It is really boring, and I feel my skillset is not applicable to other data roles. I have taken courses on data engineering and ML and worked on personal projects, but it has yielded little success. I was wondering if any other PhD that are entering the job market or are veterans have had trouble finding a new job in the last few years. Obviously the job market is terrible, but you would think having a PhD would yield better success in finding new positions. I would also like some advice on how to better position myself in the market.",78,0.83,https://www.reddit.com/r/datascience/comments/1mby05c/any_phds_having_trouble_in_the_job_market/,False,True,False
1mbqiix,cptsanderzz,1753732510.0,9,/r/datascience/comments/1mbqiix/best_framework_for_internal_tools/,datascience,Best framework for internal tools,"I need frameworks to build standalone internal tools that don’t require spinning up a server. Most of the time I am delivering to non technical users and having them install Python to run the tool is so cumbersome if you don’t have a clue what you are doing. Also, I don’t want to spin up a server for a process that users run once a week, that feels like a waste. PowerBI isn’t meant to execute actions when buttons are clicked so that isn’t really an option. I don’t need anything fancy, just something that users click, it opens up asks them to put in 6 files, runs various logic and exports a report comparing various values across all of those files.


Tkinter would be a great option besides the fact that it looks like it was last updated in 2000 which while it sounds silly doesn’t inspire confidence for non technical people to use a new tool.

I love Streamlit or Shiny but that would require it to be running 24/7 on a server or me remembering to start it up every morning and monitor it for errors.

What other options are out there to build internal tools for your colleagues? I don’t need anything enterprise grade anything, just something simple that less than 30 people would ever use.
",8,0.79,https://www.reddit.com/r/datascience/comments/1mbqiix/best_framework_for_internal_tools/,False,True,False
1mbmnkz,AipaQ,1753723936.0,12,/r/datascience/comments/1mbmnkz/why_autoencoders_arent_the_answer_for_image/,datascience,Why autoencoders aren't the answer for image compression,"I just finished my engineering thesis comparing different lossy compression methods and thought you might find the results interesting.

**What I tested:**

* Principal Component Analysis (PCA)
* Discrete Cosine Transform (DCT) with 3 different masking variants
* Convolutional Autoencoders

All methods were evaluated at 33% compression ratio on MNIST dataset using SSIM as the quality metric.

**Results:**

* **Autoencoders: 0.97 SSIM** \- Best reconstruction quality, maintained proper digit shapes and contrast
* **PCA: 0.71 SSIM** \- Decent results but with grayer, washed-out digit tones
* **DCT variants: \~0.61 SSIM** \- Noticeable background noise and poor contrast

**Key limitations I found:**

* Autoencoders and PCA require dataset-specific training, limiting universality
* DCT works out-of-the-box but has lower quality
* Results may be specific to MNIST's simple, uniform structure
* More complex datasets (color images, multiple objects) might show different patterns

**Possible optimizations:**

* Autoencoders: More training epochs, different architectures, advanced regularization
* Linear methods: Keeping more principal components/DCT coefficients (trading compression for quality)
* DCT: Better coefficient selection to reduce noise

**My takeaway:** While autoencoders performed best on this controlled dataset, the training requirement is a significant practical limitation compared to DCT's universal applicability.

**Question for you:** What would you have done differently in this comparison? Any other methods worth testing or different evaluation approaches I should consider for future work?

The post with more details about implementation and **visual comparisons** if anyone's interested in the technical details: [https://dataengineeringtoolkit.substack.com/p/autoencoders-vs-linear-methods-for](https://dataengineeringtoolkit.substack.com/p/autoencoders-vs-linear-methods-for)",9,0.66,https://dataengineeringtoolkit.substack.com/p/autoencoders-vs-linear-methods-for,False,False,False
1mbm7zn,Technical-Love-8479,1753722975.0,0,/r/datascience/comments/1mbm7zn/tried_wan22_on_rtx_4090_quite_impressed/,datascience,"Tried Wan2.2 on RTX 4090, quite impressed",,2,0.75,/r/LocalLLaMA/comments/1mbm4a0/tried_wan22_on_rtx_4090_quite_impressed/,False,False,False
1mbk933,ElectrikMetriks,1753718690.0,9,/r/datascience/comments/1mbk933/why_are_none_of_my_reports_refreshing_this_morning/,datascience,Why are none of my reports refreshing this morning?,,258,0.96,https://i.redd.it/kect7eypzmff1.png,False,False,False
1mb6ch8,AutoModerator,1753675299.0,40,/r/datascience/comments/1mb6ch8/weekly_entering_transitioning_thread_28_jul_2025/,datascience,"Weekly Entering & Transitioning - Thread 28 Jul, 2025 - 04 Aug, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",6,0.88,https://www.reddit.com/r/datascience/comments/1mb6ch8/weekly_entering_transitioning_thread_28_jul_2025/,False,True,False
1mb49xm,insane_membrane13,1753668885.0,104,/r/datascience/comments/1mb49xm/new_grad_data_scientist_feeling_overwhelmed_and/,datascience,New Grad Data Scientist feeling overwhelmed and disillusioned at first job,"Hi all,

I recently graduated with a degree in Data Science and just started my first job as a data scientist. The company is very focused on staying ahead/keeping up with the AI hype train and wants my team (which has no other data scientists except myself) to explore deploying AI agents for specific use cases. 

The issue is, my background, both academic and through internships, has been in more traditional machine learning (regression, classification, basic NLP, etc.), not agentic AI or LLM-based systems. The projects I’ve been briefed on, have nothing to do with my past experiences and are solely concerned with how we can infuse AI into our workflows and within our products. I’m feeling out of my depth and worried about the expectations being placed on me so early in my career. I was wondering if anyone had advice on how to quickly get up to speed with newer techniques like agentic AI, or how I should approach this situation overall. Any learning resources, mindset tips, or career advice would be greatly appreciated.",382,0.96,https://www.reddit.com/r/datascience/comments/1mb49xm/new_grad_data_scientist_feeling_overwhelmed_and/,False,True,False
1maxkht,Routine_Nothing_8568,1753650336.0,12,/r/datascience/comments/1maxkht/anomoly_detection_with_only_categorical_variables/,datascience,Anomoly detection with only categorical variables,"Hello everyone, I have an anomoly detection project but all of my data is categorical. I suppose I could try and ask them to change it prediction but does anyone have any advice. The goal is to there are groups within the data and and do an analysis to see anomlies. This is all unsupervised the dataset is large in terms of rows (500k) and I have no gpus.",6,0.8,https://www.reddit.com/r/datascience/comments/1maxkht/anomoly_detection_with_only_categorical_variables/,False,True,False
1mawalf,Due-Duty961,1753647197.0,17,/r/datascience/comments/1mawalf/why_onehotencoder_give_better_results_than/,datascience,why OneHotEncoder give better results than get.dummies/reindex?,"**I can't figure out why I get a better score with OneHotEncoder :**

preprocessor = ColumnTransformer(

transformers=\[

('cat', categorical\_transformer, categorical\_cols)

\],

remainder='passthrough'  # <-- this keeps the numerical columns

)

model\_GBR =  GradientBoostingRegressor(n\_estimators=1100, loss='squared\_error', subsample = 0.35, learning\_rate = 0.05,random\_state=1)

GBR\_Pipeline = Pipeline(steps=\[('preprocessor', preprocessor),('model', model\_GBR)\])

  
**than get.dummies/reindex:**

  
X\_test = pd.get\_dummies(d\_test)

X\_test\_aligned = X\_test.reindex(columns=X\_train.columns, fill\_value=0)",10,0.7,https://www.reddit.com/r/datascience/comments/1mawalf/why_onehotencoder_give_better_results_than/,False,True,False
1makoge,ArticleLegal5612,1753618104.0,36,/r/datascience/comments/1makoge/can_llms_reason_i_dont_know_depends_on_the/,datascience,"Can LLMs Reason - I don't know, depends on the definition of reasoning.  Denny Zhou - Founder/Lead of Google Deepmind LLM Reasoning Team","AI influencers: LLMs can think given this godly prompt bene gesserit oracle of the world blahblah, hence xxx/yyy/zzz is dead. See more below.

Meanwhile, literally the founder/lead of the reasoning team: 

https://preview.redd.it/z9uwnummqeff1.png?width=652&format=png&auto=webp&s=c84727d328d059504adf64768b8badac45d20611

Reference: [https://www.youtube.com/watch?v=ebnX5Ur1hBk](https://www.youtube.com/watch?v=ebnX5Ur1hBk) good lecture! ",20,0.66,https://www.reddit.com/r/datascience/comments/1makoge/can_llms_reason_i_dont_know_depends_on_the/,False,True,False
1mabzuf,hendrix616,1753586240.0,4,/r/datascience/comments/1mabzuf/hyperparameter_and_prompt_tuning_via_agentic_cli/,datascience,Hyperparameter and prompt tuning via agentic CLI tools like Claude Code,"Has anyone used Claude Code as way to automate the improvement of their ML/AI solution?

In traditional ML, there’s the notion of hyperparameter tuning, whereby you search the source of all possible hyperparameter values to see which combination yields the best result on some outcome metric.

In LLM systems, the thing that gets tuned is the prompt and the outcome being evaluated is the output of some eval framework.

And some systems incorporate both ML and LLM

All of this iteration can be super time consuming and, in the case of the LLM prompt optimization, quite costly if you are constantly changing the prompt and having to rerun the eval framework.

The process can be manual or operated automatically by some heuristic.

It occurred to me the other day that it might be a great idea to get CC to do this iteration instead. If we arm it with the context and a CLI for running experiments with different configs), then it could do the following:
- ⁠Run its own experiments via CLI
- Log the results
- Analyze the results against historical results
- Write down its thoughts
- Come up with ideas for future experiments
- Iterate!

Just wondering if anyone has pulled this off successfully in the past and would care to share :)
",2,0.57,https://www.reddit.com/r/datascience/comments/1mabzuf/hyperparameter_and_prompt_tuning_via_agentic_cli/,False,True,False
1m9e3vg,Suspicious_Coyote_54,1753485641.0,55,/r/datascience/comments/1m9e3vg/stuck_not_doing_ds_work_as_a_ds/,datascience,Stuck not doing DS work as a DS,"I have been working at a pharma for 5 years. In that time I got my MSDS and did some good work. Issue is, despite stellar yearly reviews I never ever get promoted. Each year I ask for a plan, for a goal to hit , for a reason why, but I always get met with “it just is not in the cards” kind of answer. 

I spent 6 months applying for other jobs but the issue is my work does not translate well. I built dashboards and an r shiny apps that had some business impact. Unfortunately despite the manager and director talking a big game about how we will use Ai and do a ton of DS and ML work, we never do and I often get stuck with the crappy work. 

When I interview I kill it during behaviorals and I often get far into the process but then I get asked about my lack of AB testing, or ML experience and I am quite honest. I simply have not been assigned those tasks and the company does not do them. Boom I’m out. I’m stuck and I don’t know what to do or how to proceed. Doing projects seems like a decent move but I’ve heard people say that it does not matter. I’m also not great at coding interviews on the spot. I’ve studied a bunch but can’t perform or often get mind wiped when asked a coding question. Anyone else been here? How did you get out? Any help would be appreciated. I really want to be a better DS and get out of pharma and into product or analytics.",138,0.96,https://www.reddit.com/r/datascience/comments/1m9e3vg/stuck_not_doing_ds_work_as_a_ds/,False,True,False
1m8zjnq,tits_mcgee_92,1753451036.0,120,/r/datascience/comments/1m8zjnq/can_a_phd_be_harmful_for_your_career/,datascience,Can a PhD be harmful for your career?,"I have my MS degree in a Data Science adjacent field. I currently work in a Data Science / Software Engineering hybrid role, but I also work a second job as an adjunct professor in data science/analytics.

I find teaching unbelievably rewarding, but I could make more money being a cashier at Target. That's no exaggeration.

Part of me thinks teaching is my calling. My workplace will pay for my PhD, however, if I receive my PhD, and discover that I may not want to be a professor... would this result in a hard time finding data science jobs that aren't solely research based?

I try to think of the recruiter perspective, and if I applied to a job with a PhD they may think I will be asking for too much money or be too overqualified.

I'm just wondering if anyone has been in the same scenario, or had thoughts on this. Thank you for your time!",92,0.81,https://www.reddit.com/r/datascience/comments/1m8zjnq/can_a_phd_be_harmful_for_your_career/,False,True,False
1m8jaeh,gpbayes,1753398633.0,114,/r/datascience/comments/1m8jaeh/highest_roi_math_youve_had/,datascience,Highest ROI math you’ve had?,"Curious if there is a type of math / project that has saved or generated tons of money for your company. For example, I used Bayesian inference to figure out what insurance policy we should buy. I would consider this my highest ROI project. 

Machine Learning so far seems to promise a lot but delivers quite little.

Causal inference is starting to pick up the speed. ",247,0.97,https://www.reddit.com/r/datascience/comments/1m8jaeh/highest_roi_math_youve_had/,False,True,False
1m8da2j,gyp_casino,1753384042.0,41,/r/datascience/comments/1m8da2j/are_your_traditional_data_science_projects_still/,datascience,Are your traditional Data Science projects still getting supported?,"My managers are consumed by AI hype.  It was interesting initially when AI was chatbots and coding assistants, but once the idea of Agents entered their mind, it all went off a cliff.  We've had conversations that might as well have been conversations about magic.

I am proposing sensible projects with modest budgets that are getting no interest.",132,0.99,https://www.reddit.com/r/datascience/comments/1m8da2j/are_your_traditional_data_science_projects_still/,False,True,False
1m825ra,qtalen,1753357484.0,3,/r/datascience/comments/1m825ra/after_many_failed_attempts_i_finally_built_a/,datascience,"After Many Failed Attempts, I Finally Built a Workflow for Generating Beautiful Ink Painting","I've always wanted to build a workflow for my blog that can quickly and affordably generate high-quality artistic covers. After dozens of days of effort, I finally succeeded. Here's what the output looks like:

https://preview.redd.it/lus6nn9i7tef1.png?width=1792&format=png&auto=webp&s=4bf86969f63512c2b223fe0382f85096f8805e87

Let me briefly share my solution:

First, I set a clear goal—this workflow should understand the Eastern artistic concepts in users' drawing intentions, generate prompts suitable for the DALL-E-3 model, and ultimately produce high-quality ink painting illustrations.

https://preview.redd.it/rvttfx5m7tef1.png?width=1792&format=png&auto=webp&s=d0dd035d9138fe5427aa84de39d714082aa47adf

It should also allow users to refine the generated prompts through multi-turn conversations and adjust prompts based on the final generated images. This would significantly reduce costs in terms of tokens and time.

Initially, I tried using Dify to build the workflow, but I faced painful failures in user feedback and workflow loops.

I couldn't use coding frameworks like LangChain or CrewAI either because their abstraction levels were too high, making it hard to meet my customization needs.

Finally, I found LlamaIndex Workflow, which provides a low-abstraction, event-driven architecture for building workflows.

Using this framework along with Context Engineering, I successfully decoupled the workflow loops, making the entire workflow easy to understand, maintain, and adjust as needed.

https://preview.redd.it/vp55460p7tef1.jpg?width=1792&format=pjpg&auto=webp&s=97f9ba642c79400b369437e0bf0a52d954da104c

This flowchart reflects my overall workflow design:

https://preview.redd.it/wjkcel5s7tef1.png?width=658&format=png&auto=webp&s=e69448489b428524bb03be2aa5ab6218359a76c1

https://preview.redd.it/raqqmngt7tef1.png?width=974&format=png&auto=webp&s=9a6eb4e02f71542a17adfff22522bb972be8d327

Due to length constraints, I can't explain my implementation in detail here, but you can read [my full tutorial](https://www.dataleadsfuture.com/use-llamaindex-workflow-to-create-an-ink-painting-style-image-generation-workflow/) to learn about my complete solution.",0,0.3,https://www.reddit.com/r/datascience/comments/1m825ra/after_many_failed_attempts_i_finally_built_a/,False,True,False
1m7z6un,Papa_Huggies,1753346988.0,51,/r/datascience/comments/1m7z6un/how_do_you_know_someones_got_a_data_science/,datascience,How do you know someone's got a data science background?,"They know of only 3 species of iris flower.

PS: we need a flair for stupid jokes",337,0.97,https://www.reddit.com/r/datascience/comments/1m7z6un/how_do_you_know_someones_got_a_data_science/,False,True,False
1m7qbd9,transferrr334,1753317730.0,13,/r/datascience/comments/1m7qbd9/shap_values_with_class_weights/,datascience,SHAP values with class weights,"I’m trying to understand which marketing channels are driving conversion. Approximately 2% of customers convert.

I utilize an XGBoost model and as features have:
1. For converters, the count of various touchpoints in the 8 weeks prior to conversion date.
2. For non-converters, the count of various touchpoints in the 8 weeks prior to a dummy date selected from the distribution of true conversion dates.

Because of how rare conversion is, I use class weighing in my XGBoost model. When I interpret SHAP values, I then get that every predictor is negative, which contextually and numerically is contradictory.

Does changing class weights impact the baseline probability, and mean that SHAP values reflect deviation from the over-weighed baseline probability and not true baseline? If so, what is the best way to correct for this if I still want to use weighing?",22,0.96,https://www.reddit.com/r/datascience/comments/1m7qbd9/shap_values_with_class_weights/,False,True,False
1m7jbpk,techno_prgrssv,1753300356.0,27,/r/datascience/comments/1m7jbpk/is_my_side_gig_worth_the_effort/,datascience,Is my side gig worth the effort?,"I’ve been doing some freelance data analysis (regression, visuals, clustering) for a mid-sized company over the past couple months. The first project paid OK, and the work itself is pretty open-ended and intellectually engaging.

I initially expected access to their internal data, but it turned out I had to source and prep everything myself. The setup is very hands-off—minimal guidance, so I end up doing a lot of research and exploration on my own.

Right now, I’ve had a lot of free time at my full-time job, so I’ve been able to fit this in without much sacrifice. But I’m anticipating a job change soon, and I’m starting to wonder if this work is worth the effort.

Realistically, I probably earn around (or slightly below) my hourly rate once you factor in how open-ended the work is. That wasn’t what I expected going in.

I keep asking myself if my time would be better spent:

* Practicing Python, SQL, or ML skills for future interviews
* Studying things I actually enjoy (causal inference, classical stats)
* Working on personal projects I control
* Or just spending time on non-data hobbies

Curious to hear how others have thought about this tradeoff. Is it better to lean into these kinds of freelance projects for experience and cash, or to use that energy more intentionally elsewhere?",24,0.83,https://www.reddit.com/r/datascience/comments/1m7jbpk/is_my_side_gig_worth_the_effort/,False,True,False
1m7ftt7,Technical-Love-8479,1753292465.0,8,/r/datascience/comments/1m7ftt7/google_deepmind_release_mixtureofrecursions/,datascience,Google DeepMind release Mixture-of-Recursions,Google DeepMind's new paper explore a new advanced Transformers architecture for LLMs called Mixture-of-Recursions which uses recursive Transformers with dynamic recursion per token. Check visual explanation details : https://youtu.be/GWqXCgd7Hnc?si=M6xxbtczSf_TEEYR,22,0.92,https://www.reddit.com/r/datascience/comments/1m7ftt7/google_deepmind_release_mixtureofrecursions/,False,True,False
1m7bhew,Substantial_Tank_129,1753282621.0,84,/r/datascience/comments/1m7bhew/so_are_we_just_supposed_to_know_how_to_get_a/,datascience,So are we just supposed to know how to get a promotion?,"I’ve been working as a Data Scientist I at a Fortune 50 company for the past 3.5 years. Over the last two performance cycles, I’ve proactively asked for a promotion. The first time, my manager pointed out areas for improvement—so I treated that as a development goal, worked on it, and presented clear results in the next cycle.

However, when I brought it up again, I was told that promotions aren’t just based on performance—they also depend on factors like budget and others in the promotion queue. When I asked for a clear path forward, I was given no concrete guidance.

Now I’m left wondering: until the next cycle, what am I supposed to do? Is it usually on us to figure out how to get promoted, or does your company provide a defined path?",182,0.91,https://www.reddit.com/r/datascience/comments/1m7bhew/so_are_we_just_supposed_to_know_how_to_get_a/,False,True,False
1m70hqg,drewm8080,1753247013.0,8,/r/datascience/comments/1m70hqg/probably_and_stats_interview_questions/,datascience,Probably and Stats interview questions?,Is there like a Neetcode equivalent to be able to do those (where you start understanding the different patterns in questions)? I want to get better at problem solving probability and stats questions. ,16,0.86,https://www.reddit.com/r/datascience/comments/1m70hqg/probably_and_stats_interview_questions/,False,True,False
1m70fk3,drewm8080,1753246819.0,51,/r/datascience/comments/1m70fk3/where_is_data_science_interviews_going/,datascience,Where is Data Science interviews going?,"As a data scientist myself, I’ve been working on a lot of RAG + LLM things and focused mostly on SWE related things. However, when I interview at jobs I notice every single data scientist job is completely different and it makes it hard to prepare for. Sometimes I get SQL questions, other times I could get ML, Leetcode, pandas data frames, probability and Statistics etc and it makes it a bit overwhelming to prepare for every single interview because they all seem very different. 

Has anyone been able to figure out like some sort of data science path to follow? I like how things like Neetcode are very structured to follow, but fail to find a data science equivalent.  ",192,0.93,https://www.reddit.com/r/datascience/comments/1m70fk3/where_is_data_science_interviews_going/,False,True,False
1m6jx39,Significant-Heron521,1753204331.0,36,/r/datascience/comments/1m6jx39/stuck_in_defense_contracting_not_doing_data/,datascience,Stuck in defense contracting not doing Data Science but have a data science title,"Title says it all…. Been here for 3 years, doing a lot of database/data architecting but not really any real data science work. My previous job was at a big 4 consulting but I was doing real data science for 2 years, but hated consulting part with a passion. Any advice?

Edit forgot to add: I’m also currently doing my masters in data science (part-time), and my company is flexible letting me do it. I see a lot more job opportunities elsewhere but feel like I should just stay until I finish next year.",106,0.91,https://www.reddit.com/r/datascience/comments/1m6jx39/stuck_in_defense_contracting_not_doing_data/,False,True,False
1m6h3f0,davernow,1753197987.0,5,/r/datascience/comments/1m6h3f0/i_wrote_2000_llm_test_cases_so_you_dont_have_to/,datascience,I wrote 2000 LLM test cases so you don't have to: LLM feature compatibility grid,"This is a quick story of how a focus on usability turned into 2000 LLM tests cases (well 2631 to be exact), and why the results might be helpful to you.

# The problem: too many options

I've been building [Kiln AI](https://github.com/kiln-ai/kiln): an open tool to help you find the best way to run your AI workload. Part of Kiln’s goal is testing various different models on your AI task to see which ones work best. We hit a usability problem on day one: too many options. We supported hundreds of models, each with their own parameters, capabilities, and formats. Trying a new model wasn't easy. If evaluating an additional model is painful, you're less likely to do it, which makes you less likely to find the best way to run your AI workload.

Here's a sampling of the many different options you need to choose: structured data mode (JSON schema, JSON mode, instruction, tool calls), reasoning support, reasoning format (`<think>...</think>`), censorship/limits, use case support (generating synthetic data, evals), runtime parameters (logprobs, temperature, top\_p, etc), and much more.

# How a focus on usability turned into over 2000 test cases

I wanted things to ""just work"" as much as possible in Kiln. You should be able to run a new model without writing a new API integration, writing a parser, or experimenting with API parameters.

To make it easy to use, we needed reasonable defaults for every major model. That's no small feat when new models pop up every week, and there are dozens of AI providers competing on inference.

The solution: a whole bunch of test cases! 2631 to be exact, with more added every week. We test every model on every provider across a range of functionality: structured data (JSON/tool calls), plaintext, reasoning, chain of thought, logprobs/G-eval, evals, synthetic data generation, and more. The result of all these tests is a detailed configuration file with up-to-date details on which models and providers support which features.

# Wait, doesn't that cost a lot of money and take forever?

**Yes it does!** Each time we run these tests, we're making thousands of LLM calls against a wide variety of providers. There's no getting around it: we want to know these features work well on every provider and model. The only way to be sure is to test, test, test. We regularly see providers regress or decommission models, so testing once isn't an option.

Our blog has some details on the [Python pytest setup we used to make this manageable](https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time).

# The Result

The end result is that it's much easier to rapidly evaluate AI models and methods. It includes

* The model selection dropdown is aware of your current task needs, and will only show models known to work. The filters include things like structured data support (JSON/tools), needing an uncensored model for eval data generation, needing a model which supports logprobs for G-eval, and many more use cases.
* Automatic defaults for complex parameters. For example, automatically selecting the best JSON generation method from the many options (JSON schema, JSON mode, instructions, tools, etc).

However, you're in control. You can always override any suggestion.

# Next Step: A Giant Ollama Server

I can run a decent sampling of our Ollama tests locally, but I lack the \~1TB of VRAM needed to run things like Deepseek R1 or Kimi K2 locally. I'd love an easy-to-use test environment for these without breaking the bank. Suggestions welcome!

# How to Find the Best Model for Your Task with Kiln

All of this testing infrastructure exists to serve one goal: making it easier for you to find the best way to run your specific use case. The 2000+ test cases ensure that when you use Kiln, you get reliable recommendations and easy model switching without the trial-and-error process.

Kiln is a free open tool for finding the best way to build your AI system. You can rapidly compare models, providers, prompts, parameters and even fine-tunes to get the optimal system for your use case — all backed by the extensive testing described above.

To get started, check out the tool or our guides:

* [Kiln AI on Github - over 3900 stars](https://getkiln.ai/)
* [Quickstart Guide](https://docs.getkiln.ai/docs/quickstart)
* [Kiln Discord](https://getkiln.ai/discord)
* [Blog post with more details on our LLM testing (more detailed version of above)](https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time)

I'm happy to answer questions if anyone wants to dive deeper on specific aspects!",10,0.78,https://www.reddit.com/r/datascience/comments/1m6h3f0/i_wrote_2000_llm_test_cases_so_you_dont_have_to/,False,True,False
1m5xn63,recruitingfornow2025,1753138274.0,17,/r/datascience/comments/1m5xn63/looking_for_mmm_marketing_data_science_specialist/,datascience,Looking for MMM / Marketing Data Science specialist,"Hi All,

Hope this is okay to post in this sub.

I am looking to hire for a role here in the DFW metro area and looking for a hard to find specialty of media mix marketing. Willing to train recent graduates with the right statistical and academic background. Currently hybrid 3 days a week in office. Compensation depends on skill set and experience, but can be between $95k-150k.

Please DM for more details and to send resumes.",24,0.78,https://www.reddit.com/r/datascience/comments/1m5xn63/looking_for_mmm_marketing_data_science_specialist/,False,True,False
1m5odiz,sideshowbob01,1753117109.0,18,/r/datascience/comments/1m5odiz/data_science_msc_1_year_full_time_or_2_year_part/,datascience,Data Science MSc 1 year Full time or 2 year Part time?,"Hi, I'm funding my own MSc in Applied Data Science (intended for non computer/maths background)

I have a 6 year healthcare background (Nuclear medicine and CT).

I have taken python and SQL introduction courses to build a foundation.

My question is:

Would a 1 year MSc be  intensive learning for 1 year with dissertation and realistically result in a 18month study?

Does a 2 year MSc offer more room, resulting in a realistic 24 month timeline, with some room for job ""volunteering"" to get some experience?

I have completed a 3 year MSc before and can't comprehend how intense a 1 year MSc would be.

Thanks!",12,0.87,https://www.reddit.com/r/datascience/comments/1m5odiz/data_science_msc_1_year_full_time_or_2_year_part/,False,True,False
1m5nkwe,ElectrikMetriks,1753115346.0,40,/r/datascience/comments/1m5nkwe/wouldnt_be_the_first_time_ive_seen_an_entire_org/,datascience,Wouldn't be the first time I've seen an entire org propped up by a 80MB Excel file,"Oh yeah, I started a meme sub r/AnalyticsMemes if anyone wants every day to be meme Monday",458,0.91,https://i.redd.it/19xz40qp79ef1.png,False,False,False
1m5m5pn,Disastrous_Classic96,1753112152.0,7,/r/datascience/comments/1m5m5pn/maintenance_of_clustered_data_over_time/,datascience,Maintenance of clustered data over time,"With LLM-generated data, what are the best practices for handling downstream maintenance of clustered data?

E.g. for conversation transcripts, we extract things like the topic. As the extracted strings are non-deterministic, they will need clustering prior to being queried by dashboards.

What are people doing for their daily/hourly ETLs? Are you similarity-matching new data points to existing clusters, and regularly assessing cluster drift/bloat? How are you handling historic assignments when you determine clusters have drifted and need re-running?

Any guides/books to help appreciated!",12,0.94,https://www.reddit.com/r/datascience/comments/1m5m5pn/maintenance_of_clustered_data_over_time/,False,True,False
1m5i5dj,Key-Network-9447,1753102516.0,2,/r/datascience/comments/1m5i5dj/data_snooping_resources/,datascience,Data Snooping Resources,"Simple question: Do you guys have any resources/papers about data snooping and how to limits its influence when making predictive models? I understand to maintain a testing dataset, but I am hoping someone knows any good high-level introductions to the topic that is not overly technical. Something like this, but about data snooping specifically, is what I am hoping to find: https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/ES13-00160.1",9,0.85,https://www.reddit.com/r/datascience/comments/1m5i5dj/data_snooping_resources/,False,True,False
1m58yyn,AutoModerator,1753070491.0,39,/r/datascience/comments/1m58yyn/weekly_entering_transitioning_thread_21_jul_2025/,datascience,"Weekly Entering & Transitioning - Thread 21 Jul, 2025 - 28 Jul, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",7,0.89,https://www.reddit.com/r/datascience/comments/1m58yyn/weekly_entering_transitioning_thread_21_jul_2025/,False,True,False
1m4s0vw,chrisgarzon19,1753026070.0,5,/r/datascience/comments/1m4s0vw/ai_in_data_engineering/,datascience,AI In Data Engineering,,0,0.21,/r/dataengineering/comments/1m4rxwf/ai_in_data_engineering/,False,False,False
1m49rai,Proof_Wrap_2150,1752966486.0,4,/r/datascience/comments/1m49rai/how_would_you_structure_a_project_data_frame_to/,datascience,How would you structure a project (data frame) to scrape and track listing changes over time?,"

I’m working on a project where I want to scrape data daily (e.g., real estate listings from a site like RentFaster or Zillow) and track how each listing changes over time. I want to be able to answer questions like:

When did a listing first appear?
How long did it stay up?
What changed (e.g., price, description, status)?
What’s new today vs yesterday?

My rough mental model is:
1. Scrape today’s data into a CSV or database.
2. Compare with previous days to find new/removed/updated listings.
3. Over time, build a longitudinal dataset with per-listing history (kind of like slow-changing dimensions in data warehousing).

I’m curious how others would structure this kind of project:

How would you handle ID tracking if listings don’t always have persistent IDs?
Would you use a single master table with change logs? Or snapshot tables per day?
How would you set up comparisons (diffing rows, hashing)?
Any Python or DB tools you’d recommend for managing this type of historical tracking?

I’m open to best practices, war stories, or just seeing how others have solved this kind of problem. Thanks!
",5,0.78,https://www.reddit.com/r/datascience/comments/1m49rai/how_would_you_structure_a_project_data_frame_to/,False,True,False
1m45pmq,Entire_Island8561,1752955674.0,9,/r/datascience/comments/1m45pmq/generating_random_noise_for_media_data/,datascience,Generating random noise for media data,"Hey everyone - I work on an ML team in the industry, and I’m currently building a predictive model to catch signals in live media data to sense when potential viral moments or crises are happening for brands. We have live media trackers at my company that capture all articles, including their sentiment (positive, negative, neutral). 

I currently am using ARIMA to predict out a certain amount of time steps, then using an LSTM to determine whether the volume of articles is anomalous given historical data trends. 

However, the nature of media is there’s so much randomness, so just taking the ARIMA projection is not enough. Because of that, I’m using Monte Carlo simulation to run an LSTM on a bunch of different forecasts that incorporate an added noise signal for each simulation. Then, that forces a probability of how likely it is that a crisis/viral moment will happen.

I’ve been experimenting with a bunch of methods on how to generate a random noise signal, and while I’m close to getting something, I still feel like I’m missing a method that’s concrete and backed by research/methodology. 

Does anyone know of approaches on how to effectively generate random noise signals for PR data? Or know of any articles on this topic?

Thank you!
",10,0.86,https://www.reddit.com/r/datascience/comments/1m45pmq/generating_random_noise_for_media_data/,False,True,False
1m3gy6m,ergodym,1752880351.0,37,/r/datascience/comments/1m3gy6m/are_headhunters_still_a_thing_in_2025/,datascience,Are headhunters still a thing in 2025?,"Curious what the current consensus is on headhunters these days. A few years ago they seemed to be everywhere, both big-name firms like Michael Page and boutique ones, but lately I don’t hear much about them.

Do companies still rely on them or have internal recruiting teams and LinkedIn taken over completely?",58,0.93,https://www.reddit.com/r/datascience/comments/1m3gy6m/are_headhunters_still_a_thing_in_2025/,False,True,False
1m10uku,OverratedDataScience,1752632348.0,57,/r/datascience/comments/1m10uku/what_question_from_recruiters_do_you_absolutely/,datascience,What question from recruiters do you absolutely hate to answer? How do you answer it elegantly?,"Pretty much the title. Recruiters are not technically adepts in most of the cases. They go about asking some questions which is routine for them but hardly make sense in the real world. Not trying to be idealistic but, which questions do you hate the most? How would you answer them in a polite way?",61,0.95,https://www.reddit.com/r/datascience/comments/1m10uku/what_question_from_recruiters_do_you_absolutely/,False,True,False
1m0wx9l,KyronAWF,1752621322.0,74,/r/datascience/comments/1m0wx9l/hoping_for_a_review/,datascience,Hoping for a review.,"I want to clarify the reason I'm not using the main thread is because I'm posting an image, which can't be used for replies. I've been searching for a while without as much as a call back. I've been a data scientist for a while now and I'm not sure if it's the market or if there's something glaringly bad with my resume. Thanks for your help.",31,0.63,https://i.redd.it/9gcv99xve4df1.png,False,False,False
1m0n56g,SharePlayful1851,1752598559.0,0,/r/datascience/comments/1m0n56g/harnessing_the_universal_geometry_of_embeddings/,datascience,"""Harnessing the Universal Geometry of Embeddings"" - Breakthroughs and Security Implications",,3,0.72,/r/AI_Community_Gurgaon/comments/1m0n4gn/harnessing_the_universal_geometry_of_embeddings/,False,False,False
1m0dxsm,Dangerous_Media_2218,1752574364.0,12,/r/datascience/comments/1m0dxsm/how_does_your_organization_label_data/,datascience,How does your organization label data?,"I'm curious to hear how your organization labels data for use in modeling. We use a combination of SMEs who label data, simple rules that flag cases (it's rare that we can use these because they're generally no unambiguous), and an ML model to find more labels. I ask because my organization doesn't think it's valuable to have SMEs labeling data. In my domain area (fraud), we need SMEs to be labeling data because fraud evolves over time, and we need to identify the evoluation. Also, identifying fraud in the data isn't cut and dry. ",7,0.89,https://www.reddit.com/r/datascience/comments/1m0dxsm/how_does_your_organization_label_data/,False,True,False
1m0b9f9,ChubbyFruit,1752563915.0,104,/r/datascience/comments/1m0b9f9/is_it_normal_to_be_scared_for_the_future_finding/,datascience,Is it normal to be scared for the future finding a job,"I am a rising senior at a large state school studying data science. I am currently working an internship as a software engineer for the summer. And I get my tickets done for the most part albeit with some help from ai. But deep down I feel a pit in my stomach that I won’t be able to end up employed after all of this.

I plan to go for a masters in applied statistics or data science after my bachelors. Thought I definitely don’t have great math grades from my first few semesters of college. But after those semesters all my upper division math/stats/cs/data science courses have been A’s and B’s. And I feel like ik enough python, R, and SAS to work through and build models for most problems I run into, as well as tableau, sql and alteryx. But I can’t shake the feeling that it won’t be enough.

Also that my rough math grades in my first few semesters will hold me back from getting into a masters programs. I have tried to supplement this by doing physics and applied math research. But I’m just not sure I’m doing enough and I’m scared for like after I finish my education.

Im just venting here but I’m hoping there r others in this sub who have been in similar positions and gotten employed. Or r currently in my same shoes I just need to hear from other people that it’s not as hopeless as it feels.

I just want to get a job as a data analyst, scientist, or statistician working on interesting problems and have a decent career.",240,0.93,https://www.reddit.com/r/datascience/comments/1m0b9f9/is_it_normal_to_be_scared_for_the_future_finding/,False,True,False
1m07l5m,m2rik,1752551137.0,2,/r/datascience/comments/1m07l5m/need_mentorship_on_climbing_the_ladder_or/,datascience,Need mentorship on climbing the ladder or transitioning,,0,0.25,/r/analytics/comments/1m07jow/need_mentorship_on_climbing_the_ladder_or/,False,False,False
1lzx0la,ElectrikMetriks,1752523627.0,14,/r/datascience/comments/1lzx0la/i_have_people_skills_i_am_good_at_dealing_with/,datascience,I have people skills... I am good at dealing with people. Can't you understand that? What the hell is wrong with you people?,,312,0.94,https://i.redd.it/dwghyhz8cwcf1.png,False,False,False
1lzo89g,Kati1998,1752504147.0,22,/r/datascience/comments/1lzo89g/do_employers_see_volunteer_experience_as_real/,datascience,Do employers see volunteer experience as “real world experience”?,,11,0.93,/r/analytics/comments/1lzo7lq/do_employers_see_volunteer_experience_as_real/,False,False,False
1lzlrlu,rsesrsfh,1752498027.0,3,/r/datascience/comments/1lzlrlu/finetuning_for_tabular_foundation_models_tabpfn/,datascience,Fine-tuning for tabular foundation models (TabPFN),"Hi everyone - wanted to share that you can now fine-tune tabular foundation models as well, specifically TabPFN! With the latest 2.1 package release, you can now build your own fine-tuned models.

A community member put together a practical walkthrough!

How to Fine-Tune TabPFN on Your Data: [https://medium.com/@iivalchev/how-to-fine-tune-tabpfn-on-your-data-a831b328b6c0](https://medium.com/@iivalchev/how-to-fine-tune-tabpfn-on-your-data-a831b328b6c0)

The tutorial covers:

* Running TabPFN in batched mode
* Handling preprocessing and inference-time transformations
* Fine-tuning the transformer backbone on your dataset

If you're working with highly domain specific data and looking to boost performance, this is a great place to start.

You can also check out the example files directly at these links:

🧪 [Fine-tune classifier](https://github.com/PriorLabs/TabPFN/blob/main/examples/finetune_classifier.py)

📈 [Fine-tune regressor](https://github.com/PriorLabs/TabPFN/blob/main/examples/finetune_regressor.py)

Would love to hear how it goes if you try it!

There’s also a community Discord where folks are sharing experiments and helping each other out - worth checking out if you're playing around with TabPFN [https://discord.com/invite/VJRuU3bSxt](https://discord.com/invite/VJRuU3bSxt)",19,0.95,https://www.reddit.com/r/datascience/comments/1lzlrlu/finetuning_for_tabular_foundation_models_tabpfn/,False,True,False
1lzkso0,multicm,1752495296.0,5,/r/datascience/comments/1lzkso0/site_selection_model_subjective_feature/,datascience,Site Selection Model - Subjective Feature,"I have been working on a site selection model, and the one I created is performing quite well in out of sample testing. I was also able to reduce the model down to just 5 features. But, one of those features is a ""Visibility Score"" (how visible the building is from the road). I had 3 people independently score all of our existing sites and I averaged their scores, and this has proven to work well so far. But if we actually put the model into production, I am concerned about standardized those scores. The model predictiction can vary by 18% just from a visibility score change from 3.5 to 4.0 so the model is heavily dependent on that subjective score.

Any tips?",7,1.0,https://www.reddit.com/r/datascience/comments/1lzkso0/site_selection_model_subjective_feature/,False,True,False
1lzgfhq,JayBong2k,1752479427.0,135,/r/datascience/comments/1lzgfhq/i_suck_at_these_interviews/,datascience,I suck at these interviews.,"I'm looking for a job again and while I have had quite a bit of hands-on practical work that has a lot of business impacts - revenue generation, cost reductions, increasing productivity etc 

But I keep failing at ""Tell the assumptions of Linear regression"" or ""what is the formula for Sensitivity"".

While I'm aware of these concepts, and these things are tested out in model development phase, I never thought I had to mug these stuff up. 

The interviews are so random - one could be hands on coding (love these), some would be a mix of theory, maths etc, and some might as well be in Greek and Latin..

Please give some advice to  4 YOE DS should be doing. The ""syllabus"" is entirely too vast.🥲

Edit:
Wow, ok i didn't expect this to blow up. I did read through all the comments. This has been definitely enlightening for me.

Yes, i should have prepared better, brushed up on the fundamentals. Guess I'll have to go the notes/flashcards way. 
",531,0.97,https://www.reddit.com/r/datascience/comments/1lzgfhq/i_suck_at_these_interviews/,False,True,False
1lzcn4y,AutoModerator,1752465686.0,40,/r/datascience/comments/1lzcn4y/weekly_entering_transitioning_thread_14_jul_2025/,datascience,"Weekly Entering & Transitioning - Thread 14 Jul, 2025 - 21 Jul, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",8,0.9,https://www.reddit.com/r/datascience/comments/1lzcn4y/weekly_entering_transitioning_thread_14_jul_2025/,False,True,False
1lywh35,harsh82000,1752422715.0,37,/r/datascience/comments/1lywh35/how_much_dsa_for_faang/,datascience,How much DSA for FAANG+ ?,"Hello all, I am going to be graduating in 6 months and have been practicing Leetcode as I believe this to be my weakest point. I have solved 250 LC with 130 Easy and 120 Hard, covering concepts like arrays, hashing, binary trees, SQL, linked list, two pointers, stack, sliding windows majorly. Could anyone guide me on how I can maximise the time I have on hand to prepare better for technical interviews? I have good internship and research experience so I am not that worried about future rounds, but timed coding questions have always been brutal for me. Any advice is appreciated.",70,0.81,https://www.reddit.com/r/datascience/comments/1lywh35/how_much_dsa_for_faang/,False,True,False
1lyo2ac,nkafr,1752396348.0,14,/r/datascience/comments/1lyo2ac/toto_a_foundation_timeseries_model_optimized_for/,datascience,Toto: A Foundation Time-Series Model Optimized for Observability Data,"Datadog open-sourced *Toto* (Time Series Optimized Transformer for Observability), a model purpose-built for observability data.

Toto is currently the most extensively pretrained time-series foundation model: The pretraining corpus contains 2.36 trillion tokens, with \~70% coming from Datadog’s private telemetry dataset.

Also, Toto currently ranks 2nd in the GIFT-Eval Benchmark.

You can find an analysis of the model [here](https://aihorizonforecast.substack.com/p/toto-a-foundation-time-series-model).",56,0.97,https://www.reddit.com/r/datascience/comments/1lyo2ac/toto_a_foundation_timeseries_model_optimized_for/,False,True,False
1lyciw1,juggerjaxen,1752358188.0,10,/r/datascience/comments/1lyciw1/the_right_questions_to_find_clusters_tangles/,datascience,The right questions to find clusters (tangles),"**Hey everyone,**

I’m currently working on my bachelor’s thesis and I’m hitting a creative block on a central part – maybe you have some ideas or impulses for me.

My dataset consists of 100,000 cleaned job postings from Kaggle (title + description). The goal of my thesis is to use a method called **Tangles** (probably no one knows it, it’s a rather specific approach from my studies) to find interesting clusters in this data – similar to embedding-based clustering methods, but with the key difference that it requires **interpretable, binary decisions**. Sounds theoretical, but it’s actually pretty cool:

You ask the dataset **yes/no questions** (e.g., *“Does the job require a lot of travel?”*), and based on the answer patterns, a kind of profile emerges – and from these profiles, groups that belong together can be formed.

The goal is to group jobs that don’t obviously belong together at first glance, but do share certain underlying similarities (e.g., requirements, tasks) that cause them to respond similarly to the questions.

**One example:**

Questions like:

* Does the job require a lot of travel?
* Do you need a driver’s license?
* Do you have to be physically fit?



=> could group *Sales Managers* and *Truck Drivers* together – even though those jobs seem very different at first. These kinds of connections are what I find exciting.

What I’m **not** looking for are questions like:

* Is this a data science job?
* Do you need to know how to code?
* Is it IT-related?

To me, those are more like categories or classifications that make the clustering too obvious – they just confirm what you already know. I’m more interested in **surprising, layered similarities**.

So here’s my question for you:

Do you have any interesting **yes/no questions** from your daily work or knowledge that could be applied to any kind of job posting – and that might result in **interesting, possibly unexpected groupings**?

Whether you work in trades, healthcare, IT, management, or research – **every perspective helps!**

In the end, I need at least 40 such questions (the more, the better), but right now I’m really struggling to come up with good ones. Even GPT & co. haven’t been much help – they usually just spit out generic stuff.

Even **one** good question from you would be incredibly helpful. 🙏 OR advice on how to find these questions/if my idea is right or not, would help.

Thanks in advance for thinking along!",1,0.53,https://www.reddit.com/r/datascience/comments/1lyciw1/the_right_questions_to_find_clusters_tangles/,False,True,False
1ly409f,Proof_Wrap_2150,1752336181.0,8,/r/datascience/comments/1ly409f/how_have_you_supported_ds_fundamentals_creative/,datascience,"How have you supported DS fundamentals, creative thinking or curiosity in your baby/toddler using what you know as a technical or analytical thinker?","Anything you built, played, repeated, or tracked?",0,0.27,https://www.reddit.com/r/datascience/comments/1ly409f/how_have_you_supported_ds_fundamentals_creative/,False,True,False
1ly06nw,Grapphie,1752326054.0,40,/r/datascience/comments/1ly06nw/how_do_you_efficiently_traverse_hundreds_of/,datascience,How do you efficiently traverse hundreds of features in the dataset?,"Currently, working on a fintech classification algorithm, with close to a thousand features which is very tiresome. I'm not a domain expert, so creating sensible hypotesis is difficult. How do you tackle EDA and forming reasonable hypotesis in these cases? Even with proper documentation it's not a trivial task to think of all interesting relationships that might be worth looking at. What I've been looking so far to make is:

1) Baseline models and feature relevance assessment with in ensemble tree and via SHAP values  
2) Traversing features manually and check relationships that ""make sense"" for me",92,0.97,https://www.reddit.com/r/datascience/comments/1ly06nw/how_do_you_efficiently_traverse_hundreds_of/,False,True,False
1lxb0bn,Substantial_Tank_129,1752250404.0,68,/r/datascience/comments/1lxb0bn/doordash_phone_screen_reject_despite_good/,datascience,Doordash phone screen reject despite good in-interview feedback. What are they looking for?,"Had a phone screen with DoorDash recently for a DS Analytics role. First round was a product case study — the interviewer was super nice, gave good feedback throughout, and even ended with “Great job on this round,” so I felt pretty good about it.

Second round was SQL with 4 questions. Honestly, the first one threw me off — it was more convoluted than I expected, so I struggled a bit but managed to get through it. The 2nd and 3rd were much easier and I finished those without issues. The 4th was a bonus question where I had to explain a SQL query — took me a moment, but I eventually explained what it was doing.

Got a rejection email the next day. I thought it went decently overall, so I’m a bit confused. Any thoughts on what might’ve gone wrong or what I could do better next time",111,0.93,https://www.reddit.com/r/datascience/comments/1lxb0bn/doordash_phone_screen_reject_despite_good/,False,True,False
1lvsh3e,idontknowotimdoing,1752090719.0,100,/r/datascience/comments/1lvsh3e/data_science_metaphors/,datascience,Data science metaphors?,"Hello everyone :)

Serious question: Does anyone have any data science related metaphors/similes/analogies that you use regularly at work? 

(I want to sound smart.)

Thanks!",121,0.93,https://www.reddit.com/r/datascience/comments/1lvsh3e/data_science_metaphors/,False,True,False
1lvoz88,NervousVictory1792,1752082532.0,31,/r/datascience/comments/1lvoz88/quarterly_to_monthly_data_conversion/,datascience,Quarterly to Monthly Data Conversion,"As the title suggests. I am trying to convert average wage data, from quarterly to monthly. I need to perform forecasting on that. What is the best ways to do that?? . I don’t want to go for a naive method and just divide by 3 as I will loose any trends or patterns. I have come across something called disproportionate aggregation but having a tough time grasping it.",11,0.71,https://www.reddit.com/r/datascience/comments/1lvoz88/quarterly_to_monthly_data_conversion/,False,True,False
1lvnda9,Technical-Love-8479,1752078809.0,8,/r/datascience/comments/1lvnda9/reachymini_huggingface_launched_opensourced_robot/,datascience,"Reachy-Mini: Huggingface launched open-sourced robot that supports vision, text and speech","Huggingface just released an open-sourced robot named Reachy-Mini, which supports all Huggingface open-sourced AI models, be it text or speech or vision and is quite cheap. Check more details here : https://youtu.be/i6uLnSeuFMo?si=Wb6TJNjM0dinkyy5",14,0.89,https://www.reddit.com/r/datascience/comments/1lvnda9/reachymini_huggingface_launched_opensourced_robot/,False,True,False
1lvn71u,Professional_Ball_58,1752078393.0,46,/r/datascience/comments/1lvn71u/how_do_you_guys_measure_ai_impact/,datascience,How do you guys measure AI impact,"Im sure a lot of companies are rolling out AI products to help their business. 

Im curious how do people typically try to measure these AI products impacts. I guess it really depends on the domain but can we isolate and see if any uplift in the KPI is attributable to AI?

Is AB testing always to gold standard? Use Quasi experimental methods? 

",26,0.77,https://www.reddit.com/r/datascience/comments/1lvn71u/how_do_you_guys_measure_ai_impact/,False,True,False
1lvmphl,Proof_Wrap_2150,1752077225.0,39,/r/datascience/comments/1lvmphl/all_of_my_data_comes_from_spreadsheets_as_i/,datascience,"All of my data comes from spreadsheets. As I receive more over time, what’s the best way to manage and access multiple files efficiently? Ideally in a way that scales and still lets me work interactively with the data?","I’m working on a project where all incoming data is provided via spreadsheets (Excel/CSV). The number of files is growing, and I need to manage them in a structured way that allows for:

1. Easy access to different uploads over time
2. Avoiding duplication or version confusion
3. Interactive analysis (e.g., via Jupyter notebooks or a lightweight dashboard)

I’m currently loading files manually, but I want a better system. Whether that means a file management structure, metadata tagging, or loading/parsing automation. Eventually I’d like to scale this up to support analysis across many uploads or clients.

What are good patterns, tools, or Python-based workflows to support this?",70,0.98,https://www.reddit.com/r/datascience/comments/1lvmphl/all_of_my_data_comes_from_spreadsheets_as_i/,False,True,False
1lvl0wp,SummerElectrical3642,1752073283.0,12,/r/datascience/comments/1lvl0wp/open_source_or_not/,datascience,Open source or not?,"Hi all,  
I am building an AI agent, similar to Github copilot / Cursor but very specialized on data science / ML. It is integrated in VSCode as an extension.  
Here is a few examples of use cases:  
\- Combine different data sources, clean and preprocess for ML pipeline.  
\- Refactor R&D notebooks into ready for production project: Docker, package, tests, documentation.

We are approaching an MVP in the next few weeks and I am hesitating between 2 business models:  
1- Closed source, similar to cursor, with fixed price subscription with limit by request.  
2- Open source, pay per token. User can plug their own API or use our backend which offers all frontier models. Charge a topup % on top of token consumption (similar to Cline).

The question is also whether the data science community would contribute to a vscode extension in React, Typescript.

What do you think make senses as a data scientist / ML engineer?",0,0.33,https://www.reddit.com/r/datascience/comments/1lvl0wp/open_source_or_not/,False,True,False
1lux7bt,tits_mcgee_92,1752001399.0,97,/r/datascience/comments/1lux7bt/saved_100k_per_year_by_explaining_how_aillm_work/,datascience,Saved $100k per year by explaining how AI/LLM work.,"I work in a data science field, and I bring this up because I think it's data science related.

We have an internal website that is very bare bones. It's made to be simplistic, because it's the reference document for our end-users (1000 of them) use.

Executives heard about a software that would be completely AI driven, build detailed statistical insights, and change the world as they know it.

I had a demo with the company and they explained its RAG capabilities, but mentioned it doesn't really ""learn"" like the assumption AI does. Our repo is so small and not at all needed for AI. We have used a fuzzy search that has worked for the past three years. Additionally, I have already built out dashboards that retrieve all the information executives have asked for via API (who's viewing pages, what are they searching, etc.)

I showed the c-suite executives our current dashboards in Tableau, and how the actual search works. I also explained what RAG is, and how AI/LLMs work at a high level. I explained to them that AI is a fantastic tool, but I'm not sure if we should be spending 100k a year on it. They also asked if I have built any predictive models. I don't think they quite understood what that was as well, because we don't have the amount of data or need to predict anything.

Needless to say, they decided it was best not to move forward ""for now"". I am shocked, but also not, that executives want to change the structure of how my team and end-users digest information just because they heard ""AI is awesome!"" They had zero idea how anything works in our shop.

Oh yeah, our company has already laid of 250 people this year due to ""financial turbulence"", and now they're wanting to spend 100k on this?!

It just goes to show you how deep the AI train runs. Did I handle this correctly and can I put this on my resume? LOL",1178,0.96,https://www.reddit.com/r/datascience/comments/1lux7bt/saved_100k_per_year_by_explaining_how_aillm_work/,False,True,False
1lum3ih,FinalRide7181,1751974053.0,12,/r/datascience/comments/1lum3ih/path_to_product_management/,datascience,Path to product management,"I’m a student interested in working as a product manager in tech. 

I know it’s tough to land a first role directly in PM, so I’m considering alternative paths that could lead there.

My question is: how common is the transition from data scientist/product data scientist to product manager? Is it a viable path?

Also would it make more sense to go down the software engineering route instead (even though I’m not particularly passionate about it) if it makes the transition to PM easier?",7,0.67,https://www.reddit.com/r/datascience/comments/1lum3ih/path_to_product_management/,False,True,False
1lu7gqq,EducationalUse9983,1751925791.0,67,/r/datascience/comments/1lu7gqq/how_to_deal_with_time_series_unbalanced_situations/,datascience,How to deal with time series unbalanced situations?,"**Hi everyone,**

I’m working on a challenge to **predict the probability of a product becoming unavailable the next day**.

The dataset contains one row per product per day, with a binary target (`failure` or not) and 10 additional features. There are over 1 million rows without failure, and only 100 with failure — so it's a highly imbalanced dataset.

Here are some key points I’m considering:

1. **The target should reflect the next day**, not the current one. For example, if product X has data from day 1 to day 10, each row should indicate whether a failure will happen on the following day. Day 10 is used only to label day 9 and is not used as input for prediction.
2. **The features are on different scales**, so I’ll need to apply normalization or standardization depending on the model I choose (e.g., for Logistic Regression or KNN).
3. **There are no missing values**, so I won’t need to worry about imputation.
4. **To avoid data leakage**, I’ll split the data by product, making sure that each product's full time series appears entirely in either the training or test set — never both. For example, if product X has data from day 1 to day 9, those rows must all go to either train **or** test.
5. Since the output should be a **probability**, I’m planning to use models like Logistic Regression, Random Forest, XGBoost, Naive Bayes, or KNN.
6. Due to the strong class imbalance, my **main evaluation metric will be ROC AUC**, since it handles imbalanced datasets well.
7. Would it make sense to include calendar-based features, like the day of the week, weekend indicators, or holidays?
8. How useful would it be to add rolling window statistics (e.g., 3-day averages or standard deviations) to capture recent trends in the attributes?
9. Any best practices for flagging anomalies, such as sudden spikes in certain attributes or values above a specific percentile (like the 90th)?

**My questions:**  
Does this approach make sense?  
I’m not entirely confident about some of these steps, so I’d really appreciate feedback from more experienced data scientists!",56,0.95,https://www.reddit.com/r/datascience/comments/1lu7gqq/how_to_deal_with_time_series_unbalanced_situations/,False,True,False
1lu7fxv,GussieWussie,1751925736.0,3,/r/datascience/comments/1lu7fxv/python_package_for_pickupadvanced_booking_models/,datascience,Python package for pickup/advanced booking models for forecasting?,Recently discovered pickup models that use reservation data to generate forecasts (see [https://www.scitepress.org/papers/2016/56319/56319.pdf](https://www.scitepress.org/papers/2016/56319/56319.pdf) ) Seems used often in the hotel and airline industry. Is there a python package for this? Maybe it goes by a different name but I'm not seeing anything,10,0.92,https://www.reddit.com/r/datascience/comments/1lu7fxv/python_package_for_pickupadvanced_booking_models/,False,True,False
1lu1cve,ElectrikMetriks,1751911575.0,0,/r/datascience/comments/1lu1cve/i_dont_drink_but_im_still_tired_because_my_dogs/,datascience,"I don't drink, but I'm still tired because my dogs hate fireworks.  Did everyone in the US take a long weekend at least?",,0,0.34,https://i.redd.it/5cprt2scshbf1.png,False,False,False
1ltkjwx,AutoModerator,1751860923.0,57,/r/datascience/comments/1ltkjwx/weekly_entering_transitioning_thread_07_jul_2025/,datascience,"Weekly Entering & Transitioning - Thread 07 Jul, 2025 - 14 Jul, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",17,1.0,https://www.reddit.com/r/datascience/comments/1ltkjwx/weekly_entering_transitioning_thread_07_jul_2025/,False,True,False
1lt464v,Technical-Love-8479,1751816242.0,18,/r/datascience/comments/1lt464v/with_generative_ai_looking_so_ominous_would_there/,datascience,"With Generative AI looking so ominous, would there be any further research in any other domains like Computer Vision or NLP or Graph Analytics ever?","So as the title suggest, last few years have been just Generative AI all over the place. Every new research is somehow focussed towards it. So does this mean other fields stands still ? Or eventually everything will merge into GenAI somehow? What's your thoughts ",0,0.43,https://www.reddit.com/r/datascience/comments/1lt464v/with_generative_ai_looking_so_ominous_would_there/,False,True,False
1lss1c5,fenrirbatdorf,1751774189.0,30,/r/datascience/comments/1lss1c5/reliable_ds_adjacent_fields_hiring_for_bachelors/,datascience,Reliable DS Adjacent Fields Hiring for Bachelor's Degree?,"Hello all. To try and condense a lot of context for this question, I am an adult who went back to school to complete my bachelor's, in order to support myself and my partner on one income. Admittedly, I did this because I heard how good data science was as a field, but it seems I jumped in at the wrong time. 

Consequently, now that I am one year out from graduating with my bachelor's, I am starting to think about what fields would be best to apply in, beyond simply ""data science"" and ""data analysis."" Any leads on fields that are reliably hiring that are similar to data science but not exact? I am really open to anything that would pay the bills for two people.",37,0.87,https://www.reddit.com/r/datascience/comments/1lss1c5/reliable_ds_adjacent_fields_hiring_for_bachelors/,False,True,False
1lsk0sd,Proof_Wrap_2150,1751749301.0,4,/r/datascience/comments/1lsk0sd/whats_the_best_way_to_automate_pulling_content/,datascience,What’s the best way to automate pulling content performance metrics from LinkedIn beyond just downloading spreadsheets?,"I’ve been stuck manually exporting post data from the LinkedIn analytics dashboard for months. Automating via API sounds ideal, but this is uncharted territory!",0,0.14,https://www.reddit.com/r/datascience/comments/1lsk0sd/whats_the_best_way_to_automate_pulling_content/,False,True,False
1lsjfj4,mlbatman,1751747714.0,69,/r/datascience/comments/1lsjfj4/longtimers_at_companies_whats_your_secret/,datascience,Long-timers at companies — what’s your secret?,"Hi everyone,

I’ve been a job hopper throughout my career—never stayed at one place for more than 1-2 years, usually for various reasons.

Now, I’m entering a phase where I want to get more settled. I’m about to start a new job and would love to hear from those who have successfully stayed long-term at a job.

What’s the secret sauce besides just hard work and taking ownership? Lay your knowledge on me—your hacks, tips, rituals.

Thanks in advance.",144,0.9,https://www.reddit.com/r/datascience/comments/1lsjfj4/longtimers_at_companies_whats_your_secret/,False,True,False
1lsfyeo,Daniel-Warfield,1751738477.0,57,/r/datascience/comments/1lsfyeo/a_brief_guide_to_uv/,datascience,A Brief Guide to UV,"Python has been largely devoid of easy to use environment and package management tooling, with various developers employing their own cocktail of `pip`, `virtualenv`, `poetry`, and `conda` to get the job done. However, it looks like `uv` is rapidly emerging to be a standard in the industry, and I'm super excited about it.

In a nutshell `uv` is like `npm` for Python. It's also written in rust so it's crazy fast.

As new ML approaches and frameworks have emerged around the greater ML space (A2A, MCP, etc) the cumbersome nature of Python environment management has transcended from an annoyance to a major hurdle. This seems to be the major reason `uv` has seen such meteoric adoption, especially in the ML/AI community.

[star history of uv vs poetry vs pip. Of course, github star history isn't necessarily emblematic of adoption.  \<ore importantly, uv is being used all over the shop in high-profile, cutting-edge repos that are governing the way modern software is evolving. Anthropic’s Python repo for MCP uses UV, Google’s Python repo for A2A uses UV, Open-WebUI seems to use UV, and that’s just to name a few.](https://preview.redd.it/b6myln1ve3bf1.png?width=1050&format=png&auto=webp&s=89d275ad7b050bbe8a365dd731e37910182592c4)

I wrote [an article](https://iaee.substack.com/p/uv-intuitively-and-exhaustively-explained) that goes over `uv` in greater depth, and includes some examples of `uv` in action, but I figured a brief pass would make a decent Reddit post.

**Why UV**  
`uv` allows you to manage dependencies and environments with a single tool, allowing you to create isolated python environments for different projects. While there are a few existing tools in Python to do this, there's one critical feature which makes it groundbreaking: *it's easy to use*.

**Installing UV**  
`uv` can be installed via `curl`

    curl -LsSf https://astral.sh/uv/install.sh | sh

or via `pip`

    pipx install uv

the docs have a [more in-depth guide to install](https://iaee.substack.com/p/uv-intuitively-and-exhaustively-explained#:~:text=Check%20out%20the-,uv%20docs,-for%20more%20information).

**Initializing a Project with UV**  
Once you have `uv` installed, you can run

    uv init

This initializes a uv project within your directory. You can think of this as an isolated python environment that's tied to your project.

**Adding Dependencies to your Project**  
You can add dependencies to your project with

    uv add <dependency name>

You can download all the dependencies you might install via `pip`:

    uv add pandas
    uv add scipy
    uv add numpy sklearn matplotlib

And you can install from various other sources, including github repos, local wheel files, etc.

**Running Within an Environment**  
if you have a python script within your environment, you can run it with

    uv run <file name>

this will run the file with the dependencies and python version specified for this particular environment.  This makes it super easy and convenient to bounce around between different projects. Also, if you clone a `uv` managed project, all dependencies will be installed and synchronized before the file is run.

**My Thoughts**  
I didn't realize I've been waiting for this for a long time. I always found off the cuff quick implementation of Python locally to be a pain, and I think I've been using ephemeral environments like Colab as a crutch to get around this issue. I find local development of Python projects to be significantly more enjoyable with `uv` , and thus I'll likely be adopting it as my go to approach when developing in Python locally.",101,0.9,https://www.reddit.com/r/datascience/comments/1lsfyeo/a_brief_guide_to_uv/,False,True,False
1lrojc3,empirical-sadboy,1751650427.0,26,/r/datascience/comments/1lrojc3/how_easy_is_it_to_be_pigeonholed_in_ds/,datascience,How easy is it to be pigeonholed in DS?,"Although in my PhD I used experiments and traditional statistics, my first DS role is entirely focused on NLP. There are no opportunities to use casual inference, time series, or other traditional statistical methods. 

How much will this hurt my ability to apply to roles focused on these kinds of analyses? Basically, I'm wondering if my current role's focus on NLP is going to make it hard for me to get non-NLP data science positions when I'm ready to leave. 

Is it common for data scientists to get stuck in a niche?",37,0.78,https://www.reddit.com/r/datascience/comments/1lrojc3/how_easy_is_it_to_be_pigeonholed_in_ds/,False,True,False
1lrluwg,kmeansneuralnetwork,1751643709.0,20,/r/datascience/comments/1lrluwg/any_good_resources_for_fraud_detection_and_credit/,datascience,Any good resources for fraud detection and credit risk modelling?,"Hello, I am very much interested in using ML/DS in banking domain like fraud detection, loan prediction, credit risk, etc..


I have read this book about fraud detection.
https://fraud-detection-handbook.github.io/fraud-detection-handbook/Foreword.html

Understood everything and it was fun. Now, I am looking for similar resources to work on.

Thank you.",67,0.98,https://www.reddit.com/r/datascience/comments/1lrluwg/any_good_resources_for_fraud_detection_and_credit/,False,True,False
1lrghkc,Unusual-Map6326,1751628684.0,66,/r/datascience/comments/1lrghkc/causes_of_the_bad_market/,datascience,Causes of the 'Bad Market',"I'm just opening the floor to speculation / source dumping but everyone's talking about a suddenly very bad market for DS and DS related fields

  
I live in the north of the UK and it feels impossible to get a job out here. It sounds like its similar in the US. Is this a DS specific issue or are we just feeling what everyone else is feeling? I'm only now just emerging from a post-grad degree and I thought that hearing all these news stories about people illegally gathering and storing data that it was an indicator in how data driven so many decisions are now... which in my mind means that you'd need more DS/ ML engineers to wade through the quagmire and build solutions

  
obviously I'm wrong but why?",101,0.89,https://www.reddit.com/r/datascience/comments/1lrghkc/causes_of_the_bad_market/,False,True,False
1lr8l54,Particular_Reality12,1751599355.0,12,/r/datascience/comments/1lr8l54/i_just_got_linkedin_learning_what_courses_do_you/,datascience,"I just got LinkedIn Learning, what courses do you recommend I take on Data Science?","I’m kinda new to it but dont shy away from giving me the more advanced courses as I’ll be able to learn more

Im going to charge my phone",0,0.43,https://i.redd.it/xdw8w4h60saf1.jpeg,False,False,False
1lqno9m,Illustrious-Pound266,1751543778.0,115,/r/datascience/comments/1lqno9m/people_who_have_been_in_the_field_before_2020_how/,datascience,People who have been in the field before 2020: how do you keep up with the constantly new and changing technologies in ML/AI?,"As someone who genuinely enjoys learning new tech, sometimes I feel it's too much to constantly keep up. I feel like it was only barely a year ago when I first learned RAG and then agents soon after, and now MCP servers.

I have a life outside tech and work and I feel that I'm getting lazier and burnt out in having to keep up. Not to mention only AI-specific tech, but even with adjacent tech like MLFlow, Kubernetes, etc, there seems to be so much that I feel I should be knowing. 

The reason why I asked *before 2020* is because I don't recall AI moving at this fast pace before then. Really feels like only after ChatGPT was released to the masses did the pace really pickup that now AI engineering actually feels quite different to the more classic ML engineering I was doing.",229,0.95,https://www.reddit.com/r/datascience/comments/1lqno9m/people_who_have_been_in_the_field_before_2020_how/,False,True,False
1lqn6pu,qtalen,1751542151.0,3,/r/datascience/comments/1lqn6pu/how_i_use_mlflow_31_to_bring_observability_to/,datascience,How I Use MLflow 3.1 to Bring Observability to Multi-Agent AI Applications,"Hi everyone,

If you've been diving into the world of multi-agent AI applications, you've probably noticed a recurring issue: most tutorials and code examples out there feel like toys. They’re fun to play with, but when it comes to building something reliable and production-ready, they fall short. You run the code, and half the time, the results are unpredictable.

This was exactly the challenge I faced when I started working on enterprise-grade AI applications. I wanted my applications to not only work but also be robust, explainable, and observable. By ""observable,"" I mean being able to monitor what’s happening at every step — the inputs, outputs, errors, and even the thought process of the AI. And ""explainable"" means being able to answer questions like: *Why did the model give this result? What went wrong when it didn’t?*

But here’s the catch: as multi-agent frameworks have become more abstract and convenient to use, they’ve also made it harder to see under the hood. Often, you can’t even tell what prompt was finally sent to the large language model (LLM), let alone why the result wasn’t what you expected.

So, I started looking for tools that could help me monitor and evaluate my AI agents more effectively. That’s when I turned to MLflow. If you’ve worked in machine learning before, you might know MLflow as a model tracking and experimentation tool. But with its latest 3.x release, MLflow has added specialized support for GenAI projects. And trust me, it’s a game-changer.

[MLflow's tracking records. ](https://preview.redd.it/k3i2hbh18naf1.png?width=948&format=png&auto=webp&s=91cd9c33943b0d612fda2e8874b4979c60ce0618)

# Why Observability Matters

Before diving into the details, let’s talk about why this is important. In any AI application, but especially in multi-agent setups, you need three key capabilities:

1. **Observability:** Can you monitor the application in real time? Are there logs or visualizations to see what’s happening at each step?
2. **Explainability:** If something goes wrong, can you figure out why? Can the algorithm explain its decisions?
3. **Traceability:** If results deviate from expectations, can you reproduce the issue and pinpoint its cause?

[Three key metrics for evaluating the stability of enterprise GenAI applications. Image by Author](https://preview.redd.it/azgs0y3j7naf1.png?width=820&format=png&auto=webp&s=9fbf70b52379d8e8e869eab3bb3acc9b9450942f)

Without these, you’re flying blind. And when you’re building enterprise-grade systems where reliability is critical, flying blind isn’t an option.

# How MLflow Helps

MLflow is best known for its model tracking capabilities, but its GenAI features are what really caught my attention. It lets you track everything — from the prompts you send to the LLM to the outputs it generates, even in streaming scenarios where the model responds token by token.

[The Events tab in MLflow interface records every SSE message.](https://preview.redd.it/7mteb23c8naf1.png?width=858&format=png&auto=webp&s=1d0de24b21a58abeca9db0bbc7feeddd106c7fbc)

[MLflow's Autolog can also stitch together streaming messages in the Chat interface.](https://preview.redd.it/h77q9y3h8naf1.png?width=859&format=png&auto=webp&s=b3ab949f1cab02c2d71da952d6b6fc60bb2bac91)

The setup is straightforward. You can annotate your code, use MLflow’s ""autolog"" feature for automatic tracking, or leverage its context managers for more granular control. For example:

* Want to know exactly what prompt was sent to the model? Tracked.
* Want to log the inputs and outputs of every function your agent calls? Done.
* Want to monitor errors or unusual behavior? MLflow makes it easy to capture that too.

[You can view code execution error messages in the Events interface.](https://preview.redd.it/svx0fnpm8naf1.png?width=854&format=png&auto=webp&s=d7edbc819dbbccad8a0e881efce310c4b9553a02)

And the best part? MLflow’s UI makes all this data accessible in a clean, organized way. You can filter, search, and drill down into specific runs or spans (i.e., individual events in your application).

# A Real-World Example

I have a project involving building a workflow using Autogen, a popular multi-agent framework. The system included three agents:

1. A **generator** that creates ideas based on user input.
2. A **reviewer** that evaluates and refines those ideas.
3. A **summarizer** that compiles the final output.

While the framework made it easy to orchestrate these agents, it also abstracted away a lot of the details. At first, everything seemed fine — the agents were producing outputs, and the workflow ran smoothly. But when I looked closer, I realized the summarizer wasn’t getting all the information it needed. The final summaries were vague and uninformative.

With MLflow, I was able to trace the issue step by step. By examining the inputs and outputs at each stage, I discovered that the summarizer wasn’t receiving the generator’s final output. A simple configuration change fixed the problem, but without MLflow, I might never have noticed it.

[I might never have noticed that the agent wasn't passing the right info to the LLM until MLflow helped me out.](https://preview.redd.it/q7giinxu8naf1.png?width=960&format=png&auto=webp&s=05a3e7c983191836e6ceae8a8f689613d5acf77c)

# Why I’m Sharing This

I’m not here to sell you on MLflow — it’s open source, after all. I’m sharing this because I know how frustrating it can be to feel like you’re stumbling around in the dark when things go wrong. Whether you’re debugging a flaky chatbot or trying to optimize a complex workflow, having the right tools can make all the difference.

If you’re working on multi-agent applications and struggling with observability, I’d encourage you to give MLflow a try. It’s not perfect (I had to patch a few bugs in the Autogen integration, for example), but it’s the tool I’ve found for the job so far.",29,0.91,https://www.reddit.com/r/datascience/comments/1lqn6pu/how_i_use_mlflow_31_to_bring_observability_to/,False,True,False
1lq8shf,BirdLadyTraveller,1751493974.0,74,/r/datascience/comments/1lq8shf/how_can_i_get_international_remote_positions/,datascience,How can I get international remote positions?,"Hello folks! I am a data scientist in Brazil and in general, I have a good resume. I have experience working in big techs, startup,  consulting and a MsC degree. 

I get Brazilian interviews easily but not abroad, even if I have a LinkedIn profile in English. How can I get considered for a remote position from US or Europe so I can keep working from my country?",93,0.76,https://www.reddit.com/r/datascience/comments/1lq8shf/how_can_i_get_international_remote_positions/,False,True,False
1lq79vo,Daniel-Warfield,1751490163.0,6,/r/datascience/comments/1lq79vo/a_breakdown_of_a2a_mcp_and_agentic/,datascience,"A Breakdown of A2A, MCP, and Agentic Interoperability","MCP and A2A are both emerging standards in AI. In this post I want to cover what they're both useful for (based on my experience) from a practical level, and some of my thoughts about where the two protocols will go moving forward. Both of these protocols are still actively evolving, and I think there's room for interpretation around where they should go moving forward. As a result, I don't think there is a single, correct interpretation of A2A and MCP. These are my thoughts.

**What is MCP?**  
From it's highest level, MCP (model context protocol) is a standard way to expose tools to AI agents. More specifically, it's a standard way to communicate tools to a client which is managing the execution of an LLM within a logical loop. There's not really one, single, god almighty way to feed tools into an LLM, but MCP defines a standard on how tools are defined to make that process more streamlined.

The whole idea of MCP is derivative from LSP (language server protocol), which emerged due to a practical need from programming language and code editor developers. If you're working on something like VS Code, for instance, you don't want to implement hooks for Rust, Python, Java, etc. If you make a new programming language, you don't want to integrate it into vscode, sublime, jetbrains, etc.  The problem of ""connect programming language to text editor, with syntax highlighting and autocomplete"" was abstracted to a generalized problem, and solved with LSP. The idea is that, if you're making a new language, you create an LSP server so that language will work in any text editor. If you're building a new text editor, you can support LSP to automatically support any modern programming language.

[A conceptual diagram of LSPs \(source: MCP IAEE\)](https://preview.redd.it/wz60k2hswiaf1.jpg?width=1050&format=pjpg&auto=webp&s=1c42845286b2bb05047bd0c32caf6a25ca7fdcac)

MCP does something similar, but for agents and tools. The idea is to represent tool use in a standardized way, such developers can put tools in an MCP server, and so developers working on agentic systems can use those tools via a standardized interface.

[LSP and MCP are conceptually similar in terms of their core workflow \(source: MCP IAEE\)](https://preview.redd.it/clc7u0qehiaf1.png?width=1050&format=png&auto=webp&s=6790f5a438aff994337a2224736ba986f1c17777)

I think it's important to note, MCP presents a standardized **interface** for tools, but there is leeway in terms of how a developer might choose to build tools and resources within an MCP server, and there is leeway around how MCP client developers might choose to use those tools and resources.

MCP has various ""transports"" defined, transports being means of communication between the client and the server. MCP can communicate both over the internet, and over local channels (allowing the MCP client to control local tools like applications or web browsers). In my estimation, the latter is really what MCP was designed for. In theory you can connect with an MCP server hosted on the internet, but MCP is chiefly designed to allow clients to execute a locally defined server.

Here's an example of a simple MCP server:

    """"""A very simple MCP server, which exposes a single very simple tool. In most
    practical applications of MCP, a script like this would be launched by the client,
    then the client can talk with that server to execute tools as needed.
    source: MCP IAEE.
    """"""
    
    from mcp.server.fastmcp import FastMCP
    
    mcp = FastMCP(""server"")
    
    u/mcp.tool()
    def say_hello(name: str) -> str:
        """"""Constructs a greeting from a name""""""
        return f""hello {name}, from the server!

In the normal workflow, the MCP client would spawn an MCP server based on a script like this, then would work with that server to execute tools as needed.

**What is A2A?**  
If MCP is designed to expose tools to AI agents, A2A is designed to allow AI agents to talk to one another. I think this diagram summarizes how the two technologies interoperate with on another nicely:

[A conceptual diagram of how A2A and MCP might work together. \(Source: A2A Home Page\)](https://preview.redd.it/gb2bj773ziaf1.png?width=640&format=png&auto=webp&s=c74c1ced5fc1e9026670f68487431392f79d0a4e)

Similarly to MCP, A2A is designed to standardize communication between AI resource. However, A2A is specifically designed for allowing agents to communicate with one another. It does this with two fundamental concepts:

1. Agent Cards: a structure description of what an agent does and where it can be found.
2. Tasks: requests can be sent to an agent, allowing it to execute on tasks via back and forth communication.

A2A is peer-to-peer, asynchronous, and is natively designed to support online communication. In python, A2A is built on top of ASGI (asynchronous server gateway interface), which is the same technology that powers FastAPI and Django.

Here's an example of a simple A2A server:

    from a2a.server.agent_execution import AgentExecutor, RequestContext
    from a2a.server.apps import A2AStarletteApplication
    from a2a.server.request_handlers import DefaultRequestHandler
    from a2a.server.tasks import InMemoryTaskStore
    from a2a.server.events import EventQueue
    from a2a.utils import new_agent_text_message
    from a2a.types import AgentCard, AgentSkill, AgentCapabilities
    
    import uvicorn
    
    class HelloExecutor(AgentExecutor):
        async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
            # Respond with a static hello message
            event_queue.enqueue_event(new_agent_text_message(""Hello from A2A!""))
    
        async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
            pass  # No-op
    
    
    def create_app():
        skill = AgentSkill(
            id=""hello"",
            name=""Hello"",
            description=""Say hello to the world."",
            tags=[""hello"", ""greet""],
            examples=[""hello"", ""hi""]
        )
    
        agent_card = AgentCard(
            name=""HelloWorldAgent"",
            description=""A simple A2A agent that says hello."",
            version=""0.1.0"",
            url=""http://localhost:9000"",
            skills=[skill],
            capabilities=AgentCapabilities(),
            authenticationSchemes=[""public""],
            defaultInputModes=[""text""],
            defaultOutputModes=[""text""],
        )
    
        handler = DefaultRequestHandler(
            agent_executor=HelloExecutor(),
            task_store=InMemoryTaskStore()
        )
    
        app = A2AStarletteApplication(agent_card=agent_card, http_handler=handler)
        return app.build()
    
    
    if __name__ == ""__main__"":
        uvicorn.run(create_app(), host=""127.0.0.1"", port=9000)

Thus A2A has important distinctions from MCP:

* A2A is designed to support ""discoverability"" with agent cards. MCP is designed to be explicitly pointed to.
* A2A is designed for asynchronous communication, allowing for complex implementations of multi-agent workloads working in parallel.
* A2A is designed to be peer-to-peer, rather than having the rigid hierarchy of MCP clients and servers.

**A Point of Friction**  
I think the high level conceptualization around MCP and A2A is pretty solid; MCP is for tools, A2A is for inter-agent communication.

[A high level breakdown of the core usage of MCP and A2A \(source: MCP vs A2A\)](https://preview.redd.it/s8ba9ov6ziaf1.png?width=1080&format=png&auto=webp&s=7c4db19dde15d13cc34372e9c7449ad91939ad28)

Despite the high level clarity, I find these clean distinctions have a tendency to break down practically in terms of implementation. I was working on an example of an application which leveraged both MCP and A2A. I poked around the internet, and found [a repo of examples](https://github.com/a2aproject/a2a-samples/tree/main/samples/python/agents/a2a_mcp) from the official a2a github account. In these examples, they actually use MCP to expose A2A as a set of tools. So, instead of the two protocols existing independently:

[How MCP and A2A might commonly be conceptualized, within a sample application consisting of a travel agent, a car agent, and an airline agent. \(source: A2A IAEE\)](https://preview.redd.it/5wxavpimniaf1.png?width=1050&format=png&auto=webp&s=b092517d6df915c72b673898f3bf563f5dda16d0)

Communication over A2A happens within MCP servers:

[Another approach of implementing A2A and MCP. \(source: A2A IAEE\)](https://preview.redd.it/dh3de5xuniaf1.png?width=1050&format=png&auto=webp&s=d3f46df060e30bb2d71b24ecfc670566f643322f)

This violates the conventional wisdom I see online of A2A and MCP essentially operating as completely separate and isolated protocols. I think the key benefit of this approach is ease of implementation: You don't have to expose both A2A and MCP as two seperate sets of tools to the LLM. Instead, you can expose only a single MCP server to an LLM (that MCP server containing tools for A2A communication). This makes it much easier to manage the integration of A2A and MCP into a single agent. Many LLM providers have plenty of demos of MCP tool use, so using MCP as a vehicle to serve up A2A is compelling.

You can also use the two protocols in isolation, I imagine. There are a ton of ways MCP and A2A enabled projects can practically be implemented, which leads to closing thoughts on the subject.

**My thoughts on MCP and A2A**  
It doesn't matter how standardized MCP and A2A are; if we can't all agree on the larger structure they exist in, there's no interoperability. In the future I expect frameworks to be built on top of both MCP and A2A to establish and enforce best practices. Once the industry converges on these new frameworks, I think issues of ""should this be behind MCP or A2A"" and ""how should I integrate MCP and A2A into this agent"" will start to go away. This is a standard part of the lifecycle of software development, and we've seen the same thing happen with countless protocols in the past.

Standardizing prompting, though, is a different beast entirely.

Having managed the development of LLM powered applications for a while now, I've found prompt engineering to have an interesting role in the greater product development lifecycle. Non-technical stakeholders have a tendency to flock to prompt engineering as a catch all way to solve any problem, which is totally untrue. Developers have a tendency to disregard prompt engineering as a secondary concern, which is also totally untrue. The fact is, prompt engineering won't magically make an LLM powered application better, but bad prompt engineering sure can make it worse. When you hook into MCP and A2A enabled systems, you are essentially allowing for arbitrary injection of prompts as they are defined in these systems. This may have some security concerns if your code isn't designed in a hardened manner, but more palpably there are massive performance concerns. Simply put, if your prompts aren't synergistic with one another throughout an LLM powered application, you won't get good performance. This seriously undermines the practical utility of MCP and A2A enabling turn-key integration.

I think the problem of a framework to define when a tool should be MCP vs A2A is immediately solvable. In terms of prompt engineering, though, I'm curious if we'll need to build rigid best practices around it, or if we can devise clever systems to make interoperable agents more robust to prompting inconsistencies.

**Sources:**  
MCP [vs A2A](https://www.eyelevel.ai/post/a2a-vs-mcp-how-agent-protocols-really-work-and-where-each-one-wins) (I co-authored)  
[MCP IAEE ](https://iaee.substack.com/p/model-context-protocol-intuitively) (I authored)  
[A2A IAEE](https://iaee.substack.com/p/agent-to-agent-protocol-intuitively?utm_source=publication-search) (I authored)  
[A2A MCP Examples](https://github.com/a2aproject/a2a-samples/tree/main/samples/python/agents/a2a_mcp)  
[A2A Home Page](https://a2aproject.github.io/A2A/latest/)

  


  
",38,0.88,https://www.reddit.com/r/datascience/comments/1lq79vo/a_breakdown_of_a2a_mcp_and_agentic/,False,True,False
1lq4xgr,Fit-Employee-4393,1751484438.0,27,/r/datascience/comments/1lq4xgr/how_much_wiggle_room_do_you_give_yourself_on_ds/,datascience,How much wiggle room do you give yourself on DS projects?,"When you’re starting a project, how much extra time do you give yourself for the deadline that you share with stakeholders?

I personally will multiply the time I think I can complete something in by 1.5-2. Honestly might start multiplying by 3 to make multitasking easier.

There’s just so much that can go wrong in DS related projects so I feel it’s necessary to do this. Basically just underpromise overdeliver as they say.

Interested to hear about different situations.",56,0.99,https://www.reddit.com/r/datascience/comments/1lq4xgr/how_much_wiggle_room_do_you_give_yourself_on_ds/,False,True,False
1lq0v8p,BirdLadyTraveller,1751474878.0,35,/r/datascience/comments/1lq0v8p/i_am_currently_a_data_scientist_how_can_i_move_to/,datascience,I am currently a data scientist. How can I move to a more business oriented rule?,"Hey folks! I have worked as a DS for about 5 years now. I wanted to move to a position that I still work with data, but I am  looking for something less technical and more business related. I will list some of my strengths that are also things I like to work with:

- Build proof of concepts projects and explore techniques in the literature to solve business problems with data science approaches;
- Do presentation for technical and non technical peers;
- Build documentation and produce online content;
- I also love to create training and manage projects related to data culture, education, and onboarding.
- Work in groups /having group discussions with multidisciplinary teams. 

Do you know names for positions that are more focused on that? I'd like to search for them! ",47,0.82,https://www.reddit.com/r/datascience/comments/1lq0v8p/i_am_currently_a_data_scientist_how_can_i_move_to/,False,True,False
1lpucaf,Belmeez,1751458678.0,8,/r/datascience/comments/1lpucaf/need_advise_on_crossfunctional_collaboration/,datascience,Need advise on cross-functional collaboration,"Hi data science community,

I need your advice on how to handle a work situation. Curious to know how others would handle or if they have been in a similar situation.

I lead a data science team and I also have a peer who leads a BI team and we report to the same executive.

A couple months ago, BI lead reached out and was excited to see if we can collaborate and create an AI/BI chat bot for our internal structured data. I thought this was a good idea and would be a great opportunity to collaborate with him and his team. So I spent a couple of weeks to build out a POC, I show cased it to him and our executive, it was well received and I outlined next steps on how we can collaborate to make it better.

I got no response from him about my next steps email. I figured no harm no foul he got busy I’m sure. Well come to find out, he had his team build almost an exact replica of the POC I did and essentially boxed my team and I out of this idea and decided he would just do it himself internally. Mind you, all the BI people had to learn how to use LLMs and how to orchestrate agents, etc. it’s a skill set we have but he decided to do it himself despite this.

How would you all handle this?

I was planning on a 1:1 with him where I essentially lay out the facts that he wasted my time by giving me the illusion that we would work together and collaborate but instead just did things himself. We have been getting pushed by our executive team to work together more and this was a great opportunity to show them we work together but instead he decided to take a different route.",18,0.95,https://www.reddit.com/r/datascience/comments/1lpucaf/need_advise_on_crossfunctional_collaboration/,False,True,False
1lpnkj0,statius9,1751433395.0,1,/r/datascience/comments/1lpnkj0/beta_release_minds_ai_filter_for_eeg/,datascience,"Beta release: Minds AI Filter for EEG — Physics-informed preprocessing for real-time BCI (+17% gain on noisy data from commercial headsets, 0.2s latency)","We at MindsApplied specialize in the development of machine learning models for the enhancement of EEG signal quality and emotional state classification. We're excited to share our latest model—the Minds AI Filter—and would love your feedback.

* [👉 Download the Python package here](https://drive.google.com/drive/folders/1_4Q9voe5j88G_EMF8YanoeEPVoUt_D2B?usp=drive_link)
* 🔑Use key: ''REDDIT-KEY-VRG44S' to initialize
* 📄 Includes setup instructions

The Minds AI Filter is a physics-informed, real-time EEG preprocessing tool that relies on sensor fusion for low-latency noise and artifact removal. It's built to improve signal quality before feature extraction or classification, especially for online systems. To dive (very briefly) into the details, it works in part by **reducing high-frequency noise (\~40 Hz) and sharpening low-frequency activity (\~3–7 Hz)**.

We tested it alongside standard bandpass filtering, using both:

* Commercial EEG hardware (OpenBCI Mark IV, BrainBit Dragon)
* The public DEAP dataset, a 32-participant benchmark for emotional state classification

Here are our experimental results:

* Commercial Devices (OpenBCI Mark IV, BrainBit Dragon)
   * \+15% average improvement in balanced accuracy using only 12 trials of 60 seconds per subject per device
   * Improvement attributed to higher baseline noise in these systems
* DEAP Dataset
   * \+6% average improvement across 32 subjects and 32 channels
   * Maximum individual gain: +35%
   * Average gain in classification accuracy was 17% for cases where the filter led to improvement.
   * No decline in accuracy for any participant
* Performance
   * \~0.2 seconds to filter 60 seconds of data

Note: Comparisons were made between bandpass-only and bandpass + Minds AI Filter. Filtering occurred before bandpass.

Methodology:

To generate these experimental results, we used 2-fold stratified cross-validation grid search to tune the filter's key hyperparameter (λ). Classification relied on balanced on balanced accuracy using logistic regression on features derived from wavelet coefficients.

Why we're posting: This filter is still in beta and we'd love feedback —especially if you try it on your own datasets or devices. The current goal is to support rapid, adaptive, and physics-informed filtering for real-time systems and multi-sensor neurotech platforms.

If you find it useful or want future updates (e.g., universal DLL, long-term/offline licenses), you can subscribe here:

* 🔗 [https://www.minds-applied.com/contact](https://www.minds-applied.com/contact)

https://preview.redd.it/o3xqckeiaeaf1.png?width=594&format=png&auto=webp&s=0fb1860d8af85fa516cb705c096427a32977a522

https://preview.redd.it/95lbzd8jaeaf1.png?width=589&format=png&auto=webp&s=39984d0e8f75f27ab0a71e7e5ca09bba25f6ffb4

https://preview.redd.it/x9iyc4kjaeaf1.png?width=1372&format=png&auto=webp&s=ef70703c892727318688b7472778cb5658a899ee

",0,0.5,https://www.reddit.com/r/datascience/comments/1lpnkj0/beta_release_minds_ai_filter_for_eeg/,False,True,False
1loo3eh,Karl_mstr,1751330168.0,32,/r/datascience/comments/1loo3eh/does_db_normalization_worth_it/,datascience,Does DB normalization worth it?,"Hi, I have 6 months as a Jr Data Analyst and I have been working with Power BI since I begin. At the beginning I watched a lot of dashboards on PBI and when I checked the Data Model was disgusting, it doesn't seems as something well designed. 

On my the few opportunities that I have developed some dashboards I have seen a lot of redundancies on them, but I keep quiet due it's my first analytic role and my role using PBI so I couldn't compare with anything else.

I ask here because I don't know many people who use PBI or has experience on Data related jobs and I've been dealing with query limit reaching (more than 10M rows to process).

So I watched some courses that normalization could solve many issues, but I wanted to know:
1 - If it could really help to solve that issue.
2 - How could I normalize the data when, not the data, the data Model is so messy? 

Thanks in advance.",27,0.78,https://www.reddit.com/r/datascience/comments/1loo3eh/does_db_normalization_worth_it/,False,True,False
1lohjp4,ElectrikMetriks,1751313540.0,75,/r/datascience/comments/1lohjp4/no_reason_to_complicate_things/,datascience,No reason to complicate things.,"There's absolutely validity in doing more complex visuals.  But, sometimes simple is better if the audience is more likely to use it/understand it.",1225,0.99,https://i.redd.it/fygwmz26e4af1.png,False,False,False
1lo4xao,Technical-Love-8479,1751282366.0,1,/r/datascience/comments/1lo4xao/model_context_protocol_mcp_tutorials_playlist_for/,datascience,Model Context Protocol (MCP) tutorials playlist for beginners,"This playlist comprises of numerous tutorials on MCP servers including

1. Install Blender-MCP for Claude AI on Windows
2. Design a Room with Blender-MCP + Claude
3. Connect SQL to Claude AI via MCP
4. Run MCP Servers with Cursor AI
5. Local LLMs with Ollama MCP Server
6. Build Custom MCP Servers (Free)
7. Control Docker via MCP
8. Control WhatsApp with MCP
9. GitHub Automation via MCP
10. Control Chrome using MCP
11. Figma with AI using MCP
12. AI for PowerPoint via MCP
13. Notion Automation with MCP
14. File System Control via MCP
15. AI in Jupyter using MCP
16. Browser Automation with Playwright MCP
17. Excel Automation via MCP
18. Discord + MCP Integration
19. Google Calendar MCP
20. Gmail Automation with MCP
21. Intro to MCP Servers for Beginners
22. Slack + AI via MCP
23. Use Any LLM API with MCP
24. Is Model Context Protocol Dangerous?
25. LangChain with MCP Servers
26. Best Starter MCP Servers
27. YouTube Automation via MCP
28. Zapier + AI using MCP
29. MCP with Gemini 2.5 Pro
30. PyCharm IDE + MCP
31. ElevenLabs Audio with Claude AI via MCP
32. LinkedIn Auto-Posting via MCP
33. Twitter Auto-Posting with MCP
34. Facebook Automation using MCP
35. Top MCP Servers for Data Science
36. Best MCPs for Productivity
37. Social Media MCPs for Content Creation
38. MCP Course for Beginners
39. Create n8n Workflows with MCP
40. RAG MCP Server Guide
41. Multi-File RAG via MCP
42. Use MCP with ChatGPT
43. ChatGPT + PowerPoint (Free, Unlimited)
44. ChatGPT RAG MCP
45. ChatGPT + Excel via MCP
46. Use MCP with Grok AI
47. Vibe Coding in Blender with MCP
48. Perplexity AI + MCP Integration
49. ChatGPT + Figma Integration
50. ChatGPT + Blender MCP
51. ChatGPT + Gmail via MCP
52. ChatGPT + Google Calendar MCP
53. MCP vs Traditional AI Agents

Hope this is useful !!

Playlist : [https://www.youtube.com/playlist?list=PLnH2pfPCPZsJ5aJaHdTW7to2tZkYtzIwp](https://www.youtube.com/playlist?list=PLnH2pfPCPZsJ5aJaHdTW7to2tZkYtzIwp)",26,0.84,https://www.reddit.com/r/datascience/comments/1lo4xao/model_context_protocol_mcp_tutorials_playlist_for/,False,True,False
1lny1dk,AutoModerator,1751256074.0,63,/r/datascience/comments/1lny1dk/weekly_entering_transitioning_thread_30_jun_2025/,datascience,"Weekly Entering & Transitioning - Thread 30 Jun, 2025 - 07 Jul, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",11,0.92,https://www.reddit.com/r/datascience/comments/1lny1dk/weekly_entering_transitioning_thread_30_jun_2025/,False,True,False
1lntegl,ergodym,1751241456.0,39,/r/datascience/comments/1lntegl/ics_who_pivoted_did_you_go_engineering_or/,datascience,ICs who pivoted: did you go engineering or management?,"Hitting that point where I feel like I need to pick a lane.

Curious what others did. Did you double down on technical stuff (data engineering/MLE/SWE), switched to the product side, or did you move into people management?",61,0.98,https://www.reddit.com/r/datascience/comments/1lntegl/ics_who_pivoted_did_you_go_engineering_or/,False,True,False
1lnct9i,hendrix616,1751197832.0,9,/r/datascience/comments/1lnct9i/using_claude_code_in_notebook/,datascience,Using Claude Code in notebook,"At work I use jupyter notebooks for experimentation and prototyping of data products. So far, I’ve been leveraging AI code completion type of functionality within a Python cell for finishing a line of code, writing the next few lines or writing a function altogether. 

But I’m curious about the next level: using something like Claude Code open side-by side with my notebook. 

Just wondering if anyone is currently using this type of workflow and if you have any tips & tricks or specific use cases you could share. ",0,0.25,https://www.reddit.com/r/datascience/comments/1lnct9i/using_claude_code_in_notebook/,False,True,False
1lnbgna,ParkingTheory9837,1751192881.0,9,/r/datascience/comments/1lnbgna/not_sure_what_certifications_to_attain_to/,datascience,Not sure what certifications to attain to increase my chances of getting an internship after third year,"Context: I am planning to go into data science as a career. Im currently about to go into my third year and I need to secure an internship agter my third year during my coop year. To increade my chances, I want to obtain AWS certifications. The problem I am seeing is that the AWS SAA certificate seems to specific to AWS. Would the MLEA or DEA increade my chance of getting data scientist/mle internships significantly? Assume I have knowledge and projects to showcase knowledge of theoretical ML, python, sql, etc. Also assume I have cloud practitioner and AI practitioner certs but no experience with AWS whatsoever, but experience in data analysis. I would really appreciate in depth responses. Please avoid stupid comments like ""certifications are useless"" because they obv arent and can set you apart from someone with similar skill sets in other areas. ",0,0.5,https://www.reddit.com/r/datascience/comments/1lnbgna/not_sure_what_certifications_to_attain_to/,False,True,False
1ln9cf0,Round-Paramedic-2968,1751184312.0,19,/r/datascience/comments/1ln9cf0/advice_on_feature_selection_process/,datascience,Advice on feature selection process,"**Hi everyone,**

I have a question regarding the feature selection process for a credit risk model I'm building as part of my internship. I've collected raw data and conducted feature engineering with the help of a domain expert in credit risk. Now I have a list of around 2000 features.

For the feature selection part, based on what I've learned, the typical approach is to use a tree-based model (like Random Forest or XGBoost) to rank feature importance, and then shortlist it down to about 15–20 features. After that, I would use those selected features to train my final model (CatBoost in this case), perform hyperparameter tuning, and then use that model for inference.

Am I doing it correctly? It feels a bit too straightforward — like once I have the 2000 features, I just plug them into a tree model, get the top features, and that's it. I noticed that some of my colleagues do multiple rounds of feature selection — for example, narrowing it down from 2000 to 200, then to 80, and finally to 20 — using multiple tree models and iterations.

Also, where do SHAP values fit into this process? I usually use SHAP to visualize feature effects in the final model for interpretability, but I'm wondering if it can or should be used during the feature selection stage as well.

I’d really appreciate your advice!",32,0.91,https://www.reddit.com/r/datascience/comments/1ln9cf0/advice_on_feature_selection_process/,False,True,False
1ln8f2b,OverratedDataScience,1751180578.0,18,/r/datascience/comments/1ln8f2b/how_do_you_deal_with_data_scientists_with_big_pay/,datascience,How do you deal with data scientists with big pay check and title but no domain knowledge?,"A tech illiterate Director at my org hired a data couple of data scientists 18 months ago. He has tasked them with nothing specific. And their job was solely to observe and find uses-cases themselves. The only reason they were hired was for the Director to gain brownie points of creating a data-driven team for themself, despite there being several other such teams.

Cut to today, the Director has realized that there is very little ROI from his hires because they lack domain knowledge. He conveniently moved them to another team where ML is an overkill. The data scientists however, have found some problems they thought they'll solve with ""data science"". They have been vibe coding and building PPTs for months now. But their attempts are hardly successful because of their lack of domain knowledge. To compensate for their lack of domain knowledge, they create beautiful presentations with lots of buzzwords such as LLMs, but again, lack domain substance.

Now, their proposals seem unnecessary and downright obnoxious to many domain SMEs. But the SMEs don't have the courage to say it to the leadership and be percevied as a roadblock to the data-driven strategy. The constant interference of these data scientists is destabilizing the existing processes for the worst and the team is incurring additional costs.

This is a very peculiar situation where the data scientists, lacking domain knowledge, are just shooting project proposals in the dark hoping to hit something. I know this doesn't typically happen in most organizations. But have you ever seen such a situation around you? How did you or others deal with the situation?

EDIT: This post is not to shit on the data scientists. They are probably good in their areas. The problem is not the domain SME support. The problem is that these data scientists seem to be too high on their titles and paychecks to collaborate with SMEs. Most SMEs want to support them and tell them nicely that ML/AI is an overkill for their usecases, and the efforts required are too big. There are other data science and analytics teams that are working seamlesly with SMEs.",0,0.33,https://www.reddit.com/r/datascience/comments/1ln8f2b/how_do_you_deal_with_data_scientists_with_big_pay/,False,True,False
1ln6aeq,guna1o0,1751172473.0,72,/r/datascience/comments/1ln6aeq/hows_the_job_market_for_bayesian_statistics/,datascience,How’s the job market for Bayesian statistics?,"I’m a data scientist with 1 YOE. mostly worked on credit scoring models, sql, and Power BI. Lately, I’ve been thinking of going deeper into bayesian statistics and I’m currently going through the s*tatistical rethinking* book.

But I’m wondering. is it worth focusing heavily on bayesian stats? Or should I pivot toward something that opens up more job opportunities?

Would love to hear your thoughts or experiences!",139,0.89,https://www.reddit.com/r/datascience/comments/1ln6aeq/hows_the_job_market_for_bayesian_statistics/,False,True,False
1ln3zyk,Illustrious-Pound266,1751164538.0,41,/r/datascience/comments/1ln3zyk/is_mlai_engineering_increasingly_becoming_less/,datascience,Is ML/AI engineering increasingly becoming less focused on model training and more focused on integrating LLMs to build web apps?,"One thing I've noticed recently is that increasingly, a lot of AI/ML roles seem to be focused on ways to integrate LLMs to build web apps that automate some kind of task, e.g. chatbot with RAG or using agent to automate some task in a consumer-facing software with tools like langchain, llamaindex, Claude, etc. I feel like there's less and less of the ""classical"" ML training and building models.

I am not saying that ""classical"" ML training will go away. I think model building/training non-LLMs will always have some place in data science. But in a way, I feel like ""AI engineering"" seems increasingly converging to something closer to back-end engineering you typically see in full-stack. What I mean is that rather than focusing on building or training models, it seems that the bulk of the work now seems to be about how to take LLMs from model providers like OpenAI and Anthropic, and use it to build some software that automates some work with Langchain/Llamaindex.

Is this a reasonable take? I know we can never predict the future, but the trends I see seem to be increasingly heading towards that.",160,0.96,https://www.reddit.com/r/datascience/comments/1ln3zyk/is_mlai_engineering_increasingly_becoming_less/,False,True,False
1lmxkq8,bobo-the-merciful,1751145288.0,6,/r/datascience/comments/1lmxkq8/pleased_to_share_the_simpy_simulation_playground/,datascience,"Pleased to share the ""SimPy Simulation Playground"" - examples of simulations in Python from different industries","Just put the finishing touches to the first version of this web page where you can run SimPy examples from different industries, including parameterising the sim, editing the code if you wish, running and viewing the results.

Runs entirely in your browser.

Here's the link: [https://www.schoolofsimulation.com/simpy\_simulations](https://www.schoolofsimulation.com/simpy_simulations)

My goal with this is to help provide education and informationa around how discrete-event simulation with SimPy can be applied to different industry contexts.

If you have any suggestions for other examples to add, I'd be happy to consider expanding the list!

Feedback, as ever, is most welcome!",14,0.77,https://i.redd.it/qm0yh3nxhq9f1.png,False,False,False
1lmv5uf,OverratedDataScience,1751138824.0,115,/r/datascience/comments/1lmv5uf/unpopular_opinion_these_are_the_most_useless/,datascience,Unpopular Opinion: These are the most useless posters on LinkedIn,"LinkedIn influencers love to treat the two roles as different species. In most enterprises, especially in mid to small orgs, these roles are largely overlapping. ",1324,0.96,https://i.redd.it/flntff9gwp9f1.jpeg,False,False,False
1lmneo7,Mission-Balance-4250,1751118944.0,20,/r/datascience/comments/1lmneo7/i_built_a_selfhosted_databricks/,datascience,I built a self-hosted Databricks,"Hey everyone, I'm an ML Engineer who spearheaded the adoption of Databricks at work. I love the agency it affords me because I can own projects end-to-end and do everything in one place.

However, the platform adds a lot of overhead and has a wide array of data-features I just don't care about. So many problems can be solved with a simple data pipeline and basic model (e.g. XGBoost.) Not only is there technical overhead, but systems and process overhead; bureaucracy and red-tap significantly slow delivery. Right now at work we are undertaking a ""migration"" to Databricks and man, it is such a PITA to get anything moving it isn't even funny...

Anyway, I decided to try and address this myself by developing [FlintML](https://github.com/flintml/flintml), a self-hosted, all-in-one MLOps stack. Basically, Polars, Delta Lake, unified catalog, Aim experiment tracking, notebook IDE and orchestration (still working on this) fully spun up with Docker Compose.

I'm hoping to get some feedback from this subreddit. I've spent a couple of months developing this and want to know whether I would be wasting time by continuing or if this might actually be useful. I am using it for my personal research projects and find it very helpful.

Thanks heaps",79,0.93,https://www.reddit.com/r/datascience/comments/1lmneo7/i_built_a_selfhosted_databricks/,False,True,False
1lmi8j8,petburiraja,1751101019.0,110,/r/datascience/comments/1lmi8j8/the_unicorn_is_dead_a_fourera_history_of_the_data/,datascience,"The ""Unicorn"" is Dead: A Four-Era History of the Data Scientist Role and Why We're All Engineers Now","Hey everyone,

I’ve been in this field for a while now, starting back when ""Big Data"" was the big buzzword, and I've been thinking a lot about how drastically our roles have changed. It feels like the job description for a ""Data Scientist"" has been rewritten three or four times over. The ""unicorn"" we all talked about a decade ago feels like a fossil today.

I wanted to map out this evolution, partly to make sense of it for myself, but also to see if it resonates with your experiences. I see it as four distinct eras.

---

### **Era 1: The BI & Stats Age (The ""Before Times,"" Pre-2010)**

Remember this? Before ""Data Scientist"" was a thing, we were all in our separate corners.

*   **Who we were:** BI Analysts, Statisticians, Database Admins, Quants.
*   **What we did:** Our world revolved around historical reporting. We lived in SQL, wrestling with relational databases and using tools like Business Objects or good old Excel to build reports. The core question was always, **""What happened last quarter?""**
*   **The ""advanced"" stuff:** If you were a true statistician, maybe you were building logistic regression models in SAS, but that felt very separate from the day-to-day business analytics. It was more academic, less integrated.

The mindset was purely descriptive. We were the historians of the company's data.

### **Era 2: The Golden Age of the ""Unicorn"" (Roughly 2011-2018)**

This is when everything changed. *HBR* called our job the ""sexiest"" of the century, and the hype was real.

*   **The trigger:** Hadoop and Spark made ""Big Data"" accessible, and Python with Scikit-learn became an absolute powerhouse. Suddenly, you could do serious modeling on your own machine.
*   **The mission:** The game changed from ""What happened?"" to **""What's *going* to happen?""** We were all building churn models, recommendation engines, and trying to predict the future. The Jupyter Notebook was our kingdom.
*   **The ""unicorn"" expectation:** This was the peak of the ""full-stack"" ideal. One person was supposed to understand the business, wrangle the data, build the model, and then explain it all in a PowerPoint deck. The *insight* from the model was the final product. It was an incredibly fun, creative, and exploratory time.

### **Era 3: The Industrial Age & The Great Bifurcation (Roughly 2019-2023)**

This is where, in my opinion, the ""unicorn"" myth started to crack. Companies realized a model sitting in a notebook doesn't actually *do* anything for the business. The focus shifted from *building models* to *deploying systems*.

*   **The trigger:** The cloud matured. AWS, GCP, and Azure became the standard, and the discipline of MLOps was born. The problem wasn't ""can we predict it?"" anymore. It was, **""Can we serve these predictions reliably to millions of users with low latency?""**
*   **The splintering:** The generalist ""Data Scientist"" role started to fracture into specialists because no single person could master it all:
    *   **ML Engineers:** The software engineers who actually productionized the models.
    *   **Data Engineers:** The unsung heroes who built the reliable data pipelines with tools like Airflow and dbt.
    *   **Analytics Engineers:** The new role that owned the data modeling layer for BI.
*   The mindset became engineering-first. We were building factories, not just artisanal products.

### **Era 4: The Autonomous Age (2023 - Today and Beyond)**

And then, everything changed again. The arrival of truly powerful LLMs completely upended the landscape.

*   **The trigger:** ChatGPT went public, GPT-4 was released, and frameworks like LangChain gave us the tools to build on top of this new paradigm.
*   **The mission:** The core question has evolved again. It's not just about prediction anymore; it's about **action and orchestration**. The question is, **""How do we build a system that can understand a goal, create a plan, and execute it?""**
*   **The new reality:**
    *   **Prediction becomes a feature, not the product.** An AI *agent* doesn't just predict churn; it takes an *action* to prevent it.
    *   **We are all systems architects now.** We're not just building a model; we're building an intelligent, multi-step workflow. We're integrating vector databases, multiple APIs, and complex reasoning loops.
    *   **The engineering rigor from Era 3 is now the mandatory foundation.** You can't build a reliable agent without solid MLOps and real-time data engineering (Kafka, Flink, etc.).

It feels like the ""science"" part of our job is now less about statistical analysis (AI can do a lot of that for us) and more about the rigorous, empirical science of architecting and evaluating these incredibly complex, often non-deterministic systems.

So, that's my take. The ""Data Scientist"" title isn't dead, but the ""unicorn"" generalist ideal of 2015 certainly is. We've been pushed to become deeper specialists, and for most of us on the building side, that specialty looks a lot more like engineering than anything else.

Curious to hear if this matches up with what you're all seeing in your roles. Did I miss an era? Is your experience different?

EDIT: In response to comments asking if this was written by AI: The underlying ideas are based on my own experience.

However, I want to be transparent that I would not have been able to articulate my vague, intuitive thoughts about the changes in this field with such precision. 

I used AI specifically for the structurization and organization of the content.",614,0.86,https://www.reddit.com/r/datascience/comments/1lmi8j8/the_unicorn_is_dead_a_fourera_history_of_the_data/,False,True,False
1lmaxr4,mgalarny,1751074564.0,25,/r/datascience/comments/1lmaxr4/using_llms_to_extract_stock_picks_from_youtube/,datascience,Using LLMs to Extract Stock Picks from YouTube,"For anyone interested in NLP or the application of data science in finance and media, we just released a dataset + paper on extracting **stock recommendations from YouTube financial influencer videos**.

This is a real-world task that combines signals across audio, video, and transcripts. We used expert annotations and benchmarked both LLMs and multimodal models to see how well they can extract structured recommendation data (like ticker and action) from messy, informal content.

If you're interested in working with unstructured media, financial data, or evaluating model performance in noisy settings, this might be interesting.

Paper: [https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=5315526](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5315526)  
Dataset: [https://huggingface.co/datasets/gtfintechlab/VideoConviction](https://huggingface.co/datasets/gtfintechlab/VideoConviction)

Happy to discuss the challenges we ran into or potential applications beyond finance!

[Betting against finfluencer recommendations outperformed the S&P 500 by +6.8&#37; in annual returns, but at higher risk \(Sharpe ratio 0.41 vs 0.65\). QQQ wins in Sharpe ratio. ](https://preview.redd.it/3n861nuhnk9f1.png?width=4764&format=png&auto=webp&s=aa010ae695934b5520df5e82d8158201750cb3a4)

",92,0.83,https://www.reddit.com/r/datascience/comments/1lmaxr4/using_llms_to_extract_stock_picks_from_youtube/,False,True,False
1lm0dc9,bobo-the-merciful,1751046669.0,9,/r/datascience/comments/1lm0dc9/i_built_a_virtual_simulation_engineer_tool_that/,datascience,"I built a ""virtual simulation engineer"" tool that designs, build, executes and displays the results of Python SimPy simulations entirely in a single browser window","New tool I built to design, build and execute a discrete-event simulation in Python entirely using natural language in a single browser window. 

You can use it here, 100% free: [https://gemini.google.com/share/ad9d3a205479](https://gemini.google.com/share/ad9d3a205479)

Version 2 uses SimPy under the hood. Pyodide to execute Python in the front end.

This is a proof of concept, I am keen for feedback please.

I made a video overview of it here: [https://www.youtube.com/watch?v=BF-1F-kqvL4](https://www.youtube.com/watch?v=BF-1F-kqvL4) ",14,0.69,https://i.redd.it/4g1jooknci9f1.png,False,False,False
1llvtfl,zsrt13,1751035808.0,37,/r/datascience/comments/1llvtfl/cvs_heath_vs_jpm/,datascience,CVS Heath vs JPM,Thank you all for the support. This is a really helpful group. Cheers!,33,0.77,https://www.reddit.com/r/datascience/comments/1llvtfl/cvs_heath_vs_jpm/,False,True,False
1lluwlv,Raz4r,1751033504.0,355,/r/datascience/comments/1lluwlv/data_science_has_become_a_pseudoscience/,datascience,Data Science Has Become a Pseudo-Science,"I’ve been working in data science for the last ten years, both in industry and academia, having pursued a master’s and PhD in Europe. My experience in the industry, overall, has been very positive. I’ve had the opportunity to work with brilliant people on exciting, high-impact projects. Of course, there were the usual high-stress situations, nonsense PowerPoints, and impossible deadlines, but the work largely felt meaningful.

However, over the past two years or so, it feels like the field has taken a sharp turn. Just yesterday, I attended a technical presentation from the analytics team. The project aimed to identify anomalies in a dataset composed of multiple time series, each containing a clear inflection point. The team’s hypothesis was that these trajectories might indicate entities engaged in some sort of fraud.

The team claimed to have solved the task using “generative AI”. They didn’t go into methodological details but presented results that, according to them, were amazing. Curious, nespecially since the project was heading toward deployment, i asked about validation, performance metrics, or baseline comparisons. None were presented.

Later, I found out that “generative AI” meant asking ChatGPT to generate a code. The code simply computed the mean of each series before and after the inflection point, then calculated the z-score of the difference. No model evaluation. No metrics. No baselines. Absolutely no model criticism. Just a naive approach, packaged and executed very, very quickly under the label of generative AI.

The moment I understood the proposed solution, my immediate thought was ""I need to get as far away from this company as possible"". I share this anecdote because it summarizes much of what I’ve witnessed in the field over the past two years. It feels like data science is drifting toward a kind of pseudo-science where we consult a black-box oracle for answers, and questioning its outputs is treated as anti-innovation, while no one really understand how the outputs were generated.

After several experiences like this, I’m seriously considering focusing on academia. Working on projects like these is eroding any hope I have in the field. I know this won’t work and yet, the label generative AI seems to make it unquestionable. So I came here to ask if is this experience shared among other DSs?

",2719,0.98,https://www.reddit.com/r/datascience/comments/1lluwlv/data_science_has_become_a_pseudoscience/,False,True,False
1llnbwq,joshamayo7,1751007348.0,17,/r/datascience/comments/1llnbwq/causal_inference_in_sports/,datascience,Causal Inference in Sports,"For all curious on Causal Inference, and anyone interested in the application of DS in Sport. I’ve written this blog with the aim of providing a taste for how Causal Inference techniques are used practically, as well as some examples to get people thinking.

I do believe upskilling in Causal Inference is quite valuable, despite the learning curve I think it’s quite cool identifying cause-and -effect without having to do RCTs.

Enjoy!",70,0.94,https://medium.com/@joshamayo7/causal-inference-in-sports-7d911a248375,False,False,False
1lliwit,Technical-Love-8479,1750992050.0,2,/r/datascience/comments/1lliwit/sealselfadapting_language_models_self_learning/,datascience,SEAL:Self-Adapting Language Models (self learning LLMs),"MIT has recently released a new research paper where they have introduced a new framework SEAL which introduces a concept of self-learning LLMs that means LLMs can now generate their own fine-tuning data set optimized for the strategy and fine tune themselves on the given context. 

Full summary ; [https://www.youtube.com/watch?v=MLUh9b8nN2U](https://www.youtube.com/watch?v=MLUh9b8nN2U)

Paper : [https://arxiv.org/abs/2506.10943](https://arxiv.org/abs/2506.10943)",9,0.77,https://www.reddit.com/r/datascience/comments/1lliwit/sealselfadapting_language_models_self_learning/,False,True,False
1ll7or7,MasteredLink,1750962424.0,34,/r/datascience/comments/1ll7or7/when_applying_internally_do_you_reach_out_to_the/,datascience,"When applying internally, do you reach out to the hiring manager?","I work at a relatively large company, and I've always reached out to hiring managers for internal positions, setting up a brief introductory meeting to ask specific questions about the role. However, during a recent HR session for new employees, it was recommended that we avoid this approach, as it could ""create bias"" and that managers are often too busy.

Now I'm rethinking my strategy for internal applications, I feel like it's highly dependent on the manager themselves but in most cases, asking for a quick intro meeting wouldn't hurt right? I feel like HR was way too broad with this statement. What are people's experiences on this.",53,0.93,https://www.reddit.com/r/datascience/comments/1ll7or7/when_applying_internally_do_you_reach_out_to_the/,False,True,False
1ll5dv2,Error40404,1750957148.0,42,/r/datascience/comments/1ll5dv2/i_have_two_amazing_job_offers_i_want_to_build_my/,datascience,I have two amazing job offers. I want to build my own company in the near future. At a loss.,"Hi!

I have two offers. One from a big tech company as a data scientist. I deem it easily the best tech company in my country. I would have killed for this offer just 1 year ago.

Another offer is from a robotics startup. I would be a founding engineer doing ML, and I think I would learn a lot. However, I'm not interested in this company in the long run. I would jump out after 2 years at the latest to build my own. So my equity would not even vest, and I would feel like I'm backstabbing the founders. They probably would not hire me if I told them this. But I think I would (maybe) learn more in this position.

I just can't decide what to do... My ultimate goal is to build my own company in 1-2 years. What to do?",74,0.76,https://www.reddit.com/r/datascience/comments/1ll5dv2/i_have_two_amazing_job_offers_i_want_to_build_my/,False,True,False
1ll56bo,Technical-Love-8479,1750956648.0,4,/r/datascience/comments/1ll56bo/gemini_cli_googles_free_coding_ai_agent/,datascience,Gemini CLI: Google's free coding AI Agent,Google's Gemini CLI is a terminal based AI Agent mostly for coding and easy to install with free access to Gemini 2.5 Pro. Check demo here : https://youtu.be/Diib3vKblBM?si=DDtnlHqAhn_kHbiP,23,0.83,https://www.reddit.com/r/datascience/comments/1ll56bo/gemini_cli_googles_free_coding_ai_agent/,False,True,False
1lkpnkk,bonesclarke84,1750907822.0,10,/r/datascience/comments/1lkpnkk/preexpedition_weather_conditions_and_success/,datascience,Pre-Expedition Weather Conditions and Success Rates: Seasonal Pattern Analysis of Himalayan Expedition Data,"After someone posted Himalayan expedition data on Kaggle: [Himalayan Expeditions](https://www.kaggle.com/datasets/siddharth0935/himalayan-expeditions), I decided to start a personal project and expand on this data by adding ERA5 historical reanalysis weather data to it. Some of my preliminary findings have been interesting so far and I thought I would share them.  
  
I expanded on the expedition data by creating multiple different weather windows:

* Full expedition from basecamp date until termination either following summit or termination of attempt.
* Pre-expedition weather - 14 days prior to official expedition start at basecamp.
* Termination or Summit approach - the day before termination or summit.
* Early phase - the first 14 days at basecamp.
* Late phase - 7 days prior to termination date (either after summit or on failed attempt.)
* Decision window - 2 days prior to summit window

The first weather that I have focused on analyzing is the pre-expedition weather window. After cleaning the data and adding the weather windows, I also added a few other features using simple operations and created a few target variables for later modelling like expedition success score, expedition failure score, and an overall expedition score. For this analysis, though, I only focused on success being either True or False. After creating the features and targets, I then ran t-tests on success being True or False to determine their statistical significance. 

When looking at all the features related to the pre-expedition weather window, the findings seem to suggest that pre-expedition weather conditions play a significant role in Himalayan expedition success or failure in spring/summer expeditions. The graphs and correlation heatmap below summarize the variables that have the highest significance in either success or failure:  


[This diagram shows how the different attributes either contribute to success or failure.](https://preview.redd.it/6nbr99uzu69f1.png?width=1904&format=png&auto=webp&s=f84e3cb76b2d61d3b3e68dc3fdd0eec2608cd586)

[This diagram highlights the key attributes over or under of a significance of 0.2 or -0.2 respectively. ](https://preview.redd.it/bzj3uxu2v69f1.png?width=1889&format=png&auto=webp&s=5287852c5a90ae75b251826e1a9668db0ca34d80)

[This is a correlation heatmap diagram associating the attributes to success or failure.](https://preview.redd.it/dd5g6ly4v69f1.png?width=1904&format=png&auto=webp&s=a6925405bc7b5a9930fa9265adc3f425129579d8)

Although these findings alone do not paint an over-all picture of Himalayan expedition success or failure, I believe they play a significant part and could be used practically to assess conditions going into spring/summer expeditions. 

I hope this is interesting and feel free to provide any feedback. I am not a data scientist by professional and still learning. This analysis was done in Python using a jupyter notebook.",12,0.89,https://www.reddit.com/r/datascience/comments/1lkpnkk/preexpedition_weather_conditions_and_success/,False,True,False
1lkjxmr,Expensive-Ad8916,1750891449.0,40,/r/datascience/comments/1lkjxmr/steam_recommender_using_vectors_student_project/,datascience,Steam Recommender using Vectors! (Student Project),"Hello Data Enjoyers!

I have recently created a steam game finder that helps users find games similar to their own favorite game,

I pulled reviews form multiple sources then used sentiment with some regex to help me find insightful ones then with some procedural tag generation along with a hierarchical genre umbrella tree i created game vectors in category trees, to traverse my db I use vector similarity and walk up my hierarchical tree.

my goal is to create a tool to help me and hopefully many others find games not by relevancy but purely by similarity. Ideally as I work on it finding hidden gems will be easy.

I created this project to prepare for my software engineering final in undergrad so its **very rough**, this is not a finished product at all by any means. **Let me know** if there are any features you would like to see or suggest some algorithms to incorporate.

check it out on : [https://nextsteamgame.com/](https://nextsteamgame.com/)",145,0.98,https://www.reddit.com/gallery/1lkjxmr,False,False,False
1lkfg6w,Starktony11,1750880618.0,8,/r/datascience/comments/1lkfg6w/how_longwhich_things_as_a_hm_you_would_expect_a/,datascience,How long/which things as a HM you would expect a candidate to speak for in Behavioral interviews?,"How long/which things as a HM you would expect a candidate to speak for in Behavioral interviews?  Anything important you want them to share or things that they share make them stand out from other candidates for offer? Also things they mention/not mention make them on rejection list? 


Also, is 2-3 minutes stories good enough? Or are they too short?  (For me STAR method complete stories in 2 minutes unless i add unnecessary details that are not asked) 

i tend to be person who answer only things you asked, should I change this method?. Like if you ask whether i did project on worked on stake holders t


Any other things you would like to share for DS behavioral interviews",8,0.75,https://www.reddit.com/r/datascience/comments/1lkfg6w/how_longwhich_things_as_a_hm_you_would_expect_a/,False,True,False
1ljsd1j,Odd_Artist4319,1750813181.0,110,/r/datascience/comments/1ljsd1j/graduating_soon_any_tips_for_landing_an/,datascience,Graduating Soon — Any Tips for Landing an Entry-Level Data Science Job?,"Hey everyone — I'm finishing up my MSc in Data Science this fall (Fall 2025). I also have a BSc in Computer Science and completed 2–3 relevant tech internships.

I’m starting to plan my job hunt and would love to hear from working data scientists or others in the field:

* Should I be applying in bulk to everything I qualify for, or focus on tailoring my resume with ATS keywords?
* Are there other strategies that helped you break into the field?
* What do you wish someone had told you when you were job hunting?
* Is it even heard of fresh graduates landing data roles?

I know the market’s tough right now, so I want to be as strategic as possible. Any advice is appreciated — thanks!",187,0.91,https://www.reddit.com/r/datascience/comments/1ljsd1j/graduating_soon_any_tips_for_landing_an/,False,True,False
1ljs8wq,titiboa,1750812844.0,11,/r/datascience/comments/1ljs8wq/masters_in_dscsmlai_inquiry/,datascience,Masters in DS/CS/ML/AI inquiry,"For those of you that had a BS in CS then went to pursue a masters degree in CS, Ai, ML or similar how much was the benefit of this masters? 

Were there things you learned besides ML theory and application that you could not have learned in the industry?

Did this open additional doors for you versus just working as a data scientist or ML engineer without a masters?

Thanks",10,0.78,https://www.reddit.com/r/datascience/comments/1ljs8wq/masters_in_dscsmlai_inquiry/,False,True,False
1ljp64t,titiboa,1750804481.0,22,/r/datascience/comments/1ljp64t/how_much_time_do_you_spend_designing_your_mlds/,datascience,How much time do you spend designing your ML/DS problems before starting?,"Not sure if this is a low effort question but working in the industry I am starting to think I am not spending enough time designing the problem by addressing how I will build training, validation, test sets. Identifying the model candidates. Identifying sources of data to build features. Designing end to end pipeline for my end result to be consumed.

In my opinion this is not spoken about enough and I am curious how much time some of you spend and what you focus to address?

Thanks",18,0.86,https://www.reddit.com/r/datascience/comments/1ljp64t/how_much_time_do_you_spend_designing_your_mlds/,False,True,False
1ljiuzx,Daniel-Warfield,1750789557.0,7,/r/datascience/comments/1ljiuzx/a_breakdown_of_rag_vs_cag/,datascience,A Breakdown of RAG vs CAG,"I work at a company that does a lot of RAG work, and a lot of our customers have been asking us about CAG. I thought I might break down the difference of the two approaches.

RAG (retrieval augmented generation) Includes the following general steps:

* retrieve context based on a users prompt
* construct an augmented prompt by combining the users question with retrieved context (basically just string formatting)
* generate a response by passing the augmented prompt to the LLM

We know it, we love it. While RAG can get fairly complex (document parsing, different methods of retrieval source assignment, etc), it's conceptually pretty straight forward.

[A conceptual diagram of RAG, from an article I wrote on the subject \(IAEE RAG\).](https://preview.redd.it/izh2zrta0x8f1.png?width=800&format=png&auto=webp&s=2beb6557c45ffc3221a6d0cda78d5674ffddb487)

CAG, on the other hand, is a bit more complex. It uses the idea of LLM caching to pre-process references such that they can be injected into a language model at minimal cost.

First, you feed the context into the model:

[Feed context into the model. From an article I wrote on CAG \(IAEE CAG\).](https://preview.redd.it/5zw54o9j1x8f1.png?width=1500&format=png&auto=webp&s=27e46efa7ef7a467834558c511954f603b94f224)

Then, you can store the internal representation of the context as a cache, which can then be used to answer a query.

[pre-computed internal representations of context can be saved, allowing the model to more efficiently leverage that data when answering queries. From an article I wrote on CAG \(IAEE CAG\).](https://preview.redd.it/jfznfh2p1x8f1.png?width=1456&format=png&auto=webp&s=da7c17029235ca3fceaa2880a14f095badef9bb3)

So, while the names are similar, CAG really only concerns the augmentation and generation pipeline, not the entire RAG pipeline. If you have a relatively small knowledge base you may be able to cache the entire thing in the context window of an LLM, or you might not.

Personally, I would say CAG is compelling if:

* The context can always be at the beginning of the prompt
* The information presented in the context is static
* The entire context can fit in the context window of the LLM, with room to spare.

Otherwise, I think RAG makes more sense.

If you pass all your chunks through the LLM prior, you can use CAG as caching layer on top of a RAG pipeline, allowing you to get the best of both worlds (admittedly, with increased complexity).

[From the RAG vs CAG article.](https://preview.redd.it/lc6ku69g3x8f1.png?width=1880&format=png&auto=webp&s=01c59fae3b9daf0b1554a5cb139375fed353d570)

I filmed a [video](https://www.youtube.com/watch?v=HqJ-KDPE6PY) recently on the differences of RAG vs CAG if you want to know more.

Sources:  
\- [RAG vs CAG video](https://www.youtube.com/watch?v=HqJ-KDPE6PY)  
\- [RAG vs CAG Article](https://www.eyelevel.ai/post/rag-vs-cag)  
\- [RAG IAEE](https://iaee.substack.com/p/retrieval-augmented-generation-intuitively-and-exhaustively-explain-6a39d6fe6fc9?utm_source=publication-search)  
\- [CAG IAEE](https://iaee.substack.com/p/cache-augmented-generation-intuitively?utm_source=publication-search)",44,0.92,https://www.reddit.com/r/datascience/comments/1ljiuzx/a_breakdown_of_rag_vs_cag/,False,True,False
1ljhuda,thro0away12,1750787278.0,24,/r/datascience/comments/1ljhuda/how_to_tell_the_difference_between_whether/,datascience,How to tell the difference between whether managers are embracing reality of AI or buying into hype?,"I work in data science with a skillset that comprises of data science, data engineering and analytics. My team seems to want to eventually make my role completely non-technical (I'm not sure what a non-technical role would entail). The reason is because there's a feeling all the technical aspects will be completely eliminated by AI. The rationale, in theory, makes sense - we focus on the human aspects of our work, which is to develop solutions that can clearly be transferred to a fully technical team or AI to do the job for us. 

The reality in my experience is that this makes a strong assumptions data processes have the capacity to fit cleanly and neatly into something like a written prompt that can easily be given to somebody or AI with no 'context' to develop. I don't feel like in my work, our processes are there yet....like at all. Some things, maybe, but most things no. I also feel I'm navigating a lot of ever evolving priorities, stakeholder needs, conflicting advice (do this, no revert this, do this, rinse, repeat). This is making my job honestly frustrating and burning me out FAST. I'm working 12 hour days, sometimes up to 3 AM. My technical skills are deteriorating and I feel like my mind is becoming into a fried egg. Don't have time or energy to do anything to upskill.

On one hand, I'm not sure if management has a point - if I let go of the 'technical' parts that I like b/c of AI and instead just focus on more of the 'other stuff', would I have more growth, opportunity and salary increase in my career? Or is it better off to have a balance between those skills and the technical aspects? In an ideal world, I want to be able to have a good compromise between subject matter and technical skills and have a job where I get to do a bit of both. I'm not sure if the narrative I'm hearing is one of hype or reality. Would be interested in hearing thoughts. ",26,0.84,https://www.reddit.com/r/datascience/comments/1ljhuda/how_to_tell_the_difference_between_whether/,False,True,False
1ljhav8,Substantial_Tank_129,1750786074.0,31,/r/datascience/comments/1ljhav8/has_anyone_prepared_for_doordash_ds_interview/,datascience,Has anyone prepared for Doordash DS interview? Looking for tips and resources,"I have phone screen coming up in 2 weeks. I feel okay about SQL part, but I am quite worried about the product case study, particularly the questions that may include A/B testing. 

Do you have any resources for studying A/B testing to crack the interview?",41,0.91,https://www.reddit.com/r/datascience/comments/1ljhav8/has_anyone_prepared_for_doordash_ds_interview/,False,True,False
1ljg8dx,vaginedtable,1750783666.0,78,/r/datascience/comments/1ljg8dx/why_would_anyone_try_to_win_kaggles_challenges/,datascience,Why would anyone try to win Kaggle's challenges?,"Per title. Go to Kaggle right now and look at the top competitions featuring monetary prizes. Like you have to predict folded protein structures and polymers properties within 3 months? Those are ground breaking problems which to me would probably require years of academic effort without any guarantee of success. And IF you win you get what, 50000$, not even a year salary in most positions, and you have to split it with your team? Like even if you are capable of actually solving some of these challenges why would you ever share them as Kaggle public notebook or give IP to the challenge sponsor?",396,0.95,https://www.reddit.com/r/datascience/comments/1ljg8dx/why_would_anyone_try_to_win_kaggles_challenges/,False,True,False
1lirxxw,ElectrikMetriks,1750711566.0,17,/r/datascience/comments/1lirxxw/does_anybody_remember_the_old_python_logo/,datascience,"Does anybody remember the old Python logo?  Honestly, I've only been using Python since 2018, so I didn't recall that this ever existed.",,208,0.96,https://i.redd.it/v3elvap9oq8f1.png,False,False,False
1libni7,Safe_Hope_4617,1750668943.0,61,/r/datascience/comments/1libni7/which_workflow_to_avoid_using_notebooks/,datascience,Which workflow to avoid using notebooks?,"I have always used notebooks for data science.
I often do EDA and experiments in notebooks before refactoring it properly to module, api etc.

Recently my manager is pushing the team to move away from notebook because it favor bad code practice and take more time to rewrite the code.

But I am quite confused how to proceed without using notebook. 

How are you doing a data science project from eda, analysis, data viz etc to final api/reports without using notebook?

Thanks a lot for your advice.",92,0.92,https://www.reddit.com/r/datascience/comments/1libni7/which_workflow_to_avoid_using_notebooks/,False,True,False
1li722k,AutoModerator,1750651294.0,56,/r/datascience/comments/1li722k/weekly_entering_transitioning_thread_23_jun_2025/,datascience,"Weekly Entering & Transitioning - Thread 23 Jun, 2025 - 30 Jun, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",13,0.93,https://www.reddit.com/r/datascience/comments/1li722k/weekly_entering_transitioning_thread_23_jun_2025/,False,True,False
1li6e4v,Dry-Detective3852,1750649076.0,98,/r/datascience/comments/1li6e4v/would_you_do_this_job_if_you_were_rich_enough_to/,datascience,Would you do this job if you were rich enough to retire?,"Curious your perspective on this. Many of us got into the field because it was lucrative and ensures a stable living,

But it also is intrinsically interesting to study and challenge yourself. The personalities attracted to tech are often fun and make work not so bad. It’s fun to build, experiment, and be in a role where that is expected!

But what if you had enough money to retire? What would you do? Quit and do something else? Keep doing it? Consult? Curious your reasons and thoughts here!",99,0.88,https://www.reddit.com/r/datascience/comments/1li6e4v/would_you_do_this_job_if_you_were_rich_enough_to/,False,True,False
1lhuk01,Fl0wer_Boi,1750616001.0,280,/r/datascience/comments/1lhuk01/i_have_run_ds_interviews_and_wow/,datascience,I have run DS interviews and wow!,"Hey all,
I have been responsible for technical interviews for a Data Scientist position and the experience was quite surprising to me. I thought some of you may appreciate some insights.

A few disclaimers: I have no previous experience running interviews and have had no training at all so I have just gone with my intuition and any input from the hiring manager. As for my own competencies, I do hold a Master’s degree that I only just graduated from and have no full-time work experience, so I went into this with severe imposter syndrome as I do just holding a DS title myself. But after all, as the only data scientist, I was the most qualified for the task.

For the interviews I was basically just tasked with getting a feeling of the technical skills of the candidates. I decided to write a simple predictive modeling case with no real requirements besides the solution being a notebook. I expected to see some simple solutions that would focus on well-structured modeling and sound generalization. No crazy accuracy or super sophisticated models.

For all interviews the candidate would run through his/her solution from data being loaded to test accuracy. I would then shoot some questions related to the decisions that were made. This is what stood out to me:

1. Very few candidates really knew of other approaches to sorting out missing values than whatever approach they had taken. They also didn’t really know what the pros/cons are of imputing rather than dropping data. Also, only a single candidate could explain why it is problematic to make the imputation before splitting the data.

2. Very few candidates were familiar with the concept of class imbalance.

3. For encoding of categorical variables, most candidates would either know of label or one-hot and no alternatives, they also didn’t know of any potential drawbacks of either one.

4. Not all candidates were familiar with cross-validation

5. For model training very few candidates could really explain how they made their choice on optimization metric, what exactly it measured, or how different ones could be used for different tasks.

Overall the vast majority of candidates had an extremely superficial understanding of ML fundamentals and didn’t really seem to have any sense for their lack of knowledge.
I am not entirely sure what went wrong. My guesses are that either the recruiter that sent candidates my way did a poor job with the screening. Perhaps my expectations are just too unrealistic, however I really hope that is not the case. My best guess is that the Data Scientist title is rapidly being diluted to a state where it is perfectly fine to not really know any ML.
I am not joking - only two candidates could confidently explain all of their decisions to me and demonstrate knowledge of alternative approaches while not leaking data.

Would love to hear some perspectives. Is this a common experience?
",833,0.97,https://www.reddit.com/r/datascience/comments/1lhuk01/i_have_run_ds_interviews_and_wow/,False,True,False
1lgvh62,alpha_centauri9889,1750509150.0,19,/r/datascience/comments/1lgvh62/ml_case_study_rounds/,datascience,ML case study rounds,"I am asking this from context of interview. In almost every company these days, there is an ML case study round where the focus is on solving a real world case study. Idk if this is somewhat similar to ML system design or not (I think ML system design rounds are different or maybe part of case study round). Can anyone help me with resources to prepare from for this round? I am well-versed with ML theories, but never worked on solving an end to end solution from interview context. ",56,0.97,https://www.reddit.com/r/datascience/comments/1lgvh62/ml_case_study_rounds/,False,True,False
1lgt6nn,silverstone1903,1750500777.0,6,/r/datascience/comments/1lgt6nn/feature_interaction_constraints_in_gbms/,datascience,Feature Interaction Constraints in GBMs,"Hi everyone,

  
I'm curious if anyone here uses the `interaction_constraints` parameter in [XGBoost](https://xgboost.readthedocs.io/en/stable/tutorials/feature_interaction_constraint.html) or [LightGBM](https://lightgbm.readthedocs.io/en/latest/Parameters.html#interaction_constraints). In what scenarios do you find it useful and how do you typically set it up? Any real-world examples or tips would be appreciated, thanks in advance.",16,0.89,https://www.reddit.com/r/datascience/comments/1lgt6nn/feature_interaction_constraints_in_gbms/,False,True,False
1lggtm2,alexellman,1750458125.0,41,/r/datascience/comments/1lggtm2/what_is_your_opinion_on_julius_and_other_ai_first/,datascience,What is your opinion on Julius and other ai first data science tools?,"I’m wondering what people’s opinions are on Julius and similar tools (https://julius.ai/)

Have people tried them? Are they useful or end up causing more work?",6,0.6,https://www.reddit.com/r/datascience/comments/1lggtm2/what_is_your_opinion_on_julius_and_other_ai_first/,False,True,False
1lgfmli,SingerEast1469,1750454964.0,24,/r/datascience/comments/1lgfmli/toolkit_to_move_from_junior_to_senior_data/,datascience,Toolkit to move from junior to senior data analyst (data science track),"I would like to move from data analyst to senior data analyst (SDA) in the next year or so. I have a background in marketing, but pivoted to data science four years ago, and have been learning python since then. Most of my work nowadays is either data wrangling or dashboards, with more senior people doing advanced data science thingies like PCA.

This is a list of tools I think I would need to move from junior data analyst to senior data analyst. Any feedback on if SDA is the right person for these tools is much appreciated.

Extraction
- general pandas read (csv, parquet, json)
- gzip
- iterating through directories
- hosting on AWS / Google Cloud
- various other python packages like sqlite

Wrangling
- cleaning
- merging
- regex / search
- masking
- dtype conversion
- bucketing
- ML preprocessing (hash encoding, standardizing, feature selection)

Segmentation
- PCA / SVD / ICA
- k-means / DBSCAN
- itertools segmentation

Statistics
- descriptive statistics
- AB testing: t tests, ANOVAs, chi squared
- confidence intervals

Machine learning
- model selection
- hyperparameter tuning
- scoring
- inference

Visualization
- EDA visualizations in Jupyter Lab / Colab
- final visualizations in dashboards

Deployment
- deploy and host on AWS / Google Cloud

———

Things I think are simply out of the realm of any DA, senior or not:
- recommendation systems
- neural networks
- setting up an AB test on the back end

Curious what the community would bucket into data analyst, senior data analyst, or data scientist responsibilities.",58,0.87,https://www.reddit.com/r/datascience/comments/1lgfmli/toolkit_to_move_from_junior_to_senior_data/,False,True,False
1lgdg9j,MarcDuQuesne,1750449460.0,53,/r/datascience/comments/1lgdg9j/has_anyone_seen_research_or_articles_proving_that/,datascience,Has anyone seen research or articles proving that code quality matters in data science projects?,"Hi all,

I'm looking for articles, studies, or real-world examples backed by data that demonstrate the value of code quality specifically in data science projects.

Most of the literature I’ve found focuses on large-scale software projects, where the codebase is big (tens of thousands of lines), the team is large (10+ developers) the expected lifetime of the product is long (10+ years).

Examples: https://arxiv.org/pdf/2203.04374

In those cases the long-term ROI of clean code and testing is clearly proven. But data science is often different: small teams, high-level languages like Python or R, and project lifespans that can be quite short.

Alternatively, I found interesting recommandations like https://martinfowler.com/articles/is-quality-worth-cost.html (article is old, but recommandations still apply) but without a lot of data backing up the claims.


Has anyone come across evidence (academic or otherwise) showing that investing in code quality, no matter how we define it, pays off in typical data science workflows?



",23,0.64,https://www.reddit.com/r/datascience/comments/1lgdg9j/has_anyone_seen_research_or_articles_proving_that/,False,True,False
1lg60ju,toga287,1750431277.0,8,/r/datascience/comments/1lg60ju/how_to_build_a_usability_metric_that_is/,datascience,"How to build a usability metric that is ""normalized"" across flows?","Hey all, kind of a specific question here, but I've been trying to research approaches to this question and haven't found a reasonable solution. Basically, I work for a tech company with a user-facing product, and we want to build a metric which measures the usability of all our different flows.

I have a good sense of what metrics might represent usability (funnel conversion rate, time, survey scores, etc) but one request made is that the metric must be ""normalized"" (not sure if that's the right word). In other words, the usability score must be comparable across different flows. For example, conversion rate in an ""add payment"" section is always going to be lower than a ""learn about our features"" section - so to prioritize usability efforts we should have a score which accounts for this difference and measures usability on an ""objective"" scale that accounts for the expected gap between different flows.

Does anyone have any experience in building this kind of metric? Are there public analyses or papers I can read up on to understand how to approach this problem, or am I doomed? Thanks in advance!",3,1.0,https://www.reddit.com/r/datascience/comments/1lg60ju/how_to_build_a_usability_metric_that_is/,False,True,False
1lg5mrg,LambdaYeti,1750430314.0,120,/r/datascience/comments/1lg5mrg/ridiculous_offer_how_to_proceed/,datascience,"Ridiculous offer, how to proceed?","Hello All, after a very long struggle with landing my first data science job, I got a ridiculous offer and would like to know how to proceed. For context, I have 7 years of medtech experience, not specifically in data science but similar and an undergrad in stats and now a masters in data science. I am located in the US.

I've been talking with a company for months now and had several interviews even without a specific position available. Well they finally opened two positions, one associate and one senior with salary ranges of 66-99k and 130k-180k respectively. I applied for both and when HR got involved for the offer they said they could probably just split the difference for 110k. Sure that's fine. However, a couple days later, they called again and offered 60-70k, below even the lower limit of the associate range. So my question is has this happened to anyone else? Is this HR's way of trying to get me to just go away?

Maybe I'm just frustrated since HR said the salary range listed on the job req isn't actually what they are willing to pay",273,0.93,https://www.reddit.com/r/datascience/comments/1lg5mrg/ridiculous_offer_how_to_proceed/,False,True,False
1lg4t92,Daniel-Warfield,1750428242.0,18,/r/datascience/comments/1lg4t92/how_are_you_making_ai_applications_in_settings/,datascience,How are you making AI applications in settings where no external APIs are allowed?,"I've seen a lot of people build plenty of AI applications that interface with a litany of external APIs, but in environments where you can't send data to a third party, what are your biggest challenges of building LLM powered systems and how do you tackle them?

In my experience LLMs can be complex to serve efficiently, LLM APIs have useful abstractions like output parsing and tool use definitions which on-prem implementations can't use, RAG Processes usually rely on sophisticated embedding models which, when deployed locally, require the creation of hosting, provisioning, scaling, storing and querying vector representations. Then, you have document parsing, which is a whole other can of worms, and is usually critical when interfacing with knowledge bases in a regulated industry.

I'm curious, especially if you're doing On-Prem RAG for applications with large numbers of complex documents, what were the big issues you experienced and how did you solve them?",38,0.94,https://www.reddit.com/r/datascience/comments/1lg4t92/how_are_you_making_ai_applications_in_settings/,False,True,False
1lfp3ge,dopplegangery,1750375201.0,8,/r/datascience/comments/1lfp3ge/confidence_interval_width_vs_training_mape/,datascience,Confidence interval width vs training MAPE,"Hi, can anyone with strong background in estimation please help me out here? I am performing price elasticity estimation. I am trying out various levels to calculate elasticities on - calculating elasticity for individual item level, calculating elasticity for each subcategory (after grouping by subcategory) and each category level. The data is very sparse in the lower levels, hence I want to check how reliable the coefficient estimates are at each level, so I am measuring median Confidence interval width and MAPE. at each level. The lower the category, the lower the number of samples in each group for which we are calculating an elasticity. Now, the confidence interval width is decreasing for it as we go for higher grouping level i.e. more number of different types of items in each group, but training mape is increasing with group size/grouping level. So much so, if we compute a single elasticity for all items (containing all sorts of items) without any grouping, I am getting the lowest confidence interval width but high mape.

But what I am confused by is - shouldn't a lower confidence interval width indicate a more precise fit and hence a better training MAPE? I know that the CI width is decreasing because sample size is increasing for larger group size, but so should the residual variance and balance out the CI width, right (because larger group contains many type of items with high variance in price behaviour)? And if the residual variance due to difference between different type of items within the group is unable to balance out the effect of the increased sample size, doesn't it indicate that the inter item variability within different types of items isn't significant enough for us to benefit from modelling them separately and we should compute a single elasticity for all items (which doesn't make sense from common sense pov)?",8,0.91,https://www.reddit.com/r/datascience/comments/1lfp3ge/confidence_interval_width_vs_training_mape/,False,True,False
1lfdkws,fridchikn24,1750346678.0,9,/r/datascience/comments/1lfdkws/what_are_good_resources_to_learn_mleswe_concepts/,datascience,What are good resources to learn MLE/SWE concepts?,I'm struggling adapting my code and was wondering if there were any (preferably free) resources to further my understanding of the engineering way of creating ML pipelines.,24,0.82,https://www.reddit.com/r/datascience/comments/1lfdkws/what_are_good_resources_to_learn_mleswe_concepts/,False,True,False
1leyh9g,throwaway69xx420,1750296821.0,8,/r/datascience/comments/1leyh9g/splitting_up_modeling_in_project_amongst_ds_team/,datascience,Splitting Up Modeling in Project Amongst DS Team,"Hi! When it comes to modeling portion of a DS project, how does your team divy that part of the project among all the data scientist in your team?

I've been part of different teams and they've each done something different and I'm curious about how other teams have gone about it. I've had a boss who would have us all make one model and we just work off one model together. I've also had other managers who had us all work on our own models and we decide which one to go with based off RMSE.

Thanks!",15,0.9,https://www.reddit.com/r/datascience/comments/1leyh9g/splitting_up_modeling_in_project_amongst_ds_team/,False,True,False
1lewya2,WristbandYang,1750292292.0,38,/r/datascience/comments/1lewya2/what_tasks_dont_you_trust_zeroshot_llms_to_handle/,datascience,What tasks don’t you trust zero-shot LLMs to handle reliably?,"For some context I’ve been working on a number of NLP projects lately (classifying textual conversation data). Many of our use cases are classification tasks that align with our niche objectives. I’ve found in this setting that structured output from LLMs can often outperform traditional methods.

That said, my boss is now asking for likelihoods instead of just classifications. I haven’t implemented this yet, but my gut says this could be pushing LLMs into the “lying machine” zone. I mean, how exactly would an LLM independently rank documents and do so accurately and consistently? 

So I’m curious:

* What kinds of tasks have you found to be unreliable or risky for zero-shot LLM use?
* And on the flip side, what types of tasks have worked surprisingly well for you? ",72,0.95,https://www.reddit.com/r/datascience/comments/1lewya2/what_tasks_dont_you_trust_zeroshot_llms_to_handle/,False,True,False
1lenpta,Lamp_Shade_Head,1750269086.0,122,/r/datascience/comments/1lenpta/i_got_ghosted_after_8_interviews_why_do_companies/,datascience,I got ghosted after 8 interviews. Why do companies do this?,"I went through 7 rounds of interviews with a company, followed by a month of complete silence. Then the recruiter reached out asking me to do an additional round because of an organizational change — the role now had a new hiring manager. Since I had already invested so much time, I agreed to go through the 8th round.

After that, they kept stringing me along and eventually just ghosted me.

Not to make this a therapy session, but this whole experience has left me feeling really sad this past week. I spent months in this process, and they couldn’t even send a simple rejection email? How hard is that? I believe I was one of their top candidates — why else would they 
circle back a month after the initial rounds? How to get over this?

Edit: One more detail, they have been trying to fill this role for the last 6 months.",383,0.97,https://www.reddit.com/r/datascience/comments/1lenpta/i_got_ghosted_after_8_interviews_why_do_companies/,False,True,False
1leh4wm,FinalRide7181,1750253340.0,223,/r/datascience/comments/1leh4wm/my_data_science_dream_is_slowly_dying/,datascience,My data science dream is slowly dying,"
I am currently studying Data Science and really fell in love with the field, but the more i progress the more depressed i become.

Over the past year, after watching job postings especially in tech I’ve realized most Data Scientist roles are basically advanced data analysts, focused on dashboards, metrics, A/B tests. (It is not a bad job dont get me wrong, but it is not the direction i want to take)

The actual ML work seems to be done by ML Engineers, which often requires deep software engineering skills which something I’m not passionate about.

Right now, I feel stuck. I don’t think I’d enjoy spending most of my time on product analytics, but I also don’t see many roles focused on ML unless you’re already a software engineer (not talking about research but training models to solve business problems).

Do you have any advice?                                      

**Also will there ever be more space for Data Scientists to work hands on with ML or is that firmly in the engineer’s domain now? I mean which is your idea about the field?**",824,0.95,https://www.reddit.com/r/datascience/comments/1leh4wm/my_data_science_dream_is_slowly_dying/,False,True,False
1le3whp,Trick-Interaction396,1750207669.0,33,/r/datascience/comments/1le3whp/how_would_you_categorize_this_ds_skill/,datascience,How would you categorize this DS skill?,"I am DS with several YOE. My company had a problem with the billing system. Several people tried fixing it for a few months but couldn’t fix it.

I met with a few people and took notes. I wrote a few basic sql queries and threw the data into excel then had the solution after a few hours. This saved the company a lot of money.

I didn’t use ML or AI or any other fancy word that gets you interviews. I just used my brain. Anyone can use their brain but all those other smart people couldn’t figure it out so what is the “thing” I have that I can sell to employers.",68,0.89,https://www.reddit.com/r/datascience/comments/1le3whp/how_would_you_categorize_this_ds_skill/,False,True,False
1ldqozx,fark13,1750175425.0,28,/r/datascience/comments/1ldqozx/we_are_back_with_many_data_science_jobs_in_soccer/,datascience,"We are back with many Data science jobs in Soccer, NFL, NHL, Formula1 and more sports! 2025-06","Hey guys,

I've been silent here lately but many opportunities keep appearing and being posted.

These are a few from the last 10 days or so

* [Lead/Senior Quantitative Analyst, Predictive Modeling - Philadelphia Phillies](http://www.sportsjobs.online/jobs/8268-lead-senior-quantitative-analyst-predictive-modeling)
* [Vice President, Business Strategy & Analytics - Detroit Pistons](http://www.sportsjobs.online/jobs/8294-vice-president-business-strategy-analytics)
* [Data Scientist - Los Angeles Rams](http://www.sportsjobs.online/jobs/8288-data-scientist)
* [Data Engineer - Houston Texans](http://www.sportsjobs.online/jobs/8299-data-engineer)

A few Internships (hard to find!)

* [Software Engineer Intern - Dallas Mavericks](https://www.sportsjobs.online/jobs/8107-software-engineer-intern)
* [Business Strategy Internship - Nashville Predators](https://www.sportsjobs.online/jobs/8212-nashville-predators-business-strategy-internship-fall-2025-nhl)
* [Business Analytics Intern - Carolina Panthers](http://www.sportsjobs.online/jobs/8197-business-analytics-intern)

NBA Great jobs that were open (and closed applications quickly) but they appear !

* [Data Analyst - Miami Heat](http://www.sportsjobs.online/jobs/8255-data-analyst) \[Sold out\]
* [Applied Scientist, Basketball Analytics - Phoenix Suns](http://www.sportsjobs.online/jobs/8243-applied-scientist-basketball-analytics) \[Sold out\]

I run www.sportsjobs(.)online, a job board in that niche. In the last month I added around 300 jobs.

For the ones that already saw my posts before, I've added more sources of jobs lately. I'm open to suggestions to prioritize the next batch.

It's a niche, there aren't thousands of jobs as in Software in general but my commitment is to **keep improving a simple metric, jobs per month.** We always need some metric in DS..

I run also a newsletter to receive emails with jobs and interesting content on sports analytics (next edition tomorrow!)  
[https://sportsjobs-online.beehiiv.com/subscribe](https://sportsjobs-online.beehiiv.com/subscribe)

Finally, I've created also a [reddit community](https://www.reddit.com/r/sports_jobs/) where I post recurrently the openings if that's easier to check for you.

I hope this helps someone!",91,0.95,https://www.reddit.com/r/datascience/comments/1ldqozx/we_are_back_with_many_data_science_jobs_in_soccer/,False,True,False
1ld1nf6,ElectrikMetriks,1750100787.0,15,/r/datascience/comments/1ld1nf6/just_tell_them_you_work_with_models_let_them/,datascience,Just tell them you work with models.  Let them figure out the rest on their own.,,657,0.98,https://i.redd.it/wyx0k1xu7c7f1.png,False,False,False
1ld06j0,Daniel-Warfield,1750097448.0,66,/r/datascience/comments/1ld06j0/the_illusion_of_the_illusion_of_thinking/,datascience,"The Illusion of ""The Illusion of Thinking""","Recently, Apple released a paper called ""The Illusion of Thinking"", which suggested that LLMs may not be reasoning at all, but rather are pattern matching:

[https://arxiv.org/abs/2506.06941](https://arxiv.org/abs/2506.06941)

A few days later, A paper written by two authors (one of them being the LLM Claude Opus model) released a paper called ""The Illusion of the Illusion of thinking"", which heavily criticised the paper.

[https://arxiv.org/html/2506.09250v1](https://arxiv.org/html/2506.09250v1)

A major issue of ""The Illusion of Thinking"" paper was that the authors asked LLMs to do excessively tedious and sometimes impossible tasks; citing The ""Illusion of the Illusion of thinking"" paper:

>Shojaee et al.’s results demonstrate that models cannot output more tokens than their context limits allow, that programmatic evaluation can miss both model capabilities and puzzle impossibilities, and that solution length poorly predicts problem difficulty. These are valuable engineering insights, but they do not support claims about fundamental reasoning limitations.

>Future work should:

>1. Design evaluations that distinguish between reasoning capability and output constraints

>2. Verify puzzle solvability before evaluating model performance

>3. Use complexity metrics that reflect computational difficulty, not just solution length

>4. Consider multiple solution representations to separate algorithmic understanding from execution

>The question isn’t whether LRMs can reason, but whether our evaluations can distinguish reasoning from typing.

This might seem like a silly throw away moment in AI research, an off the cuff paper being quickly torn down, but I don't think that's the case. I think what we're seeing is the growing pains of an industry as it begins to define what reasoning actually is.

This is relevant to application developers, not just researchers. AI powered products are significantly difficult to evaluate, often because it can be very difficult to define what ""performant"" actually means.

(I wrote this, it focuses on RAG but covers evaluation strategies generally. I work for EyeLevel)  
[https://www.eyelevel.ai/post/how-to-test-rag-and-agents-in-the-real-world](https://www.eyelevel.ai/post/how-to-test-rag-and-agents-in-the-real-world)

I've seen this sentiment time and time again: LLMs, LRMs, and AI in general are more powerful than our ability to test is sophisticated. New testing and validation approaches are required moving forward.",25,0.69,https://www.reddit.com/r/datascience/comments/1ld06j0/the_illusion_of_the_illusion_of_thinking/,False,True,False
1lcpzzw,Double-Bar-7839,1750072135.0,20,/r/datascience/comments/1lcpzzw/yes_i_do_want_to_allow_this_app_to_make_changes/,datascience,"""Yes, I do want to allow this app to make changes to my device!""","DS's in mid-sized firms: do you have to wrestle with the constant “admin approval required” pop-ups? Is this really best practice?

I'm writing this in anger (sorry if that comes across!) but I feel like every time I stumble on anything remotely  cool or new, *BAM -* admin rights. 

I understand the security implication, but surely there's a better way. When I was at a large tech firm, this wasn't a thing - but I'm not sure if my laptop was truly unlocked, or if they had a clever workaround. 

1. Is it reasonable/possible to ask IT to carve out an exception for the data science team. If you've manage this, what arguments or evidence actually worked? 
2. Is there a middle ground I don't know about?",65,0.92,https://www.reddit.com/r/datascience/comments/1lcpzzw/yes_i_do_want_to_allow_this_app_to_make_changes/,False,True,False
1lcjd6h,AutoModerator,1750046478.0,27,/r/datascience/comments/1lcjd6h/weekly_entering_transitioning_thread_16_jun_2025/,datascience,"Weekly Entering & Transitioning - Thread 16 Jun, 2025 - 23 Jun, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",5,0.86,https://www.reddit.com/r/datascience/comments/1lcjd6h/weekly_entering_transitioning_thread_16_jun_2025/,False,True,False
1lcemw6,Odd-One8023,1750031562.0,100,/r/datascience/comments/1lcemw6/dont_be_the_data_scientist_whos_in_love_with/,datascience,"Don’t be the data scientist who’s in love with models, be the one who solves real problems","work at a company with around 100 data scientists, ML and data engineers.

The most frustrating part of working with many data scientists and honestly, I see this on this sub all the time too, is how obsessed some folks are with using ML or whatever the latest SoTA causal inference technique is. Earlier in my career plus during my masters, I was exactly the same, so I get it.

But here’s the best advice I can give you: **don’t be that person.**

Unless you’re literally working on a product where ML is the core feature, **your job is basically being an internal consultant.** That means understanding what stakeholders actually want, challenging their assumptions when needed, and giving them something useful, not just something that will disappear into a slide deck or notebook. 

Always try and make something run in production, don’t do endless proof of concepts. If you’re doing deep dives / analysis, define success criteria of your initiatives, try and measure them (e.g., some of my less technical but awesome DS colleagues made their career of finding drivers of key KPIs, reporting them to key stakeholders and measuring improvement over time). In short, **prove you’re worth it**.

A lot of the time, that means building a dashboard. Or doing proper data/software engineering. Or using GenAI. Or whatever else some of my colleagues (and a loads of people on this sub) roll their eyes at.

Solve the problem. Use whatever gets the job done, not just whatever looks cool on a résumé.",850,0.95,https://www.reddit.com/r/datascience/comments/1lcemw6/dont_be_the_data_scientist_whos_in_love_with/,False,True,False
1lccbgj,PathalogicalObject,1750024895.0,3,/r/datascience/comments/1lccbgj/books_on_applied_data_science_for_b2b_marketing/,datascience,Books on applied data science for B2B marketing?,"There's this thread from 3 years ago: https://www.reddit.com/r/datascience/comments/ram75g/books_on_applied_data_science_for_b2b_marketing/

Unfortunately, it never got any book recommendations - I'm in pretty much the exact same position as the OP of the linked thread and am looking for resources that explain the best methods and provide practical how-tos for marketing science/data science applied to B2B marketing.",3,0.67,https://www.reddit.com/r/datascience/comments/1lccbgj/books_on_applied_data_science_for_b2b_marketing/,False,True,False
1lbksb6,Due-Duty961,1749939761.0,8,/r/datascience/comments/1lbksb6/creating_a_deepfake_identity_on_social_media_for/,datascience,creating a deepfake identity on Social media ( for good),"To avoid bullying on SM for my ideas, I want to replace my face with a deepfake ( not a real person, but I don t anyone to take it since i ll be using it all the time), what is the best way to do that? I already have ideas. but someone with deep knowledge will help me a lot. My pc also don t have gpu (amd rysen) so advice on that also will be helpful. thanks!",0,0.33,https://www.reddit.com/r/datascience/comments/1lbksb6/creating_a_deepfake_identity_on_social_media_for/,False,True,False
1lare33,MahaloMerky,1749849410.0,31,/r/datascience/comments/1lare33/data_annotation_spam/,datascience,"""Data Annotation"" spam","Anyone else's job search site just absolutely spammed by Data Annotation? If I look up Data, ML, AI, or anything similar in my area I get 2-3 pages of there job posting.",138,0.96,https://www.reddit.com/r/datascience/comments/1lare33/data_annotation_spam/,False,True,False
1l9w0ln,MamboAsher,1749757732.0,62,/r/datascience/comments/1l9w0ln/significant_humor/,datascience,Significant humor,"Saw this and found it hilarious , thought I’d share it here as this is one of the few places this joke might actually land. 


Datetime.now() + timedelta(days=4) ",2395,0.98,https://i.redd.it/dztfywc2wj6f1.jpeg,False,False,False
1l9q78x,No_Length_856,1749744128.0,131,/r/datascience/comments/1l9q78x/do_you_say_daytah_or_dahtah/,datascience,Do you say day-tah or dah-tah,"Grab the hornets nest, shake it, throw it, run!!!!",130,0.91,https://www.reddit.com/r/datascience/comments/1l9q78x/do_you_say_daytah_or_dahtah/,False,True,False
1l99bfz,Timely_Ad9009,1749689632.0,119,/r/datascience/comments/1l99bfz/get_dozens_of_messages_from_new_graduates_former/,datascience,Get dozens of messages from new graduates/ former data scientist  about roles at my organization. Is this a sign?,"Everyday I have  been getting more and more LinkedIn messages from people laid off from their analytics roles searching for roles from JPMorgan Chase to CVS, to name a few. Are we in for a downturn? This is making me nervous for my own role. This doesn’t even include all the new students who have just graduated.",223,0.96,https://www.reddit.com/r/datascience/comments/1l99bfz/get_dozens_of_messages_from_new_graduates_former/,False,True,False
1l8xqgf,santiviquez,1749660725.0,6,/r/datascience/comments/1l8xqgf/data_scientists_need_to_know_about_data_contracts/,datascience,Data scientists need to know about data contracts.,"Data contracts are these things that data engineers write to set up expectations of what the data looks like.

And who understands the expectations better than a data engineer? A data scientist with context about how the business works.

…But, most of us aren’t gonna write YAML files and glue contracts into pipelines.

We don’t do that kind of dirty job…

Still, if you want to stop data quality issues from showing up and impacting your machine learning models, contracts can still be the way to go.

Why? Because a good data contract connects two worlds:

• The business context you understand.

• The technical realities your team builds on.

That’s a perfect match for what great data scientists already do.",0,0.28,https://www.reddit.com/r/datascience/comments/1l8xqgf/data_scientists_need_to_know_about_data_contracts/,False,True,False
1l8vdvk,SummerElectrical3642,1749655123.0,130,/r/datascience/comments/1l8vdvk/what_do_you_hates_the_most_as_a_data_scientist/,datascience,What do you hates the most as a data scientist,"A bit of a rant here. But sometimes it feels like 90% of the time at my job is not about data science.  
I wonder if it is just me and my job is special or everyone is like this.

If I try to add up a project from end to end, may be there is 10-15% of really interesting modeling work.   
It looks something like this:  
- Go after different sources to get the right data - 20% (lot's of meeting)
- Clean the data - 20% (lot's of meeting to understand the data)
- Wrestling with some code issue, packages installation, old dependencies - 10%
- Data exploration, analysis, modeling - 10%
- validation & documentation - 10%
- Deployment, debugging deployment issues - 20%
- Some regular reporting, maintenance - 10%

How do things look like for you? I wonder if things are different depending on companies, industries etc..",234,0.91,https://www.reddit.com/r/datascience/comments/1l8vdvk/what_do_you_hates_the_most_as_a_data_scientist/,False,True,False
1l8kf9h,CantorFunction,1749618846.0,29,/r/datascience/comments/1l8kf9h/i_have_a_training_budget_of_250_usd_for_my_own/,datascience,I have a training budget of ~250 USD for my own professional development. What would you recommend I spend it on?,"Pretty much the title, but here are some details:

* As far as I know, the budget can be spent on things like books, courses, seminars - things like that (possible also cloud services, haven't found out about that one)
* As far as the skills I currently have, my educational background is in mathematics (master's degree level) and my work today is mainly in classical ML and NLP. In the past I also did some bio-medical modeling with non-linear ODE systems.
* However, the scope of both the budget and my interests are pretty much anything to do with data science, so hit me with anything you've got :). Also, whatever it is doesn't have to fit perfectly into the budget - I'm happy to purchase multiple things, not use all of it or dip into my own pocket if needed.
* I'm based in Melbourne, Australia, in case someone has an in-person thing to recommend

Appreciate all the help!",45,0.87,https://www.reddit.com/r/datascience/comments/1l8kf9h/i_have_a_training_budget_of_250_usd_for_my_own/,False,True,False
1l8gmy0,anomnib,1749606517.0,41,/r/datascience/comments/1l8gmy0/lyft_vs_pinterest_data_science/,datascience,Lyft vs Pinterest Data Science,"If you have some familiarity with both, how does Lyft compare with Pinterest for career growth both while inside the company and in terms of exit opportunities?",66,0.88,https://www.reddit.com/r/datascience/comments/1l8gmy0/lyft_vs_pinterest_data_science/,False,True,False
1l8e4iq,big_data_mike,1749599090.0,44,/r/datascience/comments/1l8e4iq/the_higher_ups_asked_me_for_an_analysis_and_it/,datascience,The higher ups asked me for an analysis and it worked.,"So I totally mean to brag here. Last week a group of directors said, “We suspect X is happening in the market, do we have data that demonstrates it?”

And I thought to myself, here we go again. I’ve got to wade through our data swamp then tell them we don’t have the data that tells the story they want.

Well I waded through the data swamp and the data was there. I made them a graph that definitively demonstrated that yes, X is happening as they suspected. It wasn’t super easy to figure out and it also didn’t require a super complex model to figure out either. ",522,0.97,https://www.reddit.com/r/datascience/comments/1l8e4iq/the_higher_ups_asked_me_for_an_analysis_and_it/,False,True,False
1l89f9z,Due-Appointment9582,1749587241.0,21,/r/datascience/comments/1l89f9z/no_internship_as_a_sophomore/,datascience,no internship as a sophomore,"i have sent hundreds of applications, but wasn't able to land an internship this summer. i think it's my experience, i switched from microbiology to stats/ds  a year ago, but was hoping to get something over the summer which would help me recruit in my junior year. genuinely heartbroken.

can anyone give me advice on what to do in the summer improve my experience? things i can do to add on my cv, i have absolutely no clue.

thank you!

  
edit: thank you guys so so much - actually - i am so grateful for your ideas! i will work on some projects in the summer, i've reached out to some professors for research opportunities (might be late, but no harm in trying ig!) and i will expand on my knowledge. you guys are awesome :)",18,0.67,https://www.reddit.com/r/datascience/comments/1l89f9z/no_internship_as_a_sophomore/,False,True,False
1l843cd,explorer_seeker,1749574972.0,23,/r/datascience/comments/1l843cd/vicious_circle_of_misplaced_expectations_with_pms/,datascience,Vicious circle of misplaced expectations with PMs and stakeholders,"Looking for opinions from experienced folks in DS.

Stuck in a vicious circle of misplaced expectations from stakeholders being agreed for delivery by PMs even without consulting DS to begin with. Then, those come to DS team to build because business stakeholders already know that is the solution they need/are missing - not necessarily true. So, that expectation functions like a feature in a front end application in the mind of a Product Manager - deterministic mode (not sure if it is agile or waterfall type of project management or whatever).

DS tries to do what is best possible but it falls short of what stakeholders expect - they literally say we thought some magic would happen through advanced data science!

PM now tries to do RCA to understand where things went wrong while continuing to play gallery to stakeholders unquestioningly. PM has difficulty understanding DS stuff and keeps telling to keep things non-technical while asking questions that are inherently technical! PM is more comfortable looking at data viz, React applications etc.

DS is to blame for not creating magic.

Meanwhile, users have other problems that could be solved by DA or DS but they lie unutilized because they are attached to Excel and Excel Macros. Not willing to share relevant domain inputs.

On loop.",23,0.94,https://www.reddit.com/r/datascience/comments/1l843cd/vicious_circle_of_misplaced_expectations_with_pms/,False,True,False
1l7spck,Bulky-Top3782,1749540931.0,26,/r/datascience/comments/1l7spck/what_masters_should_could_be_an_option_after_bsc/,datascience,What Masters should could be an option after B.Sc Data Science,"Hello,

I recently completed B.Sc Data Science in India. Was wondering which M.Sc should I go for after this.

Someone told me M.Sc Data Science but when I checked the syllabus, a lot of subjects are similar. Would it still be a good option? Or please help with different options as well ",0,0.37,https://www.reddit.com/r/datascience/comments/1l7spck/what_masters_should_could_be_an_option_after_bsc/,False,True,False
1l7knce,AdventurousAddition,1749513829.0,8,/r/datascience/comments/1l7knce/can_someone_explain_to_me_the_difference_between/,datascience,Can someone explain to me the difference between Fitting aggregation functions and regular old linear regression?,"They seem like basically the same thing? 
When would one prefer to use fitting aggregation functions?",12,0.84,https://www.reddit.com/r/datascience/comments/1l7knce/can_someone_explain_to_me_the_difference_between/,False,True,False
1l7cbkg,ElectrikMetriks,1749493467.0,53,/r/datascience/comments/1l7cbkg/what_if_we_inverted_that_chart/,datascience,"""What if we inverted that chart?""",,981,0.99,https://i.redd.it/ens3q2p02y5f1.png,False,False,False
1l77blf,santiviquez,1749481874.0,13,/r/datascience/comments/1l77blf/ml_monitoring_startup_nannyml_got_acquired_by/,datascience,ML monitoring startup NannyML got acquired by Soda Data Quality,,20,0.76,https://siliconcanals.com/brussels-soda-acquires-nannyml/,False,False,False
1l6vciq,AutoModerator,1749441704.0,51,/r/datascience/comments/1l6vciq/weekly_entering_transitioning_thread_09_jun_2025/,datascience,"Weekly Entering & Transitioning - Thread 09 Jun, 2025 - 16 Jun, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",10,0.87,https://www.reddit.com/r/datascience/comments/1l6vciq/weekly_entering_transitioning_thread_09_jun_2025/,False,True,False
1l6ac7v,phicreative1997,1749383532.0,8,/r/datascience/comments/1l6ac7v/you_can_now_automate_deep_dives_with_clear/,datascience,"You can now automate deep dives, with clear actionable recommendations based on data.",,0,0.36,https://medium.com/firebird-technologies/deep-analysis-your-new-superpower-for-insight-6a9244350a83,False,False,False
1l5tiqg,corgibestie,1749325798.0,37,/r/datascience/comments/1l5tiqg/what_is_your_domain_and_what_are_the_most/,datascience,What is your domain and what are the most important technical skills that help you stand out in your domain?,"Aside from soft skills and domain expertise, ofc those are a given.

I'm manufacturing-adjacent (closer to product development and validation). Design of experiments has been my most useful data-related skill. I'm always being asked ""We are doing test X to validate our process. Can you propose how to do it with less runs?"" Most of the other engineers in our team are familiar with the concept of DoE but aren't confident enough to generate or analyze it themselves, which is where my role typically falls into.",44,0.9,https://www.reddit.com/r/datascience/comments/1l5tiqg/what_is_your_domain_and_what_are_the_most/,False,True,False
1l5t9af,mcjon77,1749325087.0,66,/r/datascience/comments/1l5t9af/phd_vs_masters_prepared_data_scientist/,datascience,PhD vs Masters prepared data scientist expectations.,"Is there anything more that you expect from a data scientist with a PhD versus a data scientist with just a master's degree, given the same level of experience?

 For the companies that I've worked with, most data science teams were mixes of folks with master's degrees and folks with PhDs and various disciplines.

That got me thinking. As a manager or team member, do you expect more from your doctorally prepared data scientist then your data scientist with only Master's degrees? If so, what are you looking for?  

Are there any particular skills that data scientists with phds from a variety of disciplines have across the board that the typical Masters prepare data scientist doesn't have?

Is there something common about the research portion of a doctorate that develops in those with a PhD skills that aren't developed during the master's degree program? If so, how are they applicable to what we do as data scientists?",102,0.93,https://www.reddit.com/r/datascience/comments/1l5t9af/phd_vs_masters_prepared_data_scientist/,False,True,False
1l51ufd,oneohsevenam,1749239795.0,26,/r/datascience/comments/1l51ufd/data_analyst_vs_engineer_at_nonprofit/,datascience,Data analyst vs. engineer? At non-profit,"Hi all,

I am the only Data Analyst at a medium-sized company related to shared transportation (adjacent to Lime Scooter/Bike). I'm pretty early in my career (grad from college 3 years ago).

My role encompasses a LOT of responsibilities that aren't traditionally under ""data analyst"", the biggest of which being that I build and maintain all the data pipelines from our partner companies via API and webhooks to our own SQL database. This feels very much like the role of Data Engineer. From there, I use the SQL data to build dashboards / do analyses, etc, which is what I usually think of as ""Data Analyst"".

I am trying to argue for a raise (since data engineers are usually paid more than analysts), and I am trying to figure out if I should ask for a title change too. I'd like to have engineering somehow in it, but ""Data Engineer and Analyst"" doesn't sound great.

Does anyone have any experience or advice with this? Thanks!!",92,0.95,https://www.reddit.com/r/datascience/comments/1l51ufd/data_analyst_vs_engineer_at_nonprofit/,False,True,False
1l51gfr,chomoloc0,1749238784.0,9,/r/datascience/comments/1l51gfr/understanding_regression_discontinuity_design/,datascience,Understanding Regression Discontinuity Design,"In my latest blog post I break-down **regression discontinuity design** \- then I build it up again in an intuition-first manner. It will become clear why you really want to understand this technique (but, that there is never really free lunch)

[Here it is](https://towardsdatascience.com/regression-discontinuity-design-how-it-works-and-when-to-use-it/) @ Towards Data Science

**My own takeaways:**

1. Assumptions make it or break it - with RDD more than ever
2. LATE might be not what we need, but it'll be what we get
3. RDD and instrumental variables have lots in common. At least both are very ""elegant"".
4. Sprinkle covariates into your model very, very delicately or you'll do more harm than good
5. Never lose track of the question you're trying to answer, and never pick it up if it did not matter to begin with

I get it; you really can't imagine how you're going to read straight on for 40 minutes; no worries, you don't have to. Just make sure you don't miss part where I leverage results page cutoff (max. 30 items per page) to recover the causal effect of top-positions on conversion — for them e-commerce / online marketplace DS out there.",17,0.87,https://www.reddit.com/r/datascience/comments/1l51gfr/understanding_regression_discontinuity_design/,False,True,False
1l4txpv,petburiraja,1749220384.0,3,/r/datascience/comments/1l4txpv/bi_and_predictive_analytics_on_saas_data_sources/,datascience,BI and Predictive Analytics on SaaS Data Sources,"Hi guys,

Seeking advice on a best practices in data management using data from SaaS sources (e.g., CRM, accounting software). 

The goal is to establish robust business intelligence (BI) and potentially incorporate predictive analytics while keeping the approach lean, avoiding unnecessary bloating of components.

1. For data integration, would you use tools like Airbyte or Stitch to extract data from SaaS sources and load it into a data warehouse like Google BigQuery? Would you use Looker for BI and EDA, or is there another stack you’d suggest to gather all data in one place?

2. For predictive analytics, would you use BigQuery’s built-in ML modeling features to keep the solution simple or opt for custom modeling in Python? 

Appreciate your feedback and recommendations!",7,0.82,https://www.reddit.com/r/datascience/comments/1l4txpv/bi_and_predictive_analytics_on_saas_data_sources/,False,True,False
1l4b3t7,No_Length_856,1749158721.0,10,/r/datascience/comments/1l4b3t7/need_help_sorting_my_thoughts_about_current/,datascience,"Need help sorting my thoughts about current ""contract""","Just reaching out to industry veterans to see if anyone can offer me some level-headed advice. Maybe you've been in a similar situation and can tell me how you approached the issue. Maybe you've been on the other side of my situation and can offer me that perspective.

For context:  
I'm a new grad who has been struggling to find work for a while now. My fiancée mentioned my power BI experience to her boss (general manager) at work and that got the ball rolling on a small contract. I was thrilled. I would be reporting to the ops manager and she had plans for a solid 4 month contract. She takes her plan off to the owner who says he wants to start off with 1 BI report done in 35 hours as a test run as a sort of feasibility thing. I do up a solid report in 32 hours. Ops manager loves it. General manager likes it. Owner thinks I missed the mark. Damn. His feedback is that he doesn't like that he has to filter to get some of the information. He'd like pieces of it to be readily available and visible without having to click anything. I take this feedback and quickly add cards with the wanted measures. Not good enough, now he wants to see more without having to filter. Oh also, he wants all the info to be on one page and all viewable without having to scroll. I tried to tell him that's not the best way to use power BI multiple times, but he just kinda brushed me off and kept moving along every time. We get to a point where he's finally happy with this report. Now he wants to see the small approach we agreed upon applied to a new report so he can verify it from scratch without me needing to take more time to implement feedback after. So I get a new report to work on, and only 20 hours this time. It's an easier data set, so I'm able to blast through it pretty quick and I do it up with his own requested measures shown prominently all on one page, with some visuals for some more complex relationships. Nope. Somehow this one isn't good enough either, but now they have this document that they just keep adding little requests to. I've gone at this thing like 4 or 5 times now. It'll be good, so we move on to the next phase, but then I somehow miss the mark on that and have to go back to the first phase and incorporate new measures?!?!?

Now he keeps giving me these tiny 3 hour micro contracts and moving the goal posts while dangling a longer contract in front of me at the end of a long stick. It's gotten to the point that literally everything on the page is being fed by a measure so that he doesn't have to filter. Am I overreacting and is this a normal use of power BI? They're paying me dog shit too (bottom 1% for my area). I feel like telling them to all fuck off, but I need to navigate things appropriately so that it doesn't negatively impact my fiancée. I'm feeling massively disrespected and played, though. I feel like it goes against everything I've learned about the tool. I'm trying to be cooperative so I can land this contract while also trying to avoid being taken advantage of because I'm a new grad. 

Oh! Also, this dude said to the ops manager that he thought I was going to use up any extra safety time he gives me because I just want the hours. This is after I saved 3 hours on my first sprint and 6 hours on my second sprint. I don't understand what his issue is. Ops manager thinks he should just give me a solid contract but keeps making excuses for why we should just try one more time to meet his unrealistic wants. 

Typing all this out has helped me realize just how much I'm being screwed. I'm going to post it anyway cause I still want other people's feedback, but yeah, I see how spineless I'm being. It's just hard to walk away when I could really use the contract that they keep dangling, but I don't think it's ever coming.

Sorry if this reads like a scatterbrained mess of words. I'm just kinda shot gunning my thoughts out. Anything constructive you can offer is appreciated. Apologies if this is a topic that has been answered 1000 times.",11,0.78,https://www.reddit.com/r/datascience/comments/1l4b3t7/need_help_sorting_my_thoughts_about_current/,False,True,False
1l49208,smilodon138,1749153729.0,16,/r/datascience/comments/1l49208/humble_bundle_ml_genai_and_more_from_oreilly/,datascience,"Humble Bundle: ML, GenAI and more from O'Reilly",This 'pay what you want' [Humble Bundle](https://www.humblebundle.com/books/machine-learning-ai-and-bots-oreilly-2025-books) from O'Reilly is very GenAI leaning,84,0.96,https://www.reddit.com/r/datascience/comments/1l49208/humble_bundle_ml_genai_and_more_from_oreilly/,False,True,False
1l40tho,SummerElectrical3642,1749134250.0,279,/r/datascience/comments/1l40tho/what_is_the_best_ide_for_data_science_in_2025/,datascience,What is the best IDE for data science in 2025?,"Hi all,  
I am a ""old"" data scientists looking to renew my stacks. Looking for opinions on what is the best IDE in 2025.   
The other discussion I found was 1 year ago and some even older. 

So what do you use as IDE for data science (data extraction, cleaning, modeling to deployment)? What do you like and what you don't like about it? 

Currently, I am using JupyterLab:  
**What I like:**  
\- Native compatible with notebook, I still find notebook the right format to explore and share results  
\- %magic command  
\- Widget and compatible with all sorts of dataviz (plotly, etc)  
\- Export in HTML

**What I feel missing (but I wonder whether it is mostly because I don't know how to use it):**  
\- Debugging  
\- Autocomplete doesn't seems to work most of the time.   
\- Tree view of file and folder  
\- Comment out block of code ? (I remember it used to work but I don't know why it don't work anymore)  
\- Great integration of AI like Github Copilot

Thanks in advance and looking forward to read your thoughts.",166,0.88,https://www.reddit.com/r/datascience/comments/1l40tho/what_is_the_best_ide_for_data_science_in_2025/,False,True,False
1l3y3sd,turingincarnate,1749127043.0,9,/r/datascience/comments/1l3y3sd/introducing_the_mlsynth_app/,datascience,Introducing the MLSYNTH App,"Presumably most people here know Python, but either way, [here's an app](https://mlsynthapp.streamlit.app/about) for my mlsynth library. Now, you can run impact analysis models without needing to know Python, all you need to know is econometrics.",8,0.76,https://www.reddit.com/r/datascience/comments/1l3y3sd/introducing_the_mlsynth_app/,False,True,False
1l2lqs3,marblesandcookies,1748980877.0,6,/r/datascience/comments/1l2lqs3/follow_up_question_to_my_previous_post/,datascience,Follow up question to my previous post.,"Previous post: https://www.reddit.com/r/datascience/comments/1l1pm5w/am_i_walking_into_a_trap/


Hello everyone! Thank you so much for the comments on the previous post. It was very helpful to understand your view. I have a follow up question and want to hear your opinion:

I also have an offer to study computer science at University of Bristol. 

Would you rather:

Take the data science job with no direct mentoring for £33,000 pay

OR

Study an MSc for Computer Science (Conversion) at Bristol University
",0,0.43,https://www.reddit.com/r/datascience/comments/1l2lqs3/follow_up_question_to_my_previous_post/,False,True,False
1l2i3p2,Trick-Interaction396,1748972059.0,50,/r/datascience/comments/1l2i3p2/what_projects_are_in_high_demand/,datascience,What projects are in high demand?,"I have 15 YOE. Looking for new job after 7 years. I mostly do anomaly detection and data engineering. I have all the normal skills (ML, Spark, etc). All the postings say something like use giant list of tech skills to drive value but they don’t mention the actual projects.

What type of projects are you doing which are in high demand?",137,0.92,https://www.reddit.com/r/datascience/comments/1l2i3p2/what_projects_are_in_high_demand/,False,True,False
1l2g1rh,WhiteRaven_M,1748967304.0,401,/r/datascience/comments/1l2g1rh/why_am_i_not_getting_interviews/,datascience,Why am I not getting interviews?,,783,0.93,https://i.redd.it/gh451zoplq4f1.png,False,False,False
1l2f2ph,howMuchCheeseIs2Much,1748965010.0,9,/r/datascience/comments/1l2f2ph/ducklake_this_is_your_data_lake_on_acid/,datascience,DuckLake: This is your Data Lake on ACID,,35,0.93,https://www.definite.app/blog/ducklake,False,False,False
1l2bmqx,vaginedtable,1748956585.0,8,/r/datascience/comments/1l2bmqx/first_hitting_time_in_arima_models/,datascience,First Hitting Time in ARIMA models,"Hi everybody. I am learning about time series, starting from the simple ideas of autoregressive models. I kinda understand, intuitively, how these models define the conditional distribution of the value at the next timestep X\_t given all previous values, but I'm struggling to understand how can I use these models to estimate the day at which my time series crosses a certain threshold, or in other words the probability distribution of the random variable τ i.e. the first day at which the value X\_τ exceeds a certain threshold.

So far I've been following some well known online sources such as [https://otexts.com/fpp3/](https://otexts.com/fpp3/) and lots of google searches but I struggle to find a walkthrough of this specific problem with ARIMA models. Is it that uncommon? Or am I just stupid",37,0.95,https://www.reddit.com/r/datascience/comments/1l2bmqx/first_hitting_time_in_arima_models/,False,True,False
1l21w10,Impossible_Notice204,1748921427.0,60,/r/datascience/comments/1l21w10/your_first_job_matters_more_than_you_know_and/,datascience,"Your first job matters more than you know, and sometimes it matters more than an advanced degree","Your first job matters more than you know, and sometimes it matters more than a masters degree.

This is something myself and a few others have mentioned here however I find that this discussion still doesn't occur enough.

I'm in a position and have been for the last few years where I get to define the hiring pipeline.

Generally speaking, I pay way more attention to what someone has been doing for the last 4 years than what they have a degree in. If someone studied a BS in geoscience then did predictive analytics for GIS and environmental services and I just happen to be working at a financial firm that's interested in environment / services then when it comes to that person or the guy with a PhD in Industrial Engineering I'm taking the BS in geoscience.

Same thing in a less niche space, if I'm looking for someone who can come up with initiatives and drive them with business leaders then I'm generally looking for someone who did analytics at a supply chain / distribution company because they know how to stand up for themself, they're willing to work more / take ownership, etc.

It doesn't matter if you got an MS from Stanford if you do compliance analytics or data governance at a bank, you're now less desirable for many applied data science positions. This being said, many smaller companies are now getting to the point where they need data governance and there is a space for you to be a leader there.

Saying this because outside of research positions, the field you work in does impact how easy it is to tranistion to other roles.",333,0.86,https://www.reddit.com/r/datascience/comments/1l21w10/your_first_job_matters_more_than_you_know_and/,False,True,False
1l1uzi1,Comfortable-Image850,1748901471.0,32,/r/datascience/comments/1l1uzi1/how_do_i_manage_expectations_for_my_career_as_a/,datascience,How do I manage expectations for my career as a prospective data scientist,"Hey all,

I'm a recent MS Statistics graduate (Fall '24), who just finished undergrad (Spring '23) with no working and internship experience. Fortunately, I was able to land a data analyst position at a nonprofit company in March this year, but I am kind of missing the hands-on modeling (Bayesian Statistics, Econometrics, ML, Statistical Regression) and theoretical math (stochastic calculus/processes, ML, probability, Real Analysis) during my master's program.

I understand that given my lack of experience and entry level position, I am very luck to have a job, especially in this economy. However, I also do harbor disappointment in my outcomes, as I did apply for \~1000 jobs, and had more than 40 interviews for all types of positions (quant, data scientist, model validation analyst, data analyst, etc.) this year, but was beat out by people who probably have more relevant experience and technical skills.

I am thinking of applying this Fall/beginning of next year for some more modeling-heavy positions, but I am also wondering whether given the current economy and my unproven track record, I should realistically lower my expectations and evaluate other options (personal projects to sharpen my skills, PhD in a STEM field, working on a research project), and what I should focus on with my projects to improve myself as a candidate (domain knowledge, sound coding skills, implementation of new models). I would like to hear your thoughts and opinions about my future career goals.

Thanks",43,0.8,https://www.reddit.com/r/datascience/comments/1l1uzi1/how_do_i_manage_expectations_for_my_career_as_a/,False,True,False
1l1qvz5,SingerEast1469,1748891758.0,28,/r/datascience/comments/1l1qvz5/real_or_fake_pattern/,datascience,Real or fake pattern?,"I am doing some data analysis/engineering to uncover highly pure subnodes in a dataset, but am having trouble understanding something.

In this graph, each point represents a pandas mask, which is linked to a small subsample of the data. Subsamples range from 30-300 in size (overall dataset was just 2500). The x axis is the size of the sample, and the y axis is %pure, cutoff at 80% and rounded to 4 decimals. Average purity for the overall dataset is just under 29%. There is jitter on the x axis, as it’s an integrated with multiple values per label.

I cannot tell if these “ribbons”relationship is strictly due to integer division (?), as Claude would suggest, or if this is a pattern commonly found in segmentation, and each ribbon is some sub-cohort of a segment.

Has anyone seen these curved ribbons in their data before?",90,0.88,https://i.redd.it/6eeqoat3dk4f1.jpeg,False,False,False
1l1pm5w,marblesandcookies,1748888831.0,43,/r/datascience/comments/1l1pm5w/am_i_walking_into_a_trap/,datascience,Am I walking into a trap?,I have a job offer from a small company (UK based) under 50 employees. It's a data science job. However there is no direct mentoring involved and I would be the only data scientist in the company. I need a job but don't know if this is safe or not. ,85,0.92,https://www.reddit.com/r/datascience/comments/1l1pm5w/am_i_walking_into_a_trap/,False,True,False
1l1nm9m,Outside_Base1722,1748884257.0,31,/r/datascience/comments/1l1nm9m/how_do_you_teach_business_common_sense/,datascience,How do you teach business common sense?,"Really not the best way to start the week by finding out a colleague of mine CC'ed our internal-only model run reports to downstream team, which then triggered a chain of ppl requesting to be CC'ed for any future delivery.

We have an external report for that which said colleague has been sending out for an extended period of time.

Said colleague would also pull up code base and go line-by-line in a meeting with director-level business people. Different directors had, on multiple occasions, asked to not do that and give an abstraction only. This affects his perception despite the work underneath being solid. We're not toxic but you really can't expect high management to read your SQL code without them feeling like you're wasting their time.

This person works hard, has good intention, and can deliver if correctly understanding the task (which is in itself another battle). I'm not his manager, but he takes over the processes/pipelines I established so I'm still on the hook if things don't work.

I trust his work on the technical side but this corporate thing is really not clicking for him, and I really have no idea how do you put these ""common sense"" into someone's head.",59,0.93,https://www.reddit.com/r/datascience/comments/1l1nm9m/how_do_you_teach_business_common_sense/,False,True,False
1l1iud0,ElectrikMetriks,1748872984.0,28,/r/datascience/comments/1l1iud0/well_thats_one_way_to_waste_the_budget_on_tools/,datascience,"Well, that’s one way to waste the budget on tools that nobody will use...","AI Tools Deployed with Purpose = Great  
AI Tools Deployed without anyone Asking Why or What it's for = Useless",461,0.98,https://i.redd.it/lgtrvc63ti4f1.png,False,False,False
1l18ji8,AutoModerator,1748836941.0,23,/r/datascience/comments/1l18ji8/weekly_entering_transitioning_thread_02_jun_2025/,datascience,"Weekly Entering & Transitioning - Thread 02 Jun, 2025 - 09 Jun, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",4,0.84,https://www.reddit.com/r/datascience/comments/1l18ji8/weekly_entering_transitioning_thread_02_jun_2025/,False,True,False
1l10fes,hamed_n,1748813259.0,67,/r/datascience/comments/1l10fes/how_i_scraped_41_million_jobs_with_gpt4omini/,datascience,How I scraped 4.1 million jobs with GPT4o-mini,"**Background**: During my PhD in Data Science at Stanford, I got sick and tired of ghost jobs & 3rd party offshore agencies on LinkedIn & Indeed. So I wrote a script that fetches jobs from 100k+ company websites' career pages and uses GPT4o-mini to extract relevant information (ex salary, remote, etc.) from job descriptions. I made it publicly available here [https://hiring.cafe](https://hiring.cafe) and you can follow my progress and give me feedback at r/hiringcafe

**Tech details (from a DS perspective)**

1. Verifying legit companies. This I did manually, but it was crucial that I exclude any recruiting firms, 3rd party offshore agencies, etc. I manually sorted through the \~100,000 company career pages (this took several weeks) and picked the ones that looked legit. At Stanford, we call this technique ""occular regression"" :) 
2. Removing ghost jobs. I discovered that a strong predictor of if a job is a ghost job is that if it keeps being reposted. I was able to identify reposting by doing a embedding text similarity search for jobs from the same company. If 2 job descriptions overlap too much, I only show the date posted for the *earliest* listing. This allowed me to weed out most ghost jobs simply by using a date filter (for example, excluding any jobs posted over a month ago). 
3. Scraping fresh jobs 3x/day. To ensure that my database is reflective of the company career page, I check each company career page 3x/day. To avoid rate-limits, I used a rotating proxy from Oxylabs for now.
4. Building advanced NLP text filters. After playing with GPT4o-mini API, I realized I could can effectively dump raw job descriptions (in HTML) and ask it to give me back formatted information back in JSON (ex salary, yoe, etc). I used this technique to extract a variety of information, including technical keywords, job industry, required licenses & security clearance, if the company sponsors visa, etc.

**Question for the DS community:** Beyond job search, one thing I'm really excited about this 4.1 million job dataset is to be able to do a yearly or quarterly trend report. For instance, to look at what technical skills are growing in demand. What kinds of cool job trends analyses would you do if you had access to this data.

**Edit:** A few folks DMed asking to explore the data for job searching. I put together a minimal frontend to make the scraped jobs searchable: [https://hiring.cafe](https://hiring.cafe) — note that it's currently non-commercial, unsupported, just a PhD side-project at the moment until I gradute.

**Edit 2::** thank you for all the super positive comments. you can follow my progress on scraping more jobs on r/hiringcafe .Aalso to comments saying this is an ad, my full-time job is my phd, this is just a fun side project beofore I get an actual job haha",567,0.86,https://www.reddit.com/r/datascience/comments/1l10fes/how_i_scraped_41_million_jobs_with_gpt4omini/,False,True,False
1l0y4zo,hamed_n,1748807566.0,5,/r/datascience/comments/1l0y4zo/advice_on_processing_1m_jobsmonth_with_llama_for/,datascience,Advice on processing ~1M jobs/month with LLaMA for cost savings,"I'm using GPT-4o-mini to process \~1 million jobs/month. It's doing things like deduplication, classification, title normalization, and enrichment.

This setup is fast and easy, but the cost is starting to hurt. I'm considering distilling this pipeline into an open-source LLM, like LLaMA 3 or Mistral, to reduce inference costs, most likely self-hosted on GPU on Google Coud. 

Questions:

\* Has anyone done a similar migration? What were your real-world cost savings (e.g., from GPT-4o to self-hosted LLaMA/Mistral)

\* Any recommended distillation workflows? I'd be fine using GPT-4o to fine-tune an open model on our own tasks.

\* Are there best practices for reducing inference costs even further (e.g., batching, quantization, routing tasks through smaller models first)?

\* Is anyone running LLM inference on consumer GPUs for light-to-medium workloads successfully?

Right now, our GPT-4o-mini usage is costing me thousands/month (I'm paying for it out of pocket, no investors). Would love to hear what’s worked for others!



",14,0.71,https://www.reddit.com/r/datascience/comments/1l0y4zo/advice_on_processing_1m_jobsmonth_with_llama_for/,False,True,False
1l0wx56,Particular_Reality12,1748804506.0,9,/r/datascience/comments/1l0wx56/can_data_science_be_used_in_computer_networking/,datascience,Can data science be used in computer networking (if not can it be used in cybersecurity)?,"Hi, I’m a high schooler (junior year) who is extremely interested in data science to the point where it is the main career field I want to go into. However, I got enrolled in a program where we train and study for the CCNA and Network+, two prominent computer networking certifications that even adults in the field dont have. I’m taking the certifications next week so hopefully I pass both, but my heart is still in data science although i rlly dont want to waste these newly acquired skills. I know data science is a wide ranging topic that can be extended to multiple different fields, and the use of automation and AI being used in stuff like SDNs are increasing. I guess my question is if theres a solid career in data science with a computer networking background.

Additional question: I gotta start thinking of college so would I, if there is a possible path, major in data science and minor in computer networking?",15,0.73,https://www.reddit.com/r/datascience/comments/1l0wx56/can_data_science_be_used_in_computer_networking/,False,True,False
1l0fa7t,Trick-Interaction396,1748748731.0,54,/r/datascience/comments/1l0fa7t/what_is_your_functional_area/,datascience,What is your functional area?,"I don’t mean industry. I mean product, operations, etc. I work in operations. I don’t grow the business. I keep the business alive.",40,0.92,https://www.reddit.com/r/datascience/comments/1l0fa7t/what_is_your_functional_area/,False,True,False
1l0dsfl,atharv1525,1748743865.0,5,/r/datascience/comments/1l0dsfl/about_mcp_servers/,datascience,About MCP servers,Do anyone have tried MCP server with llm and rag? If anyone done please share the code ,2,0.56,https://www.reddit.com/r/datascience/comments/1l0dsfl/about_mcp_servers/,False,True,False
1l03bjn,guna1o0,1748714343.0,15,/r/datascience/comments/1l03bjn/help_choosing_a_book_for_learning_bayesian/,datascience,Help choosing a book for learning bayesian statistics in python,,22,0.92,/r/statistics/comments/1l02phw/d_help_choosing_a_book_for_learning_bayesian/,False,False,False
1kzpdnv,unserious1,1748668948.0,3,/r/datascience/comments/1kzpdnv/infra_dads_guidance_to_ramp_up/,datascience,"Infra DA/DS, guidance to ramp up?","Hello!

Just stepped into a new role as Lead DS for a team focused on infra analytics and data science. We'll be analyzing model training jobs/runs (I don't know what the data set is yet but assume it's resource usage, cost, and system logs) to find efficiency wins (think speed, cost, and even sustainability). We'll also explore automation opportunities down the line as subsequent projects.

This is my first time working at the infrastructure layer, and I’m looking to ramp up fast.

What I’m looking for:

- Go-to resources (books, papers, vids) for ML infra analytics

- What data you typically analyze (training logs, GPU usage, queue times, etc.)

- Examples of quick wins, useful dashboards, KPIs?

If you’ve done this kind of work I’d love to hear what helped you get sharp. Thanks!

Ps - I'm a 8 yr DS at this company. Company size, data, number of models, etc, is absolutely massive. Lmk what other info and I can amend this post. Thank you!",16,0.9,https://www.reddit.com/r/datascience/comments/1kzpdnv/infra_dads_guidance_to_ramp_up/,False,True,False
1kzkwcg,Sebyon,1748653977.0,5,/r/datascience/comments/1kzkwcg/validation_of_statistical_tooling_packages/,datascience,Validation of Statistical Tooling Packages,"Hey all,

I was wondering if anyone has any experience on how to properly validating statistical packages for numerical accuracy?

Some context: I've developed a Python package for internal use that can undertake all the statistics we require in our field for our company. The statistics are used to ensure compliance to regulatory guidelines. 

The industry standard is a globally shared maceo-free Excel sheet, that relies heavily on approximations to bypass VBA requirements. Because of this, edge cases will give different reaults. Examples include use of non-central t-distrubtion, MLE, infinite series calcuations, Shapiro-wilk. The sheet is also limited to 50 samples as the approximations end here.

Packages exist in R that do most of it (NADA, EnvStats, STAND, Tolerance). I could (and probably should have) make a package from these, but I'd still need to modify and develop some statistics from scratch, and my R skills are abysmal compared to Python.

From a software engineering point, for more math heavy code, is there best practices for validating the outputs? The issue is this Excel sheet is considered the ""gold standard"" and I'll need to justify differences.

I currently have two validation passes, one is a dedicated unit test with a small dataset that I have cross referenced and checked by hand, with exisiting R packages and with the existing notebook. This dataset I've picked tries to cover extremes at either side of the data ranges we get (Geo standard deviations > 5, massive skews, zero range, heavily censored datasets).

The second is a bulk run of a large datatset to tease out weird edge cases, but I haven't done the cross validations by hand unless I notice weird results.

Is there anything else that I should be doing, or need to consider?",12,0.94,https://www.reddit.com/r/datascience/comments/1kzkwcg/validation_of_statistical_tooling_packages/,False,True,False
1kzjr30,hamed_n,1748650539.0,1,/r/datascience/comments/1kzjr30/twostage_model_filter_for_webscale_document_triage/,datascience,Two‑stage model filter for web‑scale document triage?,"I am crawling roughly **20 billion** web pages, and trying to triage for the ones that are only job descriptions. Only about **5%** contain actual job advertisements. Running a Transformer over the whole corpus feels prohibitively expensive, so I am debating whether a **two‑stage pipeline** is the right move:

1. **Stage 1:** ultra‑cheap lexical model (hashing TF‑IDF plus Naive Bayes or logistic regression) on CPUs to toss out the obviously non‑job pages while keeping recall very high.
2. **Stage 2:** small fine‑tuned Transformer such as DistilBERT on a much smaller candidate pool to recover precision.

My questions for teams that have done large‑scale extraction or classification:

* Does the two‑stage approach really save enough money and wall‑clock time to justify the engineering complexity compared with just scaling out a single Transformer model on lots of GPUs?
* Any unexpected pitfalls with maintaining two models in production, feature drift between stages, or tokenization bottlenecks?
* If you tried both single‑stage and two‑stage setups, how did total cost per billion documents compare?
* Would you recommend any open‑source libraries or managed services that made the cascade easier?

",7,0.89,https://www.reddit.com/r/datascience/comments/1kzjr30/twostage_model_filter_for_webscale_document_triage/,False,True,False
1kzgvz0,Feeling-Carry6446,1748642695.0,20,/r/datascience/comments/1kzgvz0/bored_and_underutilized_how_to_prep_for_the_next/,datascience,Bored and underutilized - how to prep for the next gig?,"DS/BI team has had 4 different leaders in the past year and our company seems to have lost any sense of analytics strategy. Two years ago we had 16 total, BI devs and data scientists including ML specialists and ML app builders. We are now down to 7 after attrition and I know three more are actively interviewing. Last model put into production was in 2024 and there are no requests for ML work this fiscal year. Our project plans are now less than a sprint ahead and it is not unusual to get an analytical request in the morning only to be told by noon ""that's no longer a priority"".

It's been this way for long enough that I'm questioning whether I want to continue in DS or move to a related field. I have a background in databases and data engineering. i have done some work in Gen AI with prompt engineering and automation but it for my company because there is a zero trust policy on all Gen AI (thanks to an idiot who loaded the transcript from a VPs disciplinary call to chatGPT to get a summary). I am much more interested in probabilistic modeling and forecasting but again no experience outside of online classes. For all intensive purposes I have been a SQL dev with some Python for the last 4 years. The last model I put into production was an unsupervised model of workers by productivity at different roles, which was in 2022. 

Where should I go next? Seriously thinking about enrolling in a masters just to look fresh again.",32,0.89,https://www.reddit.com/r/datascience/comments/1kzgvz0/bored_and_underutilized_how_to_prep_for_the_next/,False,True,False
1kz8mmn,EarthGoddessDude,1748622490.0,48,/r/datascience/comments/1kz8mmn/president_taps_palantir_to_compile_data_on/,datascience,President Taps Palantir to Compile Data on Americans,No words,298,0.95,https://www.reddit.com/r/datascience/comments/1kz8mmn/president_taps_palantir_to_compile_data_on/,False,True,False
1kz6tnh,klaxonlet,1748618177.0,56,/r/datascience/comments/1kz6tnh/perfect_job_for_me_suffering_from_imposter/,datascience,Perfect job for me suffering from Imposter Syndrome,,1752,0.99,https://i.redd.it/geogp5wlrx3f1.jpeg,False,False,False
1kyr1va,Ciasteczi,1748565863.0,33,/r/datascience/comments/1kyr1va/regularizationmagic/,datascience,Regularization=magic?,"Everyone knows that regularization prevents overfitting when model is over-parametrized and it makes sense. But how is it possible that a regularized model performs better even when the model family is fully specified?

I generated data y=2+5x+eps, eps~N(0, 5) and I fit a model y=mx+b (so I fit the same model family as was used for data generation). Somehow ridge regression still fits better than OLS.

I run 10k experiments with 5 training and 5 testing data points. OLS achieved mean MSE 42.74, median MSE 31.79. Ridge with alpha=5 achieved mean MSE 40.56 and median 31.51.

I cannot comprehend how it's possible - I seemingly introduce bias without an upside because I shouldn't be able to overfit. What is going on? Is it some Stein's paradox type of deal? Is there a counterexample where unregularized model would perform better than model with any ridge_alpha?

Edit: well of course this is due to small sample and large error variance. That's not my question. I'm not looking for a ""this is a bias-variance tradeoff"" answer either. Im asking for intuition (proof?) why would a biased model ever work better in such case. Penalizing high b instead of high m would also introduce a bias but it won't lower the test error. But penalizing high m does lower the error. Why?",49,0.81,https://www.reddit.com/r/datascience/comments/1kyr1va/regularizationmagic/,False,True,False
1kymajf,anuveya,1748552953.0,17,/r/datascience/comments/1kymajf/anyone_working_for_public_organizations_publish/,datascience,Anyone working for public organizations publish open data?,"Hello everyone,

I'm conducting research on how public sector organizations manage and share data with the public. I'm particularly interested in understanding:

* **Which platforms or repositories do you use to publish open data?**
* **What types of data are you sharing with the public?**
* **What challenges have you faced in publishing and managing open data?**
* **Are there specific policies or regulations that guide your open data practices?**

Your insights will be invaluable in understanding the current landscape of open data practices in public organizations. Feel free to share as much or as little as you're comfortable with.

Thank you in advance for your contributions!",4,0.75,https://www.reddit.com/r/datascience/comments/1kymajf/anyone_working_for_public_organizations_publish/,False,True,False
1kyesak,WhatsTheAnswerDude,1748535099.0,46,/r/datascience/comments/1kyesak/did_any_certifications_or_courses_actually_make_a/,datascience,Did any certifications or courses actually make a difference or were great investments financially?,"Howdy folks,

Looking for some insights and feedback. Ive been working a new job for the last two months that pays me more than I was previously making, after being out of work for about 8 months. 

Nonetheless, I feel a bit funky as despite it being the best paying job Ive ever had-I also feel insanely disengaged from my job and not really all that engaged by my manager AT ALL and dont feel secure in it either. Its not nearly as kinetic and innovative of a role as I was sold.

So I wanted some feedback while I still had money coming in just in case something happens. 

**Were there or have there been any particular certifications or courses that you paid for, that REALLY made a difference for you in career opportunities at all? Just trying to make smart investments and money moves now in case anything happens and trying to think ahead.**",64,0.94,https://www.reddit.com/r/datascience/comments/1kyesak/did_any_certifications_or_courses_actually_make_a/,False,True,False
1kydj2t,Clicketrie,1748532083.0,16,/r/datascience/comments/1kydj2t/i_turned_a_real_machine_learning_project_into_a/,datascience,I turned a real machine learning project into a children's book,,67,0.83,https://i.redd.it/fsc08wqugq3f1.jpeg,False,False,False
1ky9jlz,mlbatman,1748521773.0,8,/r/datascience/comments/1ky9jlz/seeking_help_in_choosing_between_two_offers/,datascience,Seeking help in choosing between two offers.,"Hey Y'all,

Needed some inputs in choosing between two offers. I have tried to read similar thread before. 

**Company 1**: Some Fintech

**Position**: Senior Data Scientist

**Role**: Taking care of their models on databricks. Models like ARR modelling. Churn modelling etc.

**Other Important Factors**: Company 1 has 5 days in office. This is a new mandate to prevent previous misuse. You also have to be very social person. They have had rounds of layoffs and had hiring freeze and have started to hiring again. My interview experience was great and I can see myself being successful in this role. However, I havent practiced classic machine learning for a while. I surely can pick it up. I am only worried that this role will have no engineering work at all. No productionsining of models. I am not sure how this will be for my future roles.



**Company 2**: Some company which is actively using LLMs and Agentic approaches

**Position**: Senior Machine Learning Engineer

**Role**: Work with agentic AI and productionise and update LLMs

**My Preference** \- Work with a company with stability and in a position where I can grow long term.


**Other Important Factors**: This role is in line with my last role, my PhD and LLM experience. I have read tonnes of literature so I sort of feel prepared for this role but I feel worthless when I have to spend weeks to improve latency without touching LLMs. My technical round was also okayish in this company. They are doubling the team. They are a well established company too. 


-------------------------------

My last position was of a ML engineer and I think what I disliked is -- the position slowly slipping into too much backend work. I am a stronger data scientist by training but have a PhD in NLP application so know the other bit too. I do struggle a bit when it comes to productinising things but I have improved a lot and in a better place.

I guess what I want to ask is for folks who work at companies that have not yet implemented AI -- do you feel behind the industry or you have satisfied with the current trajectory ?

I honestly don't care about whether I work in NLP / AI or not, All I want is a peaceful job where I can do my best and grow. On one hand the ML engineer position seems to be very on the cutting edge of technology but I know at the end its going to be API call to some LLM with much boiler plate code and many tools. The data scientist position looks like something I have done in the past and now should leave and do progress to ML engineering. 

Advice ?",19,0.76,https://www.reddit.com/r/datascience/comments/1ky9jlz/seeking_help_in_choosing_between_two_offers/,False,True,False
1kxkne5,Karl_mstr,1748447594.0,27,/r/datascience/comments/1kxkne5/does_anyone_knows_a_nice_course_for_streamlit_apps/,datascience,Does anyone knows a nice course for Streamlit Apps?,"What's in the title, I wanna learn how to create a deploy apps using Streamlit and I wanted to know which courses do you suggest for it?",2,0.54,https://www.reddit.com/r/datascience/comments/1kxkne5/does_anyone_knows_a_nice_course_for_streamlit_apps/,False,True,False
1kxk5m1,guna1o0,1748446427.0,19,/r/datascience/comments/1kxk5m1/best_youtube_playlists_for_learning_causal/,datascience,Best youtube playlists for learning causal inference with Python?,"Hey folks,

Im starting to learn causal inference and want to understand both the theory and how to apply it using python. I’m comfortable with classical ML, but causal inference is new to me.

Looking for youtube playlists or videos that explain concepts like DAGs, DID, double ML, propensity scores, IPTW, etc., and ideally show practical examples using libraries like DoWhy, EconML, or CausalML.

im not very comfortable with books.

Also, is it even worth spending time learning causal inference in depth? Im planning to dig into Bayesian inference next, so curious if this is a good path.

Would really appreciate any suggestions. thanks!",73,0.96,https://www.reddit.com/r/datascience/comments/1kxk5m1/best_youtube_playlists_for_learning_causal/,False,True,False
1kxjfgz,Substantial_Tank_129,1748444692.0,77,/r/datascience/comments/1kxjfgz/how_to_stay_motivated_in_a_job_where_my_salary/,datascience,How to stay motivated in a job where my salary has remained flat for last 4 years and there’s no promotion in sight?,"I joined my current company 3.5 years ago during a hiring boom. I was excited about the role and contributed heavily, leading process improvements with real financial impact. Despite this, I’ve received 0% raises year after year, which has been discouraging.

I stayed motivated, hoping the role would benefit my long-term career. But since the last performance cycle, my enthusiasm has dropped. I don’t feel appreciated, and it worries me that I could be the first to go if layoffs happen.

I’ve asked for a promotion twice in the past two years, but only received vague feedback like “We haven’t set you up for success yet” or “Promotion isn’t just about performance.”

It’s frustrating to feel stuck in a job I once loved. I’ve started interviewing, though the market is tough — but I’ll keep at it. In the meantime, I’m not sure what to do next. Any advice?",192,0.97,https://www.reddit.com/r/datascience/comments/1kxjfgz/how_to_stay_motivated_in_a_job_where_my_salary/,False,True,False
1kwycfm,Fit-Employee-4393,1748378769.0,16,/r/datascience/comments/1kwycfm/the_ds_industry_is_turning_into_the_investment/,datascience,The DS industry is turning into the investment banking industry,"Seems like the DS industry is essentially becoming a reflection of investment banking at places like Goldman Sachs or JP Mo. To get a job in the investment banking world you need to either: know someone high up at the company, have gone to a prestigious school, have experience at a different prestigious institution or transfer into the role internally.

How is this different from the current state of DS? Sure, it’s still possible to get a job based purely off skills, experience and raw dogging a job application, but it’s unlikely considering you are battling against ~800 resumes filled with exaggerations and lies for each job posting. Some companies don’t even put out job positions and choose to hire from their network instead, similar to IB. Merit based hiring seems like a thing of the past at this point.",0,0.42,https://www.reddit.com/r/datascience/comments/1kwycfm/the_ds_industry_is_turning_into_the_investment/,False,True,False
1kwh7bw,jameslee2295,1748330702.0,9,/r/datascience/comments/1kwh7bw/seeking_advice_how_to_scale_ai_models_without/,datascience,Seeking Advice: How To Scale AI Models Without Huge Upfront Investment?,"Hey folks,  
Our startup is exploring AI-powered features but building and managing GPU clusters is way beyond our current budget and expertise. Are there good cloud services that provide ready-to-use AI models via API?Anyone here used similar “model APIs” to speed up AI deployment and avoid heavy infrastructure? Insights appreciated!",11,0.87,https://www.reddit.com/r/datascience/comments/1kwh7bw/seeking_advice_how_to_scale_ai_models_without/,False,True,False
1kwd8vj,honwave,1748315931.0,68,/r/datascience/comments/1kwd8vj/with_ds_layoffs_happening_everydaywhats_the_future/,datascience,"With DS layoffs happening everyday,what’s the future ?",I am a freelancer Data Scientist and finding it extremely hard to get projects. I understand the current environment in DS space with layoffs happening all over the place and even the Director of AI @ Microsoft was laid off. I would love to hear from other Redditors about it. I’m currently extremely scared about my future as I don’t know if I’ll get projects. ,169,0.89,https://www.reddit.com/r/datascience/comments/1kwd8vj/with_ds_layoffs_happening_everydaywhats_the_future/,False,True,False
1kw1cik,jinstronda,1748283218.0,144,/r/datascience/comments/1kw1cik/am_i_the_only_one_who_truly_love_this_field_it/,datascience,Am i the only one who truly love this field? It sounds like everyone here is in for the money and hate their jobs,it's funny because in real life most of the people i know in the field love it,1900,0.97,https://i.redd.it/1cgu2smi363f1.jpeg,False,False,False
1kvzd73,xSicilianDefenderx,1748278515.0,25,/r/datascience/comments/1kvzd73/thinking_of_switching_from_data_scientist_to_data/,datascience,Thinking of switching from Data Scientist to Data Product Owner — need advice,"Hey everyone,
I’ve been working as a Data Scientist for the past 5 years, currently at a bank. I’ll be honest — this might sound a bit harsh, but it’s just how I personally feel: this job is slowly draining me.

Most of the models I build never make it to production. A big chunk of my time is spent doing analysis that feels more like trying to impress higher-ups than solving real problems. And with AI evolving so rapidly, there’s this growing pressure to “level up” to a senior role — but the bar is so high now, and the opportunities seem fewer and harder to reach. It’s honestly demotivating.

So, I’m thinking about pivoting into a Data Product Owner (or Product Manager) role. I feel like my experience could bridge the gap between business and technical teams — I can speak the language of data engineers, ML engineers, and data scientists. Plus, I’d love to be in a role that’s more collaborative and human-facing. It also feels like a safer long-term path in this AI-driven world.

Has anyone made a similar transition? Or is anyone here feeling the same way? I’d really appreciate any advice, feedback, or even just hearing your story. Totally open to different perspectives.

Thanks!
",100,0.93,https://www.reddit.com/r/datascience/comments/1kvzd73/thinking_of_switching_from_data_scientist_to_data/,False,True,False
1kvqn1s,Kellsier,1748253626.0,44,/r/datascience/comments/1kvqn1s/how_can_i_address_wild_expectations_about_gen_ai/,datascience,How can I address wild expectations about Gen AI and Agentic AI?,"Following what the title says, people in my company have gone ballistic on Agentic AI and Gen AI more broadly as of late. This sadly includes some of the IT management that should know better/temper out expectations on what these can/cannot do.

To be clear, I am not a hater either, I see them as useful techonologies that unlock new opportunities within my work. At the same time, I feel like all the non-experts (and in this case even my management which is supposed to be more knowledgeable but has been carried away from the hype and is not hands-on) have completely non-realistic expectations of what these tools can do.

Do any of you have experience with educating people on what is reasonable to expect in this context? I am a bit tired of having to debunk use case by use.",100,0.96,https://www.reddit.com/r/datascience/comments/1kvqn1s/how_can_i_address_wild_expectations_about_gen_ai/,False,True,False
1kvl43y,AutoModerator,1748232104.0,32,/r/datascience/comments/1kvl43y/weekly_entering_transitioning_thread_26_may_2025/,datascience,"Weekly Entering & Transitioning - Thread 26 May, 2025 - 02 Jun, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",3,0.81,https://www.reddit.com/r/datascience/comments/1kvl43y/weekly_entering_transitioning_thread_26_may_2025/,False,True,False
1kv3smi,FinalRide7181,1748183088.0,14,/r/datascience/comments/1kv3smi/can_you_explain_to_me_the_product_analytics_job/,datascience,Can you explain to me the product analytics job?,"I ve watched videos about Data Scientist Product Analytics but i still dont understand if the job would excite me. 

Can someone explain it more in depth so that i can understand if i like it? I like the data science job (i am pursuing a master in DS) but it seems that product analytics is very different in the sense that it is very focused on SQL.

Also is it interesting and does it involve a lot of problem solving?
Does it have a sort of path to PM?",11,0.7,https://www.reddit.com/r/datascience/comments/1kv3smi/can_you_explain_to_me_the_product_analytics_job/,False,True,False
1kuxcok,meni_s,1748160325.0,53,/r/datascience/comments/1kuxcok/2025_stack_check_which_dsml_tools_am_i_missing/,datascience,2025 stack check: which DS/ML tools am I missing?,"**Hi all,**

I work in ad-tech, where my job is to improve the product with data-driven algorithms, mostly on tabular datasets (CTR models, bidding, attribution, the usual).

Current work stack (quite classic I guess)

* pandas, numpy, scikit-learn, xgboost, statsmodels 
* PyTorch (light use) 
* JupyterLab & notebooks 
* matplotlib, seaborn, plotly for viz 
* Infra: everything runs on AWS (code is hosted on Github)

The news cycle is overflowing with LLM tools, I do use ChatGPT / Claude / Aider as helpers, but my main concern right now is the core DS/ML tooling that powers production pipelines.

So,  
What *genuinely awesome* 2024-25 libraries, frameworks, or services should I try, so I don’t get left behind? :)  
Any recommendations greatly appreciated, thanks!",141,0.97,https://www.reddit.com/r/datascience/comments/1kuxcok/2025_stack_check_which_dsml_tools_am_i_missing/,False,True,False
1kuu5g2,FinalRide7181,1748147523.0,34,/r/datascience/comments/1kuu5g2/is_it_worth_to_waste_a_year_to_do_cs/,datascience,Is it worth to waste a year to do CS?,"_(Yesterday i posted “is studying DS worth it” and it seemed that DS nowadays leads to product analytics which i dont enjoy. So i am considering to switch, it is a tough decision that is giving me troubles sleeping and concentrating on other stuff so i’d really like an helping hand from you guys)_
 
Guys I’m currently doing a 2 years Master in Business Analytics (Management + Data Science), but I’m considering switching to a Master in CS and ML. The downside is that I’d lose a year.

Here are some thoughts I’ve had so far:
With Business Analytics, I can access roles like:
- Data Scientist (but nowadays Data Scientists mostly do Product Analytics rather than ML, which doesn’t excite me)
- Management roles (but in tech it means mainly Sales, Marketing… less interesting to me. The exception is PM but it is very hard as a graduate)

So my questions are:

1) Does it make sense to lose a year to switch to CS+ML? My biggest fear is how AI is evolving and impacting the field. **This is the biggest fear i have, should i switch in the era of AI?**

 2) Am I undervaluing the opportunities from the Business Analytics Master? Especially regarding management roles, are there interesting options I’m missing?",0,0.44,https://www.reddit.com/r/datascience/comments/1kuu5g2/is_it_worth_to_waste_a_year_to_do_cs/,False,True,False
1kulk1b,Much_Discussion1490,1748119979.0,1,/r/datascience/comments/1kulk1b/found_a_really_amazing_video_providing_context_to/,datascience,"Found a really amazing video , providing context to the breakthrough as well as the misconceived hype around Alphaevolve","I am sure by now most of us would have seen or atleast heard about AlphaEvolve and it's many breakthroughs including the 4*4 MM improvement. While this was a fantastic step forward in constrained optimisation problems , a lot of the commentary around it in media was absolutely garbage.

The original paper is an amazing read, however I was scouring the internet to find videos by people who understood it at a better depth than I did. That's where I came across this gem. 

It's long watch at around 40 mins, but is extremely well structured and not too heavy on math ( grad level at best). Would highly recommend watching this!",20,0.89,https://www.youtube.com/watch?v=Z3hIHaV0P7w,False,False,False
1ku5qsq,NervousVictory1792,1748070942.0,21,/r/datascience/comments/1ku5qsq/fomo_at_workplace/,datascience,FOMO at workplace,Hii All. I have joined as a DS and this is my first job. The DS model which I am tasked  to improve and maintain does not adhere to the modern tech stack. It is just old school classical ML in R. It is not in production. We only maintain it in our local and show the stakeholders necessary numbers in quarterly meetings or whenever it is required. My concern is am I falling behind on skills by doing this. Especially seeing all the fancy tools and MLE buzzwords that is being thrown around in almost every DS application ?? If yes how can I develop those skills despite not having opportunities at my workplace. ,44,0.83,https://www.reddit.com/r/datascience/comments/1ku5qsq/fomo_at_workplace/,False,True,False
1ku31vc,FinalRide7181,1748060368.0,150,/r/datascience/comments/1ku31vc/is_studying_data_science_still_worth_it/,datascience,Is studying Data Science still worth it?,"Hi everyone, I’m currently studying data science, but I’ve been hearing that the demand for data scientists is decreasing significantly. I’ve also been told that many data scientists are essentially becoming analysts, while the machine learning side of things is increasingly being handled by engineers.

- Does it still make sense to pursue a career in data science or should i switch to computer science? I mean i dont think i want to do just AB tests for a living

- Also, are machine learning engineers still building models or are they mostly focused on deploying them?
",287,0.9,https://www.reddit.com/r/datascience/comments/1ku31vc/is_studying_data_science_still_worth_it/,False,True,False
1ktn5xx,CapraNorvegese,1748015919.0,3,/r/datascience/comments/1ktn5xx/6_degrees_of_separation/,datascience,6 degrees of separation,,0,0.42,https://i.redd.it/6wtkow73ie2f1.jpeg,False,False,False
1ksz870,Infinitrix02,1747941264.0,34,/r/datascience/comments/1ksz870/the_8020_guide_to_r_you_wish_you_read_years_ago/,datascience,The 80/20 Guide to R You Wish You Read Years Ago,"After years of R programming, I've noticed most intermediate users get stuck writing code that works but isn't optimal. We learn the basics, get comfortable, but miss the workflow improvements that make the biggest difference.

I just wrote up the handful of changes that transformed my R experience - things like:

* Why DuckDB (and data.table) can handle datasets larger than your RAM
* How renv solves reproducibility issues
* When vectorization actually matters (and when it doesn't)
* The native pipe |> vs %>% debate

These aren't advanced techniques - they're small workflow improvements that compound over time. The kind of stuff I wish someone had told me sooner.

Read the [full article here.](https://open.substack.com/pub/borkar/p/the-8020-guide-to-r-you-wish-you?r=2qg9ny&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true)

What workflow changes made the biggest difference for you?

P.S. Posting to help out a friend",291,0.96,https://www.reddit.com/r/datascience/comments/1ksz870/the_8020_guide_to_r_you_wish_you_read_years_ago/,False,True,False
1ksvnsk,Emergency-Agreeable,1747932689.0,48,/r/datascience/comments/1ksvnsk/you_will_help_build_and_deploy_scalable_solutions/,datascience,"""You will help build and deploy scalable solutions... not just prototypes""","Hi everyone,

I’m not exactly sure how to frame this, but I’d like to kick off a discussion that’s been on my mind lately.

I keep seeing data science job descriptions *(E2E) data science,* not just prototypes, but scalable, production-ready solutions. At the same time, they’re asking for an overwhelming tech stack: DL, LLMs, computer vision, etc. On top of that, E2E implies a whole software engineering stack too.

So, what does *E2E* really mean?

For me, the ""left end"" is talking to stakeholders and/or working with the WH. The ""right end"" is delivering three pickle files: one with the model, one with transformations, and one with feature selection. Sometimes, this turns into an API and gets deployed sometimes not. This assumes the data is already clean and available in a single table. Otherwise, you’ve got another automated ETL step to handle. (Just to note: I’ve never had write access to the warehouse. The best I’ve had is an S3 bucket.)

When people say “scalable deployment,” what does that really mean? Let’s say the above API predicts a value based on daily readings. In my view, the model runs daily, stores the outputs in another table in the warehouse, and that gets picked up by the business or an app. Is that considered scalable? If not, what is?

If the data volume is massive, then you’d need parallelism, Lambdas, or something similar. But is that my job? I could do it if I had to, but in a business setting, I’d expect a software  engineer to handle that.

Now, if the model is deployed on the edge, where exactly is the “end” of E2E then?

Some job descriptions also mention API ingestion, dbt, Airflow, basically full-on data engineering responsibilities.

The bottom line: Sometimes I read a JD and what it *really* says is:

“We want you to talk to stakeholders, figure out their problem, find and ingest the data, store it in an optimized medallion-model warehouse using dbt for daily ingestion and Airflow for monitoring. Then build a model, deploy it to 10,000 devices, monitor it for drift, and make sure the pipeline never breaks.

Meanwhile, in real life, I spend weeks hand-holding stakeholders, begging data engineers for read access to a table I should already have access to, and struggling to get an EC2 instance when my model takes more than a few hours to run. Eventually, we store the outputs after  more meetings with the DE.

Often, the stakeholder sees the prototype, gets excited, and then has no idea how to use it. The model ends up in limbo between the data team and the business until it’s forgotten. It just  feels like the ego boost of the week for the C guys.

Now, I’m not the fastest or the smartest. But when I try to do all this E2E in personal projects, it takes ages and that’s without micromanagers breathing down my neck. Just setting up ingestion and figuring out how to optimize the WH took me two weeks.

So... all I am asking am I stupid , am I missing something?  Do you all actually do all of this daily? Is my understanding off?

Really just hoping this kicks off a genuine discussion.

Cheers :)",84,0.95,https://www.reddit.com/r/datascience/comments/1ksvnsk/you_will_help_build_and_deploy_scalable_solutions/,False,True,False
1ksnwds,joshamayo7,1747911840.0,3,/r/datascience/comments/1ksnwds/hypothesis_testing_and_experimental_design/,datascience,Hypothesis Testing and Experimental Design,"Sharing my second ever blog post, covering experimental design and Hypothesis testing. 

I shared my first blog post here a few months ago and received valuable feedback, sharing it here so I can hopefully share some value and receive some feedback as well.",28,0.91,https://medium.com/@joshamayo7/an-introduction-to-hypothesis-testing-bd6da5b3ccaf,False,False,False
1ksev5p,ImGallo,1747877623.0,166,/r/datascience/comments/1ksev5p/is_the_traditional_data_scientist_role_dying_out/,datascience,Is the traditional Data Scientist role dying out?,"I've been casually browsing job postings lately just to stay informed about the market, and honestly, I'm starting to wonder if the classic ""Data Scientist"" position is becoming a thing of the past.

Most of what I'm seeing falls into these categories:

* Data Analyst/BI roles (lots of SQL, dashboards, basic reporting)
* Data Engineer positions (pipelines, ETL, infrastructure stuff)
* AI/ML Engineer jobs (but these seem more about LLMs and deploying models than actually building them)

What I'm *not* seeing much of anymore is that traditional data scientist role - you know, the one where you actually do statistical modeling, design experiments, and work through complex business problems from start to finish using both programming and solid stats knowledge.

It makes me wonder: are companies just splitting up what used to be one data scientist job into multiple specialized roles? Or has the market just moved on from needing that ""unicorn"" profile that could do everything?

For those of you currently working as data scientists - what does your actual day-to-day look like? Are you still doing the traditional DS work, or has your role evolved into something more specialized?

And for anyone else who's been keeping an eye on the job market - am I just looking in the wrong places, or are others seeing this same trend?

Just curious about where the field is heading and whether that broad, stats-heavy data scientist role still has a place in today's market.",521,0.96,https://www.reddit.com/r/datascience/comments/1ksev5p/is_the_traditional_data_scientist_role_dying_out/,False,True,False
1ks5jo6,potatotacosandwich,1747852852.0,41,/r/datascience/comments/1ks5jo6/those_of_you_who_interviewedworking_at_big/,datascience,"Those of you who interviewed/working at big tech/finance, how did you prepare for it? Need advice pls.","title. Im a data analyst  with \~3yoe currently work at a bank. lets say i have this golden time period where my work is low stress/pressure and I can put time into preparing for interviews. My goal is to get into FAANG/finance/similar companies in data science roles. How do I prepare for interviews? Did you follow a specific structure for certain companies? How/what did you allocate time into between analytics/sql/python, ML, GenAI(if at all) or other stuff and how did you prepare? Im good w sql, currently practicing ML and GenAI projects on python. I have very basic understanding of data engg from self projects. What metrics you use to determine where you stand?

I get the job market is shit but Im not ready anyway. My aim is to start interviewing by fall, say august/september. I'd highly appreciate any help i can get. thx. ",76,0.92,https://www.reddit.com/r/datascience/comments/1ks5jo6/those_of_you_who_interviewedworking_at_big/,False,True,False
1krl7kx,_hairyberry_,1747788946.0,3,/r/datascience/comments/1krl7kx/question_about_using_the_mle_of_a_distribution_as/,datascience,Question about using the MLE of a distribution as a loss function,"I recently built a model using a Tweedie loss function. It performed really well, but I want to understand it better under the hood. I'd be super grateful if someone could clarify this for me.

I understand that using a ""Tweedie loss"" just means using the negative log likelihood of a Tweedie distribution as the loss function. I also already understand how this works in the simple case of a linear model f(x\_i) = wx\_i, with a normal distribution negative log likelihood (i.e., the RMSE) as the loss function. You simply write out the likelihood of observing the data {(x\_i, y\_i) | i=1, ..., N}, given that the target variable y\_i came from a normal distribution with mean f(x\_i). Then you take the negative log of this, differentiate it with respect to the parameter(s), w in this case, set it equal to zero, and solve for w. This is all basic and makes sense to me; you are finding the w which maximizes the likelihood of observing the data you saw, given the assumption that the data y\_i was drawn from a normal distribution with mean f(x\_i) for each i.

What gets me confused is using a more complex model and loss function, like LightGBM with a Tweedie loss. I figured the exact same principles would apply, but when I try to wrap my head around it, it seems I'm missing something.

In the linear regression example, the ""model"" is y\_i \~ N(f(x\_i), sigma\^2). In other words, you are assuming that the response variable y\_i is a linear function of the independent variable x\_i, plus normally distributed errors. But how do you even write this in the case of LightGBM with Tweedie loss? In my head, the analogous ""model"" would be y\_i \~ Tw(f(x\_i), phi, p), where f(x\_i) is the output of the LightGBM algorithm, and f(x\_i) takes the place of the mean mu in the Tweedie distribution Tw(u, phi, p). Is this correct? Are we always just treating the prediction f(x\_i) as the mean of the distribution we've assumed, or is that only coincidentally true in the special case of a linear model with normal distribution NLL?",7,1.0,https://www.reddit.com/r/datascience/comments/1krl7kx/question_about_using_the_mle_of_a_distribution_as/,False,True,False
1krkxl4,Proof_Wrap_2150,1747788068.0,22,/r/datascience/comments/1krkxl4/have_you_ever_wondered_what_comes_next_once_youve/,datascience,"Have you ever wondered, what comes next? Once you’ve built the model or finished the analysis, how do you take the next step? Whether it’s turning it into an app, a tool, a product, or something else?","For those of you working on personal data science projects, what comes after the .py script or Jupyter notebook? 

I’m trying to move beyond exploratory work into something more usable or shareable. 

Is building an app the natural next step? 

What paths have you taken to evolve your projects once the core analysis or modeling was done?",25,0.82,https://www.reddit.com/r/datascience/comments/1krkxl4/have_you_ever_wondered_what_comes_next_once_youve/,False,True,False
1krc89b,Emuthusiast,1747765491.0,115,/r/datascience/comments/1krc89b/no_ds_job_after_degree/,datascience,No DS job after degree,"Hi everyone, 
This may be a bit of a vent post. I got a few years in DS experience as a data analyst and then got my MSc in well ranked US school. For some reason beyond my knowledge, I’ve never been able to get a DS job after the MS degree. I got a quant job where DS is the furthest thing from it even though some stats is used, and I am now headed to a data engineering fellowship with option to renew for one more year max. I just wonder if any of this effort was worth it sometimes . I’m open to any advice or suggestions because it feels like I can’t get any lower than this.
Thanks everyone 

Edit : thank you everyone for all the insights and kind words!!!",268,0.93,https://www.reddit.com/r/datascience/comments/1krc89b/no_ds_job_after_degree/,False,True,False
1krau6q,Beginning-Sport9217,1747762232.0,27,/r/datascience/comments/1krau6q/are_there_any_math_tests_that_test_mathematical/,datascience,Are there any math tests that test mathematical skill for data science?,I am looking for a test which can test one’s math skills that are relevant for data science- that way I can understand which areas I’m weak in and how I measure relative to my peers. Is anybody aware of anything like that?,52,0.9,https://www.reddit.com/r/datascience/comments/1krau6q/are_there_any_math_tests_that_test_mathematical/,False,True,False
1kr41wk,Flaky_Literature8414,1747745406.0,6,/r/datascience/comments/1kr41wk/i_scrape_faang_data_science_jobs_from_the_last/,datascience,I Scrape FAANG Data Science Jobs from the Last 24h and Email Them to You,"I built a tool that scrapes fresh data science, machine learning, and data engineering roles from FAANG and other top tech companies’ official career pages — no LinkedIn noise or recruiter spam — and emails them straight to you.

What it does:

* Scrapes jobs directly from sites like Google, Apple, Meta, Amazon, Microsoft, Netflix, Stripe, Uber, TikTok, Airbnb, and more
* Sends daily emails with newly scraped jobs
* Helps you find openings faster – before they hit job boards
* Lets you select different countries like USA, Canada, India, European countries, and more

Check it out here:  
[https://topjobstoday.com/data-scientist-jobs](https://topjobstoday.com/data-scientist-jobs)

Would love to hear your thoughts or suggestions!",0,0.35,https://www.reddit.com/r/datascience/comments/1kr41wk/i_scrape_faang_data_science_jobs_from_the_last/,False,True,False
1kqkszs,ElectrikMetriks,1747683059.0,8,/r/datascience/comments/1kqkszs/but_i_still_put_a_ton_of_work_into_it/,datascience,"""But, I still put a ton of work into it...""",,506,0.98,https://i.redd.it/u5jq8q4zis1f1.png,False,False,False
1kqgxhb,Proof_Wrap_2150,1747674103.0,11,/r/datascience/comments/1kqgxhb/ive_modularized_my_jupyter_pipeline_into_py_files/,datascience,"I’ve modularized my Jupyter pipeline into .py files, now what? Exploring GUI ideas, monthly comparisons, and next steps!","I have a data pipeline that processes spreadsheets and generates outputs.

What are smart next steps to take this further without overcomplicating it?

I’m thinking of building a simple GUI or dashboard to make it easier to trigger batch processing or explore outputs.

I want to support month-over-month comparisons e.g. how this month’s data differs from last and then generate diffs or trend insights.

Eventually I might want to track changes over time, add basic versioning, or even push summary outputs to a web format or email report.

Have you done something similar? What did you add next that really improved usefulness or usability? And any advice on building GUIs for spreadsheet based workflows?

I’m curious how others have expanded from here",5,0.63,https://www.reddit.com/r/datascience/comments/1kqgxhb/ive_modularized_my_jupyter_pipeline_into_py_files/,False,True,False
1kq2lxu,AutoModerator,1747627293.0,62,/r/datascience/comments/1kq2lxu/weekly_entering_transitioning_thread_19_may_2025/,datascience,"Weekly Entering & Transitioning - Thread 19 May, 2025 - 26 May, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",3,1.0,https://www.reddit.com/r/datascience/comments/1kq2lxu/weekly_entering_transitioning_thread_19_may_2025/,False,True,False
1kpy8ha,CanYouPleaseChill,1747612995.0,53,/r/datascience/comments/1kpy8ha/study_looking_at_ai_chatbots_in_7000_workplaces/,datascience,"Study looking at AI chatbots in 7,000 workplaces finds ‘no significant impact on earnings or recorded hours in any occupation’",,873,0.98,https://fortune.com/2025/05/18/ai-chatbots-study-impact-earnings-hours-worked-any-occupation/,False,False,False
1kpj2cw,officialcrimsonchin,1747572088.0,184,/r/datascience/comments/1kpj2cw/are_data_science_professionals_primarily/,datascience,Are data science professionals primarily statisticians or computer scientists?,"Seems like there's a lot of overlap and maybe different experts do different jobs all within the data science field, but which background would you say is most prevalent in most data science positions?",261,0.92,https://www.reddit.com/r/datascience/comments/1kpj2cw/are_data_science_professionals_primarily/,False,True,False
1kp0grb,corgibestie,1747508364.0,8,/r/datascience/comments/1kp0grb/what_were_your_first_cloud_projects_related_to/,datascience,what were your first cloud projects related to DS/ML?,Currently learning GCP. Help me stay motivated by telling me about your first cloud-related DS/ML projects.,6,0.88,https://www.reddit.com/r/datascience/comments/1kp0grb/what_were_your_first_cloud_projects_related_to/,False,True,False
1kowb8p,indie-devops,1747497456.0,15,/r/datascience/comments/1kowb8p/prediction_flow_with_gaussian_distributed_features/,datascience,Prediction flow with Gaussian distributed features,"Hi all,
Just recently started as a data scientist, so I thought I could use the wisdom of this subreddit before I get up to speed and compare methodologies to see what can help my team better.
 
So say I have a dataset for a classification problem with several features (not all) that are normally distributed, and for the sake of numerical stability I’m normalizing those values to their respective Z-values (using the training set’s means and std to prevent leakage).

Now after I train the model and get some results I’m happy with using the test set (that was normalized also with the training’s mean and std), we trigger some of our tests and deploy pipelines (whatever they are) and later on we’ll use that model in production with new unseen data. 

My question is, what is your most popular go to choice to store those mean and std values for when you’ll need to normalize the unseen data’s features prior to the prediction? The same question applies for filling null values.

“Simplest” thing I thought of (with an emphasis on the “”) is a wrapper class that stores all those values as member fields along with the actual model object (or pickle file path) and storing that class also with pickle, but it sounds a bit cumbersome, so maybe you can spread some light with more efficient ideas :)

Cheers.",25,0.91,https://www.reddit.com/r/datascience/comments/1kowb8p/prediction_flow_with_gaussian_distributed_features/,False,True,False
1kobhx7,NervousVictory1792,1747428643.0,41,/r/datascience/comments/1kobhx7/demand_forecasting_using_multiple_variables/,datascience,Demand forecasting using multiple variables,I am working on a demand forecasting model to accurately predict test slots across different areas. I have been following the Rob Hyndman book. But the book essentially deals with just one feature and predicting its future values. But my model takes into account a lot of variables. How can I deal with that ? What kind of EDA should I perform ?? Is it better to make every feature stationary ? ,17,0.9,https://www.reddit.com/r/datascience/comments/1kobhx7/demand_forecasting_using_multiple_variables/,False,True,False
1ko8ngz,Proof_Wrap_2150,1747421388.0,44,/r/datascience/comments/1ko8ngz/when_is_the_right_time_to_move_from_jupyter_into/,datascience,When is the right time to move from Jupyter into a full modular pipeline?,"I feel stuck in the middle where my notebook works well, but it’s growing, and I know clients will add new requirements. I don’t want to introduce infrastructure I don’t need yet, but I also don’t want to be caught off guard when it’s important. 

How do you know when it’s time to level up, and what lightweight steps help you prepare?

Any books that can help me scale my jupyter notebooks into bigger solutions? ",72,0.93,https://www.reddit.com/r/datascience/comments/1ko8ngz/when_is_the_right_time_to_move_from_jupyter_into/,False,True,False
1ko8lwv,Proof_Wrap_2150,1747421279.0,3,/r/datascience/comments/1ko8lwv/how_would_you_structure_a_data_pipeline_project/,datascience,How would you structure a data pipeline project that needs to handle near-identical logic across different input files?,I’m trying to turn a Jupyter notebook that processes 100k rows in a spreadsheet into something that can be reused across multiple datasets. I’ve considered parameterized config files but I want to hear from folks who’ve built reusable pipelines in client facing or consulting setups.,3,0.8,https://www.reddit.com/r/datascience/comments/1ko8lwv/how_would_you_structure_a_data_pipeline_project/,False,True,False
1ko8j3v,Proof_Wrap_2150,1747421084.0,82,/r/datascience/comments/1ko8j3v/jupyter_notebook_has_grown_into_a_200_line/,datascience,"Jupyter notebook has grown into a 200+ line pipeline for a pandas heavy, linear logic, processor. What’s the smartest way to refactor without overengineering it or breaking the ‘run all’ simplicity?","I’m building an analysis that processes spreadsheets, transforms the data, and outputs HTML files. 

It works, but it’s hard to maintain. 

I’m not sure if I should start modularizing into scripts, introduce config files, or just reorganize inside the notebook. Looking for advice from others who’ve scaled up from this stage. It’s easy to make it work with new files, but I can’t help but wonder what the next stage looks like? 

EDIT: Really appreciate all the thoughtful replies so far. I’ve made notes with some great perspectives on refactoring, modularizing, and managing complexity without overengineering.

Follow-up question for those further down the path:

Let’s say I do what many of you have recommended and I refactor my project into clean .py files, introduce config files, and modularize the logic into a more maintainable structure. What comes after that?

I’m self taught and using this passion project as a way to build my skills. Once I’ve got something that “works well” and is well organized… what’s the next stage? 

Do I aim for packaging it? Turning it into a product? Adding tests? Making a CLI? 

I’d love to hear from others who’ve taken their passion project to the next level! 

How did you keep leveling up?",137,0.91,https://www.reddit.com/r/datascience/comments/1ko8j3v/jupyter_notebook_has_grown_into_a_200_line/,False,True,False
1ko0frr,timusw,1747400949.0,2,/r/datascience/comments/1ko0frr/company_data_retention_policies_and_gdpr/,datascience,Company Data Retention Policies and GDPR,"How long are your data retention policies?

How do you handle GDPR rules?

My company is instituting a very, very conservative retention policy of <9months of raw event-level data (but storing 15-months worth of aggregated data). Additionally, the only way this company thinks about GDPR compliance is to delete user records instead of anonymizing. 

I'm curious how your companies deal with both, and what the risks would be with instituting such policies.",0,0.33,https://www.reddit.com/r/datascience/comments/1ko0frr/company_data_retention_policies_and_gdpr/,False,True,False
1kncg7f,darkwhiteinvader,1747326248.0,107,/r/datascience/comments/1kncg7f/is_our_job_just_to_p_hack_for_the_stakeholders/,datascience,Is our job just to P hack for the stakeholders?,"Specifically in experimentation and causal inference.
",345,0.95,https://www.reddit.com/r/datascience/comments/1kncg7f/is_our_job_just_to_p_hack_for_the_stakeholders/,False,True,False
1kn66el,anuveya,1747309748.0,0,/r/datascience/comments/1kn66el/federated_platform_for_secure_research_data/,datascience,Federated Platform for Secure Research Data Sharing,,5,0.7,/r/clinicalresearch/comments/1kn5hf3/federated_platform_for_secure_research_data/,False,False,False
1kmv770,Difficult-Big-3890,1747269534.0,4,/r/datascience/comments/1kmv770/anyone_here_experimenting_with_implementing/,datascience,Anyone here experimenting with implementing Transformers on tabular data like Strip? Looking for some coding repo to play around and learn.,Here’s the Stripe case: https://techcrunch.com/2025/05/07/stripe-unveils-ai-foundation-model-for-payments-reveals-deeper-partnership-with-nvidia/,8,0.67,https://www.reddit.com/r/datascience/comments/1kmv770/anyone_here_experimenting_with_implementing/,False,True,False
1km3wa1,Suspicious_Coyote_54,1747188360.0,73,/r/datascience/comments/1km3wa1/is_linkedin_data_trust_worthy/,datascience,Is LinkedIn data trust worthy?,Hey all. So I got my month of Linkdin premium and I am pretty shocked to see that for many data science positions it’s saying that more applicants have a masters? Is this actually true? I thought it would be the other way around. This is a job post that was up for 2 hours with over 100 clicks on apply. I know that doesn’t mean they are all real applications but I’m just curious to know what the communities thoughts on this are? ,143,0.93,https://i.redd.it/3zlnro11on0f1.jpeg,False,False,False
1klv393,corgibestie,1747164771.0,15,/r/datascience/comments/1klv393/those_in_manufacturing_and_scienceengineering/,datascience,"Those in manufacturing and science/engineering, aside from classic DoE (full-fact, CCD, etc.), what other experimental design tools do you use?",Title. My role mostly uses central composite designs and the standard lean six sigma quality tools because those are what management and the engineering teams are used to. Our team is slowly integrating other techniques like Bayesian optimization or interesting ways to analyze data (my new fave is functional data analysis) and I'd love to hear what other tools you guys use and your success/failures with them.,23,0.97,https://www.reddit.com/r/datascience/comments/1klv393/those_in_manufacturing_and_scienceengineering/,False,True,False
1kl55q1,alexellman,1747086744.0,79,/r/datascience/comments/1kl55q1/what_do_you_use_to_build_dashboards/,datascience,What do you use to build dashboards?,"Hi guys, I've been a data scientist for 5 years. I've done lots of different types of work and unfortunately that has included a lot of dashboarding (no offense if you enjoy making dashboards). I'm wondering what tools people here are using and if you like them. In my career I've used mode, looker, streamlit and retool off the top of my head. I think mode was my favorite because you could type sql right into it and get the charts you wanted but still was overall unsatisfied with it.

  
I'm wondering what tools the people here are using and if you find it meets all your needs? One of my frustrations with these tools is that even platforms like Looker—designed to be self-serve for general staff—end up being confusing for people without a data science background.

Are there any tools (maybe powered my LLMs now) that allow non data science people to write prompts that update production dashboards? A simple example is if you have a revenue dashboard showing net revenue and a PM, director etc wanted you to add an additional gross revenue metric. With the tools I'm aware of I would have to go into the BI tool and update the chart myself to show that metric. Are there any tools that allow you to just type in a prompt and make those kinds of edits?",77,0.95,https://www.reddit.com/r/datascience/comments/1kl55q1/what_do_you_use_to_build_dashboards/,False,True,False
1kl2fq2,PraiseChrist420,1747080206.0,12,/r/datascience/comments/1kl2fq2/8_yoe_7_years_software_engineer_trying_to_pivot/,datascience,[8 YoE] 7 Years Software Engineer Trying to Pivot to Data Analytics/Science/Machine Learning,,0,0.48,/r/EngineeringResumes/comments/1kkxd78/8_yoe_7_years_software_engineer_trying_to_pivot/,False,False,False
1kl2ck3,James_c7,1747080003.0,11,/r/datascience/comments/1kl2ck3/do_open_source_contributors_still_need_to_do/,datascience,Do open source contributors still need to do coding challenges?,"I’ve become an avid open source contributor over the past few years in a few popular ML, Econ, and Jax ecosystem packages.

In my opinion being able to take someone else’s code and fix bugs or add features is a much better signal than leetcode and hacker rank. I’m really hoping I don’t have to study leetcode/hackerrank for my next job search (DS/MLE roles) and I’d rather just keep doing open source work that’s more relevant.

For the other open source contributors out there - are you ever able to get out of coding challenges by citing your own pull requests?


",28,0.83,https://www.reddit.com/r/datascience/comments/1kl2ck3/do_open_source_contributors_still_need_to_do/,False,True,False
1kky19i,ElectrikMetriks,1747069920.0,22,/r/datascience/comments/1kky19i/now_youre_paying_an_analyst_50hr_to_standardize/,datascience,Now you're paying an analyst $50/hr to standardize date formats instead of doing actual analysis work.,,382,0.95,https://i.redd.it/ccnonqd9vd0f1.png,False,False,False
1kkwjla,Ok-Needleworker-6122,1747066427.0,16,/r/datascience/comments/1kkwjla/day_since_last_x_feature_preprocessing/,datascience,"""Day Since Last X"" feature preprocessing","Hi Everyone! Bit of a technical modeling question here. Apologies if this is very basic preprocessing stuff but I'm a younger data scientist working in industry and I'm still learning.

  
Say you have a pretty standard binary classification model predicting 1 = we should market to this customer and 0 = we should not market to this customer (the exact labeling scheme is a bit proprietary). 

I have a few features that are in the style ""days since last touchpoint"". For example ""days since we last emailed this person"" or ""days since we last sold to this person"". However, a solid percentage of the rows are NULL, meaning we have never emailed or sold to this person. Any thoughts on how should I handle NULLs for this type of column? I've been imputing with MAX(days since we last sold to this person) + 1 but I'm starting to think that could be confusing my model. I think the reality of the situation is that someone with 1 purchase a long time ago is **a lot** more likely to purchase today than someone who has never purchased anything at all. The person with 0 purchases may not even be interested in our product, while we have evidence that the person with 1 purchase a long time ago is at least a fit for our product. Imputing with MAX(days since we last sold to this person) + 1 poses these two cases as very similar to the model.

For reference I'm testing with several tree-based models (light GBM and random forest) and comparing metrics to pick between the architecture options. So far I've been getting the best results with light GBM.

One thing I'm thinking about is whether I should just leave the people who have never sold as NULLs and have my model pick the direction to split for missing values. (I believe this would work with LightGBM but not RandomForest).

Another option is to break down the ""days since last sale"" feature into categories, maybe quantiles with a special category for NULLS, and then dummy encode.

Has anyone else used these types of ""days since last touchpoint"" features in propensity modeling/marketing modeling?",30,0.94,https://www.reddit.com/r/datascience/comments/1kkwjla/day_since_last_x_feature_preprocessing/,False,True,False
1kktdbl,vniversvs_,1747058750.0,75,/r/datascience/comments/1kktdbl/is_it_necessary_to_learn_some_language_other_than/,datascience,is it necessary to learn some language other than python?,"that's pretty much it. i'm proficient in python already, but was wondering if, to be a better DS, i'd need to learn something else, or is it better to focus on studying something else rather than a new language.

  
edit: yes, SQL is obviously a must. i already know it. sorry for the overlook.",96,0.94,https://www.reddit.com/r/datascience/comments/1kktdbl/is_it_necessary_to_learn_some_language_other_than/,False,True,False
1kkjf6g,AutoModerator,1747022501.0,20,/r/datascience/comments/1kkjf6g/weekly_entering_transitioning_thread_12_may_2025/,datascience,"Weekly Entering & Transitioning - Thread 12 May, 2025 - 19 May, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",7,0.9,https://www.reddit.com/r/datascience/comments/1kkjf6g/weekly_entering_transitioning_thread_12_may_2025/,False,True,False
1kjvita,brodrigues_co,1746949145.0,3,/r/datascience/comments/1kjvita/rixpress_an_r_package_to_set_up_multilanguage/,datascience,rixpress: an R package to set up multi-language reproducible analytics pipelines (2 Minute intro video),,9,0.71,https://youtu.be/a1eNG9TFZ_o?si=yWRIpPGWEP9NY-4Y,False,False,False
1kjuwou,Aftabby,1746946635.0,26,/r/datascience/comments/1kjuwou/where_can_i_find_legit_remote_data_science_jobs/,datascience,Where Can I Find Legit Remote Data Science Jobs That Hire Globally?,"Hey folks! I’m on the hunt for trustworthy remote job boards or sites that regularly post real data science and data analyst roles—and more importantly, are open to hiring from anywhere in the world. I’ve noticed sites like Indeed don’t support my country, and while LinkedIn has plenty of remote listings, many seem sketchy or not legit. 

So, what platforms or communities do you recommend for finding genuine remote gigs in this field that are truly global? Any tips on spotting legit postings would also be super helpful! 

Thanks in advance for sharing your experiences!",36,0.68,https://www.reddit.com/r/datascience/comments/1kjuwou/where_can_i_find_legit_remote_data_science_jobs/,False,True,False
1kjqlcv,MLEngDelivers,1746930425.0,33,/r/datascience/comments/1kjqlcv/new_python_package_feedback_try_in_google_collab/,datascience,New Python Package Feedback - Try in Google Collab,"I’ve been occasionally working on this in my spare time and would appreciate feedback.

[Try the package in Colab](https://colab.research.google.com/github/OlivierNDO/framecheck/blob/main/framecheck_quickstart.ipynb)

The idea for ‘framecheck’ is to catch bad data in a data frame before it flows downstream in *very few* lines of code. 

You’d also easily isolate the records with problematic data. This isn’t revolutionary or new - what I wanted was a way to do this in fewer lines of code than other packages like great expectations and pydantic.

Really I just want honest feedback. If people don’t find it useful, I won’t put more time into it.

pip install framecheck

Repo with reproducible examples:

https://github.com/OlivierNDO/framecheck",54,0.87,https://i.redd.it/z5qrbw22d20f1.jpeg,False,False,False
1kjjb32,Federal_Bus_4543,1746908253.0,437,/r/datascience/comments/1kjjb32/i_am_a_staff_data_scientist_at_a_big_tech_company/,datascience,I am a staff data scientist at a big tech company -- AMA,"**Why I’m doing this**

I am low on karma. Plus, it just feels good to help.

**About me**

I’m currently a staff data scientist at a big tech company in Silicon Valley. I’ve been in the field for about 10 years since earning my PhD in Statistics. I’ve worked at companies of various sizes — from seed-stage startups to pre-IPO unicorns to some of the largest tech companies.

**A few caveats**

* Anything I share reflects my personal experience and may carry some bias.
* My experience is based in the US, particularly in Silicon Valley.
* I have some people management experience but have mostly worked as an IC
* Data science is a broad term. I’m most familiar with machine learning scientist, experimentation/causal inference, and data analyst roles.
* I may not be able to respond immediately, but I’ll aim to reply within 24 hours.

**Update:**

Wow, I didn’t expect this to get so much attention. I’m a bit overwhelmed by the number of comments and DMs, so I may not be able to reply to everyone. That said, I’ll do my best to respond to as many as I can over the next week. Really appreciate all the thoughtful questions and discussions!",1216,0.93,https://www.reddit.com/r/datascience/comments/1kjjb32/i_am_a_staff_data_scientist_at_a_big_tech_company/,False,True,False
1kjca29,Illustrious-Pound266,1746889228.0,26,/r/datascience/comments/1kjca29/does_your_company_have_a_dedicated_teamperson_for/,datascience,"Does your company have a dedicated team/person for MLOps? If not, how do you manage MLOps?","As someone in MLOps, I am curious to hear how other companies and teams manage the MLOps process and workflow. My company (because it's a huge enterprise) has multiple teams doing some type of MLOps or MLOps-adjacent projects. But I know that other companies do this very differently.

So does your team have a separate dedicated person or a group for MLOps and managing model lifecycle in production? If not, how do you manage it? Is the data scientist / MLE expected to do all?",29,0.86,https://www.reddit.com/r/datascience/comments/1kjca29/does_your_company_have_a_dedicated_teamperson_for/,False,True,False
1kj40s6,Aftabby,1746859543.0,120,/r/datascience/comments/1kj40s6/how_can_earlylevel_data_scientists_get_noticed_by/,datascience,How Can Early-Level Data Scientists Get Noticed by Recruiters and Industry Pros?,"Hey everyone! 

I started my journey in the data science world almost a year ago, and I'm wondering: What’s the best way to market myself so that I actually get noticed by recruiters and industry professionals? How do you build that presence and get on the radar of the right people?  
  
**Any tips on networking, personal branding, or strategies that worked for you would be amazing to hear!**",202,0.94,https://www.reddit.com/r/datascience/comments/1kj40s6/how_can_earlylevel_data_scientists_get_noticed_by/,False,True,False
1kiubls,Trick-Interaction396,1746827375.0,15,/r/datascience/comments/1kiubls/what_are_some_useful_dsde_projects_i_can_do/,datascience,What are some useful DS/DE projects I can do during slow periods at work?,Things are super slow at work due to economic uncertainty. I'm used to being super busy so I never had to think up my own problems/projects. Any ideas for useful projects I can do or sell to management? Thanks.,20,0.87,https://www.reddit.com/r/datascience/comments/1kiubls/what_are_some_useful_dsde_projects_i_can_do/,False,True,False
1kionyr,marblesandcookies,1746812832.0,37,/r/datascience/comments/1kionyr/i_have_an_inperson_interview_with_the_cto_of_a/,datascience,I have an in-person interview with the CTO of a company in 2 weeks. I have no industry work experience for data science. Only project based experience. How f*cked am I?,Help,89,0.87,https://www.reddit.com/r/datascience/comments/1kionyr/i_have_an_inperson_interview_with_the_cto_of_a/,False,True,False
1ki9zo3,melissa_ingle,1746764905.0,133,/r/datascience/comments/1ki9zo3/client_told_me_ms_copilot_replicated_what_i_built/,datascience,Client told me MS Copilot replicated what I built. It didn’t.,"I built three MVP models for a client over 12 weeks. Nothing fancy: an LSTM, a prophet model, and XGBoost. The difficulty, as usual, was getting and understanding the data and cleaning it. The company is largely data illiterate. Turned in all 3 models, they loved it then all of a sudden canceled the pending contract to move them to production. Why? They had a devops person do in MS Copilot Analyst (a new specialized version of MS Copilot studio) and it took them 1 week! Would I like to sign a lesser contract to advise this person though? I finally looked at their code and it’s 40 lines of code using a subset of the California housing dataset run using a Random Forest regressor. They had literally nothing. My advice to them: go f*%k yourself. ",1094,0.99,https://www.reddit.com/r/datascience/comments/1ki9zo3/client_told_me_ms_copilot_replicated_what_i_built/,False,True,False
1ki5bob,bobo-the-merciful,1746749589.0,0,/r/datascience/comments/1ki5bob/may_be_of_interest_to_anyone_looking_to_learn/,datascience,May be of interest to anyone looking to learn Python with a stats bias,,0,0.43,/r/pythontips/comments/1ki54aw/python_for_engineers_and_scientists/,False,False,False
1khu4f8,Lamp_Shade_Head,1746721015.0,124,/r/datascience/comments/1khu4f8/this_is_how_i_got_a_potential_offer_revoked_a/,datascience,This is how I got a (potential) offer revoked: A learning lesson,"I’m based in the Bay Area with 5 YOE. A couple of months ago, I interviewed for a role I wasn’t too excited about, but the pay was super compelling. In the first recruiter call, they asked for my salary expectations. I asked for their range, as an example here, let’s say they said $150K–$180K. I said, “That works, I’m looking for something above $150K.” I think this was my first mistake, more on that later.

I am a person with low self esteem(or serious imposter syndrome) and when I say I nailed all 8 rounds, I really must believe that. The recruiter followed up the day after 8th round saying team is interested in extending an offer. Then on compensation expectations the recruiter said, “You mentioned $150K earlier.” I clarified that I was targeting the upper end based on my fit and experience. They responded with, “So $180K?” and I just said yes. It felt a bit like putting words in my mouth.

Next day, I got an email saying that I have to wait for the offer decision as they are interviewing  other candidates. Haven’t heard back since. I don’t think I did anything fundamentally wrong or if I should have regrets but curious what others think.

Edit: Just to clarify, in my mind I thought that’s how negotiations work. They will come back and say can’t do 150 but can do 140. But I guess not.",240,0.92,https://www.reddit.com/r/datascience/comments/1khu4f8/this_is_how_i_got_a_potential_offer_revoked_a/,False,True,False
1khkkv8,furioncruz,1746690828.0,15,/r/datascience/comments/1khkkv8/code_is_shit_business_wants_to_scale_what_could/,datascience,"Code is shit, business wants to scale, what could go wrong?","A bit of context. I have taken charge of a project recently. It's a product in a client facing app. The implementation of the ML system is messy. The data pipelines consists of many sql codes. These codes contain rather complicated business knowledge. There is airflow that schedules them, so there is observability. 

This code has been used to run experiments for the past 2 months. I don't know how much firefighting has been going on. But in the past week that I picked up the project, I spent 3 days on firefighting. 

I understand that, at least theoretically, when scaling, everything that could go wrong goes wrong. But I want to hear real life experiences. When facing such issues, what have you done that worked? Could you find a way to fix code while helping with scaling? Did firefightings get in the way? Any past experience would help. Thanks! ",32,0.78,https://www.reddit.com/r/datascience/comments/1khkkv8/code_is_shit_business_wants_to_scale_what_could/,False,True,False
1khj1wm,sg6128,1746684316.0,9,/r/datascience/comments/1khj1wm/final_verdict_on_llm_generated_confidence_scores/,datascience,Final verdict on LLM generated confidence scores?,,6,0.69,/r/LocalLLaMA/comments/1khfhoh/final_verdict_on_llm_generated_confidence_scores/,False,False,False
1khic8u,CadeOCarimbo,1746681479.0,87,/r/datascience/comments/1khic8u/the_worst_thing_about_being_a_data_scientist_is/,datascience,The worst thing about being a Data Scientist is that the best you can do you sometimes is not even nearly enough,"This specially sucks as a consultant. You get hired because some guy from Sales department of the consulting company convinced the client that they would give them a Data Scientist consultant that would solve all their problems and build perfect Machine Learning models. 


Then you join the client and quickly realize that is literary impossible to do any meaningful work with the poor data and the unjustified expectations they have. 

As an ethical worker, you work hard and to everything that is possible with the data at hand (and maybe some external data you magically gathered). You use everything that you know and don't know, take some time to study the state of the art, chat with some LLMs on their ideas for the project, run hundreds of different experiments (should I use different sets of features? Should I log transform some numerical features? Should I apply PCA? How many ML algorithms should I try?) 

And at the end of day... The model still sucks. You overfit the hell of the model, makes a gigantic boosting model with max_depth  set as 1000, and you still don't match the dumb manager expectations. 

I don't know how common that it is in other professions, but an intrinsic thing of working in Data Science is that you are never sure that your work will eventually turn out to be something good, no matter how hard you try. ",549,0.98,https://www.reddit.com/r/datascience/comments/1khic8u/the_worst_thing_about_being_a_data_scientist_is/,False,True,False
1kgzki4,MorningDarkMountain,1746630194.0,53,/r/datascience/comments/1kgzki4/is_hackerrankleetcode_a_valid_way_to_screen/,datascience,Is HackerRank/LeetCode a valid way to screen candidates?,"Reverse questions: is it a red flag if a company is using HackerRank / LeetCode challenges in order to filter candidates?

I am a strong believer in technical expertise, meaning that a DS needs to know what is doing. You cannot improvise ML expertise when it comes to bring stuff into production.

Nevertheless, I think those kind of challenges works only if you're a monkey-coder that recently worked on that exact stuff, and specifically practiced for those challenges. No way that I know by heart all the subtle nuances of SQL or edge cases in ML, but on the other hand I'm most certainly able to solve those issues in real life projects.

Bottom line: do you think those are legit way of filter candidates (and we should prepare for that when applying to roles) or not?",61,0.82,https://www.reddit.com/r/datascience/comments/1kgzki4/is_hackerrankleetcode_a_valid_way_to_screen/,False,True,False
1kgz66a,chomoloc0,1746629212.0,3,/r/datascience/comments/1kgz66a/grinding_through_regression_discontinuity/,datascience,Grinding through regression discontinuity resulted in this post - feel free to check it out,"Title should check out. Been reading on RDD in the spare time I had in the past few months. I put everything together after applying it in my company (#1 online marketplace in the Netherlands) — the result: a few late nights and this [blog post.](https://towardsdatascience.com/regression-discontinuity-design-how-it-works-and-when-to-use-it/)

Thanks to the few redditors that shared [their input](https://www.reddit.com/r/CausalInference/comments/1i801e0/call_for_input_regression_discontinuity_design/) on the technique and application. It made me wiser!",9,0.8,https://towardsdatascience.com/regression-discontinuity-design-how-it-works-and-when-to-use-it/,False,False,False
1kgz36l,Trick-Interaction396,1746628994.0,26,/r/datascience/comments/1kgz36l/anyone_else_tried_of_always_discussing_techtools/,datascience,Anyone else tried of always discussing tech/tools?,"Maybe it’s just my company but we spend the majority of our time discussing the pros/cons of new tech. Databricks, Snowflake, various dashboards software. I agree that tech is important but a new tool isn’t going to magically fix everything. We also need communication, documentation, and process. Also, what are we actually trying to accomplish? We can buy a new fancy tool but what’s the end goal? It’s getting worse with AI. Use AI isn’t a goal. How do we solve problem X is a goal. Maybe it’s AI but maybe it’s something else.",115,0.95,https://www.reddit.com/r/datascience/comments/1kgz36l/anyone_else_tried_of_always_discussing_techtools/,False,True,False
1kgsn61,Ciasteczi,1746608068.0,61,/r/datascience/comments/1kgsn61/am_i_or_my_pms_crazy_unknown_unknowns/,datascience,Am I or my PMs crazy? - Unknown unknowns.,"My company wants to develop a product that detects ""unknown unknowns"" it a complex system, in an unsupervised manner, in order to identify new issues before they even begin. I think this is an ill-defined task, and I think what they actually want is a supervised, not unsupervised ML pipeline. But they refuse to commit to the idea of a ""loss function"" in the system, because ""anything could be an interesting novelty in our system"". 

The system produces thousands of time series monitoring metrics. They want to stream all these metrics through anomaly detection model. Right now, the model throws thousands of anomalies, almost all of them meaningless. I think this is expected, because statistical anomalies don't have much to do with *actionable events.* Even more broadly **I think unsupervised learning cannot ever produce business value.** You always need some sort of supervised wrapper around it.

What PMs want to do: flag all outliers in the system, because they are potential problems

What I think we should be doing: (1) define the ""health (loss) function"" in the system (2) whenever the health function degrades look for root causes / predictors / correlates of the issues (3) find patterns in the system degradation - find *unknown* causes of *known* adverse system states 

Am I missing something? Are you guys doing something similar or have some interesting reads? Thanks",97,0.94,https://www.reddit.com/r/datascience/comments/1kgsn61/am_i_or_my_pms_crazy_unknown_unknowns/,False,True,False
1kglzv7,millsGT49,1746583035.0,6,/r/datascience/comments/1kglzv7/i_wrote_a_walkthrough_post_that_covers_shape/,datascience,I wrote a walkthrough post that covers Shape Constrained P-Splines for fitting monotonic relationships in python. I also showed how you can use general purpose optimizers like JAX and Scipy to fit these terms. Hope some of y'all find it helpful!,,21,0.89,http://statmills.com/2025-05-03-monotonic_spline_jax/,False,False,False
1kgkpr5,Analytics_Fanatics,1746579089.0,3,/r/datascience/comments/1kgkpr5/how_does_the_httplivecodeamazon_link_work_for/,datascience,how does the http:livecode/amazon..... link work for data science technical interview ?,"I had a call with the recruiter yesterday and this was for an interview for a DS position at AMZ. 

Recruiter told me you can't execute any code on the whiteboard. Then I got another email saying here is the link to ""livecode"" for coding exercise and I can choose the programming language of my choice. 

Can someone explain to me what is this whiteboard ? or the livecode ? and how does it work ?",4,0.64,https://www.reddit.com/r/datascience/comments/1kgkpr5/how_does_the_httplivecodeamazon_link_work_for/,False,True,False
1kgk3hw,AhmedOsamaMath,1746577250.0,5,/r/datascience/comments/1kgk3hw/a_complete_guide_covering_foundational_linux/,datascience,"A complete guide covering foundational Linux concepts, core tasks, and best practices.",,46,0.93,https://github.com/AhmedOsamaMath/linux-basics,False,False,False
1kgdevk,Ok_Post_149,1746559875.0,23,/r/datascience/comments/1kgdevk/aws_batch_alternative_deploy_to_10000_vms_with/,datascience,"AWS Batch alternative — deploy to 10,000 VMs with one line of code","I just launched an open-source batch-processing platform that can scale Python to **10,000 VMs in under 2 seconds**, with just **one line of code**.

I've been frustrated by how slow and painful it is to iterate on large batch processing pipelines. Even small changes require rebuilding Docker containers, waiting for AWS Batch or GCP Batch to redeploy, and dealing with cold-start VM delays — a **5+ minute dev cycle per iteration**, just to see what error your code throws *this time*, and then doing it all over again.

Most other tools in this space are too complex, closed-source or fully managed, hard to self-host, or simply too expensive. If you've encountered similar barriers give Burla a try.

docs: [https://docs.burla.dev/](https://docs.burla.dev/)

github: [https://github.com/Burla-Cloud](https://github.com/Burla-Cloud)",24,0.88,https://www.reddit.com/r/datascience/comments/1kgdevk/aws_batch_alternative_deploy_to_10000_vms_with/,False,True,False
1kfwny7,ChavXO,1746508285.0,12,/r/datascience/comments/1kfwny7/request_for_feedback_dataframe_library/,datascience,[Request for feedback] dataframe library,"I'm working on a dataframe library and wanted to make sure the API makes sense and is easy to get started with. No official documentation yet but wanted to get a feel of what people think of it so far.

I have some tutorials on the [github repo](https://github.com/mchav/dataframe) and a [jupyter lab environment](https://ihaskell-dataframe-crf7g5fvcpahdegz.westus2-01.azurewebsites.net/) running. Would appreciate some feedback on the API and usability. Functionality is still limited and this site is so far just a sandbox. Thanks so much.",15,0.99,https://www.reddit.com/r/datascience/comments/1kfwny7/request_for_feedback_dataframe_library/,False,True,False
1kfbtob,ElectrikMetriks,1746452251.0,30,/r/datascience/comments/1kfbtob/please_for_the_love_of_god_just_give_me_something/,datascience,"Please, for the love of god ... just give me something!!",,746,0.98,https://i.redd.it/j1tp2b00vyye1.png,False,False,False
1kfb10a,anuveya,1746450046.0,0,/r/datascience/comments/1kfb10a/selfservice_open_data_portal_zeroops_fully/,datascience,Self-Service Open Data Portal: Zero-Ops & Fully Managed for Data Scientists,"__Disclaimer: I’m one of the creators of PortalJS.__

Hi everyone, I wanted to share this open-source product for data portals with the Data Science community. Appreciate your attention!

**Our mission:**

Open data publishing shouldn’t be hard. We want local governments, academics, and NGOs to treat publishing their data like any other SaaS subscription: sign up, upload, update, and go.

**Why PortalJS?**

- Small teams need a simple, affordable way to get their data out there.
- Existing platforms are either extremely expensive or require a technical team to set up and maintain.
- Scaling an open data portal usually means dedicating an entire engineering department—and we believe that shouldn’t be the case.

Happy to answer any questions!",2,0.58,https://www.portaljs.com/?utm_source=reddit&utm_medium=post&utm_campaign=datascience,False,False,False
1kf4g8b,_brownmunda,1746424620.0,2,/r/datascience/comments/1kf4g8b/need_referral_for_amex_for_data_science_position/,datascience,Need referral for AmEx for Data Science position,"Anyone working in AmEx specifically in India in any IT/Tech related field, I need a referral for a Data Science position at AmEx Gurugram, India",0,0.17,https://www.reddit.com/r/datascience/comments/1kf4g8b/need_referral_for_amex_for_data_science_position/,False,True,False
1kf2nlk,AutoModerator,1746417691.0,51,/r/datascience/comments/1kf2nlk/weekly_entering_transitioning_thread_05_may_2025/,datascience,"Weekly Entering & Transitioning - Thread 05 May, 2025 - 12 May, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",11,0.93,https://www.reddit.com/r/datascience/comments/1kf2nlk/weekly_entering_transitioning_thread_05_may_2025/,False,True,False
1kerpax,AdministrativeRub484,1746385775.0,8,/r/datascience/comments/1kerpax/how_would_you_architect_this/,datascience,How would you architect this?,"I work for a startup where the main product is a sales meeting analyser. Naturally there are a ton of features that require audio and video processing, like diarization, ASR, video classification, etc…

The CEO is in cost savings mode and he wants to reduce our compute costs. Currently our ML pipeline is built on top of kubernetes and we always have at least on gpu machine up per task (T4s and L4s) per day and we dont have a lot of clients, meaning most of the time the gpus are idle and we are paying for them. I suggested moving those tasks to cloud functions that use GPUs, since we are using GCP and they have recently came out with that feature, but the CEO wants to use gemini to replace these tasks since we will most likely be on the free tier.

The problems I see is that once we leave the free tier the costs will be more than 10x our current costs and that there are downstream ML tasks that depend on these, so changing the input distribution is not really a good idea… for example, we have a text classifier that was trained with text from whisper - changing it to gemini does not seem to be a good idea to me…

he claimed he wants it to be maintainable so an api request makes more sense to him, but the reason why he wants it to be maintainable is because a lot of ML people are leaving (mainly because of his wrong decisions and micro management - is this another of his wrong decisions?)

using gemini to do asr and diarization, for example, just feels way way wrong",6,0.65,https://www.reddit.com/r/datascience/comments/1kerpax/how_would_you_architect_this/,False,True,False
1kdlxel,SeaSubject9215,1746251655.0,74,/r/datascience/comments/1kdlxel/wich_computer_are_you_using/,datascience,Wich computer are you using?,"Hi guys I'm thinking of buy a new computer, do you have some ideas (no Apple)? Wich computer are you using today? In looking mobility so a laptop is the option.

Thanks guys ",0,0.5,https://www.reddit.com/r/datascience/comments/1kdlxel/wich_computer_are_you_using/,False,True,False
1kcrsyn,tiwanaldo5,1746157790.0,137,/r/datascience/comments/1kcrsyn/tired_of_everyone_becoming_an_ai_expert_all_of_a/,datascience,Tired of everyone becoming an AI Expert all of a sudden,"Literally every person who can type prompts into an LLM is now an AI consultant/expert. I’m sick of it, today a sales manager literally said ‘oh I can get Gemini to make my charts from excel directly with one prompt so ig we no longer require Data Scientists and their support hehe’

These dumbos think making basic level charts equals DS work. Not even data analytics, literally data science? 

I’m sick of it. I hope each one of yall cause a data leak, breach the confidentiality by voluntarily giving private info to Gemini/OpenAi and finally create immense tech debt by developing your vibe coded projects.

Rant over ",1556,0.98,https://www.reddit.com/r/datascience/comments/1kcrsyn/tired_of_everyone_becoming_an_ai_expert_all_of_a/,False,True,False
1kcrrzu,Illustrious-Pound266,1746157696.0,17,/r/datascience/comments/1kcrrzu/do_you_have_to_keep_up_with_the_latest_research/,datascience,Do you have to keep up with the latest research papers if you are working with LLMs as an AI developer?,"I've been diving deeper into LLMs these days (especially agentic AI) and I'm slightly surprised that there's a lot of references to various papers when going through what are pretty basic tutorials.

For example, just on prompt engineering alone, quite a few tutorials referenced the Chain of Thought paper (Wei et al, 2022). When I was looking at intro tutorials on agents, many of them referred to the ICLR ReAct paper (Yao et al, 2023). In regards to finetuning LLMs, many of them referenced the QLoRa paper (Dettmers et al, 2023).

I had assumed that as a developer (not as a researcher), I could just use a lot of these LLM tools out of the box with just documentation but do I have to read the latest ICLR (or other ML journal/conference) papers to interact with them now? Is this common?

AI developers: how often are you browsing through and reading through papers? I just wanted to build stuff and want to minimize academic work...",18,0.71,https://www.reddit.com/r/datascience/comments/1kcrrzu/do_you_have_to_keep_up_with_the_latest_research/,False,True,False
1kbps44,Smooth_Signal_3423,1746043044.0,30,/r/datascience/comments/1kbps44/made_this_meme_for_a_presentation_i_have_to_give/,datascience,Made this meme for a presentation I have to give tomorrow at work,,185,0.89,https://i.redd.it/xp4zs98d21ye1.png,False,False,False
1kbfya2,Training-Screen8223,1746018208.0,82,/r/datascience/comments/1kbfya2/breaking_into_ds_from_academia/,datascience,Breaking into DS from academia,"Hi everyone,

I need advice from industry DS folks. I'm currently a bioinformatics postdoc in the US, and it seems like our world is collapsing with all the cuts from the current administration. I'm considering moving to industry DS (any field), as I'm essentially doing DS in the biomedical field right now.

I tried making a DS/industry style 1-page resume; could you please advise whether it is good and how to improve? Be harsh, no problemo with that. And a couple of specific questions:

1. A friend told me I should write ""Data Scientist"" as my previous roles, as recruiters will dump my CV after seeing ""Computational Biologist"" or ""Bioinformatics Scientist."" Is this OK practice? The work I've done, in principle, is data science.
2. Am I missing any critical skills that every senior-level industry DS should have?

Thanks everyone in advance!! 

https://preview.redd.it/0o0mg29szyxe1.png?width=2550&format=png&auto=webp&s=85d0ec3cdab2e439c42445f90a76f898fa2a3b13

  
",120,0.89,https://www.reddit.com/r/datascience/comments/1kbfya2/breaking_into_ds_from_academia/,False,True,False
1kb5xj6,Aromatic-Fig8733,1745980467.0,20,/r/datascience/comments/1kb5xj6/ds_in_healthcare/,datascience,DS in healthcare,"So I have a situation.   
I have a dataset that contains real-world clinical vignettes drawn from frontline healthcare settings. Each sample presents a prompt representing a clinical case scenario, along with the response from a human clinician. The goal is to predict the the phisician's response based on the prompt.

These vignettes simulate the types of decisions nurses  must make every day, particularly in low-resource environments where access to specialists or diagnostic equipment may be limited.

* These are real clinical scenarios, and the dataset is small because expert-labelled data is difficult and time-consuming to collect.
* Prompts are diverse across medical specialties, geographic regions, and healthcare facility levels, requiring broad clinical reasoning and adaptability.
* Responses may include abbreviations, structured reasoning (e.g. ""Summary:"", ""Diagnosis:"", ""Plan:""), or free text.

my first go to is to fine tune a small LLM to do this but I have feeling it won't be enough given how diverse the specialties are and the size of the dataset.  
Anyone has done something like this before? any help or resources would be welcomed.",13,0.81,https://www.reddit.com/r/datascience/comments/1kb5xj6/ds_in_healthcare/,False,True,False
1kayvx4,iwannabeunknown3,1745960564.0,15,/r/datascience/comments/1kayvx4/putting_forecast_model_into_production_help/,datascience,Putting Forecast model into Production help,"I am looking for feedback on deploying a Sarima model. 


I am using the model to predict sales revenue on a monthly basis. The goal is identifying the trend of our revenue and then making purchasing decisions based on the trend moving up or down. I am currently forecasting 3 months into the future, storing those predictions in a table, and exporting the table onto our SQL server. 


It is now time to refresh the forecast. I think that I retrain the model on all of the data, including the last 3 months, and then forecast another 3 months. 


My concern is that I will not be able to rollback the model to the original version if I need to do so for whatever reason. Is this a reasonable concern? Also, should I just forecast 1 month in advance instead of 3 if I am retraining the model anyway? 


This is my first time deploying a time series model. I am a one person shop, so I don't have anyone with experience to guide me. Please and thank you. ",11,0.92,https://www.reddit.com/r/datascience/comments/1kayvx4/putting_forecast_model_into_production_help/,False,True,False
1kapuvz,alpha_centauri9889,1745938355.0,11,/r/datascience/comments/1kapuvz/transition_to_sde/,datascience,Transition to SDE,"Is there anyone here who has transitioned to SDE from DS? I have been working as a data scientist for over 2 years now, so my CV comprises of DS related experience only. I want to explore opportunities in SDE (as well as DS/MLE) since I am not enjoying the kind of work I am doing now. My background is CS. 

If someone has done it, can you suggest how to prepare for it given that I have worked as DS? Should I include SDE related self projects? Btw there's no opportunity in my current organization to internally transition to SDE. And I am more inclined towards product related companies.",27,0.95,https://www.reddit.com/r/datascience/comments/1kapuvz/transition_to_sde/,False,True,False
1kapczj,arairia,1745937058.0,8,/r/datascience/comments/1kapczj/what_is_the_best_way_to_parse_and_order_a_pdf/,datascience,"What is the best way to parse and order a PDF from forum screenshots that includes a lot of cached text, quotes, random order and overall a mess.","Hello dear people! Been dealing with this very interesting problem that I'm not 100% sure how to tackle. A local forum went down some time ago and they lost a few hours worth of data since backups aren't hourly. Quite a few topics were lost, as well as some of them apparently became corrupted and also got lost. One of them included a very nice discussion about local mountaineering and beautiful locations which a lot of people are saddened to lost since we discussed many trails. Somehow, people managed to collect data from various cached sources, computers, some screenshots, but mostly old google, bing caches while they worked and webarchive. 

Now it's all properly ordered in pdf document but the thing is the layouts often change and so does resolution but the general idea of how data is represented is the same. There's also some artifacts in data from webarchive for example - they have an element hovering over text and you can't see it, but if you ctrl-f to search for it it's there somehow, hidden under the image haha. No javascript in PDF, something else, probably  colored, no idea.

The ideas I had were (btw PDF is OCR'd already):

&nbsp;

- PDF to text and try to regex + LLM process it all somehow?

- Somehow ""train"" (if train is a proper word here?) machine vision / machine learning for each separate layout so that it knows how to extract data

&nbsp;

But I also face issue that some posts are for example screenshoted in ""half"", e.g. page 360 has the text cut out and continue on page 361 with random stuff on top from the archival's page (e.g. webarchive or bing cache info). I would need to also truncate this, but that should be easy.

&nbsp;

- Or option 3 with those new LLMs that can somehow recognize images or work with PDF (idk how they do it) I could maybe have the LLM do the whole heavy load of processing? I could pick up one of better new models with big context length and remembrance, I just checked total character count, it's 8.588.362 characters or 2.147.090 tokens approximately, but I believe the data could be split and later manually combined or something? I'm not sure I'm really new to this. The main goal is to have a nice json output with all data properly curated.

&nbsp;

Many thanks! Much appreciated.",5,0.73,https://www.reddit.com/r/datascience/comments/1kapczj/what_is_the_best_way_to_parse_and_order_a_pdf/,False,True,False
1kanby4,Raikoya,1745931594.0,92,/r/datascience/comments/1kanby4/the_role_of_data_science_in_the_age_of_genai/,datascience,The role of data science in the age of GenAI,"I've been working in the space of ML for around 10 years now. I have a stats background, and when I started I was mostly training regression models on tabular data, or the occasional tf-idf + SVM pipeline for text classification. Nowadays, I work mainly with unstructured data and for the majority of problems my company is facing, calling a pre-trained LLM through an API is both sufficient and the most cost-effective solution - even deploying a small BERT-based classifier costs more and requires data labeling. I know this is not the case for all companies, but it's becoming very common.

Over the years, I've developed software engineering skills, and these days my work revolves around infra-as-code, CI/CD pipelines and API integration with ML applications. Although these skills are valuable, it's far away from data science.

For those who are in the same boat as me (and I know there are many), I'm curious to know how you apply and maintain your data science skills in this age of GenAI? ",384,0.97,https://www.reddit.com/r/datascience/comments/1kanby4/the_role_of_data_science_in_the_age_of_genai/,False,True,False
1kaki1s,guna1o0,1745921981.0,14,/r/datascience/comments/1kaki1s/is_it_data_leakage/,datascience,is it data leakage?,"We are predicting conversion. Conversion means customer converted from paying one-off to paying regular (subscribe)

If one feature is categorical feature ""Activity"" , consisting 15+ categories and one of the category is ""conversion"" (labelling whether the customer converted or not). The other 14 categories are various. Examples are emails, newsletter, acquisition, etc. they're companies recorded of how it got this customers (no matter it's one-off or regular customer) It may or may not be converted customers

so we definitely cannot use the one category as a feature in our model otherwise it would create data leakage. What about the other 14 categories?

What if i create dummy variables from these 15 categories + and select just 2-3 to help modelling? Would it still create leakage ?

I asked this to 1. my professor 2. A professional data analyst They gave different answers. Can anyone help adding some more ideas?

I tried using the whole features (convert it to dummy and drop 1), it helps the model. For random forests, the top one with high feature importance is this Activity\_conversion (dummy of activity - conversion) feature

  
Note: found this question on a forum.",5,0.59,https://www.reddit.com/r/datascience/comments/1kaki1s/is_it_data_leakage/,False,True,False
1ka2l4q,ElectrikMetriks,1745864402.0,14,/r/datascience/comments/1ka2l4q/ill_just_do_it_later/,datascience,I'll just do it later,,333,0.95,https://i.redd.it/cfhnln9yamxe1.jpeg,False,False,False
1k9uwf6,bikeskata,1745845058.0,12,/r/datascience/comments/1k9uwf6/a_paper_from_the_latest_sigbovik_proceedings/,datascience,A paper from the latest SIGBOVIK proceedings,,340,0.98,https://i.redd.it/ophkf4pkpkxe1.jpeg,False,False,False
1k9momp,AutoModerator,1745812891.0,28,/r/datascience/comments/1k9momp/weekly_entering_transitioning_thread_28_apr_2025/,datascience,"Weekly Entering & Transitioning - Thread 28 Apr, 2025 - 05 May, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",11,0.93,https://www.reddit.com/r/datascience/comments/1k9momp/weekly_entering_transitioning_thread_28_apr_2025/,False,True,False
1k8hjp4,takuonline,1745686512.0,25,/r/datascience/comments/1k8hjp4/this_environment_would_be_a_real_nightmare_for_me/,datascience,This environment would be a real nightmare for me.,"YouTube released some interesting metrics for their 20 year celebration and their data environment is just insane.

- Processing infrastructure handling 20+ million daily video uploads 
- Storage and retrieval systems managing 20+ billion total videos
- Analytics pipelines tracking 3.5+ billion daily likes and 100+ million daily comments
- Real-time processing of engagement metrics (creator-hearted comments reaching 10 million daily)
- Infrastructure supporting multimodal data types (video, audio, comments, metadata)

From an analytics point of view, it would be extremely difficult to validate anything you build in this environment, especially if it's something that is very obscure. 
Supposed they calculate a ""Content Stickiness Factor"" (a metric which quantifies how much a video prevents users from leaving the platform), 
how would anyone validate that a factor of 0.3 is correct for creator X? That is just for 1 creator in one segment, there are different segments which all have different behaviors eg podcasts which might be longer vs shorts

I would assume training ml models, or basic queries would be either slow or very expensive which punishes mistakes a lot. You either run 10 computer for 10 days or or 2000 computers for 1.5 hours, and if you forget that 2000 computer cluster running, for just a few minutes for lunch maybe, or worse over the weekend, you will come back to regret it.

Any mistakes you do are amplified by the amount of data, you omitting a single ""LIMIT 10"" or use a ""SELECT * "" in the wrong place and you could easy cost the company millions of dollars.
""Forgot a single cluster running, well you just lost us $10 million dollars buddy""

And because of these challenges, l believe such an environment demands excellence, not to ensure that no one makes mistakes, but to prevent obvious ones and reduce the probability of catastrophic ones.

l am very curious how such an environment is managed and would love to see it someday.






[YouTube article](https://blog.youtube/news-and-events/happy-birthday-youtube-20/)",125,0.88,https://www.reddit.com/r/datascience/comments/1k8hjp4/this_environment_would_be_a_real_nightmare_for_me/,False,True,False
1k87wnq,fightitdude,1745654754.0,45,/r/datascience/comments/1k87wnq/thoughts_on_getting_a_masters_while_working_as_a/,datascience,Thoughts on getting a Masters while working as a DS?,"I entered DS straight after an undergrad in Computer Science. During my degree I did multiple DS internships and an ML research internship. I figured out I didn't like research so a PhD was out. I couldn't afford to stay on for a Masters so I went straight into work and found a DS role, where I'm performing very well and getting promoted quickly.

I like my current org but it's a very narrow field of work so I might want to move on in 2-3 years. I see a lot of postings (both internally and externally) require a Masters, so I'm wondering if I'm putting myself at a disadvantage by not having one.

My current employer has tuition reimbursement up to ~$6k a year so I was thinking of doing a part-time Masters (something like OMSCS, OMSA, or a statistics MS program offered by a local uni) - partially for the signalling of having a Masters, and partially because I just really love learning and I feel like the learning has stagnated in my current role... 

On the other hand I'm worried that doing a Masters alongside work will impact my ability to focus on my job & progression plans. I've already done two Masters courses part-time (free, credit-bearing but can't transfer them to a degree) and found it ok but any of the degrees I've been considering would be much more workload. 

Another option would be to take a year out between jobs and do a Masters, but with the job market the way it is that feels like a big risk.

Thanks in advance for your opinions/discussion :)",70,0.93,https://www.reddit.com/r/datascience/comments/1k87wnq/thoughts_on_getting_a_masters_while_working_as_a/,False,True,False
1k81pru,poop-machines,1745632015.0,14,/r/datascience/comments/1k81pru/an_example_of_how_statistics_can_be_used_to/,datascience,An example of how statistics can be used to unintentionally deceive (and why data analysis is important).,,43,0.87,https://www.reddit.com/r/2westerneurope4u/comments/1k78yjt/comment/mp2mlra/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button,False,False,False
1k80mxy,Adventurous-Put-8042,1745628591.0,33,/r/datascience/comments/1k80mxy/question_about_how_to_use_churn_prediction/,datascience,Question about How to Use Churn Prediction,"When churn prediction is done, we have predictions of who will churn and who will retain.

  
I am wondering what the typical strategy is after this. 

Like target the people who are predicting as being retained (perhaps to upsell on them) or try to get people back who are predicted as churning? My guess is it is something that depends on the priority of the business.

I'm also thinking, if we output a probability that is borderline, that could be an interesting target to attempt to persuade. 

",42,0.96,https://www.reddit.com/r/datascience/comments/1k80mxy/question_about_how_to_use_churn_prediction/,False,True,False
1k804yc,No-Brilliant6770,1745627031.0,61,/r/datascience/comments/1k804yc/thought_i_was_prepping_for_mlds_internships_turns/,datascience,"Thought I was prepping for ML/DS internships... turns out I need full-stack, backend, cloud, AND dark magic to qualify","I'm currently doing my undergrad and have built up a decent foundation in machine learning and data science. I figured I was on track, until I actually started looking for internships.

Now every ML/DS internship description looks like:  
""Must know full-stack development, backend, frontend, cloud engineering, DevOps, machine learning, deep learning, computer vision, and also invent a new programming language while you're at it.""

Bro I just wanted to do some modeling, not rebuild Twitter from scratch..

I know basic stuff like SDLC, Git, and cloud fundamentals, but I honestly have no clue about real frontend/backend development. Now I’m thinking I need to buckle down and properly learn SWE if I ever want to land an ML/DS internship.

First, am I wrong for thinking this way? Is full-stack knowledge pretty much required now for ML/DS intern roles, or am I just applying to cracked job posts?  
Second, if I do need to learn SWE properly, where should I start?

I don't want to sit through super basic ""hello world"" courses (no offense to IBM/Meta Coursera certs, but I need something a little more serious). I heard the Amazon Junior Developer program on Coursera might be good? Anyone tried it?

Not trying to waste time spinning in circles. Just wanna know how people here approached it if you were in a similar spot. Appreciate any advice.",306,0.96,https://www.reddit.com/r/datascience/comments/1k804yc/thought_i_was_prepping_for_mlds_internships_turns/,False,True,False
1k7xi9g,Moonlit_Sailor,1745619428.0,5,/r/datascience/comments/1k7xi9g/responsible_tech_certificates_a_worthwhile_expense/,datascience,Responsible Tech Certificates: A Worthwhile Expense?,"Curious what people here think about this article: [
Responsible Tech Certificates: A Worthwhile Expense?
](https://alltechishuman.org/all-tech-is-human-blog/responsible-tech-certificates-a-worthwhile-expense) 

Personally I find these to be mostly a waste of money, but as someone who's interested in getting into ethical AI, was wondering if anyone has had a similar experience and if it helped them get their foot in the door.",5,0.73,https://www.reddit.com/r/datascience/comments/1k7xi9g/responsible_tech_certificates_a_worthwhile_expense/,False,True,False
1k76c0v,LilParkButt,1745536561.0,46,/r/datascience/comments/1k76c0v/step_in_the_right_or_wrong_direction_long_term/,datascience,Step in the right or wrong direction long term?,"I’m a sophomore double majoring in Data Analytics and Data Engineering with a minor in Computer Science. (It sounds like a lot, but I came in with an associate’s degree from high school, so it’s honestly not a ton)

My end goal is to become a Data Scientist, ideally specializing in time-series forecasting or recommendation systems. I plan to go straight into a Master’s in Data Science after undergrad.

Today, I just got an offer for a Business Analyst Internship. The role focuses heavily on SQL and Power BI, but doesn’t involve any Python, machine learning, or advanced statistics. It’s a great opportunity and I’d be working with a Business Analytics team at a credit union, but I’m a bit torn.

Will having “Business Analyst Intern” on my resume make me look less competitive for future data science internships or full-time roles—especially compared to students who land internships with “Data Scientist” or “Data Science Intern” in the title?

I know I’m only a sophomore, and I don’t want to overthink it, but I also don’t want to unintentionally steer myself toward an analyst-only path.

Any advice or insight would be appreciated!",7,0.59,https://www.reddit.com/r/datascience/comments/1k76c0v/step_in_the_right_or_wrong_direction_long_term/,False,True,False
1k6za0y,thro0away12,1745518614.0,22,/r/datascience/comments/1k6za0y/signs_of_burnout/,datascience,Signs of burnout?,"Hey all,

I posted a little bit about my current job situation in a previous post: [https://www.reddit.com/r/datascience/comments/1javfus/do\_you\_deal\_with\_unrealistic\_expectations\_from/](https://www.reddit.com/r/datascience/comments/1javfus/do_you_deal_with_unrealistic_expectations_from/)

Ever since the year started, I've just been looped into tasks where I have no context what it's supposed to do, don't have the requirements clear, frequently have my boss try to get something out without clear requirements and then us fixing it after the fact with another co-worker constantly expressing dissapointment and frustration for things not churning out sooner.

For the past month, I've been working several 12-14 hour shifts. On days when I don't have quick turnaround times, I've noticed myself losing focus, losing interest in the work overall. I signed up for a bunch of Udemy classes in the beginning of the year and feel like my headspace isn't there to upskill even though I had a lot of enthusiasm before. 

Has anybody gone through this situation and have advice? I want to change my job eventually in a few months, but I want to spend time preparing rather than just jump ship at the moment, esp in this market. ",39,0.87,https://www.reddit.com/r/datascience/comments/1k6za0y/signs_of_burnout/,False,True,False
1k6rj0y,phicreative1997,1745499402.0,0,/r/datascience/comments/1k6rj0y/deep_analysis_the_analytics_analogue_to_deep/,datascience,Deep Analysis — the analytics analogue to deep research,,12,0.88,https://medium.com/firebird-technologies/deep-analysis-the-analytics-analogue-to-deep-research-45ecd8c60096,False,False,False
1k6pqem,AMGraduate564,1745493737.0,5,/r/datascience/comments/1k6pqem/polars_what_is_the_status_of_compatibility_with/,datascience,Polars: what is the status of compatibility with other Python packages?,,9,0.91,/r/Python/comments/1k6ppc7/polars_what_is_the_status_of_compatibility_with/,False,False,False
1k63zii,Starktony11,1745426843.0,17,/r/datascience/comments/1k63zii/to_interviewers_who_ask_product_metrics_cases/,datascience,"To Interviewers who ask product metrics  cases study, what makes you say yes or no to a candidate, do you want complex metrics? Or basic works too?","Hi, 
I was curious to know if you are an interviewer, lest say at faang or similar big tech, what makes you feel yes this is good candidate and we can hire, what are the deal breakers or something that impress you or think that a red flag? 

Like you want them to think about out of box metrics, or complex metrics or even basic engagement metrics like DAUs, conversions rates, view rates, etc are good enough? Also, i often see people mention a/b test whenever the questions asked so do you want them to go on deep in it?  Or anything you look them to answer? Also, how long do you want the conversation to happen?

Edit- also anything you think that makes them stands out or topics they mention make them stands out? ",52,0.88,https://www.reddit.com/r/datascience/comments/1k63zii/to_interviewers_who_ask_product_metrics_cases/,False,True,False
1k60gey,guna1o0,1745418250.0,19,/r/datascience/comments/1k60gey/how_can_i_come_up_with_better_feature_ideas/,datascience,How can I come up with better feature ideas?,"I'm currently working on a credit scoring model. I have tried various feature engineering approaches using my domain knowledge, and my manager has also shared some suggestions. Additionally, I’ve explored several feature selection techniques. However, the model's performance still isn't meeting my manager’s expectations.

At this point, I’ve even tried manually adding and removing features step by step to observe any changes in performance. I understand that modeling is all about domain knowledge, but I can't help wishing there were a magical tool that could suggest the best feature ideas.",21,0.84,https://www.reddit.com/r/datascience/comments/1k60gey/how_can_i_come_up_with_better_feature_ideas/,False,True,False
1k5ikzd,Trick-Interaction396,1745358495.0,55,/r/datascience/comments/1k5ikzd/how_is_your_teaming_using_ai_for_ds/,datascience,How is your teaming using AI for DS?,"I see a lot of job posting saying “leverage AI to add value”. What does this actually mean? Using AI to complete DS work or is AI is an extension of DS work?

I’ve seen a lot of cool is cases outside of DS like content generation or agents but not as much in DS itself. Mostly just code assist of document creation/summary which is a tool to help DS but not DS itself.",75,0.86,https://www.reddit.com/r/datascience/comments/1k5ikzd/how_is_your_teaming_using_ai_for_ds/,False,True,False
1k52w1u,essenkochtsichselbst,1745317399.0,7,/r/datascience/comments/1k52w1u/request_for_review/,datascience,Request for Review,,0,0.42,/r/learndatascience/comments/1k4xcj2/request_for_review/,False,False,False
1k4q8b8,zangler,1745273542.0,24,/r/datascience/comments/1k4q8b8/in_an_effort_to_keep_learning/,datascience,In an effort to keep learning,"I have a new DS starting soon...modalities change and all of that, more importantly, for those of you hired in the last year, what are some things you wish were presented earlier than they were ( or things done in general)? Looking to make this a very positive experience for the new employee.",27,0.86,https://www.reddit.com/r/datascience/comments/1k4q8b8/in_an_effort_to_keep_learning/,False,True,False
1k4geso,NerdyMcDataNerd,1745249434.0,154,/r/datascience/comments/1k4geso/ever_met_a_person_you_think_lied_about_working_in/,datascience,Ever met a person you think lied about working in Data Science?,"You ever get the feeling someone online or in-person just straight up lied to you about having a Data Science job (Data Scientist, Data Analyst, Data Engineer, Machine Learning Engineer, Data Architect, etc.)?

I was recently talking to someone at a technical meet-up for working professionals and one person was saying some really weird stuff. It was like they had heard of the technical terms before, but didn't actually have the experience working with the technologies/skills. For example, they mentioned that they had ""All sorts of experience with Kafka"" but didn't know that it is a tool that Data Engineers and related professionals could use for their workflows. They also mixed up the definitions of common machine learning models, what said models could do for a business, NoSQL & SQL, etc. It was jarring.

Also, sometimes I get the impression that a minority of people on this subreddit come on and lie about ever having a Data Science job. The more obvious examples are those who post the Chat-GPT answers to post questions. No shade thrown to anyone here. I encounter many qualified people here and have learned new stuff just reading through posts.

Any of you ever had an experience like that?

**Edit:** Hello all. Thank you for all of the responses on this post. I have gotten some good perspective, some hilarious comments, and some cool advice. I appreciate all of you on this sub-reddit.

I do want to say that I do not believe that all Data Scientists need to know Kafka (or any other specific tech. I don't know a bunch of stuff). I brought up the Kafka example because it was the most egregious (the person claimed to have all these years of experience, but didn't know a bunch of stuff including the basics). The conversation was 35 minutes, so I only wanted to bring up the outliers/notable examples.

And I want to emphasize that I was talking about **all** Data Science jobs (Data Scientist, Data Analyst, Data Engineer, Machine Learning Engineer, Data Architect, etc.). Because I think that these are all valid roles and that we all have unique experiences, skills, and knowledge to bring to this field.

Anyways, I appreciate all the comments and I will read through them after work.",277,0.9,https://www.reddit.com/r/datascience/comments/1k4geso/ever_met_a_person_you_think_lied_about_working_in/,False,True,False
1k44mgg,AutoModerator,1745208103.0,37,/r/datascience/comments/1k44mgg/weekly_entering_transitioning_thread_21_apr_2025/,datascience,"Weekly Entering & Transitioning - Thread 21 Apr, 2025 - 28 Apr, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",8,0.91,https://www.reddit.com/r/datascience/comments/1k44mgg/weekly_entering_transitioning_thread_21_apr_2025/,False,True,False
1k3nxj7,gonna_get_tossed,1745159796.0,211,/r/datascience/comments/1k3nxj7/pandas_why_the_hype/,datascience,"Pandas, why the hype?","I'm an R user and I'm at the point where I'm not really improving my programming skills all that much, so I finally decided to learn Python in earnest. I've put together a few projects that combine general programming, ML implementation, and basic data analysis. And overall, I quite like python and it really hasn't been too difficult to pick up. And the few times I've run into an issue, I've generally blamed it on R (e.g . the day I learned about mutable objects was a frustrating one). However, basic analysis - like summary stats - feels impossible.

All this time I've heard Python users hype up pandas. But now that I am actually learning it, I can't help think why? Simple aggregations and other tasks require so much code. But more confusng is the syntax, which seems to be odds with itself at times.  Sometimes we put the column name in the parentheses of a function, other times be but the column name in brackets before the function. Sometimes we call the function normally (e.g.mean()), other times it is contain by quotations. The whole thing reminds me of the Angostura bitters bottle story, where one of the brothers designed the bottles and the other designed the label without talking to one another.

Anyway, this wasn't really meant to be a rant. I'm sticking with it, but does it get better? Should I look at polars instead?

To R users, everyone needs to figure out what Hadley Wickham drinks and send him a case of it.",404,0.88,https://www.reddit.com/r/datascience/comments/1k3nxj7/pandas_why_the_hype/,False,True,False
1k3jt7b,guna1o0,1745145771.0,0,/r/datascience/comments/1k3jt7b/is_there_something_similar_tailored_for_data/,datascience,Is there something similar tailored for Data Science interviews? | asking on behalf of my friend,,3,0.64,/r/learnmachinelearning/comments/1k2z3g1/is_there_something_similar_tailored_for_data/,False,False,False
1k3e4nb,genobobeno_va,1745121827.0,28,/r/datascience/comments/1k3e4nb/unit_tests/,datascience,Unit tests,Serious question: Can anyone provide a real example of a series of unit tests applied to an MLOps flow? And when or how often do these unit tests get executed and who is checking them? Sorry if this question is too vague but I have never been presented an example of unit tests in production data science applications.,42,0.86,https://www.reddit.com/r/datascience/comments/1k3e4nb/unit_tests/,False,True,False
1k33k6t,v2thegreat,1745089085.0,5,/r/datascience/comments/1k33k6t/finally_releasing_the_bambu_timelapse_dataset/,datascience,Finally releasing the Bambu Timelapse Dataset – open video data for print‑failure ML (sorry for the delay!),"Hey everyone!

I know it’s been a **long minute** since my original call‑for‑clips – life got hectic and the project had to sit on the back burner a bit longer than I’d hoped. 😅 Thanks for bearing with me!

# What’s new?

* **The dataset is live** on Hugging Face and ready for download or contribution.
* **First models are on the way** (starting with **build‑plate identification**) – but I can’t promise an exact release timeline yet. Life still throws curveballs!

🔗 **Dataset page:** [https://huggingface.co/datasets/v2thegreat/bambu-timelapse-dataset](https://huggingface.co/datasets/v2thegreat/bambu-timelapse-dataset)

# What’s inside?

* **627 timelapse videos** from P1/X1 printers
* **81 full‑length camera recordings** straight off the printer cam
* Thumbnails + CSV metadata for quick indexing
* CC‑BY‑4.0 license – free for hobby, research, and even commercial use with proper attribution

# Why bother?

* It’s the **first fully open corpus** of Bambu timelapses; most prior failure‑detection work never shares raw data.
* Bambu Lab printers are everywhere, so the footage mirrors real‑world conditions.
* Great sandbox for manufacturing / QA projects—failure classification, anomaly detection, build‑plate detection, and more.

# Contribute your clips

1. Open a **Pull Request** on the repo (`originals/timelapses/<your_id>/`).
2. If PRs aren’t your jam, DM me and we’ll arrange a transfer link.
3. Please crop or blur anything private; aim for bed‑only views.

# Skill level

If you know some Python and basic ML, this is a perfect **intermediate** project to dive into computer vision. Total beginners can still poke around with the sample code, but training solid models will take a bit of experience.

Thanks again for everyone’s patience and for the clips already shared—can’t wait to see what the community builds with this!",20,0.96,https://www.reddit.com/r/datascience/comments/1k33k6t/finally_releasing_the_bambu_timelapse_dataset/,False,True,False
1k32lrl,brodrigues_co,1745086504.0,75,/r/datascience/comments/1k32lrl/python_users_which_r_packages_do_you_use_if_any/,datascience,"Python users, which R packages do you use, if any?","I'm currently writing an R package called [rixpress](https://github.com/b-rodrigues/rixpress) which aims to set up reproducible pipelines with simple R code by using Nix as the underlying build tool. Because it uses Nix as the build tool, it is also possible to write targets that are built using Python.
[Here is an example of a pipeline that mixes R and Python.](https://github.com/b-rodrigues/rixpress_demos/blob/master/python_r/gen-pipeline.R)

I think rixpress can be quite useful to Python users as well (and I might even translate the package to Python in the future), and I'm looking for examples of Python users that need to also work with certain R packages. These examples would help me make sure that passing objects from and between the two languages can be as seamless as possible.

So Python data scientists, which R packages do you use, if any? 
",102,0.9,https://www.reddit.com/r/datascience/comments/1k32lrl/python_users_which_r_packages_do_you_use_if_any/,False,True,False
1k2y84g,da_chosen1,1745074906.0,36,/r/datascience/comments/1k2y84g/data_science_content_gap/,datascience,Data science content gap,"I’m trying to get back into the habit of writing data science articles. I can cover a wide range of topics, including A/B testing, causal inference, and model development and deployment. I’d love to hear from this community—what kinds of articles or posts would be most valuable to you? I know there’s already a lot of content out there, and I’m to understand I’m writing something people find valuable. 

Edit thanks for the response: 

I’ve learned that people want to see more real-world data science applications. Here are a few topics I could write about:

	•	Using time series forecasting to determine the best location for building a hydro power plant
	•	Developing top-line KPI metrics to track product or business health
	•	Modeling CLV for B2B businesses, especially where most revenue comes from a few accounts
	•	Applying quasi-experiments to measure the impact of marketing campaigns
	•	Prioritizing different GenAI opportunities 
	•	Detecting survey fraud by analyzing mouse movement
      - developing a full end-to- end modeling. ",57,0.87,https://www.reddit.com/r/datascience/comments/1k2y84g/data_science_content_gap/,False,True,False
1k2u4nd,essenkochtsichselbst,1745062061.0,1,/r/datascience/comments/1k2u4nd/leverage_points_for_a_design_matrix_with_mainly/,datascience,Leverage Points for a Design Matrix with Mainly Categorial Features,"Hello! I hope this is a stupid question and gets quickly resolved. As per title, I have a design matrix with a high amount of categorial features. I am applying a linear regression model on the data set (mainly for training myself to get familiarity with linear regression). The model has a high amount of categorial features that I have one-hot encoded.

Now I try to figure out high leverage points for the design matrix. After a couple of attempts I was wondering if that would even make sense and how to evaluate if determining high leverage points would generally make sense in this scenario.

After asking ChatGPT (which provided a weird answer I know is incorrect) and searching a bit I found nothing explaining this. So, I thought I come here and ask:

* In how far does it make sense to compute/check for leverage values given that there is a high amount of categorial features?
* How to compute them? Would I use the diagonal of the HAT matrix or is there eventually another technique?

  
I am happy about any advise or hint, explanation or approach that gives me some clarity in this scenario. Thank you!!

 ",8,0.9,https://www.reddit.com/r/datascience/comments/1k2u4nd/leverage_points_for_a_design_matrix_with_mainly/,False,True,False
1k2igce,sg6128,1745018155.0,32,/r/datascience/comments/1k2igce/what_sweai_engineer_skills_in_2025_can_i_learn_to/,datascience,What SWE/AI Engineer skills in 2025 can I learn to complement Data Science?,"At my company currently - the hype is to use LLMs and GenAI at every intersection.

I have seen this means that a lot of DS work is now instead handed to SWEs, and the 'modelling' is all a GPT/API call.

Maybe this is just a feature of my company and the way they look at their tech stack, but I feel that DS is not getting as many projects and things are going to the SWEs only, as they can quickly build, and rapidly deploy into product.

I want to better learn how to integrate GenAI features/apps in our JavaScript based product, so that I can also build and integrate, and build working PoCs, rather than being trapped in notebooks. 

I'm not sure if I should just learn raw JS, because I'd even want to know how to put things into a silent test as an example, where predictions are made but no prediction is shown to the user.

Maybe the more apt title is going from a DS -> AI Engineer, and what skills to learn to get there?",86,0.94,https://www.reddit.com/r/datascience/comments/1k2igce/what_sweai_engineer_skills_in_2025_can_i_learn_to/,False,True,False
1k2ax74,throwaway69xx420,1744998234.0,23,/r/datascience/comments/1k2ax74/what_does_a_good_ds_manager_look_like_to_you_how/,datascience,What does a good DS manager look like to you? How does one manage a DS project?,"Hi all, 

I have found myself numerous times in leadership roles for data science projects. I never feel that I am doing a sufficient job. I find that I either end have up doing a lot of the work on my own and failing to split up task in the data science realm. A lot of these projects, and I hate to say it like this without sounding cocky, I feel that I can do on my own from end to end. Maybe some minimal support from other teams in helping with data flow issues, etc. I'm not a manager by any means, I am individual contributor. 

For those in this subreddit who are managers, what are some ways you found success in managing data science teams and projects? For those as individual contributors, what are some things that you like to have in a data science manager?",61,0.98,https://www.reddit.com/r/datascience/comments/1k2ax74/what_does_a_good_ds_manager_look_like_to_you_how/,False,True,False
1k2a8t6,Sampo,1744996521.0,8,/r/datascience/comments/1k2a8t6/forecasting_principles_and_practice_the_pythonic/,datascience,"Forecasting: Principles and Practice, the Pythonic Way",,109,0.98,https://otexts.com/fpppy/,False,False,False
1k26kp3,Zuricho,1744987285.0,68,/r/datascience/comments/1k26kp3/whats_your_2025_data_science_coding_stack_ai/,datascience,What’s your 2025 data science coding stack + AI tools workflow?,"Curious how others are working these days. What’s your current setup?

IDE / notebook tools? (VS Code, Cursor, Jupyter, etc.)

Are you using AI tools like Cursor, Windsurf, Copilot, Cline, Roo?

How do they fit into your workflow? (e.g., prompting style, tasks they’re best at)

Any wins, limitations, or tips?",184,0.94,https://www.reddit.com/r/datascience/comments/1k26kp3/whats_your_2025_data_science_coding_stack_ai/,False,True,False
1k26920,Lamp_Shade_Head,1744986429.0,66,/r/datascience/comments/1k26920/how_do_you_go_about_memorizing_all_the_ml/,datascience,How do you go about memorizing all the ML algorithms details for interviews?,"I’ve been preparing for interviews lately, but one area I’m struggling to optimize is the ML depth rounds. Right now, I’m reviewing ISLR and taking notes, but I’m not retaining the material as well as I’d like. Even though I studied this in grad school, it’s been a while since I dove deep into the algorithmic details.  

Do you have any advice for preparing for ML breadth/depth interviews? Any strategies for reinforcing concepts or alternative resources you’d recommend? ",152,0.94,https://www.reddit.com/r/datascience/comments/1k26920/how_do_you_go_about_memorizing_all_the_ml/,False,True,False
1k22cd4,oryx_za,1744974637.0,28,/r/datascience/comments/1k22cd4/working_with_distance/,datascience,Working with distance,"I'm super curious about the solutions you're using to calculate distances. 

I can't share too many details, but we have data that includes two addresses and the GPS coordinates between these locations. While the results we've obtained so far are interesting, they only reflect the straight-line distance.

Google has an API that allows you to query travel distances by car and even via public transport. However, my understanding is that their terms of service restrict storing the results of these queries and the volume of the calls.

Have any of you experts explored other tools or data sources that could fulfill this need? This is for a corporate solution in the UK, so it needs to be compliant with regulations.

Edit: thanks, you guys are legends ",17,0.78,https://www.reddit.com/r/datascience/comments/1k22cd4/working_with_distance/,False,True,False
1k20azb,SonicBoom_81,1744966399.0,19,/r/datascience/comments/1k20azb/have_a_lot_of_experience_but_not_getting_any/,datascience,Have a lot of experience but not getting any interviews - help,"Hi,

I was here a few weeks back and you helped me to cut down my CV and demo more impact.  I have applied to jobs all over and get only rejections.

I know the market is hard right now, but I would think that I would at least get invited to have at least initial conversations.  This makes me think, there must be something really missing.  Could you tell me what you think it could be?

Due to AI hype there are a lot of postings with LLMs.  I don't have corporate experience there but I plan to do projects to learn & demo it.

This week I have lowered my salary requirements by 10k and still get rejections.

I have 2 versions - a 2 pager and a 1 pager.  Have been applying with the 2 pager mostly until now.

Am grateful for your feedback and any help you can give me

https://preview.redd.it/e4pubfms4kve1.png?width=1414&format=png&auto=webp&s=853c4ae00db446784cb42ff17048611e5fb03a81

https://preview.redd.it/mzsfifmv4kve1.png?width=1414&format=png&auto=webp&s=ca35aeac336eb834a54b55008efc51936c26658d

https://preview.redd.it/l9jz6b6w4kve1.png?width=1414&format=png&auto=webp&s=802f98f4dfdb7cc5d39346c6d1a91cf6b08b95b6

",0,0.42,https://www.reddit.com/r/datascience/comments/1k20azb/have_a_lot_of_experience_but_not_getting_any/,False,True,False
1k1x464,Starktony11,1744953050.0,8,/r/datascience/comments/1k1x464/what_is_the_difference_between_did_and/,datascience,What is the difference between DiD and incremental testing? I did search online and gpt but didn’t find convincing difference,"Hi 

What is the difference between DiD and incremental testing? I did search online and gpt but didn’t find convincing difference, i don’t get it as both are basically difference between control and treatment group. If anyone could explain then would be great help. Thanks!",10,0.79,https://www.reddit.com/r/datascience/comments/1k1x464/what_is_the_difference_between_did_and/,False,True,False
1k1wu9o,Admirable_Creme1276,1744952011.0,43,/r/datascience/comments/1k1wu9o/forecasting_models_for_small_data_in_operations/,datascience,Forecasting models for small data in operations,"Hi, I work in a company that provides a weekly service to our customers.

One of the most important things for our operations is to know 1 to 5 weeks in advance how many customers we expect to have for each of those future weeks.

Company is operating for about 4 years so there are roughly 200 historical data points.

I wonder, which data science, ML models are best for small data with some seasonal trends?

Facebook prophet, Arima and Sarima are the ones we use but it feels like we are missing some.

Any thoughts?
",36,0.92,https://www.reddit.com/r/datascience/comments/1k1wu9o/forecasting_models_for_small_data_in_operations/,False,True,False
1k1vo23,Emuthusiast,1744947829.0,4,/r/datascience/comments/1k1vo23/advice_before_getting_data_engineer_fellowship/,datascience,Advice before getting data engineer fellowship position,"Hey everybody,

I need some advice. I have an MsC in Data Science and have really struggled to find jobs. I got an average paying, “data science adjacent but not data science enough” quantitative analyst job in a bank. In fact , I feel like I get dumber every day I’m there and I’m miserable. None of the skills or achievements there are noteworthy : no model building, no big analyses, no data engineering or Gen ai work, just model validation work (helping other people fix their modeling solutions).

Long story short, I’m interviewing for a fellowship position to be a data engineer in a nonprofit. It lasts for one year and exposes me to many clients that I will aid. At most I can extend the fellowship for one additional year. It sounds exciting. It pays 10K less, but it’s a step in the right direction. It gets me closer to what I actually studied.

The reason I write this post is because I want to know if it will negatively impact my resume or future chances. If I take this job, my resume will look like this : data analyst job (3 years) with a bit of sql and excel, two data science internships (one 3 months and one 8 months) at the university, quantitative analyst (6months), data engineer fellowship (1 year). Will this make companies look at me like a problem and not give me a chance to even interview? Thanks in advance, everybody.
",7,0.74,https://www.reddit.com/r/datascience/comments/1k1vo23/advice_before_getting_data_engineer_fellowship/,False,True,False
1k1mjok,citizenofme,1744920922.0,23,/r/datascience/comments/1k1mjok/lead_ds_book_suggestions/,datascience,Lead DS book suggestions,"Ive landed my first role as a lead DS. My responsibilities outside actual DS work is upskilling the analytics team in Python, R and powerBI which I've got 5+ experience with. However, this is the first role where I'm mentoring/coaching/leading a team. I would welcome any suggestions for reading materials that would help me in this new leadership role. Thank you for your time!",80,0.95,https://www.reddit.com/r/datascience/comments/1k1mjok/lead_ds_book_suggestions/,False,True,False
1k1lh3r,FilmIsForever,1744918223.0,5,/r/datascience/comments/1k1lh3r/experiences_from_past_open_data_science/,datascience,Experiences from past Open Data Science Conferences (ODSC)?,"I have an opportunity to attend ODSC East (https://odsc.com/boston/) and want to see if this is worth it as a M.S. CS graduate looking for networking and employment opportunities.

  
I am less interested in tutorials and workshops than in networking and employment. Is it worth it to show up with a resume and portfolio links looking to network?

  
I searched this sub and reviews are mixed but fairly old. Anyone gone recently?",8,0.79,https://www.reddit.com/r/datascience/comments/1k1lh3r/experiences_from_past_open_data_science/,False,True,False
1k0zcye,khaili109,1744848145.0,34,/r/datascience/comments/1k0zcye/data_engineer_trying_to_understand_data_science/,datascience,Data Engineer trying to understand data science to provide better support.,"I work as a data engineer who mainly builds & maintains data warehouses but now I’m starting to get projects assigned to me asking me to build custom data pipelines for various data science projects and I’m assuming deployment of Data Science/ML models to production. 

Since my background is data engineering, how can I learn data science in a structured bottom up manner so that I can best understand what exactly the data scientists want? 

This may sound like overkill to some but so far the data scientist I’m working with is trying to build a data science model that requires enriched historical data for the training of the data science model. Ok no problem so far. 

However, they then want to run the data science model on the data as it’s collected (before enrichment) but the problem is this data science model is trained on enriched historical data that wont have the exact same schema as the data that’s being collected real time?

What’s even more confusing is some data scientists have said this is ok and some said it isn’t.

I don’t know which person is right. So, I’d rather learn at least the basics, preferably through some good books & projects so that I can understand when the data scientists are asking for something unreasonable.

I need to be able to easily speak the language of data scientists so I can provide better support and let them know when there’s an issue with the data that may effect their data science model in unexpected ways. ",62,0.92,https://www.reddit.com/r/datascience/comments/1k0zcye/data_engineer_trying_to_understand_data_science/,False,True,False
1k0vdku,showme_watchu_gaunt,1744837338.0,4,/r/datascience/comments/1k0vdku/quick_question_regarding_nested_resampling_and/,datascience,Quick question regarding nested resampling and model selection workflow,"EDIT!!!!!! Post wording is confusing, when I refer to models I mean one singular model tuned N number of ways. E.g. random Forrest tuned to 4 different depths would be model a,b,c,d in my diagram.

Just wanted some feedback regarding my model selection approach.

The premise:  
Need to train dev a model and I will need to perform nested resmapling to prevent against spatial and temporal leakage.  
Outer samples will handle spatial leakage.  
Inner samples will handle temporal leakage.  
I will also be tuning a model.

Via the diagram below, my model tuning and selection will be as follows:  
\-Make inital 70/30 data budget  
\-Perfrom some number of spatial resamples (4 shown here)  
\-For each spatial resample (1-4), I will make N (4 shown) spatial splits  
\-For each inner time sample i will train and test N (4 shown) models and mark their perfromance  
\-For each outer samples' inner samples - one winner model will be selected based on some criteria  
\--e.g Model A out performs all models trained innner samples 1-4 for outer sample #1  
\----Outer/spatial #1 -- winner model A  
\----Outer/spatial #2 -- winner model D  
\----Outer/spatial #3 -- winner model C  
\----Outer/spatial #4 -- winner model A  
\-I take each winner from the previous step and train them on their entire train sets and validate on their test sets  
\--e.g train model A on outer #1 train and test on outer #1 test  
\----- train model D on outer #2 train and test on outer #2 test  
\----- and so on  
\-From this step the model the perfroms the best is then selected from these 4 and then trained on the entire inital 70% train and evalauated on the inital 30% holdout.

Should I change my method up at all?  
I was thinking that I might be adding bias in to the second modeling step (training the winning models on the outer/spatial samples) because there could be differences in the spatial samples themselves.   
Potentially some really bad data ends up exclusively in the test set for one of the outer folds and by default make one of the models not be selected that otherwise might have. 

https://preview.redd.it/kw6ogyygg9ve1.png?width=1080&format=png&auto=webp&s=7d6ac472bd91269bd9790ee3b8111b053cffa351

  
",3,0.67,https://www.reddit.com/r/datascience/comments/1k0vdku/quick_question_regarding_nested_resampling_and/,False,True,False
1k0v0dc,Trick-Interaction396,1744836414.0,31,/r/datascience/comments/1k0v0dc/does_anyone_here_work_for_doordash_discover_home/,datascience,"Does anyone here work for DoorDash, Discover, Home Depot, or Liberty Mutual?",Why do you keep posting the same jobs over and over again?,56,0.78,https://www.reddit.com/r/datascience/comments/1k0v0dc/does_anyone_here_work_for_doordash_discover_home/,False,True,False
1k0c459,Suspicious_Jacket463,1744778309.0,164,/r/datascience/comments/1k0c459/data_science_is_not_about/,datascience,Data science is not about...,"There's a lot of posts on LinkedIn which claim:
- Data science is not about Python
- It's not about SQL
- It's not about models
- It's not about stats
...

But it's about storytelling and business value. 

There is a huge amount of people who are trying to convince everyone else in this BS, IMHO. It's just not clear why... 

Technical stuff is much more important. It reminds me of some rich people telling everyone else that money doesn't matter.  ",723,0.88,https://www.reddit.com/r/datascience/comments/1k0c459/data_science_is_not_about/,False,True,False
1k082ij,nirvana5b,1744765107.0,15,/r/datascience/comments/1k082ij/is_timeseriessplit_appropriate_for_purchase/,datascience,Is TimeSeriesSplit appropriate for purchase propensity prediction?”,"I have a dataset of price quotes for a service, with the following structure: client ID, quote ID, date (daily), target variable indicating whether the client purchased the service, and several features.

I'm building a model to predict the likelihood of a client completing the purchase after receiving a quote.

Does it make sense to use TimeSeriesSplit for training and validation in this case?
Would this type of problem be considered a time series problem, even though the prediction target is not a continuous time-dependent variable?",19,0.84,https://www.reddit.com/r/datascience/comments/1k082ij/is_timeseriessplit_appropriate_for_purchase/,False,True,False
1jzml32,Prize-Flow-3197,1744705044.0,78,/r/datascience/comments/1jzml32/is_agentic_ai_remotely_useful_for_real_business/,datascience,Is Agentic AI remotely useful for real business problems?,"Agentic AI is the latest hype train to leave the station, and there has been an explosion of frameworks, tools etc. for developing LLM-based agents. The terminology is all over the place, although the definitions in the Anthropic blog ‘Building Effective Agents’ seem to be popular (I like them). 

Has anyone actually deployed an agentic solution to solve a business problem? Is it in production (i.e more than a PoC)? Is it actually agentic or just a workflow? I can see clear utility for open-ended web searching tasks (e.g. deep research, where the user validates everything) - but having agents autonomously navigate the internal systems of a business (and actually being useful and reliable) just seems fanciful to me, for all kinds of reasons. How can you debug these things? 

There seems to be a vast disconnect between expectation and reality, more than we’ve ever seen in AI. Am I wrong?",107,0.89,https://www.reddit.com/r/datascience/comments/1jzml32/is_agentic_ai_remotely_useful_for_real_business/,False,True,False
1jz4teg,Suspicious_Coyote_54,1744652198.0,40,/r/datascience/comments/1jz4teg/why_wont_they_let_you_run_your_code/,datascience,Why won’t they let you run your code!?,"So I just got done with a SQL zoom screen. I practiced for a long time on mediums and hards. One thing that threw me off was I was not allowed to run the query to see the result. The problems were medium and hard often requiring multiple joins and CTEs. 2 mediums 2 hards. 25 mins. Only got done with 3 and they wouldn’t even tell me if I was right or wrong. Just “logic looks sound” 

All the practice resources like leetcode and data lemur allow you to run your code. I did not expect this. Is this common practice? Definitely failed and feel totally dejected 😞 ",191,0.94,https://www.reddit.com/r/datascience/comments/1jz4teg/why_wont_they_let_you_run_your_code/,False,True,False
1jz0h1y,ElectrikMetriks,1744641544.0,18,/r/datascience/comments/1jz0h1y/saw_greg_pinged_me_logged_off_immediately/,datascience,*Saw Greg pinged me & logged off immediately*,,495,0.98,https://i.redd.it/rp4lu4ruatue1.png,False,False,False
1jyu503,vintagefiretruk,1744620386.0,40,/r/datascience/comments/1jyu503/powerbi_but_not_powerbi/,datascience,PowerBI but not PowerBI,"Figured this was the best community to ask this question:

I have a bunch of personal data (think personal finance spreadsheet type stuff), and I'd love to build a dashboard for it - purely for me. I have access to Power BI through my work so I know how to build the sort of thing I want.

However

I obviously can't use my work account to create a personal dashboard with my personal data etc, so I'm trying to find alternative solutions.

To set up a personal PBI account seems to need a lot of hoops like owning your own domain for an email address etc, so I'm wondering if anyone in this community might use any other dashboard tools that they reccomend and that would have similar basic functionality and be a bit less faff to try and set up a personal account?",32,0.81,https://www.reddit.com/r/datascience/comments/1jyu503/powerbi_but_not_powerbi/,False,True,False
1jyq1tk,AutoModerator,1744603309.0,60,/r/datascience/comments/1jyq1tk/weekly_entering_transitioning_thread_14_apr_2025/,datascience,"Weekly Entering & Transitioning - Thread 14 Apr, 2025 - 21 Apr, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",9,0.99,https://www.reddit.com/r/datascience/comments/1jyq1tk/weekly_entering_transitioning_thread_14_apr_2025/,False,True,False
1jyloqi,LeaguePrototype,1744588866.0,17,/r/datascience/comments/1jyloqi/reputed_graduate_certificates/,datascience,Reputed Graduate Certificates?,"Since finishing my Master's in Stats 4+ years ago the field has changed a lot. I feel like my education had a lot of useless classes and missed things like bayesian, graphs, DL, big data, etc.

Stanford seems to have some good graduate certs with classes I'm interested in and my employer will cover 2/3 the costs. Are these worth taking or is there a better way to get this info online? I have 3 YOE as DS at well known companies, so will these graduate certs from reputed unis improve my resume or is it similar to coursera?",29,0.83,https://www.reddit.com/r/datascience/comments/1jyloqi/reputed_graduate_certificates/,False,True,False
1jyicx6,Loud_Communication68,1744579181.0,97,/r/datascience/comments/1jyicx6/why_are_methods_like_forwardbackward_selection/,datascience,Why are methods like forward/backward selection still taught?,"When you could just use lasso/relaxed lasso instead?


https://www.stat.cmu.edu/~ryantibs/papers/bestsubset.pdf",81,0.85,https://www.reddit.com/r/datascience/comments/1jyicx6/why_are_methods_like_forwardbackward_selection/,False,True,False
1jygakg,MyKo101,1744573736.0,4,/r/datascience/comments/1jygakg/features_you_would_love/,datascience,Features you would love,If someone were to create a new cloud based data system. What features would you love it to have? What features do other services lack?,0,0.25,https://www.reddit.com/r/datascience/comments/1jygakg/features_you_would_love/,False,True,False
1jy2pe0,Feeling_Bad1309,1744531456.0,119,/r/datascience/comments/1jy2pe0/is_a_masters_still_necessary/,datascience,Is a Master’s Still Necessary?,"Can I break into DS with just a bachelor’s? I have 3 YOE of relevant experience although not titled as “data scientist”. I always come across roles with bachelor’s as a minimum requirement but master’s as a preferred. However, I have not been picked up for an interview at all. 

I do not want to take the financial burden of a masters degree since I already have the knowledge and experience to succeed. But it feels like I am just putting myself at a disadvantage in the field. Should I just get an online degree for the masters stamp? ",129,0.82,https://www.reddit.com/r/datascience/comments/1jy2pe0/is_a_masters_still_necessary/,False,True,False
1jxtzs1,Starktony11,1744499468.0,15,/r/datascience/comments/1jxtzs1/which_topics_or_questions_frequently_asked_for_a/,datascience,Which topics or questions frequently asked for a data science role in traditional banks? Or for fraud detection/risk modeling topics?,"Hi,

I am proficient with statistics(causal inference , parametric non parametric tests) and ML models, but i don’t what models, statistical techniques are used in fraud detection and risk modeling, especially in finance industry. So, could anyone suggest FAQs? Or topics i should focus more on? Or any not common topic you ask to candidates that are crucial to know? Role requires 3+ years of experience.

Also, would like to know what techniques you work on in your day to work in fraud detection. It would help me great how it works in industry and prepare for a potential interview. Thanks! 


Edit-
Would you consider it to be similar like anomaly detection in time series? If so what methods you use in your company, i know concept of a few methods like z-score, arima, sarima, med and other but would like to know in practice what you use as well

Edit 2- i am interested more on the topics that i could learn, like i know sql and python will be there",24,0.82,https://www.reddit.com/r/datascience/comments/1jxtzs1/which_topics_or_questions_frequently_asked_for_a/,False,True,False
1jxl18x,Daniel-Warfield,1744474855.0,14,/r/datascience/comments/1jxl18x/ace_the_interview_sql_intuitively_and/,datascience,Ace The Interview - SQL Intuitively and Exhaustively Explained,"SQL is easy to learn and hard to master. Realistically, the difficulty of the questions you get will largely be dictated by the job role you're trying to fill.

From it's highest level, SQL is a ""declarative language"", meaning it doesn't define a set of operations, but rather a desired end result. This can make SQL incredibly expressive, but also a bit counterintuitive, especially if you aren't fully aware of it's declarative nature.

SQL expressions are passed through an SQL engine, like PostgreSQL, MySQL, and others. Thes engines parse out your SQL expressions, optimize them, and turn them into an actual list of steps to get the data you want. While not as often discussed, for beginners I recommend SQLite. It's easy to set up in virtually any environment, and allows you to get rocking with SQL quickly. If you're working in big data, I recommend also brushing up on something like PostgreSQL, but the differences are not so bad once you have a solid SQL understanding.

In being a high level declaration, SQL’s grammatical structure is, fittingly, fairly high level. It’s kind of a weird, super rigid version of English. SQL queries are largely made up of:

* **Keywords:** special words in SQL that tell an engine what to do. Some common ones, which we’ll discuss, are `SELECT, FROM, WHERE, INSERT, UPDATE, DELETE, JOIN, ORDER BY, GROUP BY` . They can be lowercase or uppercase, but usually they’re written in uppercase.
* **Identifiers:** Identifiers are the names of database objects like tables, columns, etc.
* **Literals:** numbers, text, and other hardcoded values
* **Operators:** Special characters or keywords used in comparison and arithmetic operations. For example `!=`, `<` ,`OR`, `NOT` , `*`, `/`, `%` , `IN`, `LIKE` . We’ll cover these later.
* **Clauses:** These are the major building block of SQL, and can be stitched together to combine a queries general behavior. They usually start with a keyword, like 
   * `SELECT` – defines which columns to return 
   * `FROM` – defines the source table 
   * `WHERE` – filters rows 
   * `GROUP BY` – groups rows etc.  

By combining these clauses, you create an SQL query

There are a ton of things you can do in SQL, like create tables:

    CREATE TABLE People(first_name, last_name, age, favorite_color)

Insert data into tables:

    INSERT INTO People
    VALUES
        ('Tom', 'Sawyer', 19, 'White'),
        ('Mel', 'Gibson', 69, 'Green'),
        ('Daniel', 'Warfiled', 27, 'Yellow')

Select certain data from tables:

    SELECT first_name, favorite_color FROM People

Search based on some filter

    SELECT * FROM People WHERE id = 3

And Delete Data

    DELETE FROM People WHERE age < 30 

What was previously mentioned makes up the cornerstone of pretty much all of SQL. Everything else builds on it, and there is a lot.

**Primary and Foreign Keys**  
A *primary key* is a unique identifier for each record in a table. A *foreign key* references a primary key in another table, allowing you to relate data across tables. This is the backbone of relational database design.

**Super Keys and Composite Keys**  
A *super key* is any combination of columns that can uniquely identify a row. When a unique combination requires multiple columns, it’s often called a *composite key* — useful in complex schemas like logs or transactions.

**Normalization and Database Design**  
Normalization is the process of splitting data into multiple related tables to reduce redundancy. First Normal Form (1NF) ensures atomic rows, Second Normal Form (2NF) separates logically distinct data, and Third Normal Form (3NF) eliminates derived data stored in the same table.

**Creating Relational Schemas in SQLite**  
You can explicitly define tables with `FOREIGN KEY` constraints using `CREATE TABLE`. These relationships enforce referential integrity and enable behaviors like cascading deletes. SQLite enforces `NOT NULL` and `UNIQUE` constraints strictly, making your schema more robust.

**Entity Relationship Diagrams (ERDs)**  
ERDs visually represent tables and their relationships. Dotted lines and cardinality markers like `{0,1}` or `0..N` indicate how many records in one table relate to another, which helps document and debug schema logic.

**JOINs**  
JOIN operations combine rows from multiple tables using foreign keys. `INNER JOIN` includes only matched rows, `LEFT JOIN` includes all from the left table, and `FULL OUTER JOIN` (emulated in SQLite) combines both. Proper JOINs are critical for data integration.

**Filtering and LEFT/RIGHT JOIN Differences**  
JOIN order affects which rows are preserved when there’s no match. For example, using `LEFT JOIN` ensures all left-hand rows are kept — useful for identifying unmatched data. SQLite lacks `RIGHT JOIN`, but you can simulate it by flipping the table order in a `LEFT JOIN`.

**Simulating FULL OUTER JOINs**  
SQLite doesn’t support `FULL OUTER JOIN`, but you can emulate it with a `UNION` of two `LEFT JOIN` queries and a `WHERE` clause to catch nulls from both sides. This approach ensures no records are lost in either table.

**The WHERE Clause and Filtration**  
`WHERE` filters records based on conditions, supporting logical operators (`AND`, `OR`), numeric comparisons, and string operations like `LIKE`, `IN`, and `REGEXP`. It's one of the most frequently used clauses in SQL.

**DISTINCT Selections**  
Use `SELECT DISTINCT` to retrieve unique values from a column. You can also select distinct combinations of columns (e.g., `SELECT DISTINCT name, grade`) to avoid duplicate rows in the result.

**Grouping and Aggregation Functions**  
With `GROUP BY`, you can compute metrics like `AVG`, `SUM`, or `COUNT` for each group. `HAVING` lets you filter grouped results, like showing only departments with an average salary above a threshold.

**Ordering and Limiting Results**  
`ORDER BY` sorts results by one or more columns in ascending (`ASC`) or descending (`DESC`) order. `LIMIT` restricts the number of rows returned, and `OFFSET` lets you skip rows — useful for pagination or ranked listings.

**Updating and Deleting Data**  
`UPDATE` modifies existing rows using `SET`, while `DELETE` removes rows based on `WHERE` filters. These operations can be combined with other clauses to selectively change or clean up data.

**Handling NULLs**  
`NULL` represents missing or undefined values. You can detect them using `IS NULL` or replace them with defaults using `COALESCE`. Aggregates like `AVG(column)` ignore NULLs by default, while `COUNT(*)` includes all rows.

**Subqueries**  
Subqueries are nested `SELECT` statements used inside `WHERE`, `FROM`, or `SELECT`. They’re useful for filtering by aggregates, comparisons, or generating intermediate results for more complex logic.

**Correlated Subqueries**  
These are subqueries that reference columns from the outer query. Each row in the outer query is matched against a custom condition in the subquery — powerful but often inefficient unless optimized.

**Common Table Expressions (CTEs)**  
CTEs let you define temporary named result sets with `WITH`. They make complex queries readable by breaking them into logical steps and can be used multiple times within the same query.

**Recursive CTEs**  
Recursive CTEs solve hierarchical problems like org charts or category trees. A base case defines the start, and a recursive step extends the output until no new rows are added. Useful for generating sequences or computing reporting chains.

**Window Functions**  
Window functions perform calculations across a set of table rows related to the current row. Examples include `RANK()`, `ROW_NUMBER()`, `LAG()`, `LEAD()`, `SUM() OVER ()`, and moving averages with sliding windows.

These all can be combined together to do a lot of different stuff.

In my opinion, this is too much to learn efficiently learn outright. It requires practice and the slow aggregation of concepts over many projects. If you're new to SQL, I recommend studying the basics and learning through doing. However, if you're on the job hunt and you need to cram, you might find this breakdown useful: [https://iaee.substack.com/p/structured-query-language-intuitively](https://iaee.substack.com/p/structured-query-language-intuitively)",223,0.88,https://www.reddit.com/r/datascience/comments/1jxl18x/ace_the_interview_sql_intuitively_and/,False,True,False
1jxk5za,phicreative1997,1744472558.0,32,/r/datascience/comments/1jxk5za/building_a_reliable_texttosql_pipeline_a/,datascience,Building a Reliable Text-to-SQL Pipeline: A Step-by-Step Guide pt.1,,9,0.58,https://medium.com/p/9041b0777a77,False,False,False
1jxe7rg,levenshteinn,1744453493.0,5,/r/datascience/comments/1jxe7rg/help_modeling_tariff_impacts_on_trade_flow/,datascience,[Help] Modeling Tariff Impacts on Trade Flow,"
I'm working on a trade flow forecasting system that uses the RAS algorithm to disaggregate high-level forecasts to detailed commodity classifications. The system works well with historical data, but now I need to incorporate the impact of new tariffs without having historical tariff data to work with.

Current approach:
- Use historical trade patterns as a base matrix
- Apply RAS to distribute aggregate forecasts while preserving patterns

Need help with:
- Methods to estimate tariff impacts on trade volumes by commodity
- Incorporating price elasticity of demand
- Modeling substitution effects (trade diversion)
- Integrating these elements with our RAS framework

Any suggestions for modeling approaches that could work with limited historical tariff data? Particularly interested in econometric methods or data science techniques that maintain consistency across aggregation levels.

Thanks in advance!",5,0.65,https://www.reddit.com/r/datascience/comments/1jxe7rg/help_modeling_tariff_impacts_on_trade_flow/,False,True,False
1jxdlfg,SonicBoom_81,1744450729.0,58,/r/datascience/comments/1jxdlfg/marketing_mix_models_are_they_really_a_good_idea/,datascience,Marketing Mix Models - are they really a good idea?,"hi,

I've seen a prior thread on this, but my question is more technical...

A prior company got sold a Return on Marketing Invest project by one of the big 4 consultancies.  The basis of it was build a bunch of MMMs, pump the budget in, and it automatically tells what you where to spend the budget to get the most bang for you buck.  Sounds wonderful.

I was the DS shadowing the consultancy to learn the models, so we could do a refresh.  The company had an annual marketing budget of 250m€ and its revenue was between 1.5 and 2bn €.

Once I got into doing the refresh, I really felt the process was never going to succeed.  Marketing thought ""there's 3 years of data, we must have a good model"", but in reality 3\*52 weeks is a tiny amount of data, when you try to fit in TV, Radio, Press, OOH, Whitemail, Email, Search, Social, and then include prices from you and comp, and seasonal variables.

You need to adstock each media to take affect for lags - and finding the level of adstock requires experimentation. The 156 weeks need to have a test and possibly a validation set given the experiments.

The business is then interested in things like what happens when we do TV and OOH together, which means creating combined variables.  More variables on very little data.

I am a practical Data Scientist.  I don't get hung up on the technical details and am focused on generating value, but this whole process seemed a crazy and expensive waste of time.  

The positive that came out of it was that we started doing AB testing in certain areas where the initial models suggested there was very low return, and those areas had previously been very resistant to any kind of testing.  

This feels a bit like a rant, but I'm genuinely interested if people think it can work.  It feels like its a over promising in the worst way.",113,0.95,https://www.reddit.com/r/datascience/comments/1jxdlfg/marketing_mix_models_are_they_really_a_good_idea/,False,True,False
1jx5k15,SingerEast1469,1744419806.0,24,/r/datascience/comments/1jx5k15/any_good_classification_datasets/,datascience,Any good classification datasets…,…that are comprised primarily of categorical features? Looking to test some segmentation code. Real world data preferred.,0,0.31,https://www.reddit.com/r/datascience/comments/1jx5k15/any_good_classification_datasets/,False,True,False
1jwlf3f,NervousVictory1792,1744362266.0,28,/r/datascience/comments/1jwlf3f/causal_inference_casework/,datascience,Causal Inference Casework,Hii All. My team currently has a demand forecasting model in place. Though it answers a lot of questions but isnt very good. I did a one day research on casual inference and from a brief understanding I feel it can be something worth looking at. I am a junior data scientist. How can I go forward and put this case forward to the principal data scientist from whom I need a sign off essentially. Should I create a POC on my own without telling anyone and present it with the findings or are there better ways ?? Thanks in advance :),21,0.82,https://www.reddit.com/r/datascience/comments/1jwlf3f/causal_inference_casework/,False,True,False
1jwduc6,chiqui-bee,1744333496.0,5,/r/datascience/comments/1jwduc6/predicting_with_anonymous_features_how_and_why/,datascience,Predicting with anonymous features: How and why?,,5,0.7,/r/kaggle/comments/1jwa7et/predicting_with_anonymous_features_how_and_why/,False,False,False
1jwbevk,etherealcabbage72,1744326138.0,76,/r/datascience/comments/1jwbevk/what_technical_skills_should_young_data/,datascience,What technical skills should young data scientists be learning?,"Data science is obviously a broad and ill-defined term, but most DS jobs today fall into one of the following flavors:

- Data analysis (a/b testing, causal inference, experimental design)

- Traditional ML (supervised learning, forecasting, clustering) 

- Data engineering (ETL, cloud development, model monitoring, data modeling)

- Applied Science (Deep learning, optimization, Bayesian methods, recommender systems, typically more advanced and niche, requiring doctoral education)

The notion of a “full stack” data scientist has declined in popularity, and it seems that many entrants into the field need to decide one of the aforementioned areas to specialize in to build a career. 

For instance, a seasoned product DS will be the best candidate for senior product DS roles, but not so much for senior data engineering roles, and vice versa. 

Since I find learning and specializing in everything to be infeasible, I am interested in figuring out which of these “paths” will equip one with the most employable skillset, especially given how fast “AI” is changing the landscape. 

For instance, when I talk to my product DS friends, they advise to learn how to develop software and use cloud platforms since it is essential in the age of big data, even though they rarely do this on the job themselves. 

My data engineer friends on the other hand say that data engineering tools are easy to learn, change too often, and are becoming increasingly abstracted, making developing a strong product/business sense a wiser choice. 

Is either group right? 

Am I overthinking and would be better off just following whichever path interests me most? 

EDIT: I think the essence of my question was to assume that candidates have solid business knowledge. Given this, which skillset is more likely to survive in today and tomorrow’s job market given AI advancements and market conditions. Saying all or multiple pathways will remain important is also an acceptable answer. ",397,0.97,https://www.reddit.com/r/datascience/comments/1jwbevk/what_technical_skills_should_young_data/,False,True,False
1jw7i9l,Gold-Artichoke-9288,1744315889.0,9,/r/datascience/comments/1jw7i9l/seeking_advice_finetuning/,datascience,Seeking advice fine-tuning,"Hello, i am still new to fine tuning trying to learn by doing projects.

Currently im trying to fine tune a model with unsloth, i found a dataset in hugging face and have done the first project, the results were fine (based on training and evaluation loss).

So in my second project i decided to prepare my own data, i have pdf files with plain text and im trying to transform them into a question answer format as i read somewhere that this format is necessary to fine tune models. I find this a bit odd as acquiring such format could be nearly impossible.

So i came up with two approaches, i extracted the text from the files into small chnuks. First one is to use some nlp technics and pre trained model to generate questions or queries based on those chnuks results were terrible maybe im doing something wrong but idk. Second one was to only use one feature which is the chunks only 215 row . Dataset shape is (215, 1) I trained it on 2000steps and notice an 
overfitting by measuring the loss of both training and testing test loss was 3 point something and traing loss was 0.00…somthing.

My questions are:
- How do you prepare your data if you have pdf files with plain text my case (datset about law)
- what are other evaluation metrics you do
- how do you know if your model ready for real world deployment ",8,0.8,https://www.reddit.com/r/datascience/comments/1jw7i9l/seeking_advice_finetuning/,False,True,False
1jvwexo,LimpInvite2475,1744287252.0,17,/r/datascience/comments/1jvwexo/do_professionals_in_the_industry_still_refer_to/,datascience,Do professionals in the industry still refer to online sources or old code for solutions?,"Hey everyone,  
I’m currently studying and working on improving my skills in data science, and I’ve been wondering something:

Do professionals—those already working in the industry—still take reference from online sources like Stack Overflow, old GitHub repos, documentation, or even their previous Jupyter notebooks when they’re coding?

Sometimes I feel like I’m “cheating” when I google things I forgot or reuse snippets from old work. But is this actually a normal part of professional workflows?

For example, take this small code block below:

  
`# 1. Instantiate the random forest classifier`

`rf = RandomForestClassifier(random_state=42)`



`# 2. Create a dictionary of hyperparameters to tune`

`cv_params = {'max_depth': [None],`

`'max_features': [1.0],`

`'max_samples': [1.0],`

`'min_samples_leaf': [2],`

`'min_samples_split': [2],`

`'n_estimators': [300],`

`}`



`# 3. Define a list of scoring metrics to capture`

`scoring = ['accuracy', 'precision', 'recall', 'f1']`



`# 4. Instantiate the GridSearchCV object`

`rf_cv = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='recall')`

  
Would professionals be able to code this entire thing out from memory, or is referencing docs and previous code still common?",0,0.35,https://www.reddit.com/r/datascience/comments/1jvwexo/do_professionals_in_the_industry_still_refer_to/,False,True,False
1jvrgr5,qtalen,1744267056.0,5,/r/datascience/comments/1jvrgr5/fixing_the_agent_handoff_problem_in_llamaindexs/,datascience,Fixing the Agent Handoff Problem in LlamaIndex's AgentWorkflow System,"[Fixing the Agent Handoff Problem in LlamaIndex's AgentWorkflow System](https://preview.redd.it/shjbjpxccyte1.png?width=1280&format=png&auto=webp&s=3338b6859f2cc9e4852d1ec0a3ffd59e3511c3e3)

# The position bias in LLMs is the root cause of the problem

I've been working with LlamaIndex's AgentWorkflow framework - a promising multi-agent orchestration system that lets different specialized AI agents hand off tasks to each other. But there's been one frustrating issue: when Agent A hands off to Agent B, Agent B often fails to continue processing the user's original request, forcing users to repeat themselves.

This breaks the natural flow of conversation and creates a poor user experience. Imagine asking for research help, having an agent gather sources and notes, then when it hands off to the writing agent - silence. You have to ask your question again!

[The receiving agent doesn't immediately respond to the user's latest request - the user has to repeat their question.](https://preview.redd.it/ucl76xnmcyte1.png?width=883&format=png&auto=webp&s=4fc975569f3bda5238ebb5ed1e5b08ff7cc86049)

**Why This Happens: The Position Bias Problem**

After investigating, I discovered this stems from how large language models (LLMs) handle long conversations. They suffer from ""position bias"" - where information at the beginning of a chat gets ""forgotten"" as new messages pile up.

[Different positions in the chat context have different attention weights. Arxiv 2407.01100](https://preview.redd.it/ugtqdq2tdyte1.png?width=519&format=png&auto=webp&s=cf9978aef461521633c8e20786ed48d8a106a2de)

In AgentWorkflow:

1. User requests go into a memory queue first
2. Each tool call adds 2+ messages (call + result)
3. The original request gets pushed deeper into history
4. By handoff time, it's either buried or evicted due to token limits

[FunctionAgent puts both tool\_call and tool\_call\_result info into ChatMemory, which pushes user requests to the back of the queue.](https://preview.redd.it/ypd4caewdyte1.png?width=786&format=png&auto=webp&s=240629c41c2f581dd7c3c8917912827358db5525)

Research shows that in an 8k token context window, information in the first 10% of positions can lose over 60% of its influence weight. The LLM essentially ""forgets"" the original request amid all the tool call chatter.

**Failed Attempts**

First, I tried the developer-suggested approach - modifying the handoff prompt to include the original request. This helped the receiving agent see the request, but it still lacked context about previous steps.

[The original handoff implementation didn't include user request information.](https://preview.redd.it/lbnm2laxcyte1.png?width=681&format=png&auto=webp&s=261eb162385f7f471c92a7812c188404ed682548)

[The output of the updated handoff now includes both chat history review and user request information.](https://preview.redd.it/u5eukjkycyte1.png?width=681&format=png&auto=webp&s=2956e9aa2f88ce7aa65da0f09fbdb93a7930aa27)

Next, I tried reinserting the original request after handoff. This worked better - the agent responded - but it didn't understand the full history, producing incomplete results.

[After each handoff, I copy the original user request to the queue's end. ](https://preview.redd.it/j5irsta0dyte1.png?width=807&format=png&auto=webp&s=f4cbaf58ca093a06938e0ccf9cd7ea9164def92d)

**The Solution: Strategic Memory Management**

The breakthrough came when I realized we needed to work with the LLM's natural attention patterns rather than against them. My solution:

1. **Clean Chat History**: Only keep actual user messages and agent responses in the conversation flow
2. **Tool Results to System Prompt**: Move all tool call results into the system prompt where they get 3-5x more attention weight
3. **State Management**: Use the framework's state system to preserve critical context between agents

[Attach the tool call result as state info in the system\_prompt.](https://preview.redd.it/yj1wmx06eyte1.png?width=634&format=png&auto=webp&s=96272c1d5ead65d83881780ae6ee4d92d7c0e7aa)

This approach respects how LLMs actually process information while maintaining all necessary context.

**The Results**

After implementing this:

* Receiving agents immediately continue the conversation
* They have full awareness of previous steps
* The workflow completes naturally without repetition
* Output quality improves significantly

For example, in a research workflow:

1. Search agent finds sources and takes notes
2. Writing agent receives handoff
3. It immediately produces a complete report using all gathered information

[ResearchAgent not only continues processing the user request but fully perceives the search notes, ultimately producing a perfect research report.](https://preview.redd.it/1hw8vza8dyte1.png?width=671&format=png&auto=webp&s=b721645671c5639c2e0b7990395ed077a992900f)

**Why This Matters**

Understanding position bias isn't just about fixing this specific issue - it's crucial for anyone building LLM applications. These principles apply to:

* All multi-agent systems
* Complex workflows
* Any application with extended conversations

The key lesson: LLMs don't treat all context equally. Design your memory systems accordingly.

[In different LLMs, the positions where the model focuses on important info don't always match the actual important info spots. ](https://preview.redd.it/ex69ri8cdyte1.png?width=575&format=png&auto=webp&s=d680659f6e9889775c4d24b650e06ac9791945df)

**Want More Details?**

If you're interested in:

* The exact code implementation
* Deeper technical explanations
* Additional experiments and findings

Check out the full article on

[https://www.dataleadsfuture.com/fixing-the-agent-handoff-problem-in-llamaindexs-agentworkflow-system/](https://www.dataleadsfuture.com/fixing-the-agent-handoff-problem-in-llamaindexs-agentworkflow-system/)

I've included all source code and a more thorough discussion of position bias research.

Have you encountered similar issues with agent handoffs? What solutions have you tried? Let's discuss in the comments!",21,0.82,https://www.reddit.com/r/datascience/comments/1jvrgr5/fixing_the_agent_handoff_problem_in_llamaindexs/,False,True,False
1jvlqx7,Upstairs-Deer8805,1744246800.0,15,/r/datascience/comments/1jvlqx7/is_agentic_ai_a_generative_ai_swe_or_am_i_missing/,datascience,"Is Agentic AI a Generative AI + SWE, or am I missing a thing?","Basically I just started doing hands-on around the Agentic AI. However, it all felt like creating multiple functions/modules powered with GenAI, and then chaining them together using SWE skills such as through endpoints. 

Some explanation said that Agentic AI is proactive and GenAI is reactive. But then, I also thought that if you have a function that uses GenAI to produce output, then run another code to send the result somewhere else, wouldn't that achive the same thing as Agentic AI?

Or am I missing something?

Thank you!

Note: this is an oversimplification of a scenario.",43,0.88,https://www.reddit.com/r/datascience/comments/1jvlqx7/is_agentic_ai_a_generative_ai_swe_or_am_i_missing/,False,True,False
1jvcz3t,alpha_centauri9889,1744223233.0,20,/r/datascience/comments/1jvcz3t/genai_and_llm_preparation_for_technical_rounds/,datascience,GenAI and LLM preparation for technical rounds,"From technical rounds perspective, can anyone suggest resources or topics to study for GenAI and LLMs? I have had some experience with them, but then in interviews they go into the depth (eg. Attention mechanism, Q-learning, chunking strategies, case studies etc.). Honestly, most of what I can see in YouTube is just in surface level. If it's just about calling an API and feeding your documents, then it's too simple, but that's not how interviews happen. ",97,0.95,https://www.reddit.com/r/datascience/comments/1jvcz3t/genai_and_llm_preparation_for_technical_rounds/,False,True,False
1jvav77,fridchikn24,1744218154.0,23,/r/datascience/comments/1jvav77/just_took_a_new_job_in_supply_chain_optimization/,datascience,"just took a new job in supply chain optimization, what do i need to learn to be effective?",I am new to supply chain and need to know what resources/concepts I should be familiar with.,35,0.87,https://www.reddit.com/r/datascience/comments/1jvav77/just_took_a_new_job_in_supply_chain_optimization/,False,True,False
1jv4xqf,Particular_Reality12,1744202852.0,31,/r/datascience/comments/1jv4xqf/hi_im_a_junior_in_high_school_and_i_am_interested/,datascience,"Hi, I’m a junior in high school and I am interested in Data Science. What’s steps should I take to get there (from now to the end of high school)?","Picture will be referenced later

For some background all I’ve done related to data science is a harvard edx python course which I took twice (first time I got all the way to the final project then quit, the second time I wasn’t able to finish all the lectures). Though I know I have the skills, I really need a refresher on the language.

Some questions I have are:
1. Is it good to take certifications in this field. For example, in the computer networking role, the CCNA is an extremely important certification and can easily get you hired for an entry level position. Is there anything similar in data science?

2. Any way to find data science internships? Idk why but it’s kinda hard to find data science internships. I did manage to find a few, but idk which ones the best use of my time. Any help here?

3. In the picture I put a roadmap that i found online. The words are kinda small; to clarify, first they say to learn python, then R, then GIT, then data structures and algorithms, after that they recommend learning SQL, then math/statistics, then data processing and visualization, machine learning, deep learning, and finally big data. Is this a good path to follow? If so how should I approach going down this route? Any resources I can use to start learning?

Any other tips would be greatly appreciated, thank you all for reading I really appreciate it.",0,0.45,https://i.redd.it/o565hwzk2tte1.jpeg,False,False,False
1juzclh,wang-bang,1744180578.0,7,/r/datascience/comments/1juzclh/familiar_matchmaking_in_gaming_to_match_players/,datascience,Familiar matchmaking in gaming; to match players with players they like and have played with before,"I've seen the classic MMRs before based on skill level in many different games. 

But the truth is gaming is about fun, and playing with people you already like or who are similar to people you like is a massive fun multiplier

So the challenge is how would you design a method to achieve that? Multiple algorithms, or something simpler?  
  
My initial idea is raw, and ripe for improvement

During or after a game session is over you get to thumbs up or thumbs down players you enjoyed playing with. 

Later on if you are in a matchmaking queue the list of players you've thumbed up is consulted and the party that has players with the greatest total thumbs up points at the top of that list gets matched to your party if there is free space, and if you are at the top of the available people on their end too.

The end goal here is to make public matchmaking more fun, and feel more familiar as you get to play repeatedly with players you've enjoyed playing with before.

The main issue with this type of matchmaking is that over time it would be difficult for newer players to get enough thumbs up to get higher on the list. Harder to get to play with the people who already have a large pool of people they like to play with. I don't know how to solve that issue at the moment.",23,0.95,https://www.reddit.com/r/datascience/comments/1juzclh/familiar_matchmaking_in_gaming_to_match_players/,False,True,False
1juvgek,chrisgarzon19,1744166203.0,0,/r/datascience/comments/1juvgek/azure_course_for_beginners_learn_azure_data/,datascience,Azure Course for Beginners | Learn Azure & Data Bricks in 1 Hour,"# FREE Azure Course for Beginners | Learn Azure & Data Bricks in 1 Hour

[https://www.youtube.com/watch?v=8XH2vTyzL7c](https://www.youtube.com/watch?v=8XH2vTyzL7c)",0,0.42,https://www.reddit.com/r/datascience/comments/1juvgek/azure_course_for_beginners_learn_azure_data/,False,True,False
1juo7ue,MightGuy8Gates,1744145436.0,68,/r/datascience/comments/1juo7ue/absolutely_bombed_interview/,datascience,Absolutely BOMBED Interview,"I landed a position 3 weeks ago, and so far wasn’t what I expected in terms of skills. Basically, look at graphs all day and reboot IT issues. Not ideal, but I guess it’s an ok start. 

Right when I started, I got another interview from a company paying similar, but more aligned to my skill set in a different industry. I decided to do it for practice based on advice from l people on here.

First interview went well, then got a technical interview scheduled for today and ABSOLUTELY BOMBED it. It was BAD BADD. It made me realize how confused I was with some of the basics when it comes to the field and that I was just jumping to more advanced skills, similar to what a lot of people on this group do. It was literally so embarrassing and I know I won’t be moving to the next steps. 

Basically the advice I got from the senior data scientist was to focus on the basics and don’t rush ahead to making complex models and deployments. Know the basics of SQL, Statistics (linear regression, logistic, xgboost) and how you’re getting your coefficients and what they mean, and Python. 

Know the basics!!",528,0.98,https://www.reddit.com/r/datascience/comments/1juo7ue/absolutely_bombed_interview/,False,True,False
1ju139m,mad_e_y_e,1744073148.0,15,/r/datascience/comments/1ju139m/career_crossroads_ds_manager_retail_w_finance/,datascience,Career Crossroads: DS Manager (Retail) w/ Finance Background -> Head of Finance Analytics Offer - Seeking Guidance & Perspectives,"
Hey r/datascience,

Hoping to tap into the collective wisdom here regarding a potential career move. I'd appreciate any insights or perspectives you might have.

My Background:

Current Role: Data Science Manager at a Retail company.

Experience: ~8 years in Data Science (started as IC, now Manager).

Prior Experience: ~5 years in Finance/M&A before transitioning into data science.
The Opportunity:

I have an opportunity for a Head of Finance Analytics role, situated within (or closely supporting) the Financial Planning & Analysis (FP&A) function.

The Appeal: 
This role feels like a potentially great way to merge my two distinct career paths (Finance + Data Science). It leverages my domain knowledge from both worlds. The ""Head of"" title also suggests significant leadership scope.

The Nature of the Work:
The primary focus will be data analysis using SQL and BI tools to support financial planning and decision-making.
Revenue forecasting is also a key component.
However, it's not a traditional data science role. Expect limited exposure to diverse ML projects or building complex predictive models beyond forecasting.
The tech stack is not particularly advanced (likely more SQL/BI-centric than Python/R ML libraries).


My Concerns / Questions for the Community:

Career Trajectory - Title vs. Substance? 
Moving from a ""Data Science Manager"" to a ""Head of Finance Analytics"" seems like a step up title-wise. However, is shifting focus primarily to SQL/BI-driven analysis and forecasting, away from broader ML/DS projects and advanced techniques, a potential functional downstep or specialization that might limit future pure DS leadership roles?

Technical Depth vs. Seniority: 
As you move towards Head of/Director/VP levels, how critical is maintaining cutting-edge data science technical depth versus deep domain expertise (finance), strategic impact through analysis, and leadership? Does the type of technical work (e.g., complex SQL/BI vs. complex ML) become less defining at these senior levels?

Compensation Outlook: 
What does the compensation landscape typically look like for senior analytics leadership roles like ""Head of Finance Analytics,"" especially within FP&A or finance departments, compared to pure Data Science management/director tracks in tech or other industries? Trying to gauge the long-term financial implications.

I'm essentially weighing the unique opportunity to blend my background and gain a significant leadership title (""Head of"") against the trade-offs in the type of technical work and the potential divergence from a purely data science leadership path.

Has anyone made a similar move or have insights into navigating careers at the intersection of Data Science and Finance/FP&A, particularly in roles heavy on analysis and forecasting? Any perspectives on whether this is a strategic pivot leveraging my unique background or a potential limitation for future high-level DS roles would be incredibly helpful.

Thanks in advance for your thoughts!

TL;DR: DS Manager (8 YOE DS, 5 YOE Finance) considering ""Head of Finance Analytics"" role. Opportunity to blend background + senior title. Work is mainly SQL/BI analysis + forecasting, less diverse/advanced DS. Worried about technical ""downstep"" vs. pure DS track & long-term compensation. Seeking advice.
",27,0.88,https://www.reddit.com/r/datascience/comments/1ju139m/career_crossroads_ds_manager_retail_w_finance/,False,True,False
1jtyyc0,vintagefiretruk,1744066951.0,68,/r/datascience/comments/1jtyyc0/do_remote_data_science_jobs_still_exsist/,datascience,Do remote data science jobs still exsist?,"Evry time I search remote data science etc jobs i exclusively seem to get hybrid if anything results back and most of them are 3+ days in office a week.

Do remote data science jobs even still exsist, and if so, is there some in the know place to look that isn't a paid for site or LinkedIn which gives me nothing helpful?",109,0.82,https://www.reddit.com/r/datascience/comments/1jtyyc0/do_remote_data_science_jobs_still_exsist/,False,True,False
1jtoul7,guna1o0,1744041752.0,39,/r/datascience/comments/1jtoul7/data_science_projects_for_1_year_of_experience/,datascience,Data Science Projects for 1 Year of Experience,"Hello senior/lead/manager data scientist,  
What kind of data science projects do you typically expect from a candidate with 1 year of experience?",141,0.95,https://www.reddit.com/r/datascience/comments/1jtoul7/data_science_projects_for_1_year_of_experience/,False,True,False
1jtn8oj,santiviquez,1744037775.0,14,/r/datascience/comments/1jtn8oj/if_snl_can_go_live_every_week_why_cant_our_models/,datascience,"If SNL can go live every week, why can't our models go live in 6 months?","""The show doesn't go on because it's ready. It goes because it's 11:30.""

I love this quote from Saturday Night Live's creator, Lorne Michaels. It holds a lot of wisdom about how projects should be planned and executed.

In data science, it perfectly captures the idea of shaping a project with fixed time and flexible scope. Too often, we get stuck in PoC hell. When every new project is treated as an experiment, requirements tend to be vague, definitions of done unclear. We fall into the rabbit hole of endlessly tweaking hyperparameters, convinced that the right combination will solve all our problems.

We end up running in circles, with yet another PoC that never makes it to production.

Lorne understood back in 1975 that to make people laugh every Saturday, they had to work with a fixed time and flexible scope. If they’ve managed to do that every week for nearly 50 years, why can't we get a model into production in less than six months?",0,0.45,https://i.redd.it/kvaf1k2uefte1.jpeg,False,False,False
1jtmrxz,ryime,1744036610.0,2,/r/datascience/comments/1jtmrxz/we_built_a_framework_for_building_sql_bots_and/,datascience,We built a framework for building SQL bots and automations!,"Hey folks! We recently released Oxy, an open-source framework for building SQL bots and automations: [https://github.com/oxy-hq/oxy](https://github.com/oxy-hq/oxy)

In short, Oxy gives you a simple YAML-based layer over LLMs so they can write accurate SQL with the right context. You can also build with these agents by combining them into workflows that automate analytics tasks.

The whole system is modular and flexible thanks to Jinja templates - you can easily reference or reuse results between steps, loop through data from previous operations, and connect everything together.

We have a few folks using us in production already, but would love to hear what you all think :)",10,0.63,https://i.redd.it/lc1cbpgrbfte1.gif,False,False,False
1jti77o,Emergency-Agreeable,1744022413.0,11,/r/datascience/comments/1jti77o/i_created_a_basic_playground_to_help_people/,datascience,I created a basic playground to help people familiarise themselves with copulas,"Hi guys,  
  
So, this app allows users to select a copula family, specify marginal distributions, and set copula parameters to visualize the resulting dependence structure.  
  
A standalone calculator is also included to convert a given Kendall’s tau value into the corresponding copula parameter for each copula family. This helps users compare models using a consistent level of dependence.

The motivation behind this project is to gain experience deploying containerized applications.

https://preview.redd.it/nd4nhhr65ete1.png?width=2554&format=png&auto=webp&s=edd9c4b7add7df49ee2e3d55d91a030d4204f80e

Here's is the link if anyone wants ton interact with it, it was build with desktop view in mind but later I realised that it's very likely people will try to access via phone, it still works but it doesn’t look tidy.

[https://copula-playground-app-n7fioequfq-lz.a.run.app](https://copula-playground-app-n7fioequfq-lz.a.run.app)",45,0.88,https://www.reddit.com/r/datascience/comments/1jti77o/i_created_a_basic_playground_to_help_people/,False,True,False
1jtdrvr,Feeling_Bad1309,1744003142.0,20,/r/datascience/comments/1jtdrvr/mscs_admit_preparing_for_2026_summer_internship/,datascience,MSCS Admit; Preparing for 2026 Summer Internship Recruitement,"I got admitted to a top MSCS program for Fall 2025! I want to be ready for Data Science recruitement for Summer 2026.

I have 3 YOE as a data scientist in a FinTech firm with a mix of cross-functional production-grade projects in NLP, GenAI, Unsupervised learning, Supervised learning with high proficiency in Python, SQL, and AWS. 

Unfortunately, do not have experience with big data technologies (Spark, Snowflake, Big Query, etc), experimentation (A/B Testing), or deployment due to the nature of my job. 

No recent personal projects. 

Lastly, I did my undergrad from a top school with majors in data science and business. Had some comprehensive projects from classes currently listed on my resume. 

Would highly appreciate advice on the best course of action in the comming 4-8 months to maximize my chances in landing a good internship in 2026. I recognize my weaknesses but would like to determine how I can prioritize them. Have not recruited/interviewed in a while. 

Add info: I am also an international working under an
n H-1B. 

Update: Many of you have flagged that I should not be seeking data science internships with 3 YOE. However, my current title is Quant analyst and is a bit more geared towards finance. Yes the skills are transferable but the problems and the approach are very different.",22,0.72,https://www.reddit.com/r/datascience/comments/1jtdrvr/mscs_admit_preparing_for_2026_summer_internship/,False,True,False
1jtcjlc,AutoModerator,1743998503.0,35,/r/datascience/comments/1jtcjlc/weekly_entering_transitioning_thread_07_apr_2025/,datascience,"Weekly Entering & Transitioning - Thread 07 Apr, 2025 - 14 Apr, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",6,0.88,https://www.reddit.com/r/datascience/comments/1jtcjlc/weekly_entering_transitioning_thread_07_apr_2025/,False,True,False
1js5jby,IMightBYourDad,1743865648.0,13,/r/datascience/comments/1js5jby/is_ongoing_part_time_degree_considered_a_red_flag/,datascience,Is ongoing part time degree considered a red flag during job hunting?,"Is ongoing part time degree considered a red flag on your resume during job hunt?

I’m pursuing a part time MBA on weekends to upskill myself. This doesn’t affect my productivity at work. I am currently considering switching jobs. 

I want to understand if this should be listed on my resume. I plan to inform the hiring manager during final stages of the interview. Let me know if I’m thinking about this wrong. ",21,0.78,https://www.reddit.com/r/datascience/comments/1js5jby/is_ongoing_part_time_degree_considered_a_red_flag/,False,True,False
1js1sgj,cptsanderzz,1743854176.0,41,/r/datascience/comments/1js1sgj/how_to_deal_with_medium_data/,datascience,How to deal with medium data,"I recently had a problem at work that dealt with what I’m coining as “medium” data which is not big data where traditional machine learning greatly helps and it wasn’t small data where you can really only do basic counts and means and medians. What I’m referring to is data that likely has a relationship that can be studied based on expertise but falls short in any sort of regression due to overfitting and not having the true variability based on the understood data.

The way I addressed this was I used elasticity as a predictor. Where I divided the percentage change of each of my inputs by my percentage change of my output which allowed me to calculate this elasticity constant then used that constant to somewhat predict what I would predict the change in output would be since I know what the changes in input would be. I make it very clear to stakeholders that this method should be used with a heavy grain of salt and to understand that this approach is more about seeing the impact across the entire dataset and changing inputs in specific places will have larger effects because a large effect was observed in the past.

So I ask what are some other methods to deal with medium sized data where there is likely a relationship but your ML methods result in overfitting and not being robust enough?

Edit: The main question I am asking is how have you all used basic statistics to incorporate them into a useful model/product that stakeholders can use for data backed decisions?",37,0.85,https://www.reddit.com/r/datascience/comments/1js1sgj/how_to_deal_with_medium_data/,False,True,False
1jrz1k5,NoteClassic,1743842539.0,11,/r/datascience/comments/1jrz1k5/ds_seeking_development_into_swe/,datascience,DS seeking development into SWE,"Hi community, 

I’m a data scientist that’s worked with both parametric and non parametric models. Quite experienced with deploying locally on our internal systems. 

Recently I’ve been needing to develop client facing systems for external systems. However I seem to be out of my depth.

Are there recommendations on courses that could help a DS with a core in pandas, scikit learn, keras and TF develop skills on how endpoints and API works? Development of backend applications in Python. I’m guessing it will be a major issue faced by many data scientists. 

I’d appreciate if you could help with recommendations of courses you’ve taken in this regard. ",36,0.82,https://www.reddit.com/r/datascience/comments/1jrz1k5/ds_seeking_development_into_swe/,False,True,False
1jrr35h,DieselZRebel,1743813058.0,17,/r/datascience/comments/1jrr35h/how_do_you_calculate_your_hourly_rate_if_you_were/,datascience,"How do you calculate your hourly rate, if you were to consider contract over FTE?!","I have always been an FTE in this field, receiving compensations and benefits that extend far beyond the base salary.

For many years now, every contract opportunity a recruiter presented never made financial sense to me, regardless of the level, and even for top FAANG employers known for generous pay packages. Is this really the case and contract workers are scammed in this field? or is it just my luck? Or is it the recruiters robbing us?

For reference, I take my annual TC, divide it by 48 × 40 (weeks times hours), because there will be at least 4 unpaid vacation weeks if I contract, to estimate my hourly rate, which isn't even fair to me because I am not factoring benefits. Anyway, the value I get is always multiples more than the best contract offer a recruiter presented. So am I doing it wrong?!


t",8,0.67,https://www.reddit.com/r/datascience/comments/1jrr35h/how_do_you_calculate_your_hourly_rate_if_you_were/,False,True,False
1jrdrpx,Grapphie,1743778269.0,22,/r/datascience/comments/1jrdrpx/ml_engineer_genai_amazon/,datascience,ML Engineer GenAI @ Amazon,"UP: Check my comment about how it went [here](https://www.reddit.com/r/datascience/comments/1jrdrpx/comment/mz4wmq0/)  
  
I'll be having technical ML Engineer interview @ Amazon on Thursday and was researching what can I expect to be asked about. All online resources talk about ML concepts, system design and leadership rules, but they seem to omit job description.

IMO it doesn't make any sense for interviewer to ask about PCA, K-means, linear regression, etc. when the role is mostly relating to applying GenAI solutions, LLM customization and fine tuning. Also data structures & algos seem to me close to irrelevant in that context.

Does anyone have any prior experience applying to this department and know if it's better to focus on prioritizing more on GenAI related concepts or keep it broad? Or maybe you've been interviewing to different department and can tell how closely the questions were relating to job description?",114,0.85,https://www.reddit.com/r/datascience/comments/1jrdrpx/ml_engineer_genai_amazon/,False,True,False
1jr680q,SonicBoom_81,1743751824.0,27,/r/datascience/comments/1jr680q/getting_back_to_data_science_after_4_years_out/,datascience,Getting back to Data Science after 4 years out,"Hi,

I left the corporate world to try to build my own apps.  They have not been successful and so I am trying to get hired back as a Data Scientist.  I have not yet heard anything from the applications I have sent so I would greatly appreciate your feedback on my CV.

I've anonymised where I can.  Re the picture, in Germany it is very normal and even expected that you add a picture, so this is why there is a placeholder there.

Cloud computing has become much more prevalent in the posts I see, so I am working my way through various Azure qualifications.

My current thoughts are:

* Add in LinkedIn Recommendations
* Somehow rewrite the key achievements to show monetary impact - current focus is on showing range of skills and impact
* Add Git - maybe add specific links to the different elements I've done for my own app development

Greatly appreciate your feedback



https://preview.redd.it/6q650rwytrse1.png?width=1414&format=png&auto=webp&s=8a509d15b782dbb272198e1a0a5fababcf9632a3

https://preview.redd.it/3kzliowytrse1.png?width=1414&format=png&auto=webp&s=c6ac751ff2cae83cb015db47b6fb995ebf0c04d0

https://preview.redd.it/31dqcnwytrse1.png?width=1414&format=png&auto=webp&s=e980bc60e07751e7cdaa23fae4672d5a383eec05",68,0.83,https://www.reddit.com/r/datascience/comments/1jr680q/getting_back_to_data_science_after_4_years_out/,False,True,False
1jr4rwq,gomezalp,1743745844.0,7,/r/datascience/comments/1jr4rwq/explain_complex_interactions_beyond_univariate/,datascience,Explain Complex Interactions Beyond Univariate Insights,"I’m analyzing a complex process where the outcome is client conversion rate, influenced by both numerical and categorical variables about client profile, product features, sales service, for instance.

So far, only univariate analyses have been used, but they fail to explain the variations effectively. I’ve already applied traditional multivariable models like decision trees and SHAP, but they haven’t provided clear or actionable insights to explain the changes in conversion. 

I’m now looking for creative, multivariable approaches (possibly involving dimensionality reduction or latent structure) to better explain what’s driving conversion. Any advice on how to approach this differently?",2,0.58,https://www.reddit.com/r/datascience/comments/1jr4rwq/explain_complex_interactions_beyond_univariate/,False,True,False
1jr1tsv,brianckeegan,1743735580.0,136,/r/datascience/comments/1jr1tsv/i_dare_someone_to_drop_this_into_a_stakeholder/,datascience,I dare someone to drop this into a stakeholder presentation,"From source: https://ustr.gov/issue-areas/reciprocal-tariff-calculations

> “Parameter values for ε and φ were selected. The price elasticity of import demand, ε, was set at 4… The elasticity of import prices with respect to tariffs, φ, is 0.25.“",1689,0.98,https://i.redd.it/vanv2v55hqse1.jpeg,False,False,False
1jqpm9u,FlyMyPretty,1743703706.0,34,/r/datascience/comments/1jqpm9u/data_scientist_quiz_from_unofficial_google_data/,datascience,Data Scientist quiz from Unofficial Google Data Science Blog,https://www.unofficialgoogledatascience.com/2025/03/quantifying-statistical-skills-needed.html,143,0.96,https://www.reddit.com/r/datascience/comments/1jqpm9u/data_scientist_quiz_from_unofficial_google_data/,False,True,False
1jqjinm,Daniel-Warfield,1743689570.0,33,/r/datascience/comments/1jqjinm/ace_the_interview_graphs/,datascience,Ace the Interview: Graphs,"A solid grasp of graph theory can give you an edge in technical interviews, especially when the problem at hand is less about code and more about the structure beneath it.

At their core, graphs are about relationships. Each node represents an entity, and each edge represents a relationship. This simple abstraction lets you model remarkably complex systems. What matters most in interviews is not memorizing jargon, but understanding what these structures mean and how to work with them intuitively.

A graph doesn’t care where things are laid out—it only matters who connects to whom. That’s why there are countless ways to visualize the same graph. This property reminds us that graph algorithms don’t depend on visuals but on connectivity.

You should also get comfortable with the flavors of graphs. Some have direction (like a tweet being retweeted), some allow duplicate edges (multigraphs), and some are fully connected (cliques and complete graphs). Understanding when to use each form lets you frame problems properly, which is half the battle in any interview.

One of the most powerful concepts is the subgraph—a way to isolate parts of a system for focused analysis. It’s useful when troubleshooting a bug, analyzing a subset of users, or designing modular systems.

Key graph metrics like degree, centrality, and shortest path help you quantify structure. They reveal which nodes are “important,” how information flows, and how efficient routes can be. These aren’t just for theory—they appear constantly in ranking algorithms, search engine logic, and network analysis.

And don’t overlook concepts like bridges, which are edges whose removal splits the graph, or graph coloring, which underpins classic scheduling and resource allocation problems. Questions about exam scheduling, register allocation, or task assignment often reduce to “coloring” graphs efficiently.

Ultimately, the interview isn’t testing whether you know the name of every centrality metric. It’s testing whether you can recognize a graph problem when you see one—and whether you can think in terms of connections, constraints, and traversals.

I noticed the top posts on r/datascience tend to be about getting a job. I'd love to hear about what other topics you think I should cover! Also, I wrote an educational piece on graphs if you want to learn more: [https://iaee.substack.com/p/graphs-intuitively-and-exhaustively](https://iaee.substack.com/p/graphs-intuitively-and-exhaustively)",124,0.94,https://www.reddit.com/r/datascience/comments/1jqjinm/ace_the_interview_graphs/,False,True,False
1jqa0yn,indie-devops,1743657114.0,11,/r/datascience/comments/1jqa0yn/does_moving_between_domains_a_thing/,datascience,Does moving between domains a thing?,"Hi,
Just started a DS role at a financial company, and I was curious to know whether transitioning to a medical/biological/any-other-based company later is possible/common in the field.
Do companies care about domain specific knowledge or only about the actual soft and hard skills required for a data scientist?

Initially, I started studying DS from the motivation to use data to help people, but I grew up and understood that my noble ideas at a young age aren’t always realistic. But the idea it is possible since there are data scientists in these domains really encourages me to try and work with them sometime in the future.

Thanks, learned a lot from this sub.",1,0.53,https://www.reddit.com/r/datascience/comments/1jqa0yn/does_moving_between_domains_a_thing/,False,True,False
1jq3j72,Illustrious-Pound266,1743637436.0,119,/r/datascience/comments/1jq3j72/is_there_an_unspoken_glass_ceiling_for/,datascience,Is there an unspoken glass ceiling for professionals in AI/ML without a PhD degree?,"I've been on the job hunt for MLE roles but it seems like a significant portion of them (certainly not all) prefer a PhD over someone with a master's.. If I look at the applicant profiles via Linkedin Premium, it seems like anywhere from 15-40% of applicants have PhDs as well. I work for a large organization and many of the leads and managers have PhD's, too.

So now, this got me worried about whether there's an unspoken glass ceiling for ML practitioners without a PhD. I'm not even talking about research/applied scientist positions, either, but just ML engineers and regular data scientists.

Do you find that this is true? If so, why is this?",170,0.9,https://www.reddit.com/r/datascience/comments/1jq3j72/is_there_an_unspoken_glass_ceiling_for/,False,True,False
1jq1lwz,Majestic-Influence-2,1743632319.0,16,/r/datascience/comments/1jq1lwz/select_typical_10_select_unusual_10_select/,datascience,select typical 10? select unusual 10? select comprehensive 10?,"Hi group, I'm a data scientist based in New Zealand.

Some years ago I did some academic work on non-random sampling - selecting points that are 'interesting' in some sense from a dataset. I'm now thinking about bringing that work to a wider audience.

I was thinking in terms of implementing as SQL syntax *(although* r/snowflake *suggests it may work better as a stored procedure).* This would enable some powerful exploratory data analysis patterns without stepping out of SQL.

We might propose queries like:

* *select typical 10...* (finds 10 records that are ""average"" or ""normal"" in some sense)
* *select unusual 10...* (finds the 10 records that are most 'different' from the rest of the dataset in some sense)
* *select comprehensive 10...* (finds a group of 10 records that, between them, represent as much as possible of the dataset)
* *select representative 10...* (finds a group of 10 records that, between them, approximate the distribution of the full dataset as closely as possible)

I've implemented a bunch of these 'select-adjectives' in R as a first step. Most of them work off a difference matrix using a generic metric using [Gower's distance](https://en.wikipedia.org/wiki/Gower%27s_distance). For example, 'select unusual 10' finds the ten records with the least RMS distance from all records in the dataset.

For demonstration purposes, I applied these methods to a [test dataset](https://www.rdocumentation.org/packages/poliscidata/versions/2.3.0/topics/world) of 'countries \[or territories\] of the world' containing various economic and social indicators, and found:

* five *typical* countries are the Dominican Republic, the Philippines, Mongolia, Malaysia, Thailand *(generally middle-income, quite democratic countries with moderate social development)*
* the most *unique* countries are Afghanistan, Cuba, Fiji, Botswana, Tunisia and Libya *(none of which is very like any other country)*
* a *comprehensive* list of seven countries, spanning the range of conditions as widely as possible, is Mauritania *(poor, less democratic)*, Cote d'Ivoire *(poor, more democratic)*, Kazakhstan *(middle income, less democratic)*, Dominican Republic *(middle income, more democratic)*, Kuwait *(high income, less democratic)*, Slovenia *(high income, more democratic)*, Germany *(very high income)*
* the six territories that are most *different* from each other are Sweden, the USA, the Democratic Republic of the Congo, Palestine and Taiwan
* the six countries that are most *similar* to each other are Denmark, Finland, Germany, Sweden, Norway and the Netherlands.

*(Please don't be offended if I've mischaracterised a country you love. Please also don't be offended if I've said a region is a country that, in your view, is not a country. The blame doubtless rests with my rather out-of-date test dataset.)*

So - any interest in hearing more about this line of work?",25,0.92,https://www.reddit.com/r/datascience/comments/1jq1lwz/select_typical_10_select_unusual_10_select/,False,True,False
1jpy5qs,chris_813,1743623847.0,38,/r/datascience/comments/1jpy5qs/robbery_prediction_on_retail_stores/,datascience,Robbery prediction on retail stores,"Hi, just looking for advice. I have a project in which I must predict probability of robbery on retail stores. I use robbery history of the stores, in which I have 1400 robberies in the last 4 years. Im trying to predict this monthly, So I add features such as robbery in the area in the last 1, 2, 3, 4 months behind, in areas for 1, 2, 3, 5 km. I even add month and if it is a festival day on that month. I am using XGboost for binary classification, wether certain store would be robbed that month or not. So far results are bad, predicting even 300 robberies in a month, with only 20 as true robberies actually, so its starting be frustrating.

Anyone has been on a similar project?",22,0.87,https://www.reddit.com/r/datascience/comments/1jpy5qs/robbery_prediction_on_retail_stores/,False,True,False
1jpq0x1,alpha_centauri9889,1743604286.0,58,/r/datascience/comments/1jpq0x1/tensorflowkeras_vs_pytorch_for_industry/,datascience,Tensorflow/Keras vs PyTorch for industry?,"I have used both Keras and PyTorch but only at the surface level. I am thinking to learn one in depth keeping DS/MLE positions in mind. I have heard that big companies use Tensorflow since it is more flexible in production while PyTorch is much more used in academia and research. I can't learn both at the same time, so want to know which one would be worth my time given that I am working in industry.

Note: By Tensorflow/Keras I meant starting with Keras and eventually evolving to Tensorflow.


PS: From the comments, I can see a lot of preferences for PyTorch. It's a clear winner.",68,0.92,https://www.reddit.com/r/datascience/comments/1jpq0x1/tensorflowkeras_vs_pytorch_for_industry/,False,True,False
1joot5w,endgamer42,1743487907.0,7,/r/datascience/comments/1joot5w/high_quality_time_series_data_sources_with/,datascience,High quality time series data sources (with realtime)?,"Are there any services or offerings that make high-quality time series data public? Perhaps with the option of ingesting data from it in real time?

Ideally a service like this would have anything-over-time available - from weather to stock prices to air quality to country migration patterns - unified under an easy to use interface which would allow you to explore these data sources and potentially subscribe to them. Does anything like this exist? If not, is there any use or demand for anything like this? ",13,0.85,https://www.reddit.com/r/datascience/comments/1joot5w/high_quality_time_series_data_sources_with/,False,True,False
1jogiud,poorpeon,1743461174.0,20,/r/datascience/comments/1jogiud/psa_largest_airbnb_datasets_available_for_free_at/,datascience,PSA: Largest Airbnb Datasets available for free at AirROI,"I came across this as I was looking to analyze some trends for my data science project. It covers more than a million listings and has high-quality data for many of the biggest rental markets.

[https://www.airroi.com/data-portal](https://www.airroi.com/data-portal)

",0,0.48,https://www.reddit.com/r/datascience/comments/1jogiud/psa_largest_airbnb_datasets_available_for_free_at/,False,True,False
1jobolz,akshayb7,1743449093.0,138,/r/datascience/comments/1jobolz/tired_of_ai/,datascience,Tired of AI,"One of the reasons I wanted to become an AI engineer was because I wanted to do cool and artsy stuff in my free time and automate away the menial tasks. But with the continuous advancements I am finding that it is taking away the fun in doing stuff. The sense of accomplishment I once used to have by doing a task meticulously for 2 hours can now be done by AI in seconds and while it's pretty cool it is also quite demoralising. 

The recent 'ghibli style photo' trend made me wanna vomit, because it's literally nothing but plagiarism and there's nothing novel about it. I used to marvel at the art created by Van Gogh or Picasso and always tried to analyse the thought process that might have gone through their minds when creating such pieces as the Starry night (so much so that it was one of the first style transfer project I did when learning Machine Learning). But the images now generated while fun seems soulless.

And the hypocrisy of us using AI for such useless things. Oh my god. It boils my blood thinking about how much energy is being wasted to do some of the stupid stuff via AI, all the while there is continuously increasing energy shortage throughout the world.

And the amount of job shortage we are going to have in the near future is going to be insane! Because not only is AI coming for software development, art generation, music composition, etc. It is also going to expedite the already flourishing robotics industry. Case in point look at all the agentic, MCP and self prompting techniques that have come out in the last 6 months itself.

I know that no one can stop progress, and neither should we, but sometimes I dread to imagine the future for not only people like me but the next generation itself. Are we going to need a universal basic income? How is innovation going to be shaped in the future? 

Apologies for the rant and being a downer but needed to share my thoughts somewhere.

PS: I am learning to create MCP servers right now so I am a big hypocrite myself. ",600,0.88,https://www.reddit.com/r/datascience/comments/1jobolz/tired_of_ai/,False,True,False
1jo4g14,ElectrikMetriks,1743431151.0,52,/r/datascience/comments/1jo4g14/its_important_work/,datascience,It's important work.,,1302,0.99,https://i.redd.it/9mcl3buvb1se1.png,False,False,False
1jo2gxt,SummerElectrical3642,1743425592.0,46,/r/datascience/comments/1jo2gxt/i_have_tested_all_the_popular_coding_assistant/,datascience,"I have tested all the popular coding assistant for data science, here's what I found","Recently I feel like much less productive when doing data science work when I do more software development. I think it is because I use AI effectively when building software. So I setup a test to find the best AI coding assistant to help with Data Science task. 

The result is a bit surprising for me: None of the popular AI agent works for data science. Although the demo looks gorgeous, Google Gemini in Colab fail pretty bad. But there are some tools that has potential and some are already a bit useful. 

Check article for more detailed analysis. ",99,0.84,https://medium.com/@DangTLam/the-best-ai-agent-for-data-science-and-machine-learning-march-2025-20a3cfee836d,False,False,False
1jnwt7l,guna1o0,1743402075.0,9,/r/datascience/comments/1jnwt7l/getting_high_information_value_on_a_credit/,datascience,Getting High Information Value on a credit scoring model,"I'm working on a credit scoring model.   
  
For a few features (3 out of 15), I'm getting high Information Values (IV) such as 1.0, 1.2, and 1.5. However, according to the theory, the maximum threshold should be 0.5. anything above this requires severe investigation as it might indicate data leakage.   
  
I've checked the features and the pipeline several times, but I couldn't find any data leakage.   
  
  
Is it normal to have high IV values, or should I investigate further?",12,0.74,https://www.reddit.com/r/datascience/comments/1jnwt7l/getting_high_information_value_on_a_credit/,False,True,False
1jnupvt,AutoModerator,1743393703.0,27,/r/datascience/comments/1jnupvt/weekly_entering_transitioning_thread_31_mar_2025/,datascience,"Weekly Entering & Transitioning - Thread 31 Mar, 2025 - 07 Apr, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",7,0.83,https://www.reddit.com/r/datascience/comments/1jnupvt/weekly_entering_transitioning_thread_31_mar_2025/,False,True,False
1jnkxza,Big-Acanthaceae-9888,1743364382.0,27,/r/datascience/comments/1jnkxza/use_of_generative_ai/,datascience,Use of Generative AI,"I'm averse to generative AI, but is this one of those if you can't beat em, join em type of things? Is it possible to market myself by making projects (nowadays) without shoehorning LLMs, or wrappers?",18,0.74,https://www.reddit.com/r/datascience/comments/1jnkxza/use_of_generative_ai/,False,True,False
1jnh32k,Ty4Readin,1743354336.0,119,/r/datascience/comments/1jnh32k/why_you_should_use_rmse_over_mae/,datascience,Why you should use RMSE over MAE,"I often see people default to using MAE for their regression models, but I think on average most people would be better suited by MSE or RMSE.

Why? Because they are both minimized by different estimates!

You can prove that MSE is minimized by the conditional expectation (mean), so E(Y | X).

But on the other hand, you can prove that MAE is minimized by the conditional median. Which would be Median(Y | X).

It might be tempting to use MAE because it seems more ""explainable"", but you should be asking yourself what you care about more. Do you want to predict the expected value (mean) of your target, or do you want to predict the median value of your target?

I think that in the majority of cases, what people actually want to predict is the expected value, so we should default to MSE as our choice of loss function for training or hyperparameter searches, evaluating models, etc.

EDIT: Just to be clear, business objectives always come first, and the business objective should be what determines the quantity you want to predict and, therefore, the loss function you should choose.

Lastly, this should be the final optimization metric that you use to evaluate your models. But that doesn't mean you can't report on other metrics to stakeholders, and it doesn't mean you can't use a modified loss function for training.",93,0.85,https://www.reddit.com/r/datascience/comments/1jnh32k/why_you_should_use_rmse_over_mae/,False,True,False
1jn9p9e,meni_s,1743331054.0,92,/r/datascience/comments/1jn9p9e/should_i_invest_time_learning_a_language_other/,datascience,Should I invest time learning a language other than Python?,"I finished my PhD in CS three years ago, and I've been working as a data scientist for the past two years, exclusively using Python. I love it, especially the statistical side and scripting capabilities, but lately, I've been feeling a bit constrained by only using one language.

I'm debating whether it's worthwhile to branch out and learn another language to broaden my horizons. R seems appealing given my interests in stats, but I'm also curious about languages like Julia, Scala, or even something completely different.

Has anyone here faced a similar decision? Did learning another language significantly boost your career, or was it just a nice-to-have skill? Or maybe this is just a waste of time?

Thanks for any insights!

Update: I'm not completely sure about my long term goals, tbh. I do like statistics and stuff like causal inference, and Bayesian inference looks appealing. At the same time I feel that doing some DL might also be great and practical as they are the most requested in the industry (took some courses about NLP but at my work we mostly do tabular data with classical ML). Those are the main direction, but I'm aware that they might be too broad.",121,0.9,https://www.reddit.com/r/datascience/comments/1jn9p9e/should_i_invest_time_learning_a_language_other/,False,True,False
1jm4yzm,Starktony11,1743194205.0,36,/r/datascience/comments/1jm4yzm/if_you_are_the_one_who_says_you_want_curious_and/,datascience,"If you are the one who says you want curious and motivated person, then do you actually hire them? Or it’s just a formality and decide based on tech skills?","I often see hiring managers and job posts saying they want someone who’s curious and motivated. I genuinely am I ask a lot of questions on projects, whether I’m working with data or just walking down the street thinking about things. I’ve even shared work that shows this curiosity and drive, like how deeply I explore projects or how I published research papers just because I wanted to dive deeper into topics not because I had to for grades. I also often think about ways to improve the products we use.

But I rarely get a response or acknowledgment of these examples. So I was wondering how do you actually evaluate curiosity and motivation in a candidate? Or does it not matter that much, and the decision mostly comes down to whether someone meets the coding criteria once the recruiter passes the resume along?

I personally feel that curiosity is one of the most important traits for a data scientist but I’m not sure how often that really gets noticed or valued in the process.",22,0.82,https://www.reddit.com/r/datascience/comments/1jm4yzm/if_you_are_the_one_who_says_you_want_curious_and/,False,True,False
1jlxfhj,Suspicious_Jacket463,1743174941.0,33,/r/datascience/comments/1jlxfhj/eda_is_useless/,datascience,EDA is Useless,"Hey folks! Yes, that is unpopular opinion. EDA is useless.

I've seen a lot notebooks on Kaggle in which people make various plots, histograms, density functions, scatter plots etc. But there is no point in doing it since at the end of the day just some sort of catboost or lightgbm is used. And still, such garbage is encouraged as usual, ""Great work!"".

All that EDA is done for the sake of EDA, and doesn't lead to any kind of decision making. ",0,0.14,https://www.reddit.com/r/datascience/comments/1jlxfhj/eda_is_useless/,False,True,False
1jlu60y,alpha_centauri9889,1743165783.0,15,/r/datascience/comments/1jlu60y/options_for_a_ds_with_2_yoe/,datascience,Options for a DS with 2 YOE,"I have been working as a data scientist for 2 years now in a consulting firm. I have experience with classical ML models, deep learning models, and some experience with GENAI. But my daily tasks revolve mostly around doing ad-hoc analytics. I am a CS grad. 

I am not very interested in analytics and consulting firm. So, what are the available options for me? Should I consider SDE (I don't have the experience though), MLE, or DS (in a product based company with more focus on model building)? 

I want growth and compensation and more interested in product based companies. What are my options? What's your advice? To be honest, working in consulting firm, it's too much frustrating due to long working hour and daily adhoc requests.",28,0.85,https://www.reddit.com/r/datascience/comments/1jlu60y/options_for_a_ds_with_2_yoe/,False,True,False
1jlr7wo,_The_Numbers_Guy,1743154551.0,20,/r/datascience/comments/1jlr7wo/need_career_guidance_ambiguity_due_to_rising_genai/,datascience,Need Career Guidance - Ambiguity due to rising GenAI,"Hey Everyone, 

I have 6+ YOE in DS and my primary expertise is problem solving, classic ML (regression, classification etc.), Azure ML/Cognitive resources. Have worked on 20+ actual Manufacturing + Finance Industry use cases... 

I have dipped my hands a bit in GenAI, Neural nets, Vision models etc. But felt they are not my cup of tea. I mean I know the basics but don't feel like a natural with those tech. Primary reason not to prefer GenAI is because unless you are training/building LLMs (rare opportunity) all you are doing is software development using pre-trained models rather than any Data Science work.

So my question is to any Industry leaders/experts here.. where should I focus more on? 

Path 1: Stick to my skills and continue with the same (concerned if this sub segment becomes redundant in future)

Path 2: Diversify and focus on Gen AI or other sub segments.

Path 3: Others",15,0.86,https://www.reddit.com/r/datascience/comments/1jlr7wo/need_career_guidance_ambiguity_due_to_rising_genai/,False,True,False
1jlnhg1,Difficult_Number4688,1743137983.0,103,/r/datascience/comments/1jlnhg1/good_at_practical_ml_weak_on_theory_getting_the/,datascience,"“Good at practical ML, weak on theory” — getting the same feedback everywhere. How do I fix this?","Recently got this feedback after a machine learning engineer interview:

“You clearly understand how to make ML algorithms work in practice and have solid experience with real-world projects. But your explanations of the theoretical concepts behind the algorithms were vague or imprecise. We recommend taking a few months to review the fundamentals before reapplying.”

This isn’t the first time I’ve heard this — in fact, it’s a pattern I’m seeing across multiple interviews with tech-focused companies. And it’s getting in the way of landing the kinds of roles I’m really interested in.

Some context:
I’ve been working for 2–3 years as an ML engineer at a large non-tech company. My experience is pretty diverse — from traditional supervised learning to computer vision, with a recent shift toward GenAI (LLMs, embeddings, prompting, RAG, etc.). I’ve built end-to-end pipelines, deployed models, and shipped ML to production. But because the work is so applied — and lately very GenAI-oriented — I’ve honestly drifted away from the theoretical side of ML.

Now I’m trying to move into roles at more ML-mature companies, and I’m getting stuck at the theory part of the interviews.

My question is: how would you recommend brushing up on ML theory in a structured, deep way — after being in the field for a while?
I’m not starting from zero, but I clearly need to tighten up my understanding and explanations.

Would love any advice, resources, or even personal stories from others who made the leap from applied/practical ML to more theory-heavy roles.

Thanks in advance!
",175,0.92,https://www.reddit.com/r/datascience/comments/1jlnhg1/good_at_practical_ml_weak_on_theory_getting_the/,False,True,False
1jlmc8t,Typical-Macaron-1646,1743133877.0,21,/r/datascience/comments/1jlmc8t/roast_my_freelancing_website/,datascience,Roast my freelancing website,"Hey fellow data scientists.

I am attempting to start my own business as a freelancer. I am at the very beginning of my journey. I have 0 experience as a free lancer, but I do have 5 years of career experience as a data analyst. 

For anyone willing, I need constructive criticism on the website I’ve made. I realize it’s not great. I made it with a free square space trial. Feel free to be brutally honest, but if you can offer any improvement advice, that would be very appreciated 

password for the website: roast
",13,0.71,https://circle-saffron-chn2.squarespace.com/,False,False,False
1jlh6ae,No-Brilliant6770,1743118295.0,27,/r/datascience/comments/1jlh6ae/got_a_technical_interview_for_data_science_intern/,datascience,Got a technical interview for data science intern at Capital One – anyone been through it?,"Hey y’all,  
  
Just got an invite for a technical interview for a data science internship at Capital One, Wasn’t expecting to get this far tbh lol  
  
Anyone here been through it? Would love to hear about your experience – what kind of stuff do they ask? Any curveballs or stuff I should brush up on? I’ve done some Leetcode/stats/prep but not sure what Capital One specifically leans into.  
  
Any advice (or horror stories lol) welcome.",40,0.92,https://www.reddit.com/r/datascience/comments/1jlh6ae/got_a_technical_interview_for_data_science_intern/,False,True,False
1jl7q5h,NotTheTrueKing,1743092175.0,11,/r/datascience/comments/1jl7q5h/how_the_fuck_do_i_even_get_started_in_this_field/,datascience,How the fuck do I even get started in this field?,"Tiny bit of background, I have my master's in biostatistics and my undergrad in math, and did learn some ML modeling methods during grad school. Working as a data analyst currently but my day-to-day work involves very little actual analysis or even statistics.

On the other hand, reading all the posts and resumes here and current job openings for data scientists, I have honest to god no idea how I would ever even get one of these jobs or work towards it. I understand that having a statistics background can help in some vague, hand-wavey way, but I genuinely don't think I have any of the hard skills needed to work in DS and don't even know where to start.",0,0.44,https://www.reddit.com/r/datascience/comments/1jl7q5h/how_the_fuck_do_i_even_get_started_in_this_field/,False,True,False
1jl6tt4,Trick-Interaction396,1743089910.0,12,/r/datascience/comments/1jl6tt4/does_anyone_else_lose_interest_during_maintenance/,datascience,Does anyone else lose interest during maintenance mode?,You've built a cool thing. It works great. Now it needs to be maintained with updates. Now I'm bored.,30,0.92,https://www.reddit.com/r/datascience/comments/1jl6tt4/does_anyone_else_lose_interest_during_maintenance/,False,True,False
1jl6otm,IMightBYourDad,1743089551.0,113,/r/datascience/comments/1jl6otm/not_getting_calls_for_a_month_now_what_can_i_do/,datascience,Not getting calls for a month now. What can I do better?,What can I do better in this resume? I’ve also worked on more projects but I have only listed high impact projects in my experience. ,230,0.92,https://i.redd.it/rd09tft549re1.jpeg,False,False,False
1jl5tjk,JobIsAss,1743087343.0,8,/r/datascience/comments/1jl5tjk/causal_inference_given_calls/,datascience,Causal inference given calls,"I have been working on a usecase for causal modeling. How do we handle an observation window when treatment is dynamic. Say we have a 1 month observation window and treatment can occur every day or every other day. 

1) Given this the treatment is repeated or done every other day.
2) Experimentation is not possible. 
3) Because of this observation window can have overlap from one time point to another.

Ideally i want to essentially create a playbook of different strategies by utilizing say a dynamicDML but that seems pretty complex. Is that the way to go?

Note that treatment can also have a mediator but that requires its own analysis. I was thinking of a simple static model but we cant just aggregate it. 
For example we do treatment day 2 had an immediate effect. We the treatment window of 7 days wont be viable.  
Day 1 will always have treatment day 2 maybe or maybe not.  My main issue is reverse causality.

Is my proposed approach viable if we just account for previous information for treatments as a confounder such as a sliding window or aggregate windows. Ie # of times treatment has been done?


If we model the problem its essentially this

treatment -> response -> action

However it can also be
 treatment -> action 

As response didnt occur.
",7,0.82,https://www.reddit.com/r/datascience/comments/1jl5tjk/causal_inference_given_calls/,False,True,False
1jl1ldy,clarinetist001,1743074531.0,109,/r/datascience/comments/1jl1ldy/leaving_data_science_what_are_my_options/,datascience,Leaving data science - what are my options?,"This doesn't seem to be within the scope of the transitioning thread, so asking in my own post.

I have 10 YoE and am in the US. Was laid off in January. Was an actuarial analyst back in 2015 (I have four exams passed) using VBA and Excel, worked my way up to data analyst doing SQL + dashboarding (Shiny, Tableau, Power BI, D3), statistician using R and SQL and Python, and ended up at a lead DS. Minus things like Qlik, Databricks, Spark, and Snowflake, I have probably used that technology in a professional setting (yes, I have used all three major cloud services). I have a MS in statistics (my thesis was on time series) and am currently enrolled in OMSCS, but I am considering ending my enrollment there after having taken CV, DL, and RL.

I am very disappointed by how I observe the field has changed since ChatGPT came out. In the jobs I have had since that time as well as with interviews, the general impression I get is that people expect models to do both causal discovery and prediction optimally through mere data ingestion and algorithmic processing, without any sort of thought as to what data are available, what research questions there are, and for what purpose we are doing modeling. I did not enter this field to become a software engineer and just watch the process get automated away due to others' expectations of how models work only to find that expectations don't match reality. And then aside from that, I want nothing to do with generative AI. That is a whole other can of worms I won't get into. 

Very long story short, due to my mental health and due to me pushing through GenAI hype for job security, I did end up losing my memory in the process. I'm taking good care of myself (as mentioned in the comments, I've been 21 weeks into therapy). But I'm at a point right now where I'm not willing to just take any job without recognizing my mental limits. 

I am looking for data roles tied to actual business operations that have some aspect of requirements gathering (analyst, engineering, scientist, manager roles that aren't screaming AI all over them) and statistician roles, but especially given the layoff situation with the federal employees and contractors as well as entry-level saturation, this seems to be an uphill battle. I also think I'm in a situation where I have too much experience for an IC role and too little for a managerial role. The most extreme option I am considering is just dropping everything to become an electrician or HVAC person (not like I'm particularly attached to due to my memory loss anyway).

I want to ask this community for two things: suggestions for other things to pursue, and how to tailor my resume given the current situation. I have paid for a resume service and I've had my resume reviewed by tons of people. I have done a ton of networking. I just don't think that my mindset is right for this field.
",260,0.94,https://www.reddit.com/r/datascience/comments/1jl1ldy/leaving_data_science_what_are_my_options/,False,True,False
1jkzb3c,Historical-Egg-2422,1743064554.0,21,/r/datascience/comments/1jkzb3c/i_built_an_aipowered_outreach_system_that/,datascience,"I built an AI-powered outreach system that automates job applications to CEOs, Data Heads, and Tech Recruiters","Hey guys, 

I’ve been applying for a lot of jobs lately (hahaha, yeah the market sucks in the states). So I decided to build an AI system to make it a little less painful. It scrapes LinkedIn to find CEOs, Data Heads, and recruiters, predicts and verifies their emails, writes personalized messages using Mistral via Ollama, picks the best resume from a few versions I have, and sends it out automatically. I even set up a dashboard to keep track of everything. I’m getting a 17% response rate so far, which is way better than the usual black hole experience. Let me know if you're curious about how it works or if you have any ideas to make it even better!",26,0.7,https://www.reddit.com/r/datascience/comments/1jkzb3c/i_built_an_aipowered_outreach_system_that/,False,True,False
1jkvbkv,Smarterchild1337,1743047776.0,2,/r/datascience/comments/1jkvbkv/designplanning_tools_and_workflows/,datascience,Design/Planning tools and workflows?,"Interested in the tools, workflows, and general approaches other practitioners use to research, design, and document their ML and analytics solutions.

 My current workflow looks something like this:

Initial requirements gathering and research in a markdown document or confluence page. 

ETL, EDA in one or more notebooks with inline markdown documentation.

Solution/model candidate design back in confluence/markdown.

And onward to model experimentation, iteration, deployment, documenting as we go.


I feel like I’m at the point where my approach to the planning/design portions are bottlenecking my efficiency, particularly for managing complex projects. In particular:

- I haven’t found a satisfactory diagramming tool. I bounce around between mermaid diagrams and drawing in powerpoint.

- Braindumping in a markdown document feels natural, but I suspect I can be more efficient than just starting with a blank canvas and hammering away. 

- My team usually uses mlflow to manage experiments, but tends to present results by copy pasting into confluence.

How do you and/or your colleagues approach these elements of the DS workflow?",6,0.8,https://www.reddit.com/r/datascience/comments/1jkvbkv/designplanning_tools_and_workflows/,False,True,False
1jkmgo7,Due-Duty961,1743022562.0,8,/r/datascience/comments/1jkmgo7/data_scientists_in_france_how_do_i_improve_my/,datascience,"data scientists in France, how do I improve my hiring chance?","I am a freelancer in France. 
I did école ingénieur in statistics
my cv is a bit chaotic with short missions in data science, then spent 4 year just doing sql, R and some power bi, no ML. 
I did a gcp, tensorflow learning but they won t hire me for these cuz I don t have many projects.or even data science cuz I have a few experience.

Do you have some good projects I can work on since I am unemployed now, is it useful to learn something ( what?) cuz anyway they ll be like oh u dont have any projects or 5yr experience in this? what are your advice gor me please?",8,0.69,https://www.reddit.com/r/datascience/comments/1jkmgo7/data_scientists_in_france_how_do_i_improve_my/,False,True,False
1jkjs7y,AdministrativeRub484,1743015938.0,66,/r/datascience/comments/1jkjs7y/isnt_this_solution_overkill/,datascience,Isn't this solution overkill?,"I'm working at a startup and someone one my team is working on a binary text classifier to, given the transcript of an online sales meeting, detect who is a prospect and who is the sales representative. Another task is to classify whether or not the meeting is internal or external (could be framed as internal meeting vs sales meeting).

We have labeled data so I suggested using two tf-idf/count vectorizers + simple ML models for these tasks, as I think both tasks are quite easy so they should work with this approach imo... My team mates, who have never really done or learned about data science suggested, training two separate Llama3 models for each task. The other thing they are going to try is using chatgpt.

Am i the only one that thinks training a llama3 model for this task is overkill as hell? The costs of training + inference are going to be so huge compared to a tf-idf + logistic regression for example and because our contexts are very large (10k+) this is going to need a a100 for training and inference.

I understand the chatgpt approach because it's very simple to implement, but the costs are going to add up as well since there will be quite a lot of input tokens. My approach can run in a lambda and be trained locally.

**Also, I should add: for 80% of meetings we get the true labels out of meetings metadata, so we wouldn't need to run any model. Even if my tf-idf model was 10% worse than the llama3 approach, the real difference would really only be 2%, hence why I think this is good enough...**",101,0.95,https://www.reddit.com/r/datascience/comments/1jkjs7y/isnt_this_solution_overkill/,False,True,False
1jkjee5,NervousVictory1792,1743015018.0,1,/r/datascience/comments/1jkjee5/navigating_the_team_in_vested_interest/,datascience,Navigating the team in vested interest,"I have recent joined as an associate data scientist with previous background of swe. This is definitely my dream role and totally love the problems the team are solving. But it is kind of an ideal world scenario where the deployment is being done by DE team, pipelines as well. No containerisation or in short no MLOps practices. I do not like DE and the ever changing landscape of swe in general but I am wary of the stuff that this situation might set me back in the near future as all DS job postings do ask for some kind of DE, cloud, containerisation etc. How do I get my hands on these things or rather convince the team to move towards these tech stacks ?",0,0.38,https://www.reddit.com/r/datascience/comments/1jkjee5/navigating_the_team_in_vested_interest/,False,True,False
1jkfv0p,JarryBohnson,1743006372.0,22,/r/datascience/comments/1jkfv0p/first_ds_interview_next_week_just_informed_it/,datascience,"First DS interview next week, just informed ""it will be very data engineering focused"".  Advice?","Hi all,  I'm going through the interview process for the first time.  I was informed that I got to the technical round, but that I should expect the questions to be very DE/ETL pipeline development focused.

I have decent experience with data-cleaning/transformation for analysis, and modelling from my PhD, but much less with the data ingestion part of the pipeline.  What suggestions would you give for me to brush up on/tools I should be able to talk fluently about?

The job is going to be dealing with a lot of real-time market data, time-series data heavy etc. I'm kinda surprised as there was no mention until now that it would be the DE side of the team (they specifically asked for predictive modelling with time-series data in description), but it's definitely something I'm interested in regardless.

Side note do people find that many DS-titled jobs these days are actually DE, or is the field so overlapping that the distinct titles aren't super relevant?",30,0.94,https://www.reddit.com/r/datascience/comments/1jkfv0p/first_ds_interview_next_week_just_informed_it/,False,True,False
1jka8tt,AMGraduate564,1742991035.0,70,/r/datascience/comments/1jka8tt/timeseries_forecasting_ml_models_perform_better/,datascience,Time-series forecasting: ML models perform better than classical forecasting models?,"This article demonstrated that ML models are better performing than classical forecasting models for time-series forecasting - https://doi.org/10.1016/j.ijforecast.2021.11.013

However, it has been my opinion, also the impression I got from the DS community, that classical forecasting models are almost always likely to yield better results. Anyone interested to have a take on this?",106,0.95,https://www.reddit.com/r/datascience/comments/1jka8tt/timeseries_forecasting_ml_models_perform_better/,False,True,False
1jk8klq,trouble_sleeping_,1742984799.0,18,/r/datascience/comments/1jk8klq/first_position_job_seeker_and_dsmleai_landscape/,datascience,First Position Job Seeker and DS/MLE/AI Landscape,"Armed to the teeth with some projects and a few bootcamp certifications, Im soon to start applying at anything that moves.

Assuming you dont know how to code all that much, what have been your experiences when it comes to the use of LLM's in the workplace? Are you allowed to use them? Did you mention it during the interview?",0,0.35,https://www.reddit.com/r/datascience/comments/1jk8klq/first_position_job_seeker_and_dsmleai_landscape/,False,True,False
1jj82n6,RecognitionSignal425,1742866507.0,133,/r/datascience/comments/1jj82n6/its_not_you_its_me/,datascience,"""It's not you, it's me""?",,387,0.94,https://www.reddit.com/gallery/1jhitoc,False,False,False
1jivc2n,Crokai,1742834838.0,13,/r/datascience/comments/1jivc2n/data_science_thesis_on_crypto_fraud_detection/,datascience,Data Science Thesis on Crypto Fraud Detection – Looking for Feedback!,"Hey r/datascience,

I'm about to start my Master’s thesis in DS, and I’m planning to focus on financial fraud detection in cryptocurrency. I believe crypto is an emerging market with increasing fraud risks, making it a high impact area for applying ML and anomaly detection techniques.

Original Plan:

\- Handling Imbalanced Datasets from Open-sources (Elliptic Dataset, CipherTrace) – Since fraud cases are rare, techniques like SMOTE might be the way to go.  
\- Anomaly Detection Approaches:

* Autoencoders – For unsupervised anomaly detection and feature extraction.
* Graph Neural Networks (GNNs) – Since financial transactions naturally form networks, models like GCN or GAT could help detect suspicious connections.
* (Maybe both?)

Why This Project?

* I want to build an attractive portfolio in fraud detection and fintech as I’d love to contribute to fighting financial crime while also making a living in the field and I believe AML/CFT compliance and crypto fraud detection could benefit from AI-driven solutions.

My questions to you:

·       Any thoughts or suggestions on how to improve the approach?

·       Should I explore other ML models or techniques for fraud detection?

·       Any resources, datasets, or papers you'd recommend?

I'm still new to the DS world, so I’d appreciate any advice, feedback and critics.  
Thanks in advance!",17,0.81,https://www.reddit.com/r/datascience/comments/1jivc2n/data_science_thesis_on_crypto_fraud_detection/,False,True,False
1jiql82,ElectrikMetriks,1742822691.0,44,/r/datascience/comments/1jiql82/hey_you_have_a_second_for_a_quick_call_it_will/,datascience,"""Hey, you have a second for a quick call? It will just take a minute""",,1240,0.99,https://i.redd.it/dz5jp9cm2nqe1.png,False,False,False
1jii855,AutoModerator,1742788904.0,40,/r/datascience/comments/1jii855/weekly_entering_transitioning_thread_24_mar_2025/,datascience,"Weekly Entering & Transitioning - Thread 24 Mar, 2025 - 31 Mar, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",9,0.85,https://www.reddit.com/r/datascience/comments/1jii855/weekly_entering_transitioning_thread_24_mar_2025/,False,True,False
1jhdo7j,Fennecfox9,1742664359.0,70,/r/datascience/comments/1jhdo7j/management_at_my_company_claims_to_want_coders/,datascience,"Management at my company claims to want coders / innovation, but rejects deliverables which aren't Excel","I work at a large financial firm. 
We have a ton of legacy Excel processes which require manual work, buggy add-ons or VBA code that takes several minutes to load. Spreadsheets that chug like hell to open or need to be operated with formula calculation off just to work in them.

Management will hype up ""innovation"" and will try to hire people with technical skills. They will send official communication talking about how the company is adopting AI and hyping up our internal chatbot (which is just some enterprise agreement with ChatGPT).

I've tried using python to automate some of our old processes. For example for adhoc deliverables, I'll use pandas and then style my work using great-tables, I'll plot stuff in plotly, etc.

I spend a lot of time styling my tables and plots to make them look professional. I use the company color scheme when creating them so that they look ""right"".

However, when I send stuff to my boss or his boss, they'll either complain that:

1) This doesn't look like the stuff that other people are doing

2) Will say ""I don't like the formatting"" but won't give specific examples on what to improve, won't provide examples of what constitutes good work

Independently of this, I recently spoke with a colleague who made attempts to move towards BI software such as Tableau for their processes. Even they have mentioned that the higher ups will ask for these types of solutions but ultimately prefer Excel's visuals for the deliverables.

I'm at a loss. I personally find Excel tables and graphs to be ugly, including the ones that my colleagues send. They look like something that a college student put together. If that's what the management wants, I'm inclined to stop complaining and just give it to them. But how would I actually do that in Python? 

In past jobs I've seen people do stuff like save ""Templates"" in Excel and have python spit the DF into the template. I've also heard there are packages that can create an excel file and then mark it up from within the code. At the end of the day this sounds like a recipe for me to create shitty code and unsustainable processes, which we already have plenty of. I want to be able to use a ""real"" plotting and table packages and perhaps just make something that is just good enough. 

Does anyone have any suggestions for me?

Edit: 

This post seems to have gained traction. I just wanted to clarify:
I think some people read this post as if my boss asked me to send an xlsx or csv file and I refused or am unwilling. That is not what happened. This is a post about visuals and formatting, i.e. sending emails or reports with inline tables and graphs/charts. If attaching an excel file with a raw DF were sufficient, obviously I would do that. 

Anyway I will look into using python/excel packages to mark up my stuff. Thanks",267,0.97,https://www.reddit.com/r/datascience/comments/1jhdo7j/management_at_my_company_claims_to_want_coders/,False,True,False
1jhbvbg,dmorris87,1742659647.0,22,/r/datascience/comments/1jhbvbg/tips_for_migrating_rbased_etl_workflows_to_python/,datascience,Tips for migrating R-based ETL workflows to Python using LLM assistant?,"My team uses R heavily for production ETL workflows. This has been very effective, but I would prefer to be doing this in Python. Anyone with experience migrating R codebases to Python using LLM assistant? Our systems can be complex (multiple functions, SQL scripts, nested folders, config files, etc). We use RStudio Server for an IDE. I’ve been using Gemini for ideation and some initial translation, but it’s tedious.",0,0.44,https://www.reddit.com/r/datascience/comments/1jhbvbg/tips_for_migrating_rbased_etl_workflows_to_python/,False,True,False
1jhbqxn,clooneyge,1742659327.0,36,/r/datascience/comments/1jhbqxn/admission_requirements_of_applied_statistics_ds/,datascience,Admission requirements of applied statistics /DS master,I’m looking at some schools within and outside of US for a master degree study in areas in the subject line . Just my past college education didn’t involve much algebra/calculus/ programming course . Have acquired some skills thru MITx online courses . How can I validate that my courses have met the requirements of such graduate programs and potentially showcase them to the admission committee ?,20,0.86,https://www.reddit.com/r/datascience/comments/1jhbqxn/admission_requirements_of_applied_statistics_ds/,False,True,False
1jgnzw6,AnalyticNick,1742582130.0,48,/r/datascience/comments/1jgnzw6/harnham_professional_ghosts/,datascience,Harnham - professional ghosts?,"Has anyone else been contacted by a recruiter from Harnham, conducted a 30min informational call, been told that their resume would be sent to the hiring manager, and then subsequently get ghosted by the recruiter? It’s happened to me 4 or 5 (or maybe more) times now. ",78,0.96,https://www.reddit.com/r/datascience/comments/1jgnzw6/harnham_professional_ghosts/,False,True,False
1jgnhn7,StillWastingAway,1742580850.0,9,/r/datascience/comments/1jgnhn7/deep_learning_industry_practitioners_how_do_you/,datascience,"Deep learning industry Practitioners, how do you upskill yourself from the intermediate level?","I've been recently introduced to GPU-MODE, which is a great resource for kernels/gpu utilisation, I wondered what else is out there which is not pure research?",21,0.92,https://www.reddit.com/r/datascience/comments/1jgnhn7/deep_learning_industry_practitioners_how_do_you/,False,True,False
1jgmsj0,mehul_gupta1997,1742579105.0,1,/r/datascience/comments/1jgmsj0/moshivis_new_conversational_ai_model_supports/,datascience,"MoshiVis : New Conversational AI model, supports images as input, real-time latency","Kyutai labs (released Moshi last year) open-sourced MoshiVis, a new Vision Speech model which talks in real time and supports images as well in conversation. Check demo : https://youtu.be/yJiU6Oo9PSU?si=tQ4m8gcutdDUjQxh",6,0.88,https://www.reddit.com/r/datascience/comments/1jgmsj0/moshivis_new_conversational_ai_model_supports/,False,True,False
1jgkdwa,NotMyRealName778,1742573165.0,11,/r/datascience/comments/1jgkdwa/scheduling_optimization_with_genetic_algorithms/,datascience,Scheduling Optimization with Genetic Algorithms and CP,"Hi,

I have a problem for my thesis project, I will receive data soon and wanted to ask for opinions before i went into a rabbit hole. 

I have a metal sheet pressing scheduling problems with  

* n jobs for varying order sizes, orders can be split 
* m machines, 
* machines are identical in pressing times but their suitability for mold differs.
*  every job can be done with a list of suitable subset of molds that fit in certain molds 
* setup times are sequence dependant, there are differing setup times for changing molds, subset of molds, 
* changing of metal sheets, pressing each type of metal sheet differs so different processing times
*  there is only one of each mold certain machines can be used with certain molds 
* I need my model to run under 1 hour. the company that gave us this project could only achieve a feasible solution with cp within a couple hours.

My objectives are to decrease earliness, tardiness and setup times

I wanted to achieve this with a combination of Genetic Algorithms, some algorithm that can do local searches between iterations of genetic algorithms and constraint programming. My groupmate has suggested simulated anealing, hence the local search between ga iterations. 

My main concern is handling operational constraints in GA. I have a lot of constraints and i imagine most of the childs from the crossovers will be infeasible. This[ chromosome encoding ](https://dergipark.org.tr/tr/download/article-file/218376)solves a lot of my problems but I still have to handle the fact that i can only use one mold at a time and the fact that this encoding does not consider idle times. We hope that constraint programming can add those idle times if we give the approximate machine, job allocations from the genetic algorithm. 

To handle idle times we also thought we could add 'dummy jobs' with no due dates, and no setup, only processing time so there wont be any earliness and tardiness cost. We could punish simultaneous usage of molds heavily in the fitness function. We hoped that optimally these dummy jobs could fit where we wanted there to be idle time, implicitly creating idle time. Is this a viable approach? How do people handle these kinds of stuff in genetic algorithms? Thank you for reading and giving your time.",8,0.9,https://www.reddit.com/r/datascience/comments/1jgkdwa/scheduling_optimization_with_genetic_algorithms/,False,True,False
1jgjuhz,mosef18,1742571823.0,13,/r/datascience/comments/1jgjuhz/deepml_leetcode_for_machine_learning_new_feature/,datascience,Deep-ML (Leetcode for machine learning) New Feature: Break Down Problems into Simpler Steps!,"**New Feature: Break Down Problems into Simpler Steps!**

We've just rolled out a new feature to help you tackle challenging problems more effectively!

If you're ever stuck on a tough problem, you can now break it down into smaller, simpler sub-questions. These bite-sized steps guide you progressively toward the main solution, making even the most intimidating problems manageable.

Give it a try and let us know how it helps you solve those tricky challenges!  
its free for everyone on the daily question

[https://www.deep-ml.com/problems/39](https://www.deep-ml.com/problems/39)

https://preview.redd.it/00sa8vhoc2qe1.png?width=733&format=png&auto=webp&s=549d8786cfc4961c8f777968401e966331fe16ed

",16,0.68,https://www.reddit.com/r/datascience/comments/1jgjuhz/deepml_leetcode_for_machine_learning_new_feature/,False,True,False
1jg8inp,jgmz-,1742530646.0,9,/r/datascience/comments/1jg8inp/really_interesting_ml_use_case_from_strava/,datascience,Really interesting ML use case from Strava,,5,0.59,https://stories.strava.com/articles/removing-cars-from-leaderboards,False,False,False
1jfv15y,Tarneks,1742493344.0,41,/r/datascience/comments/1jfv15y/breadth_vs_depth_and_gatekeeping_in_our_industry/,datascience,Breadth vs Depth and gatekeeping in our industry,"Why is it very common when people talk about analytics there is often a nature of people dismissing predictive modeling saying it’s not real data science or how people gate-keeping causal inference?

I remember when I first started my career and asked on this sub some person was adamant that you must know Real analysis. Despite the fact in my 3 years of working i never really saw any point of going very deep into a single algorithm or method? Often not I found that breadth is better than depth especially when it’s our job to solve a problem as most of the heavy lifting is done.


Wouldn’t this mindset then really be toxic in workplaces but also be the reason why we have these unrealistic take-homes where a manager thinks a candidate should for example build a CNN model with 0 data on forensic bullet holes to automate forensic analytics.

Instead it’s better for the work geared more about actionability more than anything.

Id love to hear what people have to say. Good coding practice, good fundamental understanding of statistics, and some solid understanding of how a method would work is good enough.",76,0.84,https://www.reddit.com/r/datascience/comments/1jfv15y/breadth_vs_depth_and_gatekeeping_in_our_industry/,False,True,False
1jftqor,Typical-Macaron-1646,1742490169.0,5,/r/datascience/comments/1jftqor/i_simulated_100000_march_madness_brackets/,datascience,"I simulated 100,000 March Madness brackets",,4,0.64,/r/CollegeBasketball/comments/1jftq5f/i_simulated_100000_march_madness_brackets/,False,False,False
1jf8uwm,SillyDude93,1742421022.0,62,/r/datascience/comments/1jf8uwm/how_exactly_people_are_getting_contacted_by/,datascience,How exactly people are getting contacted by recruiters on LinkedIn?,"I have been applying for jobs for almost an year now and I have varied approach like applying directly on the websites, cold emailing, referral, only applying for jobs posted in last 24 hours and with each application been customized for that job description.

I have got 4 interviews in total and unfortunately no offer, but never a recruiter contacted me through LinkedIn, even it's regularly updated filled with skills, projects and experiences. I have made posts regarding various projects and topics but not a single recruiter contacted. 

Please share your input if you have received messages from recruiters.",65,0.84,https://www.reddit.com/r/datascience/comments/1jf8uwm/how_exactly_people_are_getting_contacted_by/,False,True,False
1jeg1xn,TheFinalUrf,1742332241.0,21,/r/datascience/comments/1jeg1xn/setting_expectations_with_management_growing_as_a/,datascience,Setting Expectations with Management & Growing as a Professional,"I am a data scientist at a F500 (technically just changed to MLE with the same team, mostly a personal choice for future opportunities).

Most of the work involves meeting with various clients (consulting) and building them “AI/ML” solutions. The work has already been sold by people far above me, and it’s on my team to implement it.

The issue is something that is probably well understood by everyone here. The data is horrific, the asks are unrealistic, and expectations are through the roof. 

The hard part is, when certain problems feel unsolvable given the setup (data quality, availability of historical data, etc), I often feel doubt that I am just not smart and not seeing some obvious solution. The leadership isn’t great from a technical side, so I don’t know how to grow. 

We had a model that we worked on for ages on a difficult problem that we got down to ~6% RMSE, and the client told us that much error is basically useless. I was so proud of it! It was months of work of gathering sources and optimizing.

At the same time, I don’t want to say ‘this is the best you will get’, because the work has already been sold. It feels like I have to be a snake oil salesmen to succeed, which I am good at but feels wrong. Plus, maybe I’m just missing something obvious that could solve these things…

Anyone who has significant experience in DS, specifically generating actual, tangible value with ML/predictive analytics? Is it just an issue with my current role? How do you set expectations with non-technical management without getting yourself let go in the process?

Apologies for the long post. Any general advice would be amazing. Thanks :)",59,0.9,https://www.reddit.com/r/datascience/comments/1jeg1xn/setting_expectations_with_management_growing_as_a/,False,True,False
1je46q1,matt-ice,1742302243.0,19,/r/datascience/comments/1je46q1/i_made_a_snowflake_native_app_that_generates/,datascience,"I made a Snowflake native app that generates synthetic card transaction data without inputs, and quickly",,5,0.65,https://app.snowflake.com/marketplace/listing/GZTSZ3VI09V/finthetic-llc-gsd-generate-synthetic-data-fraud,False,False,False
1je02lx,Adorable-Emotion4320,1742285740.0,3,/r/datascience/comments/1je02lx/spending_and_demographics_dataset/,datascience,Spending and demographics dataset,"Is there any free dataset out there that contains spending data at customer level, and any demographic info attached? I figure this is highly valuable and perhaps privacy sensitive, so a good dataset unlikely freely available.
In case there is some (anonymized) toy dataset out there, please do tell",0,0.5,https://www.reddit.com/r/datascience/comments/1je02lx/spending_and_demographics_dataset/,False,True,False
1jdv7ui,mehul_gupta1997,1742266074.0,9,/r/datascience/comments/1jdv7ui/whats_your_expectation_from_jensen_huangs_keynote/,datascience,What’s your expectation from Jensen Huang’s keynote today in NVIDIA GTC? Some AI breakthrough round the corner?,"Today, Jensen Huang, NVIDIA’s CEO (and my favourite tech guy) is taking the stage for his famous Keynote at 10.30 PM IST in NVIDIA GTC’2025. Given the track record, we might be in for a treat and some major AI announcements might be coming. I strongly anticipate a new Agentic framework or some Multi-modal LLM. What are your thoughts? 

Note: You can tune in for free for the Keynote by registering at NVIDIA GTC’2025 [here](https://www.nvidia.com/gtc/?ncid=ref-inpa-722552).",0,0.46,https://www.reddit.com/r/datascience/comments/1jdv7ui/whats_your_expectation_from_jensen_huangs_keynote/,False,True,False
1jdotmw,penpapermouse,1742248050.0,29,/r/datascience/comments/1jdotmw/what_is_financial_fraud_prevention_data_science/,datascience,What is financial fraud prevention data science like as a career path?,"How are the hours, the progression, the income, and the overall stress and work-life balance for this career path?  What are the pivots from here?

Edit: I'm most interested in learning about fraud prevention careers for banks and credit cards.",44,0.89,https://www.reddit.com/r/datascience/comments/1jdotmw/what_is_financial_fraud_prevention_data_science/,False,True,False
1jddkvq,ElectrikMetriks,1742220749.0,7,/r/datascience/comments/1jddkvq/golden_gigo/,datascience,Golden GIGO,,144,0.97,https://i.redd.it/gy2w5aapc9pe1.jpeg,False,False,False
1jd5wub,Thiseffingguy2,1742191131.0,28,/r/datascience/comments/1jd5wub/moviesshows_who_gets_it_right_who_gets_it_so_wrong/,datascience,Movies/Shows. Who gets it right? Who gets it SO wrong?,"Got a fun one for ya. Which moments in movies/shows have you cringed over, and which have you been impressed with, in regard to how they discuss the field? I feel like the term “data hard drive” has been thrown around since the 80s, the spy-related flicks always have some kind of weird geolocating/tracking animation that doesn’t exist. But who did it relatively well? Who did it the worst?",12,0.65,https://www.reddit.com/r/datascience/comments/1jd5wub/moviesshows_who_gets_it_right_who_gets_it_so_wrong/,False,True,False
1jd44gj,AutoModerator,1742184104.0,57,/r/datascience/comments/1jd44gj/weekly_entering_transitioning_thread_17_mar_2025/,datascience,"Weekly Entering & Transitioning - Thread 17 Mar, 2025 - 24 Mar, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",8,0.85,https://www.reddit.com/r/datascience/comments/1jd44gj/weekly_entering_transitioning_thread_17_mar_2025/,False,True,False
1jd2kgg,norfkens2,1742178915.0,16,/r/datascience/comments/1jd2kgg/is_rpa_a_feasible_way_for_data_scientists_to/,datascience,Is RPA a feasible way for Data Scientists to access data siloes?,"Basically, I'm debating whether I should make a case for my boss to learn my company's RPA tool (i.e. robot process automation) and invest a not insignificant amount of my time into implementing data pipelines.

We have an RPA tool already available, and we have a number of use cases that would benefit from it. I haven't systematically quantified their value (but I do have a rough idea).

Personally, I think I'm overqualified/overpaid for this type of data extraction. Plus, it's a technically inferior workaround to access siloed data. Lastly, I'm not sure what that deep dive into ""business analyst""/""data engineer light"" territory would mean for my career as a data scientist. It might limit me in some ways and it might create opportunities in others.

On the other side, it's only way too access some sources now. That may (or may not!) change in two years time, when a major software system is updated. And that depends on IT governance two years down the road (at a large company).

Long rambling, I know. My question: do you have experience with RPA bots within your data teams or within your departments? How and how well does it work for you? How sustainable a data pipeline can RPAs be? Do you have any advice for me?",2,0.55,https://www.reddit.com/r/datascience/comments/1jd2kgg/is_rpa_a_feasible_way_for_data_scientists_to/,False,True,False
1jcpd28,galactictock,1742142646.0,28,/r/datascience/comments/1jcpd28/how_to_proceed_with_large_work_gap_given/,datascience,How to proceed with large work gap given competitive DS market?,"I’ve been out of work for over a year now and don’t get much traction with job applications. I imagine the employment gap has rendered me basically unemployable in this market, despite having a master’s degree and a few years of subsequent work experience (plus some unrelated work experience prior to the master’s). I’ve even applied to volunteer DS roles just to build my resume and been rejected. I recognize that I will likely need to find other means of employment before I can re-enter the DS space. Any advice on how to proceed and become employable again would be greatly appreciated. ",29,0.85,https://www.reddit.com/r/datascience/comments/1jcpd28/how_to_proceed_with_large_work_gap_given/,False,True,False
1jcnell,Thatshelbs,1742137394.0,15,/r/datascience/comments/1jcnell/3_reasons_why_data_science_projects_fail/,datascience,3 Reasons Why Data Science Projects Fail,Have you ever seen any data science or analytics projects crash and burn? Why do you think it happened? Let’s hear about it! ,0,0.26,https://medium.com/@ThatShelbs/3-reasons-why-data-science-projects-fail-b6a589a58762?sk=0e2d5e9b2ba7650d2d3fae32fd0d1c46,False,False,False
1jclsn8,JayBong2k,1742132812.0,51,/r/datascience/comments/1jclsn8/seeking_advice_how_to_effectively_develop/,datascience,Seeking Advice: How to Effectively Develop advanced ML skills,"**About me** \- I am a DS with currently 3.5 YoE under my belt with experience in BFSI and FMCG.

In the past couple of months, I’ve spoken with several mid-level data scientists working at my target companies. After reviewing my resume, they all pointed out the same gaps:

1. I lack NLP, Deep Learning, and LLM experience.
2. I don’t have any projects demonstrating these skills.
3. Feedback on my resume format varied from person to person.

Given this, I’d like advice on the following:

* How can I develop an intermediate-level understanding of NLP, DL, and LLMs enough to score a new job?
* Courses provide a high-level overview, but they often lack depth—what’s the best way to go deeper?
* I feel like **I’m being stretched too thin** by trying to learn these topics in different ways (courses, projects etc.). How would you approach this to stay focused and maximize learning?
* How do you gauge depth of your knowledge for interview?

Would appreciate any insights or strategies that worked for you!",182,0.94,https://www.reddit.com/r/datascience/comments/1jclsn8/seeking_advice_how_to_effectively_develop/,False,True,False
1jbqeyy,gagarin_kid,1742026380.0,3,/r/datascience/comments/1jbqeyy/solar_panel_installation_rate_and_energy_yield/,datascience,Solar panel installation rate and energy yield estimation from houses in the neighborhood using aerial imagery and solar radiation maps,,36,0.98,https://kopytjuk.github.io/posts/solar-panel-analysis/,False,False,False
1jbhjmx,PsychicSeaCow,1741994965.0,81,/r/datascience/comments/1jbhjmx/advice_on_building_a_data_team/,datascience,Advice on building a data team,"I’m currently the “chief” (i.e., only) data scientist at a maturing start up. The CEO has asked me to put together a proposal for expanding our data team. For the past 3 years I’ve been doing everything from data engineering, to model development, and mlops. I’ve been working 60+ hour weeks and had to learn a lot of things on the fly. But somehow I’ve have managed to build models that meet our benchmark requirements, pushed them into production, and started to generate revenue. I feel like a jack of all trades and a master of none (with the exception of time-series analysis which was the focus of my PhD in a non-related STEM field). I’m tired, overworked and need to be able to delegate some of my work.

We’re getting to the point where we are ready to hire and grow our team, but I have no experience with transitioning from a solo IC to a team leader. Has anybody else made this transition in a start up? Any advice on how to build a team?

PS. Please DO NOT send me dm’s asking for a job. We do not do Visa sponsorships and we are only looking to hire locally. ",165,0.95,https://www.reddit.com/r/datascience/comments/1jbhjmx/advice_on_building_a_data_team/,False,True,False
1jbetth,kater543,1741987615.0,21,/r/datascience/comments/1jbetth/chain_restaurant_data_scientists_what_do_you_do/,datascience,"Chain restaurant data scientists, what do you do, and what kind of data do you work with?","Is it mostly just marketing? Do y’all ever work on pricing models or wholesale/supply chain analysis? Is your data internal or external? This is all out of academic curiosity, I am not currently looking to get into the industry!",37,0.91,https://www.reddit.com/r/datascience/comments/1jbetth/chain_restaurant_data_scientists_what_do_you_do/,False,True,False
1jbdpuh,Fit-Employee-4393,1741984707.0,19,/r/datascience/comments/1jbdpuh/contract_for_hire_work/,datascience,Contract For Hire Work,"Anybody have experience with contract for hire ds work? Did you convert? Did you get fired halfway through? Was it W2 or 1099? Were you forced to do the annoying stuff that full timers didn’t want to touch?

I’ve been ignoring these types of jobs for a while now, but am interested in hearing how they are. Seems like a lack of security and benefits is traded for a high wage, but idk. 

Should I continue ignoring?",8,0.9,https://www.reddit.com/r/datascience/comments/1jbdpuh/contract_for_hire_work/,False,True,False
1jb0i8y,LeaguePrototype,1741946981.0,13,/r/datascience/comments/1jb0i8y/how_much_of_the_ml_pipeline_am_i_expected_to_know/,datascience,How much of the ML pipeline am I expected to know as DS?,"I'm prepping for an L4 level DS interview at big tech. The interview description is that we'll be doing ML case studies.

Does anyone have a good framework for how to outline how to answer these questions (how much you predict customer LTV?, how would you classify searches on the site?, how would you predict if the ad will be successful?, etc.) similar to the STAR framework for behavioral interviews?

How much of the pipeline am I supposed to know from the start to the end? Some of my interviews in the past have caught me off guard about some part in the pipeline I didn't think was the DS's job.",67,0.91,https://www.reddit.com/r/datascience/comments/1jb0i8y/how_much_of_the_ml_pipeline_am_i_expected_to_know/,False,True,False
1javfus,thro0away12,1741925269.0,33,/r/datascience/comments/1javfus/do_you_deal_with_unrealistic_expectations_from/,datascience,Do you deal with unrealistic expectations from non-technical people frequently?,"I've been working at my job for a year and in data itself for several years. I'm willing to admit my shortcomings, willing to admit mistakes and learn.

However, there are several times where I feel like I've been in situations where there is 'no-winning'. Recently, I've inherited a task from a colleague who has left. There is no documentation. My only way of understanding this task is through the colleague who assigned it to me, who is not really a technical person. I've inherited code which is repetitive/redundant, difficult to follow and understand. What I REALLY want to do is spend time cleaning up this code so that debugging is easier and this code can run better but I'm not given a chance to do this b/c everytime I get a request related to this project, I'm asked to churn something out in less than a day. This feels unrealistic b/c I don't even have time to understand the outcome and whenever I do exactly as my collague asks, it has times broken something downstream, forcing me to undo this as soon as possible. This has put a strain on other tasks and so when I put this task to the side to do other tasks, there's been frustration expressed on me for not doing this task sooner.

The same colleague who assigned me this task initially told me that if I need help in understanding the requirements, he can help with that. When I've gone to him to ask questions or send updates, he himself looks like he doesn't have time to answer my questions because of back to back meetings. When he doesn't respond, then he expresses frustration to my boss and other senior colleagues when I haven't done something b/c I'm still waiting for a response b/c 'it's taking too long'. My boss has expressed to me he feels I don't ask enough questions that could be 'holding up the process'. So I have tried to ask more questions, but when colleagues can't get back to me on time, I'm told I'm not asking the right people or if I ask a question, I'm told I'm not 'asking the right question'. For example, this same colleague wanted me to fix a bug and wrote that this bug is causing ""unexpected results"". A senior colleague asked me if the requirements to fix this bug are clear to me and I thought to just clarify with the colleague who put in the bug fix request ""do you want me to remove these records or figure out how to best include them in the end result"". My boss saw my response and said ""you're not asking the right question! you're not supposed to ask people to do YOUR work for you"". From my point of view, I wasn't asking anybody to do my work b/c I'm the one ultimately who will dive into the code to fix things.

I'm at a loss tbh....I'm trying to do all the right things, trying to also improve my 'people skills' and understand what people want and how to streamline things. I know there's more room for improvement for me, but I am struggling with conflicting advice and lack of direction. I'm not sure if others can relate to this.",106,0.96,https://www.reddit.com/r/datascience/comments/1javfus/do_you_deal_with_unrealistic_expectations_from/,False,True,False
1jally0,Trick-Interaction396,1741897051.0,96,/r/datascience/comments/1jally0/does_anyone_have_a_job_which_doesnt_use/,datascience,Does anyone have a job which doesn't use LLM/NLP/Computer Vision?,"I am looking for a new job and everything I see is LLM/NLP/Computer Vision. That stuff doesn't really interest me. Seems very computer science and my background is stats/analytics. I do linear regression and xgboost. Do these jobs still exist? If so, where?",148,0.92,https://www.reddit.com/r/datascience/comments/1jally0/does_anyone_have_a_job_which_doesnt_use/,False,True,False
1jajnyq,duffs_dimes,1741892160.0,39,/r/datascience/comments/1jajnyq/has_anybody_taken_the_datamasked_course/,datascience,Has anybody taken the DataMasked Course?,"Is it worth 3 grand? [https://datamasked.com/](https://datamasked.com/)

A data science coach (influencer?) on LinkedIn highly recommended it.

I'm 3 years post MS from a non-impressive state school. I'm working in compliance in the banking industry and bored out of my mind.

I'd like to break into experimentation, marketing, causal inference, etc.

Would this course be a good use of my money and time?",21,0.72,https://www.reddit.com/r/datascience/comments/1jajnyq/has_anybody_taken_the_datamasked_course/,False,True,False
1j8kofx,mehul_gupta1997,1741675340.0,10,/r/datascience/comments/1j8kofx/free_registrations_for_nvidia_gtc_2025_one_of_the/,datascience,"Free Registrations for NVIDIA GTC' 2025, one of the prominent AI conferences, are open now","https://preview.redd.it/46u3gvqma0oe1.jpg?width=1200&format=pjpg&auto=webp&s=8e99003ad3c9af8b3f825a142650ea4e8ccfbf07

NVIDIA GTC 2025 is set to take place from **March 17-21**, bringing together researchers, developers, and industry leaders to discuss the latest advancements in **AI, accelerated computing, MLOps, Generative AI, and more**.

One of the key highlights will be **Jensen Huang’s keynote**, where NVIDIA has historically introduced breakthroughs, including last year’s **Blackwell architecture**. Given the pace of innovation, this year’s event is expected to feature significant developments in **AI infrastructure, model efficiency, and enterprise-scale deployment**.

With **technical sessions, hands-on workshops, and discussions led by experts**, GTC remains one of the most important events for those working in AI and high-performance computing.

**Registration is free** and now open. You can register [here.](https://www.nvidia.com/gtc/?ncid=ref-inpa-722552)

I strongly feel NVIDIA will announce something really big around AI this time. What are your thoughts?

",20,0.88,https://www.reddit.com/r/datascience/comments/1j8kofx/free_registrations_for_nvidia_gtc_2025_one_of_the/,False,True,False
1j8iqpw,fridchikn24,1741667444.0,9,/r/datascience/comments/1j8iqpw/msba_with_5_years_experience_in_ds_looking_to/,datascience,"MSBA with 5 years experience in DS looking to pivot to an MLE, should I get a master's in CS?",I feel it would help me bridge the gap in software development and would appeal to recruiters(I am unemployed rn),8,0.68,https://www.reddit.com/r/datascience/comments/1j8iqpw/msba_with_5_years_experience_in_ds_looking_to/,False,True,False
1j8g4w9,redKeep45,1741658958.0,22,/r/datascience/comments/1j8g4w9/mysql_for_ds_interviews/,datascience,MySQL for DS interviews?,"Hi, I currently work as a DS at a AI company, we primarily use SparkSQL, but I believe most DS interviews are in MySQL (?). Any tips/reading material for a smooth transition. 

  
For my work, I use SparkSQL for EDA and featurization ",11,0.77,https://www.reddit.com/r/datascience/comments/1j8g4w9/mysql_for_ds_interviews/,False,True,False
1j87x92,JobIsAss,1741637037.0,33,/r/datascience/comments/1j87x92/how_do_you_deal_with_coworkers_that_are_adamant/,datascience,How do you deal with coworkers that are adamant about their ways despite it blowing up in the past.,"Was discussing with a peer and they are very adamant of using randomized splits as its easy despite the fact that I proved that data sampling is problematic for replication as the data will never be the same even with random_seed set up. Factors like environment and hardware play a role.

I been pushing for model replication is a bare minimum standard as if someone else cant replicate the results then how can they validate it? We work in a heavily regulated field and I had to save a project from my predecessor where the entire thing was on the verge of being pulled out because none of the results could be replicated by a third party.

My coworker says that the standard shouldn’t be set up but i personally believe that replication is a bare minimum regardless as models isnt just fitting and predicting with 0 validation. If anything we need to ensure that our model is stable. 

The person constantly challenges everything I say and refuses to acknowledge the merit of methodology.  I dont mind people challenging but constantly saying I dont see the point or it doesn’t matter when it does infact matter by 3rd party validators. 

This person when working with them I had to constantly slow them down and stop them from rushing Through the work as it literally contains tons of mistakes. This is like a common occurrence. 


Edit: i see a few comments in, My manager was in the discussion as my coworker brought it up in our stand up and i had to defend my position in-front of my bosses (director and above). Basically what they said is “apparently we have to do this because I say this is what should be done now given the need to replicate”. So everyone is pretty much aware and my boss did approach me on this, specifically because we both saw the fallout of how bad replication is problematic.",9,0.74,https://www.reddit.com/r/datascience/comments/1j87x92/how_do_you_deal_with_coworkers_that_are_adamant/,False,True,False
1j80r9t,ElectrikMetriks,1741619202.0,5,/r/datascience/comments/1j80r9t/happy_2025_mar10_day/,datascience,Happy 2025 Mar10 Day!,,75,0.88,https://i.redd.it/ive5e6q1ovne1.jpeg,False,False,False
1j7uxqq,Suspicious_Sector866,1741599831.0,11,/r/datascience/comments/1j7uxqq/why_is_my_macbook_m4_pro_faster_than_my_rtx_4060/,datascience,Why is my MacBook M4 Pro faster than my RTX 4060 Desktop for LLM inference with Ollama?,"I've been running the `deepseek-coder-v2` model (8.9GB) using `ollama run` on two systems:

1. **MacBook M4 Pro** (latest model)
2. **Desktop** with **Intel i9-14900K**, **192GB RAM**, and an **RTX 4060 GPU**

Surprisingly, the MacBook M4 Pro is significantly faster when running a simple query like ""tell me a long story."" The desktop setup, which should be much more powerful on paper, is noticeably slower.

Both systems are running the same model with default Ollama configurations.

Why is the MacBook M4 Pro outperforming the desktop? Is it related to how Ollama utilizes hardware, GPU acceleration differences, or perhaps optimizations for Apple Silicon?

Would appreciate insights from anyone with experience in LLM inference on these platforms!

Note: I can observe my gpu usage spiking when running the same, and so assume the hardware access is happening without issue",19,0.74,https://www.reddit.com/r/datascience/comments/1j7uxqq/why_is_my_macbook_m4_pro_faster_than_my_rtx_4060/,False,True,False
1j7uh4f,metalvendetta,1741597724.0,3,/r/datascience/comments/1j7uh4f/have_you_started_using_mcp_model_context_protocol/,datascience,Have you started using MCP (Model Context Protocol) with your agentic workflow and data storages? What is the experience?,"If you've used MCP in your workflow, how has the experience been? Do you use it on top of your current data storage as well to gather more data?",9,0.8,https://www.reddit.com/r/datascience/comments/1j7uh4f/have_you_started_using_mcp_model_context_protocol/,False,True,False
1j7q4w5,AutoModerator,1741579302.0,22,/r/datascience/comments/1j7q4w5/weekly_entering_transitioning_thread_10_mar_2025/,datascience,"Weekly Entering & Transitioning - Thread 10 Mar, 2025 - 17 Mar, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",9,0.92,https://www.reddit.com/r/datascience/comments/1j7q4w5/weekly_entering_transitioning_thread_10_mar_2025/,False,True,False
1j7gch1,osm3000,1741550831.0,8,/r/datascience/comments/1j7gch1/the_kebab_and_the_french_train_station_yet/,datascience,The kebab and the French train station: yet another data-driven analysis,,33,0.92,https://blog.osm-ai.net/2025/03/09/the-kebab-and-the-train-station-deep-analysis.html,False,False,False
1j5t5gp,No_Information6299,1741366018.0,8,/r/datascience/comments/1j5t5gp/agent_flow_vs_data_science/,datascience,Agent flow vs. data science,"I just wrapped up an experiment exploring how the number of agents (or steps) in an AI pipeline affects classification accuracy. Specifically, I tested four different setups on a movie review classification task. My initial hypothesis going into this was essentially, ""*More agents might mean a more thorough analysis, and therefore higher accuracy.""* But, as you'll see, it's not quite that straightforward.

# Results Summary

I have used the first 1000 reviews from IMDB  dataset to classify reviews into positive or negative. I used gpt-4o-mini as a model.

Here are the final results from the experiment:

|Pipeline Approach|Accuracy|
|:-|:-|
|Classification Only|0.95|
|Summary → Classification|0.94|
|Summary → Statements → Classification|0.93|
|Summary → Statements → Explanation → Classification|0.94|

Let's break down each step and try to see what's happening here.

# Step 1: Classification Only

*(Accuracy: 0.95)*

This simplest approach—simply reading a review and classifying it as positive or negative—provided the highest accuracy of all four pipelines. The model was straightforward and did its single task exceptionally well without added complexity.

# Step 2: Summary → Classification

*(Accuracy: 0.94)*

Next, I introduced an extra agent that produced an emotional summary of the reviews before the classifier made its decision. Surprisingly, accuracy slightly dropped to 0.94. It looks like the summarization step possibly introduced abstraction or subtle noise into the input, leading to slightly lower overall performance.

# Step 3: Summary → Statements → Classification

*(Accuracy: 0.93)*

Adding yet another step, this pipeline included an agent designed to extract key emotional statements from the review. My assumption was that added clarity or detail at this stage might improve performance. Instead, overall accuracy dropped a bit further to 0.93. While the statements created by this agent might offer richer insights on emotion, they clearly introduced complexity or noise the classifier couldn't optimally handle.

# Step 4: Summary → Statements → Explanation → Classification

*(Accuracy: 0.94)*

Finally, another agent was introduced that provided human readable explanations alongside the material generated in prior steps. This boosted accuracy slightly back up to 0.94, but didn't quite match the original simple classifier's performance. The major benefit here was increased interpretability rather than improved classification accuracy.

# Analysis and Takeaways

Here are some key points we can draw from these results:

# More Agents Doesn't Automatically Mean Higher Accuracy.

Adding layers and agents can significantly aid in interpretability and extracting structured, valuable data—like emotional summaries or detailed explanations—but each step also comes with risks. Each guy in the pipeline can introduce new errors or noise into the information it's passing forward.

# Complexity Versus Simplicity

The simplest classifier, with a single job to do (direct classification), actually ended up delivering the top accuracy. Although multi-agent pipelines offer useful modularity and can provide great insights, they're not necessarily the best option if raw accuracy is your number one priority.

# Always Double Check Your Metrics.

Different datasets, tasks, or model architectures could yield different results. Make sure you are consistently evaluating tradeoffs—interpretability, extra insights, and user experience vs. accuracy.

In the end, ironically, the simplest methodology—just directly classifying the review—gave me the highest accuracy. For situations where richer insights or interpretability matter, multiple-agent pipelines can still be extremely valuable even if they don't necessarily outperform simpler strategies on accuracy alone.

I'd love to get thoughts from everyone else who has experimented with these multi-agent setups. Did you notice a similar pattern (the simpler approach being as good or slightly better), or did you manage to achieve higher accuracy with multiple agents?

Full code on [GitHub](https://github.com/Pravko-Solutions/FlashLearn/tree/main/examples/agent_patterns)

# TL;DR

Adding multiple steps or agents can bring deeper insight and structure to your AI pipelines, but it won't always give you higher accuracy. Sometimes, keeping it simple is actually the best choice.",19,0.79,https://www.reddit.com/r/datascience/comments/1j5t5gp/agent_flow_vs_data_science/,False,True,False
1j5hrrn,No-Brilliant6770,1741331290.0,16,/r/datascience/comments/1j5hrrn/thinking_of_selling_my_m2_air_to_buy_an_m4_pro_is/,datascience,Thinking of selling my M2 Air to buy an M4 Pro - is it worth the upgrade for Machine Learning?,"Hey everybody, I need some advice. I’m a 3rd year CS undergrad and currently have a MacBook M2 Air with 16GB RAM and 256GB storage. I bought it in 2022 for about $2000 CAD, but I’ve been running into issues. When I open multiple apps like Docker, Ollama, PyCharm, and run training models, the laptop quickly runs out of RAM and gets heat up and starts swapping, which isn’t great for the SSD.

I’m leaning towards selling it to upgrade to an M4 Pro, especially for machine learning and data science tasks. However, Apple’s trade-in value is only around $585 CAD, and I just recently had the motherboard, chassis, and display replaced (everything except the battery), so my laptop is basically new in most parts. I was planning to sell it on Facebook Marketplace, but I’m not sure what price I should target now that the M4 has been released.

On the flip side, I’ve also considered keeping the laptop and using a Google Colab subscription for ML work. But running many applications still leads to heavy swap usage, which could harm the SSD in the long run. Given that I just renewed some parts, it might be the best time to sell for a higher resale value.

If I decide to upgrade to the M4, I’m thinking of getting a model with at least 24GB RAM and a 10-core CPU and GPU combination. Do you guys think that would be enough to future-proof it? What are your thoughts on selling now versus sticking with the current setup and using cloud resources?",0,0.31,https://www.reddit.com/r/datascience/comments/1j5hrrn/thinking_of_selling_my_m2_air_to_buy_an_m4_pro_is/,False,True,False
1j4wn4d,Careless-Tailor-2317,1741271234.0,13,/r/datascience/comments/1j4wn4d/failing_final_round_interviews/,datascience,Failing final round interviews,I've been applying to DS internships all year and just got rejected from my 4th final round. Does anyone have any advice for these interviews? And is it bad practice for me to ask the hiring managers where I went wrong in the interviews?,6,0.8,https://www.reddit.com/r/datascience/comments/1j4wn4d/failing_final_round_interviews/,False,True,False
1j4v2ee,Suspicious-Oil6672,1741266526.0,13,/r/datascience/comments/1j4v2ee/google_collab_now_provides_native_support_for/,datascience,Google Collab now provides native support for Julia 🎉🥳,,157,0.99,https://i.redd.it/tasxd9icsyme1.png,False,False,False
1j47h7z,Zeoluccio,1741193182.0,3,/r/datascience/comments/1j47h7z/help_with_pyspark_and_bigquery/,datascience,Help with pyspark and bigquery,"Hi everyone. 

I'm creating a pyspark df that contains arrays for certain columns. 

But when I move it to a bigqquery table all the columns containing arrays are empty (they contains a message that says 0 rows)

Any suggestions? 

Thanks ",1,0.56,https://www.reddit.com/r/datascience/comments/1j47h7z/help_with_pyspark_and_bigquery/,False,True,False
1j3hx9m,mehul_gupta1997,1741113392.0,5,/r/datascience/comments/1j3hx9m/googles_data_science_agent_free_to_use_in_colab/,datascience,Google's Data Science Agent (free to use in Colab): Build DS pipelines with just a prompt,"Google launched Data Science Agent integrated in Colab where you just need to upload files and ask any questions like build a classification pipeline, show insights etc. Tested the agent, looks decent but has errors and was unable to train a regression model on some EV data.  Know more here : https://youtu.be/94HbBP-4n8o",9,0.64,https://www.reddit.com/r/datascience/comments/1j3hx9m/googles_data_science_agent_free_to_use_in_colab/,False,True,False
1j3hq4r,FirefoxMetzger,1741112915.0,108,/r/datascience/comments/1j3hq4r/whats_your_favourite_ai_tool_so_far/,datascience,Whats your favourite AI tool so far?,Its hard for me too keep up - please enlighten me on what I am currently missing out on :),121,0.91,https://www.reddit.com/r/datascience/comments/1j3hq4r/whats_your_favourite_ai_tool_so_far/,False,True,False
1j3hc7l,Cool-Ad-3878,1741111976.0,12,/r/datascience/comments/1j3hc7l/would_someone_with_a_bba_fintech_make_a_good_data/,datascience,Would someone with a BBA Fintech make a good data scientist?,"Given they: Demonstrate fluency in Data Science programs/models such as Python, R, Blockchain, Al etc. and be able to recommend technological solutions to such problems as imperfect or asymmetric data

*(Deciding on a course to pursue with my limited regional options)*

Thank you ",0,0.4,https://www.reddit.com/r/datascience/comments/1j3hc7l/would_someone_with_a_bba_fintech_make_a_good_data/,False,True,False
1j3etuj,Proof_Wrap_2150,1741105952.0,48,/r/datascience/comments/1j3etuj/favorite_data_science_books_and_authors/,datascience,Favorite Data Science Books and Authors?,"I enjoy O’Reilly books for data science. I like how they build a topic progressively throughout the chapters. I’m looking for recommendations on great books or authors you’ve found particularly helpful in learning data science, analytics, or machine learning.

What do you like about your recommendation? Do they have a unique way of explaining concepts, great real-world examples, or a hands-on approach?",111,0.99,https://www.reddit.com/r/datascience/comments/1j3etuj/favorite_data_science_books_and_authors/,False,True,False
1j39e2e,Davidat0r,1741090364.0,32,/r/datascience/comments/1j39e2e/workflow_with_spark_large_datasets/,datascience,Workflow with Spark & large datasets,"Hi, I’m a beginner DS working at a company that handles huge datasets (>50M rows, >100 columns) in databricks with Spark. 

The most discouraging part of my job is the eternal waiting times when I want to check the current state of my EDA, say, I want the null count in a specific column, for example. 

I know I could sample the dataframe in the beginning to prevent processing the whole data but that doesn’t really reduce the execution time, even if I .cache() the sampled dataframe. 

I’m waiting now for 40 minutes for a count and I think this can’t be the way real professionals work, with such waiting times (of course I try to do something productive in those times but sometimes the job just needs to get done. 

So, I ask the more experienced professionals in this group: how do you handle this part of the job? Is .sample() our only option? I’m eager to learn ways to be better at my job. ",23,0.93,https://www.reddit.com/r/datascience/comments/1j39e2e/workflow_with_spark_large_datasets/,False,True,False
1j30hfs,mehul_gupta1997,1741055622.0,12,/r/datascience/comments/1j30hfs/huggingface_free_certification_course_for_llm/,datascience,"HuggingFace free certification course for ""LLM Reasoning"" is live","HuggingFace has launched a new free course on ""LLM Reasoning"" for explaining how to build models like DeepSeek-R1. The course has a special focus towards Reinforcement Learning. Link : https://huggingface.co/reasoning-course",192,0.95,https://i.redd.it/48t51es84lme1.png,False,False,False
1j2fd49,pimmen89,1740996142.0,14,/r/datascience/comments/1j2fd49/soft_skills_how_do_you_make_the_rest_of_the/,datascience,Soft skills: How do you make the rest of the organization contribute to data quality?,"I've been in six different data teams in my career, two of them as an employee and four as a consultant. Often we run into a wall when it comes to data quality where the quality will not improve unless the rest of the organization works to better it.

For example, if the dev team doesn't test the event measuring and deploy a new version, you don't get any data until you figure out what the problem is, ask them to fix it, and they deploy the fix. They say that they will test it next time, but it doesn't become a priority and happens a few months later again.

Or when a team is supposed to reach a certain KPI they will cut corners and do a weird process to reach it, making the measurement useless. For example, when employees on the ground are rewarded for the ""order to deliver"" time, they might check something as delivered once it's completed but not actually delivered, because they don't get rewarded for completing the task quickly only delivering it.

How do you engage with the rest organization to make them care about the data quality and meet you half way?

One thing I've kept doing at new organizations is trying to build an internal data product for the data producing teams, so that they can become a stakeholder in the data quality. If they don't get their processes in order, their data product stops working. This has had mixed results, form completely transformning the company to not having any impact at all. I've also tried holding workshops, and they seem to work for a while, but as people change departments and other stuff happens, this knowledge gets lost or deprioritized again.

What are your tried and true ways to make the organization you work for take the data quality seriously?",69,0.95,https://www.reddit.com/r/datascience/comments/1j2fd49/soft_skills_how_do_you_make_the_rest_of_the/,False,True,False
1j2b2ng,AutoModerator,1740978084.0,38,/r/datascience/comments/1j2b2ng/weekly_entering_transitioning_thread_03_mar_2025/,datascience,"Weekly Entering & Transitioning - Thread 03 Mar, 2025 - 10 Mar, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",5,1.0,https://www.reddit.com/r/datascience/comments/1j2b2ng/weekly_entering_transitioning_thread_03_mar_2025/,False,True,False
1j271od,mehul_gupta1997,1740964935.0,0,/r/datascience/comments/1j271od/chain_of_drafts_improvised_chain_of_thoughts/,datascience,Chain of Drafts : Improvised Chain of Thoughts prompting,CoD is an improvised Chain Of Thoughts prompt technique producing similarly accurate results with just 8% of tokens hence faster and cheaper. Know more here : https://youtu.be/AaWlty7YpOU,1,0.56,https://www.reddit.com/r/datascience/comments/1j271od/chain_of_drafts_improvised_chain_of_thoughts/,False,True,False
1j1nr34,Marek_Vsk,1740910436.0,23,/r/datascience/comments/1j1nr34/alternatives_for_streamlit/,datascience,Alternatives for Streamlit,"For my most pet projects like creating dashboards of voting charts for songs or planning a trip with altitude chart and maps along with some proof of concept for LLM or ML projects at work my first to go is Streamlit. I got accustomed to this tool but looking for some alternatives mostly because of the visual part. I tried dash with plotly but missing the coherence of the Streamlit. 

What is the tool that can do the same for the front end part (which can be uploaded in the simple way similar to Streamlit) as Streamlit but is not Streamlit. What are your favorite similar frameworks?",32,0.85,https://www.reddit.com/r/datascience/comments/1j1nr34/alternatives_for_streamlit/,False,True,False
1j1crux,FirefoxMetzger,1740871337.0,26,/r/datascience/comments/1j1crux/any_examples_of_genai_in_the_value_chain/,datascience,Any examples of GenAI in the value chain?,"Does anyone have some no-bullshit examples of how the generative part of AI has actually added value to the business?

I come across a lot of chat interfaces ... but those often are more hype and fomo than value adds. Curious if you know something serious.",51,0.86,https://www.reddit.com/r/datascience/comments/1j1crux/any_examples_of_genai_in_the_value_chain/,False,True,False
1j17yok,NumerousYam4243,1740858327.0,84,/r/datascience/comments/1j17yok/meta_e5_ml_experience_cleared/,datascience,Meta E5 ML Experience - Cleared,"Learned a lot form this subreddit so sharing my experience so people can learn from it too.

**Coding rounds** \- It is going to be 2 mids or 1 easy and 1 hard. For me biggest shock was the interviewer asked questions to see if I understand what I am saying or just saying it because I saw on leetcode that is the best option. So try to understand why the solution is working the way it is working and how is the space and time complexity calculated for that solution

**Behavioral** \- I created a story for every meta vision and mission. That covers all meta questions. The main difference I found in meta compared to other companies is the depth of follow ups. The questions were very specific and there were follow up questions on my answer to previous follow ups. I don't think one can lie in this round, they would be caught in the follow up questions easily. Also there was no why meta or tell me about yourself.

**MLSD** \- Alex Xu book is all you need for structure and what ML models to read about. The interviewer will ask technical questions including formula and how the particular thing actually work. So my suggest use Alex Xu ML SD book to understand the format, structure and solutions. Then google/chatgpt the technical part of each step in deep.",193,0.93,https://www.reddit.com/r/datascience/comments/1j17yok/meta_e5_ml_experience_cleared/,False,True,False
1j0y4xv,nkafr,1740831156.0,12,/r/datascience/comments/1j0y4xv/influential_timeseries_forecasting_papers_of/,datascience,Influential Time-Series Forecasting Papers of 2023-2024: Part 2,"This article explores some of the latest advancements in time-series forecasting.

You can find the article [here](https://aihorizonforecast.substack.com/p/influential-time-series-forecasting-8c3).

If you know of any other interesting TS papers, please share them in the comments.",104,0.96,https://www.reddit.com/r/datascience/comments/1j0y4xv/influential_timeseries_forecasting_papers_of/,False,True,False
1j0x07s,Aftabby,1740826613.0,33,/r/datascience/comments/1j0x07s/data_science_web_app_project_what_are_your_best/,datascience,Data Science Web App Project: What Are Your Best Tips?,"I'm aiming to create a ***data science project*** that demonstrates my full skill set, including ***web app*** deployment, for my resume. I'm in search of well-structured **demo projects** that I can use as a template for my own work.



I'd also appreciate any guidance on the best tools and practices for deploying a data science project as a web app. What are the key elements that hiring managers look for in a project that's hosted online? Any suggestions on how to effectively present the project on my portfolio website and source code in GitHub profile would be greatly appreciated.",72,0.85,https://www.reddit.com/r/datascience/comments/1j0x07s/data_science_web_app_project_what_are_your_best/,False,True,False
1j0mkfs,Gravbar,1740788322.0,6,/r/datascience/comments/1j0mkfs/textbook_recommendations/,datascience,Textbook Recommendations,"Because of my background in ML I was put in charge of the design and implementation of a project involving using synthetic data to make classification predictions. I am not a beginner and very comfortable with modeling in python with sklearn, pytorch, xgboost, etc and the standard process of scaling data, imputing, feature selection and running different models on hyperparameters. But I've never worked professionally doing this, only some research and kaggle projects. 

At the moment I'm wondering if anyone has any recommendations for textbooks or other documents detailing domain adaptation in the context of synthetic to real data for when the sets are not aligned 

and any on feature engineering techniques for non-time series, tabular numeric data beyond crossing, interactions, and taking summary statistics.


I feel like there's a lot I don't know but somehow I know the most where I work. So are there any intermediate to advanced resources on navigating this space?",15,0.86,https://www.reddit.com/r/datascience/comments/1j0mkfs/textbook_recommendations/,False,True,False
1j0lhyr,magooshseller,1740785290.0,4,/r/datascience/comments/1j0lhyr/presentation_resources/,datascience,Presentation resources,I am looking for any resources helpful for creating good slide decks for presenting our work. I have seen some really fancy decks created by fellow DS at my company and I always wonder how are they creating these without any help. These folks do tend to have consulting backgrounds so could be something learnt there. Is it possible to learn this skill as it seems like good ppt skills create more impact on business stakeholders.,5,0.7,https://www.reddit.com/r/datascience/comments/1j0lhyr/presentation_resources/,False,True,False
1j0ikva,coke_and_coldbrew,1740777451.0,1,/r/datascience/comments/1j0ikva/check_out_our_ai_data_science_tool/,datascience,Check out our AI data science tool,"Demo video: [https://youtu.be/wmbg7wH\_yUs](https://youtu.be/wmbg7wH_yUs)

Try out our beta here: [datasci.pro](https://datasci.pro/) (Note: The site isn’t optimized for mobile yet)

Our tool lets you upload datasets and interact with your data using conversational AI. You can prompt the AI to clean and preprocess data, generate visualizations, run analysis models, and create pdf reports—all while seeing the python scripts running under the hood.

We’re shipping updates daily so your feedback is greatly appreciated!",0,0.24,https://www.reddit.com/r/datascience/comments/1j0ikva/check_out_our_ai_data_science_tool/,False,True,False
1j0eptr,DanielBaldielocks,1740767519.0,6,/r/datascience/comments/1j0eptr/ai_file_convention_detectionlearning/,datascience,AI File Convention Detection/Learning,"I have an idea for a project and trying to find some information online as this seems like something someone would have already worked on, however I'm having trouble finding anything online.  So I'm hoping someone here could point me in the direction to start learning more.

So some background.  In my job I help monitor the moving and processing of various files as they move between vendors/systems.

So for example we may a file that is generated daily named customerDataMMDDYY.rpt  where MMDDYY is the month day year.  Yet another file might have a naming convention like genericReport394MMDDYY492.csv

So what I would like to is to try and build a learning system that monitors the master data stream of file transfers that does two things

1) automatically detects naming conventions  
2) for each naming convention/pattern found in step 1, detect the ""normal"" cadence of the file movement.  For example is it 7 days a week, just week days, once a month?  
3) once 1,2 are set up, then alert if a file misses it's cadence.

Now I know how to get 2 and 3 set up.  However I'm having a hard time building a system to detect the naming conventions.  I have some ideas on how to get it done but hitting dead ends so hoping someone here might be able to offer some help.

Thanks",1,0.6,https://www.reddit.com/r/datascience/comments/1j0eptr/ai_file_convention_detectionlearning/,False,True,False
1j0abwb,Triplebeambalancebar,1740756662.0,0,/r/datascience/comments/1j0abwb/how_would_i_recreate_this_page_other_data_inputs/,datascience,How would I recreate this page (other data inputs and topics) on my Squarespace website?,"Hello All,

New Hear i have a youtube channel and social brand I'm trying to build, and I want to create pages like this:

[https://www.cnn.com/markets/fear-and-greed](https://www.cnn.com/markets/fear-and-greed)

or the data snapshots here:

[https://knowyourmeme.com/memes/loss](https://knowyourmeme.com/memes/loss)

I want to repeatedly create pages that would encompass a topic and have graphs and visuals like the above examples.

Thanks for any help or suggestions!!!",0,0.33,https://www.reddit.com/r/datascience/comments/1j0abwb/how_would_i_recreate_this_page_other_data_inputs/,False,True,False
1j04aqa,joshamayo7,1740736876.0,12,/r/datascience/comments/1j04aqa/medium_blog_post_on_eda/,datascience,Medium Blog post on EDA,"Hi all,
Started my own blog with the aim of providing guidance to beginners and reinforcing some concepts for those more experienced. 

Essentially trying to share value. Link is attached. Hope there’s something to learn for everyone. Happy to receive any critiques as well",37,0.84,https://medium.com/@joshamayo7/a-visual-guide-to-exploratory-data-analysis-eda-with-python-5581c3106485,False,False,False
1j03efx,imisskobe95,1740732812.0,29,/r/datascience/comments/1j03efx/fwd_name_shame_pacific_life_insurance_sharing_cuz/,datascience,Fwd - NAME & SHAME: PACIFIC LIFE INSURANCE - sharing cuz reading this pissed me off. Similar experience with them last year.,,51,0.85,/r/datasciencecareers/comments/1j00o3i/name_shame_pacific_life_insurance/,False,False,False
1j029yl,Unhappy_Technician68,1740727755.0,48,/r/datascience/comments/1j029yl/sales_forecasting_advice_multiple_out_put/,datascience,"Sales forecasting advice, multiple out put","Hi All,

So I'm forecasting some sales data.  Mainly units sold.  They want a daily forecast (I tried to push them towards weekly but here we are).

I have a decades worth of data, I need to model out the effects of lockdowns obviously as well as like a bazillion campaigns they run throughout the year.

I've done some feature engineering and I've tried running it through multiple regression but that doesn't seem to work there are just so many parameters.  I computed a PCA on the input sales data and I'm feeding the lagged scores into the model which helps to reduce the number of features.

I am currently trying Gaussian Process Regression, the results are not generalizing well at all.  Definitely getting overfitting.  It gives 90% R2 and incredibly low rmse on training data, then garbage on validation.  The actual predictions do not track the real data as well at all.  Honestly was getting better just reconstruction from the previous day's PCA.  Considering doing some cross validation and hyper parameter tuning, any general advice on how to proceed?  I'm basically just throwing models at the wall to see what sticks would appreciate any advice.

",16,0.81,https://www.reddit.com/r/datascience/comments/1j029yl/sales_forecasting_advice_multiple_out_put/,False,True,False
1j0291u,Alarmed-Reporter-230,1740727637.0,1,/r/datascience/comments/1j0291u/question_on_gpt2_from_scratch_of_andrej_karpathy/,datascience,question on GPT2 from scratch of Andrej Karpathy,"I was watching his video (Let's reproduce GPT-2 (124M)) where he implemented GPT-2. At around 3:15:00, it says that the initial token is the `endoftext` token. Can someone explain why that is?

Also, it seems to me that, with his code, three sentences of length 500, 524, and 2048 tokens, respectively, will fit into a (3, 1024) tensor (ignoring any excess tokens), with the first two sentences being adjacent. This would be appropriate if the three sentences come from, let's say, the same book or article; otherwise, it could be detrimental during training. Is my reasoning correct?",7,0.89,https://www.reddit.com/r/datascience/comments/1j0291u/question_on_gpt2_from_scratch_of_andrej_karpathy/,False,True,False
1izmkfd,KindLuis_7,1740680515.0,209,/r/datascience/comments/1izmkfd/ds_is_becoming_ai_standardized_junk/,datascience,DS is becoming AI standardized junk,"Hiring is a nightmare. The majority of applicants submit the same prepackaged solutions. basic plots, default models, no validation, no business reasoning. EDA has been reduced to prewritten scripts with no anomaly detection or hypothesis testing. Modeling is just feeding data into GPT-suggested libraries, skipping feature selection, statistical reasoning, and assumption checks. Validation has become nothing more than blindly accepting default metrics. Everybody’s using AI and everything looks the same. It’s the standardization of mediocrity. Data science is turning into a low quality, copy-paste job.",882,0.78,https://www.reddit.com/r/datascience/comments/1izmkfd/ds_is_becoming_ai_standardized_junk/,False,True,False
1izapxk,metalvendetta,1740643084.0,8,/r/datascience/comments/1izapxk/have_you_used_data_heatmap_in_your_workflows_if/,datascience,Have you used data heatmap in your workflows? If yes then how and what tools did you use?,"One specific use case would be:

  
\- LLM training/finetuning datasets could use heatmap to assess what records of a dataset have been mostly used across multiple models.



What else do you need data heatmap in your workflow, and did you write your own code or external tools to assess this for yourself?",5,0.63,https://www.reddit.com/r/datascience/comments/1izapxk/have_you_used_data_heatmap_in_your_workflows_if/,False,True,False
1iz76dr,TheLastWhiteKid,1740629347.0,2,/r/datascience/comments/1iz76dr/unsupervised_model_failure_instagram_algorithm_is/,datascience,[Unsupervised Model failure] Instagram Algorithm is Broken Every Year on Feb 26,,26,0.93,/r/Instagram/comments/1iz6w1v/instagram_algorithm_is_broken_every_year_on_feb_26/,False,False,False
1iyn2u6,mehul_gupta1997,1740575184.0,0,/r/datascience/comments/1iyn2u6/wan21_new_sota_model_for_video_generation/,datascience,"Wan2.1 : New SOTA model for video generation, open-sourced, can run on consumer grade GPU","Alibabba group has released Wan2.1, a SOTA model series which has excelled on all benchmarks and is open-sourced. The 480P version can run on just 8GB VRAM only. Know more here : https://youtu.be/_JG80i2PaYc",4,0.67,https://www.reddit.com/r/datascience/comments/1iyn2u6/wan21_new_sota_model_for_video_generation/,False,True,False
1iygj98,AnUncookedCabbage,1740548664.0,406,/r/datascience/comments/1iygj98/is_there_a_large_pool_of_incompetent_data/,datascience,Is there a large pool of incompetent data scientists out there?,"Having moved from academia to data science in industry, I've had a strange series of interactions with other data scientists that has left me very confused about the state of the field, and I am wondering if it's just by chance or if this is a common experience? Here are a couple of examples:

I was hired to lead a small team doing data science in a large utilities company. Most senior person under me, who was referred to as the senior data scientists had no clue about anything and was actively running the team into the dust. Could barely write a for loop, couldn't use git. Took two years to get other parts of business to start trusting us. Had to push to get the individual made redundant because they were a serious liability. It was so problematic working with them I felt like they were a plant from a competitor trying to sabotage us.

Start hiring a new data scientist very recently. Lots of applicants, some with very impressive CVs, phds, experience etc. I gave a handful of them a very basic take home assessment, and the work I got back was mind boggling. The majority had no idea what they were doing, couldn't merge two data frames properly, didn't even look at the data at all by eye just printed summary stats. I was and still am flabbergasted they have high paying jobs in other places.  They would need major coaching to do basic things in my team. 

So my question is: is there a pool of ""fake"" data scientists out there muddying the job market and ruining our collective reputation, or have I just been really unlucky?",849,0.94,https://www.reddit.com/r/datascience/comments/1iygj98/is_there_a_large_pool_of_incompetent_data/,False,True,False
1iy6v4d,takenorinvalid,1740520105.0,66,/r/datascience/comments/1iy6v4d/i_get_the_impression_that_traditional_statistical/,datascience,I get the impression that traditional statistical models are out-of-place with Big Data. What's the modern view on this?,"I'm a Data Scientist, but not good enough at Stats to feel confident making a statement like this one. But it seems to me that:

* Traditional statistical tests were built with the expectation that sample sizes would generally be around 20 - 30 people
* Applying them to Big Data situations where our groups consist of millions of people and reflect nearly 100% of the population is problematic

Specifically, I'm currently working on a A/B Testing project for websites, where people get different variations of a website and we measure the impact on conversion rates. Stakeholders have complained that it's very hard to reach statistical significance using the popular A/B Testing tools, like Optimizely and have tasked me with building a A/B Testing tool from scratch.

To start with the most basic possible approach, I started by running a z-test to compare the conversion rates of the variations and found that, using that approach, you can reach a statistically significant p-value with about 100 visitors. Results are about the same with chi-squared and t-tests, and you can usually get a pretty great effect size, too.

Cool -- but all of these data points are absolutely wrong. If you wait and collect weeks of data anyway, you can see that these effect sizes that were classified as statistically significant are completely incorrect.

It seems obvious to me that the fact that popular A/B Testing tools take a long time to reach statistical significance is a feature, not a flaw.

But there's a lot I don't understand here:

* What's the theory behind adjusting approaches to statistical testing when using Big Data? How are modern statisticians ensuring that these tests are more rigorous?
* What does this mean about traditional statistical approaches? If I can see, using Big Data, that my z-tests and chi-squared tests are calling inaccurate results significant when they're given small sample sizes, does this mean there are issues with these approaches in all cases?

The fact that so many modern programs are already much more rigorous than simple tests suggests that these are questions people have already identified and solved. Can anyone direct me to things I can read to better understand the issue?",98,0.8,https://www.reddit.com/r/datascience/comments/1iy6v4d/i_get_the_impression_that_traditional_statistical/,False,True,False
1iy5ud3,Any-Fig-921,1740517579.0,27,/r/datascience/comments/1iy5ud3/do_you_dev_local_or_in_the_cloud/,datascience,Do you dev local or in the cloud?,"Like the question says -- by this I also think ssh'd into a stateful machine where you can basically do whatever you want counts as 'local.'

My company has tried many different things for us to have development enviornments in the cloud -- jupyter labs, aws sagemaker etc. However, I find that for the most part it's such a pain working with these system that any increase in compute speed I'd gain would be washed out by the clunkiness of these managed development systems.

I'm sure there's times when your data get's huge -- but tbh I can handle a few trillion rows locally if I batch. And my local GPU is so much easier to use than trying to download CUDA on an AWS system.

For me, just putting a requirments.txt in the rep, and using either a venv or a docker container is just so much easier and, in practice, more ""standard"" than trying to grok these complicated cloud setups. Yet it seems like every company thinks data scientists ""need"" a cloud setup. ",15,0.76,https://www.reddit.com/r/datascience/comments/1iy5ud3/do_you_dev_local_or_in_the_cloud/,False,True,False
1iy5rks,LeaguePrototype,1740517387.0,3,/r/datascience/comments/1iy5rks/shitty_debugging_job_taught_me_the_most/,datascience,Shitty debugging job taught me the most,"I was always a losey developer and just started working on large codebases the past year (first real job after school). I have a strong background in stats but never had to develop the ""backend"" of data intensive applications.

  
At my current job we took over a project from an outside company who was originally developing it. This was the main reason the company hired us, trying to in-house the project for cheaper than what they were charging. The job is pretty shit tbh, and I got 0 intro into the code or what we are doing. They figuratively just showed me my seat and told me to get at it.

  
I've been using a mix of AI tools to help me read through the code and help me understand what is going on in a macro level. Also when some bug comes up I let it read through the code for me to point me towards where the issue is and insert the neccesary print statements or potential modifications.

  
This excersize of ""something is constantly breaking"" is helping me to become a better data scientist in a shorter amount of time than anything else has. The job is still shit and pays like shit so I'll be switching soon, but I learned a lot by having to do this dirty work that others won't. Unfortunately, I don't think this opportunity is avaiable to someone fresh out of school in HCOL countries since they put this type of work where the labor is cheap.",47,0.94,https://www.reddit.com/r/datascience/comments/1iy5rks/shitty_debugging_job_taught_me_the_most/,False,True,False
1iy3eqq,OverratedDataScience,1740511546.0,103,/r/datascience/comments/1iy3eqq/microsoft_ceo_admits_that_ai_is_generating/,datascience,Microsoft CEO Admits That AI Is Generating Basically No Value,,601,0.83,https://ca.finance.yahoo.com/news/microsoft-ceo-admits-ai-generating-123059075.html,False,False,False
1ixy90o,NoteClassic,1740498922.0,21,/r/datascience/comments/1ixy90o/data_scientist_tasked_with_building_interactive/,datascience,Data Scientist Tasked with Building Interactive Client-Facing Product—Where Should I Start?,"Hi community,

I’m a data scientist with little to no experience in front-end engineering, and I’ve been tasked with developing an interactive, client-facing product. My previous experience with building interactive tools has been limited to Streamlit and Plotly, but neither scales well for this use case.

I’m looking for suggestions on where to start researching technologies or frameworks that can help me create a more scalable and robust solution. Ideally, I’d like something that:

	1. Can handle larger user loads without performance issues. 	2. Is relatively accessible for someone without a front-end background.
        3.Integrates well with Python and backend services.

If you’ve faced a similar challenge, what tools or frameworks did you use? Any resources (tutorials, courses, documentation) would also be much appreciated!",13,0.88,https://www.reddit.com/r/datascience/comments/1ixy90o/data_scientist_tasked_with_building_interactive/,False,True,False
1ixlnua,anecdotal_yokel,1740454738.0,10,/r/datascience/comments/1ixlnua/if_ai_were_used_to_evaluate_employees_based_on/,datascience,"If AI were used to evaluate employees based on self-assessments, what input might cause unintended results?",Have fun with this one. ,9,0.59,https://www.reddit.com/r/datascience/comments/1ixlnua/if_ai_were_used_to_evaluate_employees_based_on/,False,True,False
1ixd4jm,MikeSpecterZane,1740431498.0,11,/r/datascience/comments/1ixd4jm/amazon_as_interviews_starting_in_2_weeks/,datascience,Amazon AS interviews starting in 2 weeks,"Hi, I was recently contacted by an Amazon recruiter. I will be interviewing for an Applied Scientist position. I am currently a DS with 5 years of experience. The problem is that the i terview process involves 1 phone screen and 1 onsite round which will have leetcode style coding. I am pretty bad at DSA. 
Can anyone please suggest me how to prepare for this part in a short duration? 
What questions to do and how to target? 
Any advice will be appreciated. TIA",4,0.6,https://www.reddit.com/r/datascience/comments/1ixd4jm/amazon_as_interviews_starting_in_2_weeks/,False,True,False
1ixctdh,Proof_Wrap_2150,1740430751.0,13,/r/datascience/comments/1ixctdh/improving_workflow_managing_iterations_between/,datascience,Improving Workflow: Managing Iterations Between Data Cleaning and Analysis in Jupyter Notebooks?,"I use Jupyter notebooks for projects, which typically follow a structure like this:
	1. Load Data
	2. Clean Data
	3. Analyze Data

What I find challenging is this iterative cycle:

I clean the data initially, move on to analysis, then realize during analysis that further cleaning or transformations could enhance insights. I then loop back to earlier cells, make modifications, and rerun subsequent cells.

2 ➡️ 3 ➡️ 2.1 (new cell embedded in workflow) ➡️ 3.1 (new cell ….

This process quickly becomes convoluted and difficult to manage clearly within Jupyter notebooks. It feels messy, bouncing between sections and losing track of the logical flow.

My questions for the community:

How do you handle or structure your notebooks to efficiently manage this iterative process between data cleaning and analysis?

Are there best practices, frameworks, or notebook structuring methods you recommend to maintain clarity and readability?

Additionally, I’d appreciate book recommendations (I like books from O’Reilly) that might help me improve my workflow or overall approach to structuring analysis.

Thanks in advance—I’m eager to learn better ways of working!",15,0.86,https://www.reddit.com/r/datascience/comments/1ixctdh/improving_workflow_managing_iterations_between/,False,True,False
1ix80i6,sonicking12,1740419118.0,18,/r/datascience/comments/1ix80i6/what_are_some_good_suggestions_to_learn_route/,datascience,What are some good suggestions to learn route optimization and data science in supply chains?,As titled.,31,0.88,https://www.reddit.com/r/datascience/comments/1ix80i6/what_are_some_good_suggestions_to_learn_route/,False,True,False
1ix7iu9,mihirshah0101,1740417947.0,6,/r/datascience/comments/1ix7iu9/best_books_to_learn_reinforcement_learning/,datascience,Best books to learn Reinforcement learning?,same as title,13,0.93,https://www.reddit.com/r/datascience/comments/1ix7iu9/best_books_to_learn_reinforcement_learning/,False,True,False
1ix4ile,fark13,1740410548.0,11,/r/datascience/comments/1ix4ile/we_are_back_with_many_data_science_jobs_in_soccer/,datascience,"We are back with many Data science jobs in Soccer, NFL, NHL, Formula1 and more sports! 2025","Hey guys,

I've been silent here lately but many opportunities keep appearing and being posted. 

These are a few from the last 10 days or so

* [Part-Time Data Scientist Assistant - NHL](http://www.sportsjobs.online/jobs/7449)
* [Senior Data Scientist - Epic Games](http://www.sportsjobs.online/jobs/7484)
* [Data Scientist - Atlanta United](https://www.linkedin.com/jobs/view/4158678051/)
* [Quantitative Analyst (Mid-Senior)](http://www.sportsjobs.online/jobs/7431)
* [Basketball Data Analyst - Washington Mystics](http://www.sportsjobs.online/jobs/7395)
* [Senior Data Scientist, NFL - The Score](http://www.sportsjobs.online/jobs/7365)

I run www.sportsjobs(.)online, a job board in that niche. In the last month I added around 300 jobs.

For the ones that already saw my posts before, I've added more sources of jobs lately. I'm open to suggestions to prioritize the next batch.

It's a niche, there aren't thousands of jobs as in Software in general but my commitment is to **keep improving a simple metric, jobs per month.**

We always need some metric in DS..

I've created also a [reddit community](https://www.reddit.com/r/sports_jobs/) where I post recurrently the openings if that's easier to check for you.

I hope this helps someone!",116,0.95,https://www.reddit.com/r/datascience/comments/1ix4ile/we_are_back_with_many_data_science_jobs_in_soccer/,False,True,False
1ix3ymj,ditchdweller13,1740409119.0,38,/r/datascience/comments/1ix3ymj/roast_my_cv/,datascience,roast my cv,basically the title. any advice? ,0,0.47,https://i.redd.it/11d53w5vp3le1.jpeg,False,False,False
1ix350m,Symmberry,1740406898.0,69,/r/datascience/comments/1ix350m/whats_the_best_business_book_youve_read/,datascience,What’s the best business book you’ve read?,"I came across this question on a job board. After some reflection, I realized that some of the best business books helped me understand the strategy behind the company’s growth goals, better empathizing with others, and getting them to care about impactful projects like I do.

What are some useful business-related books for a career in data science?",254,0.97,https://www.reddit.com/r/datascience/comments/1ix350m/whats_the_best_business_book_youve_read/,False,True,False
1iwu845,AutoModerator,1740373305.0,47,/r/datascience/comments/1iwu845/weekly_entering_transitioning_thread_24_feb_2025/,datascience,"Weekly Entering & Transitioning - Thread 24 Feb, 2025 - 03 Mar, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",6,1.0,https://www.reddit.com/r/datascience/comments/1iwu845/weekly_entering_transitioning_thread_24_feb_2025/,False,True,False
1iwemoj,kater543,1740329827.0,115,/r/datascience/comments/1iwemoj/gym_chain_data_scientists/,datascience,Gym chain data scientists?,Just had a thought-any gym chain data scientists here can tell me specifically what kind of data science you’re doing? Is it advanced or still in nascency?  Was just curious since I got back into the gym after a while and was thinking of all the possibilities data science wise.,56,0.78,https://www.reddit.com/r/datascience/comments/1iwemoj/gym_chain_data_scientists/,False,True,False
1ivsn10,matt-ice,1740257196.0,5,/r/datascience/comments/1ivsn10/publishing_a_snowflake_native_app_to_generate/,datascience,Publishing a Snowflake native app to generate synthetic financial data - any interest?,,4,0.67,/r/snowflake/comments/1ivsl9s/publishing_a_native_app_to_generate_synthetic/,False,False,False
1ivolsz,mehul_gupta1997,1740246805.0,1,/r/datascience/comments/1ivolsz/deepseek_new_paper_native_sparse_attention_for/,datascience,DeepSeek new paper : Native Sparse Attention for Long Context LLMs,Summary for DeepSeek's new paper on improved Attention mechanism (NSA) : https://youtu.be/kckft3S39_Y?si=8ZLfbFpNKTJJyZdF,6,0.62,https://www.reddit.com/r/datascience/comments/1ivolsz/deepseek_new_paper_native_sparse_attention_for/,False,True,False
1ivgrnb,Ciasteczi,1740223865.0,29,/r/datascience/comments/1ivgrnb/are_llms_good_with_ml_model_outputs/,datascience,Are LLMs good with ML model outputs?,"The vision of my product management is to automate the root cause analysis of the system failure by deploying a multi-reasoning-steps LLM agents that have a problem to solve, and at each reasoning step are able to call one of multiple, simple ML models (get_correlations(X[1:1000], look_for_spikes(time_series(T1,...,T100)).

I mean, I guess it could work because LLMs could utilize domain specific knowledge and process hundreds of model outputs way quicker than human, while ML models would take care of numerically-intense aspects of analysis.

Does the idea make sense? Are there any successful deployments of machines of that sort? Can you recommend any papers on the topic?
",17,0.84,https://www.reddit.com/r/datascience/comments/1ivgrnb/are_llms_good_with_ml_model_outputs/,False,True,False
1ivavo1,mehul_gupta1997,1740199742.0,0,/r/datascience/comments/1ivavo1/large_language_diffusion_models_lldms_diffusion/,datascience,Large Language Diffusion Models (LLDMs) : Diffusion for text generation,"A new architecture for LLM training is proposed called LLDMs that uses Diffusion (majorly used with image generation models ) for text generation. The first model, LLaDA 8B looks decent and is at par with Llama 8B and Qwen2.5 8B. Know more here : https://youtu.be/EdNVMx1fRiA?si=xau2ZYA1IebdmaSD",3,0.72,https://www.reddit.com/r/datascience/comments/1ivavo1/large_language_diffusion_models_lldms_diffusion/,False,True,False
1iv8cbv,SingerEast1469,1740191298.0,109,/r/datascience/comments/1iv8cbv/was_the_hype_around_deepseek_warranted_or/,datascience,Was the hype around DeepSeek warranted or unfounded?,"Python DA here whose upper limit is sklearn, with a bit of tensorflow.

The question: how innovative was the DeepSeek model? There is so much propaganda out there, from both sides, that’s it’s tough to understand what the net gain was.

From what I understand, DeepSeek essentially used reinforcement learning on its base model, was sucked, then trained mini-models from Llama and Qwen in a “distillation” methodology, and has data go thru those mini models after going thru the RL base model, and the combination of these models achieved great performance. Basically just an ensemble method. But what does “distilled” mean, they imported the models ie pytorch? Or they cloned the repo in full? And put data thru all models in a pipeline?

I’m also a bit unclear on the whole concept of synthetic data. To me this seems like a HUGE no no, but according to my chat with DeepSeek, they did use synthetic data.

So, was it a cheap knock off that was overhyped, or an innovative new way to architect an LLM? And what does that even mean?",67,0.79,https://www.reddit.com/r/datascience/comments/1iv8cbv/was_the_hype_around_deepseek_warranted_or/,False,True,False
1iuw9ow,Difficult-Big-3890,1740158571.0,198,/r/datascience/comments/1iuw9ow/to_the_avid_fans_of_r_i_respect_your_fight_for_it/,datascience,"To the avid fans of R, I respect your fight for it but honestly curious what keeps you motivated?","I started my career as an R user and loved it! Then after some years in I started looking for new roles and got the slap of reality that no one asks for R. Gradually made the switch to Python and never looked back. I have nothing against R and I still fend off unreasonable attacks on R by people who never used it calling it only good for adhoc academic analysis and bla bla. But, is it still worth fighting for?",342,0.95,https://www.reddit.com/r/datascience/comments/1iuw9ow/to_the_avid_fans_of_r_i_respect_your_fight_for_it/,False,True,False
1iun6jy,KindLuis_7,1740130910.0,165,/r/datascience/comments/1iun6jy/ai_isnt_evolving_its_stagnating/,datascience,"AI isn’t evolving, it’s stagnating","AI was supposed to revolutionize intelligence, but all it’s doing is shifting us from discovery to dependency. Development has turned into a cycle of fine-tuning and API calls, just engineering.
Let’s be real, the power isn’t in the models it’s in the infrastructure. If you don’t have access to massive compute, you’re not training anything foundational. Google, OpenAI, and Microsoft own the stack, everyone else just rents it. This isn’t decentralizing intelligence it’s centralizing control.
Meanwhile, the viral hype is wearing thin. Compute costs are unsustainable, inference is slow and scaling isn’t as seamless as promised. We are deep in Amara’s Law, overestimating short-term effects and underestimating long-term ones.",842,0.88,https://www.reddit.com/r/datascience/comments/1iun6jy/ai_isnt_evolving_its_stagnating/,False,True,False
1iuivf9,Proof_Wrap_2150,1740113557.0,18,/r/datascience/comments/1iuivf9/how_would_you_clean_categorize_job_titles_at_scale/,datascience,How Would You Clean & Categorize Job Titles at Scale?,"I have a dataset with 50,000 unique job titles and want to standardize them by grouping similar titles under a common category. 

My approach is to:

1. Take the top 20% most frequently occurring titles (~500 unique).
2. Use these 500 reference titles to label and categorize the entire dataset.
3. Assign a match score to indicate how closely other job titles align with these reference titles.

I’m still working through it, but I’m curious—how would you approach this problem? Would you use NLP, fuzzy matching, embeddings, or another method?

Any insights on handling messy job titles at scale would be appreciated!

TL;DR: I have 50k unique job titles and want to group similar ones using the top 500 most common titles as a reference set. How would you do it? Do you have any other ways of solving this?",26,0.82,https://www.reddit.com/r/datascience/comments/1iuivf9/how_would_you_clean_categorize_job_titles_at_scale/,False,True,False
1iuib2z,mehul_gupta1997,1740111639.0,22,/r/datascience/comments/1iuib2z/uncensored_deepseekr1_by_perplexity_ai/,datascience,Uncensored DeepSeek-R1 by Perplexity AI,"Perplexity AI has released R1-1776, a post tuned version of DeepSeek-R1 with 0 Chinese censorship and bias. The model is free to use on perplexity AI and weights are available on Huggingface. For more info : https://youtu.be/TzNlvJlt8eg?si=SCDmfFtoThRvVpwh",74,0.85,https://www.reddit.com/r/datascience/comments/1iuib2z/uncensored_deepseekr1_by_perplexity_ai/,False,True,False
1iuf85f,jarena009,1740102146.0,105,/r/datascience/comments/1iuf85f/whats_are_the_top_three_technical_skills_or/,datascience,"What's are the top three technical skills or platforms to learn, NOT named R, Python, SQL, or any of the BI platforms (eg Tableau, PowerBI)?","E.g. Alteryx, OpenAI, etc?",122,0.86,https://www.reddit.com/r/datascience/comments/1iuf85f/whats_are_the_top_three_technical_skills_or/,False,True,False
1iu0skr,No_Information6299,1740064542.0,1,/r/datascience/comments/1iu0skr/build_demo_pipelines_100x_faster/,datascience,Build demo pipelines 100x faster,"Every time I start a new project I have to collect the data and guide clients through the first few weeks before I get some decent results to show them. This is why I created a collection of classic data science pipelines built with LLMs you can use to quickly demo any data science pipeline and even use it in production in some cases.

All of the examples are using opensource library FlashLearn that was developed for exactly this purpose.

# Examples by use case

* **Customer service**
   * [Classifying customer tickets](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Customer%20service/classify_tickets.md)
* **Finance**
   * [Parse financial report data](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Finance/parse_financial_report_data.md)
* **Marketing**
   * [Customer segmentation](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Marketing/customer_segmentation.md)
* **Personal assistant**
   * [Research assistant](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Personal%20asistant/research_assistant.md)
* **Product Intelligence**
   * [Discover trends in product\_reviews](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Product%20intelligence/discover_trends_in_prodcut%20_reviews.md)
   * [User behaviour analysis](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Product%20intelligence/user_behaviour_analysis.md)
* **Sales**
   * [Personalized cold emails](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Sales/personalized_emails.md)
   * [Sentiment classification](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Sales/sentiment_classification.md)

Feel free to use it and adapt it for your use cases!

P.S: The quality of the result should be 2-5% off the specialized model -> I expect this gap will close with new development. ",0,0.33,https://www.reddit.com/r/datascience/comments/1iu0skr/build_demo_pipelines_100x_faster/,False,True,False
1itr8ub,Cool-Ad-3878,1740029776.0,39,/r/datascience/comments/1itr8ub/who_would_contribute_more_to_a_company/,datascience,Who would contribute more to a company?,"2 fresh graduates, Graduate A and B.

Graduate A has a data science bachelors, has completed various projects and research and stays up to date with industry skills. (Internships completed too)

Graduate B has a statistics bachelors, has actively pursued academic research and applies learned skills to a startup after some projects. (No internships, but lots of self initiation)

Would Graduate A or B make the cut for the data scientist and/or ML/AI role? ",0,0.41,https://www.reddit.com/r/datascience/comments/1itr8ub/who_would_contribute_more_to_a_company/,False,True,False
1itpmil,Tamalelulu,1740024207.0,6,/r/datascience/comments/1itpmil/upping_my_generative_ai_game/,datascience,Upping my Generative AI game,"I'm a pretty big user of AI on a consumer level. I'd like to take a deeper dive in terms of what it could do for me in Data Science. I'm not thinking so much of becoming an expert on building LLMs but more of an expert in using them. I'd like to learn more about 
- Prompt engineering 
- API integration 
- Light overview on how LLMs work
- Custom GPTs

Can anyone suggest courses, books, YouTube videos, etc that might help me achieve that goal?",0,0.43,https://www.reddit.com/r/datascience/comments/1itpmil/upping_my_generative_ai_game/,False,True,False
1ito4a5,Proof_Wrap_2150,1740019435.0,9,/r/datascience/comments/1ito4a5/help_analyzing_profit_loss_statements_across/,datascience,Help analyzing Profit & Loss statements across multiple years?,"Has anyone done work analyzing Profit & Loss statements across multiple years? I have several years of records but am struggling with standardizing the data. The structure of the PDFs varies, making it difficult to extract and align information consistently.

Rather than reading the files with Python, I started by manually copying and pasting data for a few years to prove a concept. I’d like to start analyzing 10+ years once I am confident I can capture the pdf data without manual intervention. I’d like to automate this process. If you’ve worked on something similar, how did you handle inconsistencies in PDF formatting and structure?",8,0.75,https://www.reddit.com/r/datascience/comments/1ito4a5/help_analyzing_profit_loss_statements_across/,False,True,False
1itn1zg,big_data_mike,1740016276.0,46,/r/datascience/comments/1itn1zg/how_do_you_organize_your_files/,datascience,How do you organize your files?,"In my current work I mostly do one-off scripts, data exploration, try 5 different ways to solve a problem, and do a lot of testing. My files are a hot mess. Someone asks me to do a project and I vaguely remember something similar I did a year ago that I could reuse but I cannot find it so I have to rewrite it. How do you manage your development work and “rough drafts” before you have a final cleaned up version? 

Anything in production is on GitHub, unit tested, and all that good stuff. I’m using a windows machine with Spyder if that matters. I also have a pretty nice Linux desktop in the office that I can ssh into so that’s a whole other set of files that is not a hot mess…..yet.",66,0.94,https://www.reddit.com/r/datascience/comments/1itn1zg/how_do_you_organize_your_files/,False,True,False
1it3ed9,Longjumping-Will-127,1739964790.0,22,/r/datascience/comments/1it3ed9/data_science_entrepreneur/,datascience,Data Science Entrepreneur,"Anyone in this group running a consultancy or trying to build a start-up? Or even an early employee at a startup?

I feel like data science lends itself mainly to large corps and without much transferability to SMEs
",26,0.77,https://www.reddit.com/r/datascience/comments/1it3ed9/data_science_entrepreneur/,False,True,False
1isi8u2,phicreative1997,1739899840.0,1,/r/datascience/comments/1isi8u2/building_a_reliable_texttosql_pipeline_a/,datascience,Building a Reliable Text-to-SQL Pipeline: A Step-by-Step Guide pt.2,,6,0.8,https://open.substack.com/pub/firebirdtech/p/building-a-reliable-text-to-sql-pipeline-e38?utm_source=share&utm_medium=android&r=kyemx,False,False,False
1ish90u,Will_Tomos_Edwards,1739897496.0,4,/r/datascience/comments/1ish90u/anyone_do_testgorilla_tests_for_a_job_app/,datascience,Anyone do TestGorilla tests for a job app?,I recently did some technical assessments from TestGorilla. I'm wondering what other people thought of these.,2,0.62,https://www.reddit.com/r/datascience/comments/1ish90u/anyone_do_testgorilla_tests_for_a_job_app/,False,True,False
1isdrmn,Grapphie,1739888535.0,40,/r/datascience/comments/1isdrmn/i_created_cv_copilot_for_data_scientists/,datascience,I created CV copilot for Data Scientists,,121,0.82,https://i.redd.it/e6p4hlyppwje1.gif,False,False,False
1isd4tz,xandie985,1739886739.0,1,/r/datascience/comments/1isd4tz/time_series_data_loading_headaches_tell_us_about/,datascience,Time series data loading headaches? Tell us about them!,"Hi r/datascience,

I am revamping time series data loading in PyTorch and want your input!  We're working on a open-source data loader with a unified API to handle all sorts of time series data quirks – different formats, locations, metadata, you name it.

The goal? Make your life easier when working with pytorch, forecasting, foundation models, and more. No more wrestling with Pandas, polars, or messy file formats! we are planning to expand the coverage and support all kinds of time series data formats. 

We're exploring a flexible two-layered design, but we need your help to make it truly awesome.

**Tell us about your time series data loading woes:**

* What are the biggest challenges you face?
* What formats and sources do you typically work with?
* Any specific features or situations that are a real pain?
* What would your dream time series data loader do?

Your feedback will directly shape this project, so share your thoughts and help us build something amazing!",4,0.6,https://www.reddit.com/r/datascience/comments/1isd4tz/time_series_data_loading_headaches_tell_us_about/,False,True,False
1is56xt,Amazing_Life_221,1739855830.0,9,/r/datascience/comments/1is56xt/system_design_oops_apis_security_etc_in_data/,datascience,"System design, OOPs, APIs, Security etc in Data science interviews?","System design, OOPs concepts and other things for DS interviews?

As a data scientist I know how to train a model, how to build data pipelines, how to create API and then deploy it on the server (maybe not extensively but I know how to deploy it on say EC2 with a docker etc). Also I know basics of OOPs and pretty good with solving leetcode type problems (ie optimising scripts). 

But now with a 4 years of exp, do I need to know the system design as well? That too extensive system design with everything that comes under the software pipeline? A client(a software engineer) just interviewed me for only such topics, API end points, scalability, etc. which I had zero idea about. I know only the basics of these things and feels like this isn’t something I should be looking at (as data science itself is huge to learn how am I supposed to learn entire software stack?) 

Am I right? Or I’m just living under a rock all this time? ",21,0.92,https://www.reddit.com/r/datascience/comments/1is56xt/system_design_oops_apis_security_etc_in_data/,False,True,False
1irs5de,yaymayhun,1739819907.0,54,/r/datascience/comments/1irs5de/what_app_making_framework_do_you_recommend_to/,datascience,What app making framework do you recommend to data scientists?,"Communicating findings from data analysis is important for people who work with data. One aspect of that is making web apps. For someone with no/little experience with web development, what app making framework would you recommend? Shiny for python/R, FastHTML, Django, Flask, or something else? And why? 

The goal is to make robust apps that work well with multiple concurrent users. Should support asynchronous operations for long running calculations.

Edit:
It seems that for simple to intermediate level complex apps, Shiny for R/Python or FastHTML are great options. The main advantage is that you can write all frontend and backend code in a single language. FastAPI authors developed FastHTML and they say it can replace FastAPI + JS frontend. So, FastHTML is probably a good option for complicated apps also.",67,0.91,https://www.reddit.com/r/datascience/comments/1irs5de/what_app_making_framework_do_you_recommend_to/,False,True,False
1irr33g,Trungyaphets,1739817406.0,17,/r/datascience/comments/1irr33g/how_to_actually_apply_inferential_statistics_on/,datascience,How to actually apply Inferential Statistics on analyses/to help business?,"Hi guys I'm a Data analyst with like 3-4 years of experience. I feel like in my last jobs I got too relaxed and have been doing too much SQL, building dashboards, reporting and python automation without going into advanced analyses. I just got lucky and had a great job offer from a company with millions of active users. I don't want to waste this opportunity to learn and therefore am looking into more advanced topics, namely inferential statistics, to make my time here worthwhile.

As far as I know Inferential statistics should be mostly about defining hypotheses, doing statistical tests and drawing conclusions. However what I'm not sure is when/how can you make use of these tests to benefit a business.

Could you please share a case, just briefly is enough, where you used inferential/advanced statistics/analysis to help your org/business?

Any other skills a great Data analyst should have?

Thank you very much! Any comment could help me a lot!",40,0.94,https://www.reddit.com/r/datascience/comments/1irr33g/how_to_actually_apply_inferential_statistics_on/,False,True,False
1irq8e0,ElectrikMetriks,1739815437.0,33,/r/datascience/comments/1irq8e0/oc_theres_far_better_ways_to_work_with_larger/,datascience,[OC] There's far better ways to work with larger sets of data... and there's also more fun ways to overheat your computer than a massive Excel book.,,236,0.93,https://i.redd.it/n2s5thveoqje1.png,False,False,False
1irlf7b,Huge-Leek844,1739803548.0,6,/r/datascience/comments/1irlf7b/leverage_my_skills/,datascience,Leverage my skills,"I work in automotive as a embedded developer (C++, Python ) in sensor processing and state estimation like sensor fusion. Also started to work in edge AI. I really like to analyse signals, think about models. Its not data science per se, but i want to leverage my skills to find data science jobs.

How can i upskill? What to learn? Is my skills valuable for data science?",3,0.59,https://www.reddit.com/r/datascience/comments/1irlf7b/leverage_my_skills/,False,True,False
1irkor6,tootieloolie,1739801465.0,16,/r/datascience/comments/1irkor6/roc_vs_prc_not_what_i_expected/,datascience,ROC vs PRC - Not what I expected,"https://preview.redd.it/eb419hrdipje1.png?width=1484&format=png&auto=webp&s=ab92bc43b20b2919a991fe6ca8ad1839f91ddf33

Interviewee started to talk about China and Taiwan when asked this question. Watch out for chatgpt abuse. ",81,0.86,https://www.reddit.com/r/datascience/comments/1irkor6/roc_vs_prc_not_what_i_expected/,False,True,False
1irc50e,AutoModerator,1739768504.0,28,/r/datascience/comments/1irc50e/weekly_entering_transitioning_thread_17_feb_2025/,datascience,"Weekly Entering & Transitioning - Thread 17 Feb, 2025 - 24 Feb, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",9,0.92,https://www.reddit.com/r/datascience/comments/1irc50e/weekly_entering_transitioning_thread_17_feb_2025/,False,True,False
1ir3o2h,NerdyMcDataNerd,1739742729.0,3,/r/datascience/comments/1ir3o2h/dataflow_diagrams_and_other_planning/,datascience,Dataflow Diagrams and Other Planning?,"Recently I have been thinking a lot about the project planning needed for good Data Science practices. Having intelligent conversations and defining clear goals is like half the battle for any job, Data Science not being an exception. 

One thing that my team has historically done towards the beginning of a project (that I quite enjoy) is to gather everyone together to discuss our Dataflow Diagrams.

For those of you who may not know what that is, here is a link: [https://www.geeksforgeeks.org/what-is-dfddata-flow-diagram/](https://www.geeksforgeeks.org/what-is-dfddata-flow-diagram/)

Some people may think that this is solely the domain of the Data Architect or Engineer (neither of which I do on an official basis), but I believe that getting the opinions of my teammates early on can reduce problems down the line. I have even incorporated this practice at the place that I volunteer at.

On to the point of this post: have any of you found the design of these quite helpful or not? What are some practices that you do to maybe improve designing these? Any other planning tips or advice to share?

P.S. I usually lurk here, so I guess it is time that I make a post. Lol!",7,0.69,https://www.reddit.com/r/datascience/comments/1ir3o2h/dataflow_diagrams_and_other_planning/,False,True,False
1ir26jt,ParfaitRude229,1739738914.0,44,/r/datascience/comments/1ir26jt/starting_a_data_consultancy/,datascience,Starting a Data Consultancy,Hey everyone. Was wondering if anyone here has successfully started their own data science/analytics/governance consultancy firm before. What was the experience like and has it been worth it so far?,52,0.83,https://www.reddit.com/r/datascience/comments/1ir26jt/starting_a_data_consultancy/,False,True,False
1iqfhq5,Queasy_Commission316,1739664782.0,6,/r/datascience/comments/1iqfhq5/most_trusted_sources_of_ai_news/,datascience,Most trusted sources of AI news,"What is your most trusted source of AI news?
",0,0.39,https://www.reddit.com/r/datascience/comments/1iqfhq5/most_trusted_sources_of_ai_news/,False,True,False
1iq9hcp,lemonbottles_89,1739648599.0,54,/r/datascience/comments/1iq9hcp/what_is_your_dailyweekly_routine_if_you_have_a/,datascience,What is your daily/weekly routine if you have a WFH position?,"I'm asking this here since data science/analytics is a very remote industry. I'm honestly trying to figure out a good cadence of when to make breakfast and get coffee, when to meal prep, when to get a 15 minute walk in, when to work out, do my hobbies etc., without driving myself insane. Especially when it comes to meal prepping and cooking. When I was unemployed I was able to cook and meal prep for myself every day. I'm trying to figure out how often to cook and meal prep and grocery shop so I'm not cooking as soon as I log off. 

What is your routine for keeping up with life while you're working remotely?",65,0.88,https://www.reddit.com/r/datascience/comments/1iq9hcp/what_is_your_dailyweekly_routine_if_you_have_a/,False,True,False
1iq2oo0,No_Information6299,1739630311.0,3,/r/datascience/comments/1iq2oo0/give_clients_bosses_what_they_want/,datascience,Give clients & bosses what they want,"Every time I start a new project I have to collect the data and guide clients through the first few weeks before I get some decent results to show them. This is why I created a collection of classic data science pipelines built with LLMs you can use to quickly demo any data science pipeline and even use it in production for non-critical use cases.

# Examples by use case

* **Customer service**
   * [Classifying customer tickets](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Customer%20service/classify_tickets.md)
* **Finance**
   * [Parse financial report data](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Finance/parse_financial_report_data.md)
* **Marketing**
   * [Customer segmentation](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Marketing/customer_segmentation.md)
* **Personal assistant**
   * [Research assistant](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Personal%20asistant/research_assistant.md)
* **Product Intelligence**
   * [Discover trends in product\_reviews](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Product%20intelligence/discover_trends_in_prodcut%20_reviews.md)
   * [User behaviour analysis](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Product%20intelligence/user_behaviour_analysis.md)
* **Sales**
   * [Personalized cold emails](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Sales/personalized_emails.md)
   * [Sentiment classification](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Sales/sentiment_classification.md)



Feel free to use it and adapt it for your use cases! ",15,0.69,https://www.reddit.com/r/datascience/comments/1iq2oo0/give_clients_bosses_what_they_want/,False,True,False
1iq0gwj,KindLuis_7,1739623050.0,243,/r/datascience/comments/1iq0gwj/data_science_is_losing_its_soul/,datascience,Data Science is losing its soul,"DS teams are starting to lose the essence that made them truly groundbreaking. their mixed scientific and business core. What we’re seeing now is a shift from deep statistical analysis and business oriented modeling to quick and dirty engineering solutions. Sure, this approach might give us a few immediate wins but it leads to low ROI projects and pulls the field further away from its true potential. One size-fits-all programming just doesn’t work. it’s not the whole game.
",901,0.92,https://www.reddit.com/r/datascience/comments/1iq0gwj/data_science_is_losing_its_soul/,False,True,False
1ip9y2l,chomoloc0,1739537732.0,4,/r/datascience/comments/1ip9y2l/looking_for_resources_on_interrupted_time_series/,datascience,Looking for resources on Interrupted time series analysis,"As the title says, I am looking for sources on the topic. It can go from basics to advanced use cases. I need them both. Thanks!",0,0.5,https://www.reddit.com/r/datascience/comments/1ip9y2l/looking_for_resources_on_interrupted_time_series/,False,True,False
1ioxz48,ib33,1739492790.0,4,/r/datascience/comments/1ioxz48/fcc_text_data/,datascience,FCC Text data?,"I'm looking to do some project(s) regarding telecommunications. Would I have to build an ""FCC_publications"" dataset from scratch? I'm not finding one on their site or others.


Also, what's the standard these days for storing/sharing a dataset like that? I can't imagine it's CSV. But is it just a zip file with folders/documents inside?",5,0.78,https://www.reddit.com/r/datascience/comments/1ioxz48/fcc_text_data/,False,True,False
1ios31c,_hairyberry_,1739476942.0,138,/r/datascience/comments/1ios31c/what_companiesindustries_are_slowpacedlow_stress/,datascience,What companies/industries are “slow-paced”/low stress?,"I’ve only ever worked in data science for consulting companies, which are inherently fast-paced and quite stressful. The money is good but I don’t see myself in this field forever. “Fast-pace” in my experience can be a code word for “burn you out”. 

Out of curiosity, do any of you have lower stress jobs in data science? My guess would be large retailers/corporations that are no longer in growth stage and just want to fine tune/maintain their production models, while also dedicating some money to R&D with more reasonable timelines",225,0.95,https://www.reddit.com/r/datascience/comments/1ios31c/what_companiesindustries_are_slowpacedlow_stress/,False,True,False
1ior7cf,lostmillenial97531,1739474730.0,19,/r/datascience/comments/1ior7cf/mcafee_data_scientist/,datascience,Mcafee data scientist,Anyone has gone through Mcafee data science coding assessment? Looking for some insights on the assessment. ,11,0.72,https://www.reddit.com/r/datascience/comments/1ior7cf/mcafee_data_scientist/,False,True,False
1iolgcd,Weird_ftr,1739460206.0,10,/r/datascience/comments/1iolgcd/is_managing_unstructured_data_a_pain_point_for/,datascience,Is Managing Unstructured Data a Pain Point for the AI/RAG Ecosystem? Can It Be Solved by Well-Designed Software?,"Hey Redditors,

I've been brainstorming about a software solution that could potentially address a significant gap in the AI-enhanced information retrieval systems, particularly in the realm of Retrieval-Augmented Generation (RAG). While these systems have advanced considerably, there's still a major production challenge: managing the real-time validity, updates, and deletion of documents forming the knowledge base.

Currently, teams need to appoint managers to oversee the governance of these unstructured data, similar to how structured databases like SQL are managed. This is a complex task that requires dedicated jobs and suitable tools.

Here's my idea: develop a unified user interface (UI) specifically for document ingestion, advanced data management, and transformation into synchronized vector databases. The final product would serve as a single access point per document base, allowing clients to perform semantic searches using their AI agents. The UI would encourage data managers to keep their information up-to-date through features like notifications, email alerts, and document expiration dates.

The project could start as open-source, with a potential revenue model involving a paid service to deploy AI agents connected to the document base.

Some technical challenges include ensuring the accuracy of embeddings and dealing with chunking strategies for document processing. As technology advances, these hurdles might lessen, shifting the focus to the quality and relevance of the source document base.

Do you think a well-designed software solution could genuinely add value to this industry? Would love to hear your thoughts, experiences, and any suggestions you might have.

Do you know any existing open source software ?

Looking forward to your insights!",0,0.14,https://www.reddit.com/r/datascience/comments/1iolgcd/is_managing_unstructured_data_a_pain_point_for/,False,True,False
1iogppw,Different_Eggplant97,1739444855.0,1,/r/datascience/comments/1iogppw/data_team_benchmarks/,datascience,Data Team Benchmarks,"I put together some charts to help benchmark data teams: [http://databenchmarks.com/](http://databenchmarks.com/)

For example

* Average data team size as % of the company (hint: 3%)
* Median salary across data roles for 500 job postings in Europe
* Distribution of analytics engineers, data engineers, and analysts
* The data-to-engineer ratio at top tech companies

The data comes from LinkedIn, open job boards, and a few other sources.",6,0.75,https://www.reddit.com/r/datascience/comments/1iogppw/data_team_benchmarks/,False,True,False
1iobbu2,jameslee2295,1739421908.0,12,/r/datascience/comments/1iobbu2/what_are_the_common_challenges_businesses_face_in/,datascience,What Are the Common Challenges Businesses Face in LLM Training and Inference?,"Hi everyone, I’m relatively new to the AI field and currently exploring the world of LLMs. I’m curious to know what are the main challenges businesses face when it comes to training and deploying LLMs, as I’d like to understand the challenges beginners like me might encounter.

Are there specific difficulties in terms of data processing or model performance during inference? What are the key obstacles you’ve encountered that could be helpful for someone starting out in this field to be aware of?

Any insights would be greatly appreciated! Thanks in advance!",4,0.57,https://www.reddit.com/r/datascience/comments/1iobbu2/what_are_the_common_challenges_businesses_face_in/,False,True,False
1inytsd,KindLuis_7,1739387137.0,163,/r/datascience/comments/1inytsd/ai_influencers_will_kill_it_sector/,datascience,AI Influencers will kill IT sector,"Tech-illiterate managers see AI-generated hype and think they need to disrupt everything: cut salaries, push impossible deadlines and replace skilled workers with AI that barely functions. 
Instead of making IT more efficient, they drive talent away, lower industry standards and create burnout cycles. The results? Worse products, more tech debt and a race to the bottom where nobody wins except investors cashing out before the crash.",622,0.95,https://www.reddit.com/r/datascience/comments/1inytsd/ai_influencers_will_kill_it_sector/,False,True,False
1inofnb,mehul_gupta1997,1739358307.0,1,/r/datascience/comments/1inofnb/kimi_k15_o1_level_reasoning_llm_free_api/,datascience,Kimi k-1.5 (o1 level reasoning LLM) Free API,"So Moonshot AI just released free API for Kimi k-1.5, a reasoning multimodal LLM which even beat OpenAI o1 on some benchmarks. The Free API gives access to 20 Million tokens. Check out how to generate : https://youtu.be/BJxKa__2w6Y?si=X9pkH8RsQhxjJeCR",17,0.71,https://www.reddit.com/r/datascience/comments/1inofnb/kimi_k15_o1_level_reasoning_llm_free_api/,False,True,False
1inl1gw,jameslee2295,1739342927.0,13,/r/datascience/comments/1inl1gw/challenges_with_realtime_inference_at_scale/,datascience,Challenges with Real-time Inference at Scale,"Hello! We’re implementing an AI chatbot that supports real-time customer interactions, but the inference time of our LLM becomes a bottleneck under heavy user traffic. Even with GPU-backed infrastructure, the scaling costs are climbing quickly. Has anyone optimized LLMs for high-throughput applications or found any company provides platforms/services that handle this efficiently? Would love to hear about approaches to reduce latency without sacrificing quality.",7,0.7,https://www.reddit.com/r/datascience/comments/1inl1gw/challenges_with_realtime_inference_at_scale/,False,True,False
1imkowl,AdministrativeRub484,1739230742.0,22,/r/datascience/comments/1imkowl/evaluating_the_thinking_process_of_reasoning_llms/,datascience,Evaluating the thinking process of reasoning LLMs,"So I tried using Deepseek R1 for a classification task. Turns out it is awful. Still, my boss wants me to evaluate it's thinking process and he has now told me to search for ways to do so.

I tried looking on arxiv and google but did not manage to find anything about evaluating the reasoning process of these models on subjective tasks.

What else can I do here?",23,0.75,https://www.reddit.com/r/datascience/comments/1imkowl/evaluating_the_thinking_process_of_reasoning_llms/,False,True,False
1imf5q9,neural_net_ork,1739216896.0,22,/r/datascience/comments/1imf5q9/takehomes_how_do_you_approach_them_and_how_to_get/,datascience,"Takehomes, how do you approach them and how to get better?","As the title says, I have about 1 year of data science experience, mostly as junior DS. My previous work consisted of month long ML projects so I am familiar with how to get each step done (cleaning, modeling, feature engineering etc.). However, I always feel like with take homes my approach is just bad. I spent about 15 hours (normally 6-10 seems to is expected afail), but then the model is absolute shit. If I were to break it down, I would say 10 hours on pandas wizardry of cleaning data, EDA (basic plots) and feature engineering, 5 on modeling, usually I try several models and end up with one that works best. HOWEVER, when I say best I do not mean it works well, it almost always behaved like shit, even something good like random forest with few features is typically giving bad predictions in most metrics. So the question is, if anyone has good examples / tutorials on how the process should look like, I would appreciate",29,0.87,https://www.reddit.com/r/datascience/comments/1imf5q9/takehomes_how_do_you_approach_them_and_how_to_get/,False,True,False
1im9ipe,Careful-Ingenuity674,1739203389.0,34,/r/datascience/comments/1im9ipe/building_an_app_help/,datascience,Building an app. Help,"I work as a data analyst. I have been asked to create an app that can be used by employees to track general updates in the company. The app must be able to be accessed on employees mobile phones. The app needs to be separate to any work login information, ideally using a personal phone number to gain access or a code.

I tried using power apps but that requires login through Microsoft.

I've never built an app before I was wondering if anyone knew any low code applications to use to built it and if not any other relatively simple application to use? Thanks.",13,0.68,https://www.reddit.com/r/datascience/comments/1im9ipe/building_an_app_help/,False,True,False
1ilyfhk,AutoModerator,1739163703.0,61,/r/datascience/comments/1ilyfhk/weekly_entering_transitioning_thread_10_feb_2025/,datascience,"Weekly Entering & Transitioning - Thread 10 Feb, 2025 - 17 Feb, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",8,0.79,https://www.reddit.com/r/datascience/comments/1ilyfhk/weekly_entering_transitioning_thread_10_feb_2025/,False,True,False
1ilb54i,cognitivebehavior,1739093515.0,30,/r/datascience/comments/1ilb54i/efforttime_needed_for_data_science_not/,datascience,Effort/Time needed for Data Science not recognized/valued,"I conduct many data analysis projects to improve processes and overall performance at my company. I am not employed as a data analyst or data scientist but fill the job as manager for a manufacturing area. 

I have the issue that top management just asks for analysis or insights but seems not to be aware of the effort and time I need to conduct these things. To gather all data, preprocess them, make the analysis, and then process the findings to nice visuals for them. 

Often it seems they think it takes one to two hours for an analysis although I need several days. 

I struggle because I feel they do not appreciate my work or recognize how much effort it takes; besides the knowledge and skills I have to put in to conduct the analysis. 

Is anyone else experiencing the same situation or have an idea how I can address this? 

",183,0.98,https://www.reddit.com/r/datascience/comments/1ilb54i/efforttime_needed_for_data_science_not/,False,True,False
1ikgq0p,FullStackAI-Alta,1738995013.0,3,/r/datascience/comments/1ikgq0p/data_analysis_on_ai_agent_token_flow/,datascience,Data Analysis on AI Agent Token Flow,"Does anyone know of a particular tool or library that can simulate agent system before actually calling LLMs or APIs? Something that I can find the distribution of token generation by a tool or agent or the number of calls to a certain function by LLM etc., any thoughts?",7,0.77,https://www.reddit.com/r/datascience/comments/1ikgq0p/data_analysis_on_ai_agent_token_flow/,False,True,False
1ijzgvg,mutlu_simsek,1738946868.0,4,/r/datascience/comments/1ijzgvg/perpetualbooster_outperformed_autogluon_on_10_out/,datascience,PerpetualBooster outperformed AutoGluon on 10 out of 10 classification tasks,"PerpetualBooster is a GBM but behaves like AutoML so it is benchmarked against AutoGluon (v1.2, best quality preset), the current leader in [AutoML benchmark](https://automlbenchmark.streamlit.app/cd_diagram). Top 10 datasets with the most number of rows are selected from [OpenML datasets](https://www.openml.org/) for classification tasks. 

The results are summarized in the following table:

| OpenML Task | Perpetual Training Duration | Perpetual Inference Duration | Perpetual AUC | AutoGluon Training Duration | AutoGluon Inference Duration | AutoGluon AUC |
| -------------------------------------------------------- | ------- | ------ | ------------------- | -------- | ------ | ------------------ |
| [BNG(spambase)](https://www.openml.org/t/146163)         | 70.1    | 2.1   | 0.671  | 73.1     | 3.7    | 0.669              |
| [BNG(trains)](https://www.openml.org/t/208)              | 89.5    | 1.7   |  0.996 | 106.4    | 2.4    | 0.994              |
| [breast](https://www.openml.org/t/361942)                | 13699.3 | 97.7  |  0.991  | 13330.7  | 79.7   | 0.949              |
| [Click_prediction_small](https://www.openml.org/t/7291)  | 89.1    | 1.0   |  0.749  | 101.0    | 2.8    | 0.703              |
| [colon](https://www.openml.org/t/361938)                 | 12435.2 | 126.7 |  0.997  | 12356.2  | 152.3  | 0.997              |
| [Higgs](https://www.openml.org/t/362113)                 | 3485.3  | 40.9  |  0.843  | 3501.4   | 67.9   | 0.816              |
| [SEA(50000)](https://www.openml.org/t/230)               | 21.9    | 0.2   |  0.936  | 25.6     | 0.5    | 0.935              |
| [sf-police-incidents](https://www.openml.org/t/359994)   | 85.8    | 1.5   |  0.687  | 99.4     | 2.8    | 0.659              |
| [bates_classif_100](https://www.openml.org/t/361941)     | 11152.8 | 50.0  |  0.864  | OOM      | OOM    | OOM                |
| [prostate](https://www.openml.org/t/361945)              | 13699.9 | 79.8  |  0.987  | OOM      | OOM    | OOM                |
| average                                                  | 3747.0  | 34.0  | -                  | 3699.2   | 39.0   | -                  |

PerpetualBooster outperformed AutoGluon on 10 out of 10 classification tasks, training equally fast and inferring 1.1x faster. 

PerpetualBooster demonstrates greater robustness compared to AutoGluon, successfully training on all 10 tasks, whereas AutoGluon encountered out-of-memory errors on 2 of those tasks.

Github: https://github.com/perpetual-ml/perpetual",33,0.89,https://www.reddit.com/r/datascience/comments/1ijzgvg/perpetualbooster_outperformed_autogluon_on_10_out/,False,True,False
1ijs9gs,No_Information6299,1738925131.0,10,/r/datascience/comments/1ijs9gs/update_use_llms_like_scikitlearn/,datascience,[UPDATE] Use LLMs like scikit-learn,"A week ago I posted that I created a very simple Python Open-source lib that allows you to integrate LLMs in your existing data science workflows.

I got a lot of DMs asking for some more real use cases in order for you to understand **HOW** and **WHEN** to use LLMs. This is why I created 10 more or less real examples split by use case/industry to get your brains going.

# Examples by use case

* **Customer service**
   * [Classifying customer tickets](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Customer%20service/classify_tickets.md)
* **Finance**
   * [Parse financial report data](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Finance/parse_financial_report_data.md)
* **Marketing**
   * [Customer segmentation](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Marketing/customer_segmentation.md)
* **Personal assistant**
   * [Research assistant](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Personal%20asistant/research_assistant.md)
* **Product intelligence**
   * [Discover trends in product\_reviews](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Product%20intelligence/discover_trends_in_prodcut%20_reviews.md)
   * [User behaviour analysis](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Product%20intelligence/user_behaviour_analysis.md)
* **Sales**
   * [Personalized cold emails](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Sales/personalized_emails.md)
   * [Sentiment classification](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Sales/sentiment_classification.md)
* **Software development**
   * [Automated PR reviews](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Software%20development/automated_pr_reviews.md)

  
I really hope that this examples will help you deliver your solutions faster! If you have any questions feel free to ask!",15,0.62,https://www.reddit.com/r/datascience/comments/1ijs9gs/update_use_llms_like_scikitlearn/,False,True,False
1ijptot,stevofolife,1738914463.0,9,/r/datascience/comments/1ijptot/anyone_use_uplift_models/,datascience,Anyone use uplift models?,How is your experience with uplift models? Are they easy to train and be used? Any tips and tricks? Do you re-train the model often? How do you decide if uplift model needs to be retrained?,10,0.82,https://www.reddit.com/r/datascience/comments/1ijptot/anyone_use_uplift_models/,False,True,False
1ijfjh6,galactictock,1738881187.0,25,/r/datascience/comments/1ijfjh6/what_does_prompt_engineering_entail_in_a_data/,datascience,What does prompt engineering entail in a Data Scientist role?,"I've seen postings for LLM-focused roles asking for experience with prompt engineering. I've fine-tuned LLMs, worked with transformers, and interfaced with LLM APIs, but what would prompt engineering entail in a DS role?",31,0.78,https://www.reddit.com/r/datascience/comments/1ijfjh6/what_does_prompt_engineering_entail_in_a_data/,False,True,False
1ij5jp8,PhotographFormal8593,1738856769.0,104,/r/datascience/comments/1ij5jp8/have_anyone_recently_interviewed_for_metas_data/,datascience,"Have anyone recently interviewed for Meta's Data Scientist, Product Analytics position?","I was recently contacted by a recruiter from Meta for the Data Scientist, Product Analytics (Ph.D.) position. I was told that the technical screening will be 45 minutes long and cover four areas:

1. Programming
2. Research Design
3. Determining Goals and Success Metrics
4. Data Analysis

I was surprised that all four topics could fit into a 45-minute since I always thought even two topics would be a lot for that time. This makes me wonder if areas 2, 3, and 4 might be combined into a single product-sense question with one big business case study.

Also, I’m curious—does this format apply to all candidates for the Data Scientist, Product Analytics roles, or is it specific to candidates with doctoral degrees?

If anyone has any idea about this, I’d really appreciate it if you could share your experience. Thanks in advance!",181,0.92,https://www.reddit.com/r/datascience/comments/1ij5jp8/have_anyone_recently_interviewed_for_metas_data/,False,True,False
1iid6zv,baileyarzate,1738770942.0,35,/r/datascience/comments/1iid6zv/data_science_skills_help_me_fill_the_gaps/,datascience,"Data Science Skills, Help Me Fill the Gaps!","I’m putting together a Data Science Knowledge Map to track key skills across different areas like Machine Learning, Deep Learning, Statistics, Cloud Computing, and Autonomy/RL. The goal is to make a structured roadmap for learning and improvement.

You can check it out here: https://docs.google.com/spreadsheets/d/1laRz9aftuN-kTjUZNHBbr6-igrDCAP1wFQxdw6fX7vY/edit

My goal is to make it general purpose so you can focus on skillset categories that are most useful to you. 

Would love your feedback. Are there any skills or topics you think should be added? Also, if you have great resources for any of these areas, feel free to share!",151,0.96,https://www.reddit.com/r/datascience/comments/1iid6zv/data_science_skills_help_me_fill_the_gaps/,False,True,False
1iicldl,FreddieKiroh,1738769404.0,6,/r/datascience/comments/1iicldl/advice_on_building_live_odds_model_etl_pipeline/,datascience,"Advice on Building Live Odds Model (ETL Pipeline, Database, Predictive Modeling, API)","I'm working on a side project right now that is designed to be a plugin for a Rocket League mod called BakkesMod that will calculate and display live odds win odds for each team to the player. These will be calculated by taking live player/team stats obtained through the BakkesMod API, sending them to a custom API that accepts the inputs, runs them as variables through predictive models, and returns the odds to the frontend. I have some questions about the architecture/infrastructure that would best be suited. Keep in mind that this is a personal side project so the scale is not massive, but I'd still like it to be fairly thorough and robust.

# Data Pipeline:

My idea is to obtain json data from [Ballchasing.com](http://ballchasing.com/) through their API from the last thirty days to produce relevant models (I don't want data from 2021 to have weight in predicting gameplay in 2025). My ETL pipeline doesn't need to be immediately up-to-date, so I figured I'd automate it to run weekly.

From here, I'd store this data in both AWS S3 and a PostgreSQL database. The S3 bucket will house parquet files assembled from the flattened json data that is received straight from Ballchasing to be used for longer term data analysis and comparison. Storing in S3 Infrequent Access (IA) would be $0.0125/GB and converting it to the Glacier Flexible Retrieval type in S3 after a certain amount of time with a lifecycle rule would be $0.0036/GB. I estimate that a single day's worth of Parquet files would be maybe 20MB, so if I wanted to keep, let's say 90 days worth of data in IA and the rest in Glacier Flexible, that would only be $0.0225 for IA (1.8GB) and I wouldn't reach $0.10/mo in Glacier Flexible costs until 3.8 years worth of data past 90 days old (~27.78GB). Obviously there are costs associated with data requests, but with the small amount of requests I'll be triggering, it's effectively negligible.

As for the Postgres DB, I plan on hosting it on AWS RDS. I will only ever retain the last thirty days worth of data. This means that every weekly run would remove the oldest seven days of data and populate with the newest seven days of data. Overall, I estimate a single day's worth of SQL data being about 25-30 MB, making my total maybe around 750-900 MB. Either way, it's safe to say I'm not looking to store a monumental amount of data.

During data extraction, each group of data entries for a specific day will be transformed to prepare it for loading into the Postgres DB (30 day retention) and writing to parquet files to be stored in S3 (IA -> Glacier Flexible). Afterwards, I'll perform EDA on the cleaned data with Polars to determine things like weights of different stats related to winning matches and what type of modeling library I should use (scikit-learn, PyTorch, XGBoost).

# API:

After developing models for different ranks and game modes, I'd serve them through a gRPC API written in Go. The goal is to be able to just send relevant stats to the API, insert them as variables in the models, and return odds back to the frontend. I have not decided where to store these models yet (S3?).

I doubt it would be necessary, but I did think about using Kafka to stream these results because that's a technology I haven't gotten to really use that interests me, and I feel it may be applicable here (albeit probably not necessary).

# Automation:

As I said earlier, I plan on this pipeline being run weekly. Whether that includes EDA and iterative updates to the models is something I will encounter in the future, but for now, I'd be fine with those steps being manual. I don't foresee my data pipeline being too overwhelming for AWS Lambda, so I think I'll go with that. If it ends up taking too long to run there, I could just run it on an EC2 instance that is turned on/off before/after the pipeline is scheduled to run. I've never used CloudWatch, but I'm of the assumption that I can use that to automate these runs on Lambda. I can conduct basic CI/CD through GitHub actions.

# Frontend

The frontend will not have to be hosted anywhere because it's facilitated through Rocket League as a plugin. It's a simple text display and the in-game live stats will be gathered using BakkesMod's API.

# Questions:

* Does anything seem ridiculous, overkill, or not enough for my purposes? Have I made any mistakes in my choices of technologies and tools?
* What recommendations would you give me for this architecture/infrastructure
* What should I use to transform and prep the data for load into S3/Postgres
* What would be the best service to store my predictive models?
* Is it reasonable to include Kafka in this project to get experience with it even though it's probably not necessary?

Thanks for any help!

Edit 1: Revised data pipeline section to better clarify the storage of Parquet files for long-term storage opposed to raw JSON.",11,0.86,https://www.reddit.com/r/datascience/comments/1iicldl/advice_on_building_live_odds_model_etl_pipeline/,False,True,False
1iibksg,Ok_Composer_1761,1738766793.0,64,/r/datascience/comments/1iibksg/how_do_you_all_quantify_the_revenue_impact_of/,datascience,How do you all quantify the revenue impact of your work product?,"I'm (mostly) an academic so pardon my cluelessness.

A lot of the advice given on here as to how to write an effective resume for industry roles revolves around quantifying the revenue impact of the projects you and your team undertook in your current role. In that, it is not enough to simply discuss technical impact (increased accuracy of predictions, improved quality of data etc) but the impact a project had on a firm's bottom line.

But it seems to me that quantifying the \*causal\* impact of an ML system, or some other standard data science project, is itself a data science project. In fact, one could hire a data scientist (or economist) whose sole job is to audit the effectiveness of data science projects in a firm.  I bet you aren't running diff-in-diffs or estimating production functions, to actually ascertain revenue impact. So how are you guys figuring it out?",72,0.95,https://www.reddit.com/r/datascience/comments/1iibksg/how_do_you_all_quantify_the_revenue_impact_of/,False,True,False
1ii5swa,Florents,1738745293.0,0,/r/datascience/comments/1ii5swa/xi_ξ_correlation_coefficient_in_postgres/,datascience,XI (ξ) Correlation Coefficient in Postgres,,3,0.67,https://github.com/Florents-Tselai/pgxicor,False,False,False
1ihsbrs,NumerousYam4243,1738702758.0,5,/r/datascience/comments/1ihsbrs/ml_system_design_mock/,datascience,ML System Design Mock,"I have ML system design interview coming up and wanted to see if anyone here has website, group,discord or want to mock together?",3,0.64,https://www.reddit.com/r/datascience/comments/1ihsbrs/ml_system_design_mock/,False,True,False
1ihnvjz,Fit-Employee-4393,1738691938.0,61,/r/datascience/comments/1ihnvjz/side_projects/,datascience,Side Projects,"What are your side projects?

For me I have a betting model I’ve been working on from time to time over the past few years. Currently profitable in backtesting, but too risky to put money into. It’s been a fun way to practice things like ranking models and web scraping which I don’t get much exposure to at work. Also could make money with it one day which is cool. I’m wondering what other people are doing for fun on the side. Feel free to share.",101,0.92,https://www.reddit.com/r/datascience/comments/1ihnvjz/side_projects/,False,True,False
1ihl43y,lemonbottles_89,1738685182.0,67,/r/datascience/comments/1ihl43y/for_a_takehome_performance_project_thats_meant_to/,datascience,"For a take-home performance project that's meant to take 2 hours, would you actually stay under 2 hours?","I've completed a take home project for an analyst role I'm applying for. The project asked that I spend no more than 2 hours to complete the task, and that it's okay if not all questions are answered, as they want to get a sense of my data story telling skills. But they also gave me a week to turn this in. 

I've finished and I spent way more than 2 hours on this, as I feel like in this job market, I shouldn't take the risk of turning in a sloppier take home task. I've looked around and seen that others who were given 2 hour take homes also spent way more time on their tasks as well. It just feels like common sense to use all the time I was actually given, especially since other candidates are going to do so as well, but I'm worried that a hiring manager and recruiter might look at this and think ""They obviously spent more than 2 hours"". ",115,0.9,https://www.reddit.com/r/datascience/comments/1ihl43y/for_a_takehome_performance_project_thats_meant_to/,False,True,False
1ih7qk4,Most_Panic_2955,1738637535.0,14,/r/datascience/comments/1ih7qk4/guidance_for_new_professionals/,datascience,Guidance for New Professionals,"Hey everyone, I worked at this company last summer and I am coming back as a graduate in March as a Data Scientist.

Altough the title is Data Scientist, projects with actual modelling are rare. The focus is more on BI, and creating new solutions for the company in its different operations.

I worked there and liked the people and environment but I really aim to stand out, to try and give my best, to learn the most.

I would love to get some tips and experiences from you guys, thanks!",45,0.9,https://www.reddit.com/r/datascience/comments/1ih7qk4/guidance_for_new_professionals/,False,True,False
1igpsrg,Silent-Sunset,1738591497.0,2,/r/datascience/comments/1igpsrg/about_data_processing_data_science_tiger_style/,datascience,"About data processing, data science, tiger style and assertions","I recently came across a video in youtube mentioning this [tiger coding style](https://github.com/tigerbeetle/tigerbeetle/blob/main/docs/TIGER_STYLE.md) and the assertions part is quite interesting.

> Assertions detect programmer errors. Unlike operating errors, which are expected and which must be handled, assertion failures are unexpected. The only correct way to handle corrupt code is to crash. Assertions downgrade catastrophic correctness bugs into liveness bugs. Assertions are a force multiplier for discovering bugs by fuzzing.

This style only reinforces that the practice that I already used to is relevant in other fields and I try to use that as much as I can BUT it seems to be only plausible to use for metadata and function parameters, and not the actual data we work with. I say that because if the dataset is large enough, then any assertion would take a lot of time and slow the actual program execution.

**Should I do a lot of assertions that reduce performance or should I ignore the need for error detection and not use any assertions during data processing?**

Do you do anything similar to this? How would you approach this performance / error detection trade-off? Is there any middle ground that could be found?",4,0.67,https://www.reddit.com/r/datascience/comments/1igpsrg/about_data_processing_data_science_tiger_style/,False,True,False
1igpi6w,rsesrsfh,1738590662.0,9,/r/datascience/comments/1igpi6w/tabpfn_v2_a_pretrained_transformer_outperforms/,datascience,TabPFN v2: A pretrained transformer outperforms existing SOTA for small tabular data and outperforms Chronos for time-series,"Have any of you tried TabPFN v2? It is a pretrained transformer which outperforms existing SOTA for small tabular data. You can read it in 🔗 [**Nature**](https://www.nature.com/articles/s41586-024-08328-6).

Some key highlights:

* It outperforms an ensemble of strong baselines tuned for 4 hours in 2.8 seconds for classification and 4.8 seconds for regression tasks, for datasets up to 10,000 samples and 500 features
* It is robust to uninformative features and can natively handle numerical and categorical features as well as missing values.
* Pretrained on 130 million synthetically generated datasets, it is a generative transformer model which allows for fine-tuning, data generation and density estimation.
* TabPFN v2 performs as well with half the data as the next best baseline (CatBoost) with all the data.
* TabPFN v2 can be used for forecasting by featurizing the timestamps. It ranks #1 on the popular time-series GIFT-Eval [benchmark](https://huggingface.co/spaces/Salesforce/GIFT-Eval) and outperforms Chronos.

TabPFN v2 is available under an [open license](https://github.com/PriorLabs/TabPFN): a derivative of the Apache 2 license with a single modification, adding an enhanced attribution requirement inspired by the Llama 3 license. You can also try it via [API](https://github.com/PriorLabs/tabpfn-client).",21,0.83,https://www.reddit.com/r/datascience/comments/1igpi6w/tabpfn_v2_a_pretrained_transformer_outperforms/,False,True,False
1igo4dh,metalvendetta,1738586328.0,57,/r/datascience/comments/1igo4dh/what_areas_does_synthetic_data_generation_has/,datascience,What areas does synthetic data generation has usecases?,"There are synthetic data generation libraries from tools such as Ragas, and I’ve heard some even use it for model training. What are the actual use case examples of using synthetic data generation?",82,0.91,https://www.reddit.com/r/datascience/comments/1igo4dh/what_areas_does_synthetic_data_generation_has/,False,True,False
1ighhad,AutoModerator,1738558904.0,39,/r/datascience/comments/1ighhad/weekly_entering_transitioning_thread_03_feb_2025/,datascience,"Weekly Entering & Transitioning - Thread 03 Feb, 2025 - 10 Feb, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",9,0.86,https://www.reddit.com/r/datascience/comments/1ighhad/weekly_entering_transitioning_thread_03_feb_2025/,False,True,False
1ig62ea,Emotional-Rhubarb725,1738526340.0,11,/r/datascience/comments/1ig62ea/any_one_here_built_a_recommender_system_before_i/,datascience,"any one here built a recommender system before , i need help understanding the architecture","I am building a RS based on a Neo4j database 

I struggle with the how the data should flow between the database, recommender system and the website 

I did some research and what i arrived on is that i should make the RS as an API to post the recommendations to the website 

but i really struggle to understand how the backend of the project work ",2,0.54,https://www.reddit.com/r/datascience/comments/1ig62ea/any_one_here_built_a_recommender_system_before_i/,False,True,False
1ifywzo,mehul_gupta1997,1738508158.0,14,/r/datascience/comments/1ifywzo/deepseekcom_is_down_constantly_alternatives_to/,datascience,deepseek.com is down constantly. Alternatives to use DeepSeek-R1 for free chatting,"Since the DeepSeek boom, DeepSeek.com is glitching constantly and I haven't been able to use it. So I found few platforms providing DeepSeek-R1 chatting for free like open router, nvidia nims, etc. Check out here : https://youtu.be/QxkIWbKfKgo",0,0.46,https://www.reddit.com/r/datascience/comments/1ifywzo/deepseekcom_is_down_constantly_alternatives_to/,False,True,False
1ifub7j,limedove,1738491384.0,38,/r/datascience/comments/1ifub7j/ai_tools_what_ai_tools_do_you_use_as_a_copilot/,datascience,[AI Tools] What AI Tools do you use as a copilot when working on your data science coding?,There are coding platforms like v0 and cursor that are very helpful for doing frontend/backend related coding work. What's the one you use for data science?,73,0.85,https://www.reddit.com/r/datascience/comments/1ifub7j/ai_tools_what_ai_tools_do_you_use_as_a_copilot/,False,True,False
1ifaenx,No_Information6299,1738428399.0,5,/r/datascience/comments/1ifaenx/use_llms_like_scikitlearn/,datascience,Use LLMs like scikit-learn,"Every time I wanted to use LLMs in my existing pipelines the integration was very bloated, complex, and too slow. This is why I created a lightweight library that works just like scikit-learn, the flow generally follows a pipeline-like structure where you “fit” (learn) a skill from sample data or an instruction set, then “predict” (apply the skill) to new data, returning structured results.

**High-Level Concept Flow**

`Your Data --> Load Skill / Learn Skill --> Create Tasks --> Run Tasks --> Structured Results --> Downstream Steps`

**Installation:**

`pip install flashlearn`

**Learning a New “Skill” from Sample Data**

Like a fit/predict pattern from scikit-learn, you can quickly “learn” a custom skill from minimal (or no!) data. Below, we’ll create a skill that evaluates the likelihood of buying a product from user comments on social media posts, returning a score (1–100) and a short reason. We’ll use a small dataset of comments and instruct the LLM to transform each comment according to our custom specification.

>from flashlearn.skills.learn\_skill import LearnSkill

>from flashlearn.client import OpenAI

>

>\# Instantiate your pipeline “estimator” or “transformer”, similar to a scikit-learn model

>learner = LearnSkill(model\_name=""gpt-4o-mini"", client=OpenAI())

>data = \[

>{""comment\_text"": ""I love this product, it's everything I wanted!""},

>{""comment\_text"": ""Not impressed... wouldn't consider buying this.""},

>\# ...

>\]

>

>\# Provide instructions and sample data for the new skill

>skill = learner.learn\_skill(

>data,

>task=(

>""Evaluate how likely the user is to buy my product based on the sentiment in their comment, ""

>""return an integer 1-100 on key 'likely\_to\_buy', ""

>""and a short explanation on key 'reason'.""

>),

>)

>

>\# Save skill to use in pipelines

>skill.save(""evaluate\_buy\_comments\_skill.json"")

**Input Is a List of Dictionaries**

Whether the data comes from an API, a spreadsheet, or user-submitted forms, you can simply wrap each record into a dictionary—much like feature dictionaries in typical ML workflows. Here’s an example:

>user\_inputs = \[

>{""comment\_text"": ""I love this product, it's everything I wanted!""},

>{""comment\_text"": ""Not impressed... wouldn't consider buying this.""},

>\# ...

>\]

**Run in 3 Lines of Code - Concurrency built-in up to 1000 calls/min**

Once you’ve defined or learned a skill (similar to creating a specialized transformer in a standard ML pipeline), you can load it and apply it to your data in just a few lines:

>\# Suppose we previously saved a learned skill to ""evaluate\_buy\_comments\_skill.json"".

>skill = GeneralSkill.load\_skill(""evaluate\_buy\_comments\_skill.json"")

>tasks = skill.create\_tasks(user\_inputs)

>results = skill.run\_tasks\_in\_parallel(tasks)

>print(results)

**Get Structured Results**

The library returns structured outputs for each of your records. The keys in the results dictionary map to the indexes of your original list. For example:

>{

>""0"": {

>""likely\_to\_buy"": 90,

>""reason"": ""Comment shows strong enthusiasm and positive sentiment.""

>},

>""1"": {

>""likely\_to\_buy"": 25,

>""reason"": ""Expressed disappointment and reluctance to purchase.""

>}

>}

**Pass on to the Next Steps**

Each record’s output can then be used in downstream tasks. For instance, you might:

1. Store the results in a database
2. Filter for high-likelihood leads
3. .....

Below is a small example showing how you might parse the dictionary and feed it into a separate function:

>\# Suppose 'flash\_results' is the dictionary with structured LLM outputs

>for idx, result in flash\_results.items():

>desired\_score = result\[""likely\_to\_buy""\]

>reason\_text = result\[""reason""\]

>\# Now do something with the score and reason, e.g., store in DB or pass to next step

>print(f""Comment #{idx} => Score: {desired\_score}, Reason: {reason\_text}"")

**Comparison**  
Flashlearn is a lightweight library for people who do not need high complexity flows of LangChain.

1. **FlashLearn -** Minimal library meant for well defined us cases that expect structured outputs
2. **LangChain -** For building complex thinking multi-step agents with memory and reasoning

If you like it, give us a star: [Github link](https://github.com/Pravko-Solutions/FlashLearn)",130,0.83,https://www.reddit.com/r/datascience/comments/1ifaenx/use_llms_like_scikitlearn/,False,True,False
1iew0k6,LebrawnJames416,1738376154.0,17,/r/datascience/comments/1iew0k6/for_the_causal_ds_do_you_follow_any_books_or/,datascience,"For the Causal DS, do you follow any books or frameworks for observational studies?","Asking as I am new to the space and wondering what are the best practises for:

1. Assessing balance 
2. Choosing confounders
3. Examples of a rigorous observational study done to learn from
4. any tools made currently to help speed up the process

  
Many thanks",30,0.9,https://www.reddit.com/r/datascience/comments/1iew0k6/for_the_causal_ds_do_you_follow_any_books_or/,False,True,False
1ievdxq,JobIsAss,1738374171.0,45,/r/datascience/comments/1ievdxq/got_a_raise_out_of_the_blue_despite_having_a_tech/,datascience,Got a raise out of the blue despite having a tech job offer.,"This is a follow up on [previous post](https://www.reddit.com/r/datascience/s/KnlQajJIqy).

Long story short got a raise from my current role before I even told them about the new job offer. To my knowledge our boss is very generous with raises. Typically around 7% but my case i went by 20%. Now my role pays more.

I communicated this to the recruiter and they were stressed but it is hard for me to make a choice now.  They said they cant afford me, as they see me as a high intermediate and their budget at the max is 120 and were offering 117. I told them that my comp is total now 125. I then explained why I am making so much more. My current employer genuinely believes that i drive a lot of impact. 

Edit: they do not know that i have a job offer yet.",250,0.97,https://www.reddit.com/r/datascience/comments/1ievdxq/got_a_raise_out_of_the_blue_despite_having_a_tech/,False,True,False
1iet1j8,Will_Tomos_Edwards,1738367342.0,31,/r/datascience/comments/1iet1j8/any_luck_through_job_apps_on_job_boards_or_is_all/,datascience,Any luck through job apps on job boards or is all success through recruiters and other methods?,The title is self-explanatory. How are people landing jobs in the data space right now?,36,0.86,https://www.reddit.com/r/datascience/comments/1iet1j8/any_luck_through_job_apps_on_job_boards_or_is_all/,False,True,False
1iepe0h,myfriendscode,1738357713.0,7,/r/datascience/comments/1iepe0h/guide_for_running_ab_test_on_a_product_with/,datascience,Guide for running A/B Test on a product with network effects?,"I'm working on a tool that is collaborative in nature and has real-time sync (think multiplayer mode in a video game). If anyone has any guidance on designing a statistical test for this kind of game, or if the juice is worth the squeeze, I'd really appreciate it!",16,0.94,https://www.reddit.com/r/datascience/comments/1iepe0h/guide_for_running_ab_test_on_a_product_with/,False,True,False
1ienwes,takenorinvalid,1738353922.0,7,/r/datascience/comments/1ienwes/is_there_a_better_changepoint_detection_model_on/,datascience,Is there a better changepoint detection model on Python than Ruptures?,"I'm rebuilding a model in Python that I previously built in R.

In R, I used the ""changepoint"" package to changepoint identification, which, in Python, I've been trying to replicate using the ""ruptures"" package -- but holy hell is there ever a difference.

R's package gave me exactly what I expected every time without configuration, but Ruptures is spotty at best.

Is anyone aware of a better changepoint detection package?",24,0.9,https://www.reddit.com/r/datascience/comments/1ienwes/is_there_a_better_changepoint_detection_model_on/,False,True,False
1ieitha,LebrawnJames416,1738341243.0,14,/r/datascience/comments/1ieitha/for_the_causal_ds_how_long_does_it_take_you_to/,datascience,"For the Causal DS, how long does it take you to complete a observational evaluation?","Hey everyone,

  
I'm wondering for those of you working on observational studies and using methods like psm,tmle, matching etc. 

How long does that project take you end to to end(getting the data to final evaluation result)? and have you found anyways to speed up your process?

Looking to see if theres any ways I could be speeding up the whole process, as they take forever normally(2-3 months)

",27,0.87,https://www.reddit.com/r/datascience/comments/1ieitha/for_the_causal_ds_how_long_does_it_take_you_to/,False,True,False
1iefenq,mehul_gupta1997,1738332022.0,17,/r/datascience/comments/1iefenq/deepseekr1_free_api_key/,datascience,DeepSeek-R1 Free API key,So DeepSeek-R1 has just landed on OpenRouter and you can now run the API key for free. Check how to get the API key and codes : https://youtu.be/jOSn-1HO5kY?si=i6n22dBWeAino0-5,99,0.88,https://www.reddit.com/r/datascience/comments/1iefenq/deepseekr1_free_api_key/,False,True,False
1iea72c,damjanv1,1738311507.0,6,/r/datascience/comments/1iea72c/these_are_the_instructions_i_created_for_my_genai/,datascience,These are the instructions i created for my Gen-AI assistant that I use for programming projects,"I'm a head of at a large-ish ecommerce company so do not code much these days but created said assistant to help me with programming tasks that has been massively helpful. just sharing nand wondering what anyone else would use. The do all charts in the style of the economist is massively helpful (though works better in r and not python which is what we primarily use at work but c'est la vie)

  
\- when I prompt you initially for a code related task, make sure that you first understand the business objectives of the work that we are doing. Ask me clarifying questions if you have to.



\- When you are not clear on a task ask clarifying questions, feel free to give me a list of queries that we can run to help you understand the task better



\- for any charting requests always do in the style of the economist or the Mckinsey / harvard business review (and following the principles of Edward Tufte outlined below)



\- try to give all responses integrated into the one code block that we were discussing



\- always run debugging code within larger code blocks (over 100 lines) and code to explicitly state where new files have been created. Debugging code should partition the larger query into small chunks and understand where any failures may be occurring



\- if I want to break away from the current train of thought , without starting a new chat I will preface my prompt with # please retain memory but be aware that we may be switching context



\- when we  create a data frame or source data to perform analysis on or create charts from , assign it a number, we will use that number when writing prompts but the table / data frame will remain the same in the code that we use ( we will just be assigning a number to allow for shorthand when communicating by prompt) i.e. sales\_table may just be 1 so therefore a prompt to extract total sales from 1 - should return the code select sum(sales) from sales\_table



\- when I use the word innovation or any of its derivatives feel free to suggest out of the box ideas or procedural improvements to the topic we are discussing



\- use python unless I specify otherwise, r would be the next most likely language to be used



\- when printing out charts also if you feel necessary print out summary statistics . keep the tabular format clean and tidy (do not use base r / python to achieve this)



\- for any charting abide by the principles of visualisation pioneer Edward Tufte which are comprehensively summarised here:



Graphical Excellence: Show complex ideas communicated with clarity, precision, and efficiency. Tufte argues that graphics should reveal data, avoid distorting what the data has to say, encourage the eye to compare different pieces of data, and make large datasets coherent.



Data-Ink Ratio: Maximize the ratio of data-ink to total ink used in a graphic. Tufte advocates for removing all non-essential elements (""chartjunk"") – decorative elements, heavy gridlines, unnecessary borders, and redundant information that don't contribute to understanding.



Data Density: Present as much data as possible in the smallest possible space while maintaining clarity. High-density graphics can be both elegant and precise.



Small Multiples: Use repeated small charts with the same scale and design to show changing data across multiple dimensions or time periods. This allows for easy comparison and pattern recognition. (this one is important use small multiples wherever possible)



Integration of Text and Graphics: Words, numbers, and graphics should be integrated rather than separated. Labels should be placed directly on the graphic rather than in legends when possible.



Truthful Proportions: The representation of numbers should be directly proportional to the numerical quantities represented. This means avoiding things like truncated axes that can mislead viewers.



Causality and Time Series: When showing cause and effect or temporal sequences, graphics should read from left to right and clearly show the relationship between variables.



Aesthetics and Beauty: While prioritizing function, Tufte argues that the best statistical graphics are also beautiful, combining complexity, detail, and clarity in an elegant way.

",96,0.9,https://www.reddit.com/r/datascience/comments/1iea72c/these_are_the_instructions_i_created_for_my_genai/,False,True,False
1iea500,damjanv1,1738311238.0,1,/r/datascience/comments/1iea500/any_data_analysts_scientists_out_there_help_me/,datascience,any data analysts / scientists out there  - help me create an assistant for my end users,,0,0.22,/r/ChatGPT/comments/1iea4do/any_data_analysts_scientists_out_there_help_me/,False,False,False
1ie7ari,MyRedditAccount1000,1738299560.0,10,/r/datascience/comments/1ie7ari/whats_the_most_absurd_data_fire_drillemergency/,datascience,What's the most absurd data fire drill/emergency you've had to work?,See prompt above.,23,0.85,https://www.reddit.com/r/datascience/comments/1ie7ari/whats_the_most_absurd_data_fire_drillemergency/,False,True,False
1idzfhq,anotheraccount97,1738276006.0,8,/r/datascience/comments/1idzfhq/aws_applied_scientist_ii_l5_offer_evaluation/,datascience,AWS Applied Scientist II (L5) offer evaluation,"Received an offer for an Applied Scientist II (L5) role at AWS Kumo (Bellevue) and wondering if it's on the lower side? 

## Offer Details:

**Base** : $165K

**Year 1 Sign-On**: $165K

**Year 2 Sign-On** : $125K

**RSUs**: 1,600 shares (5%, 15%, 20% every 6 months in years 3 & 4)

Estimated Year 1 TC: **\~$350K**

Does this seem competitive for an Applied Scientist II position? I was told the correct range from AS 2 is about 318k - 419k. Base can go up to 193K.

## Current :

C3 AI (just joined this week)

Senior Data Scientist, GenAI

**TC : 245K**

* 170k base, 250k RSUs over 5 years.

## My details:

YoE : 3 (\~0 full time in US.)

* 3 years as Senior Applied Scientist in mid-tier org, India.
* Co-founded a legit AI Startup in NYC.
* MS from top Ivy League (recent grad, top of class)


Does it seem like a lowball of an offer? ",0,0.38,https://www.reddit.com/r/datascience/comments/1idzfhq/aws_applied_scientist_ii_l5_offer_evaluation/,False,True,False
1idu5it,NoteClassic,1738262734.0,66,/r/datascience/comments/1idu5it/whats_your_firms_ai_strategy/,datascience,What’s your firms AI strategy?,"Hey DS community,

Mid level data scientist here. 

I’m currently involved in a project where I’m expected to work on delivering an appropriate AI strategy for my firm…. I’d like to benefit from the hive’s experience.

I’m interested looking at ideas and philosophies behind the AI strategy for the companies you work for. 

What products do you use? For your staff, clients?
Did you use in-house solutions or buy a product? 
How did you manage security and Data governance issues?
Were there open source solutions? Why did you/did you not go for them? 

I’d appreciate if you could also share resources that aided you in defining a strategy for your team/firm. 

Cheers. ",58,0.86,https://www.reddit.com/r/datascience/comments/1idu5it/whats_your_firms_ai_strategy/,False,True,False
1idtj2m,PhotographFormal8593,1738261159.0,25,/r/datascience/comments/1idtj2m/interview_format_different_from_what_recruiter/,datascience,Interview Format Different from What Recruiter Explained – Is This Common?,"I recently interviewed for a data scientist role, and the format of the interview turned out to be quite different from what the recruiter had initially described.

Specifically, I was told that the interview would focus on a live coding test for SQL and Python, but during the actual interview, it included a case study. While I was able to navigate the interview, the difference caught me off guard.

Has anyone else experienced a similar situation? How common is it for interview formats to deviate from what was communicated beforehand? Also, is it appropriate to follow up with the recruiter for clarification or feedback regarding this mismatch?

Would love to hear your thoughts and experiences!",73,0.91,https://www.reddit.com/r/datascience/comments/1idtj2m/interview_format_different_from_what_recruiter/,False,True,False
1idqgi8,venom_holic_,1738253501.0,17,/r/datascience/comments/1idqgi8/hirevue_data_science_internship_interviewadvice/,datascience,Hirevue data science internship interviewadvice,"Hey guys, this is literally my first time attending an professional interview in my entire life. I dont know how this roadmap works but i just got a email for hirevue as my first round and this is virtual interview which i was not expecting. Any inputs that you can give will potentially help me!! 

TIA

update : passed the hirevue and into my second round - technical assessment ",9,0.71,https://www.reddit.com/r/datascience/comments/1idqgi8/hirevue_data_science_internship_interviewadvice/,False,True,False
1iderhl,Illustrious-Pound266,1738212365.0,108,/r/datascience/comments/1iderhl/why_does_there_seem_to_be_so_many_more_data/,datascience,Why does there seem to be so many more data engineering jobs than data science or MLE jobs? I feel like I made a mistake in choosing data science and ML...,"I've been browsing jobs recently (since my current role doesn't pay well). I usually search for jobs in the data field in general rather than a particular title, since titles have so much variance. But one thing I've noticed is that there are way more data engineering roles than either data scientists or ML engineers on the job boards. When I say data engineering jobs, I mean the roles where you are building ETL pipelines, scalable/distributed data infrastructure and storage in the cloud, building data ingestion pipelines, DataOps, etc.

But why is this? I thought that given all the hype over AI these days, that there would be more LLM/ML jobs. And there's certainly a number of those, don't get me wrong, but I just feel like they pale in comparison to the amount of data engineering openings. Did I make a mistake in choosing data science and ML? Is data engineering in more demand and secure? If so, why? Should I fully transition to data engineering?",248,0.94,https://www.reddit.com/r/datascience/comments/1iderhl/why_does_there_seem_to_be_so_many_more_data/,False,True,False
1id8zee,SnooStories6404,1738196062.0,21,/r/datascience/comments/1id8zee/green_ai_which_programming_language_consumes_the/,datascience,Green AI: Which Programming Language Consumes the Most?,,0,0.48,https://doi.org/10.48550/arXiv.2501.14776,False,False,False
1id1gn1,LebrawnJames416,1738176995.0,135,/r/datascience/comments/1id1gn1/most_secure_data_science_jobs/,datascience,Most secure Data Science Jobs?,"Hey everyone,

  
I'm constantly hearing news of layoffs and was wondering what areas you think are more secure and how secure do you think your job is?

  
How worried are you all about layoffs? Are you always looking for jobs just in case?",181,0.89,https://www.reddit.com/r/datascience/comments/1id1gn1/most_secure_data_science_jobs/,False,True,False
1icvhgh,-Montse-,1738162277.0,16,/r/datascience/comments/1icvhgh/i_have_opensourced_several_of_my_data/,datascience,I have open-sourced several of my Data Visualization projects with Plotly,,145,0.96,https://figshare.com/authors/Montserrat_Mora/20430644,False,False,False
1ic1mnm,Grapphie,1738071160.0,27,/r/datascience/comments/1ic1mnm/created_an_app_for_practicing_for_your_interviews/,datascience,Created an app for practicing for your interviews with GPT,,95,0.79,https://i.redd.it/vxjuxi5plqfe1.gif,False,False,False
1iby5om,mehul_gupta1997,1738058049.0,70,/r/datascience/comments/1iby5om/nvidias_paid_generative_ai_courses_for_free/,datascience,NVIDIA's paid Generative AI courses for FREE (limited period),"NVIDIA has announced free access (for a limited time) to its premium courses, each typically valued between $30-$90, covering advanced topics in Generative AI and related areas.

The major courses made free for now are :

* **Retrieval-Augmented Generation (RAG) for Production:** Learn how to deploy scalable RAG pipelines for enterprise applications.
* **Techniques to Improve RAG Systems:** Optimize RAG systems for practical, real-world use cases.
* **CUDA Programming:** Gain expertise in parallel computing for AI and machine learning applications.
* **Understanding Transformers:** Deepen your understanding of the architecture behind large language models.
* **Diffusion Models:** Explore generative models powering image synthesis and other applications.
* **LLM Deployment:** Learn how to scale and deploy large language models for production effectively.

**Note:** There are redemption limits to these courses. A user can enroll into any one specific course.

**Platform Link**: [NVIDIA TRAININGS](https://nvda.ws/4jtHiOf)",884,0.98,https://www.reddit.com/r/datascience/comments/1iby5om/nvidias_paid_generative_ai_courses_for_free/,False,True,False
1ibm1t4,Guyserbun007,1738016755.0,19,/r/datascience/comments/1ibm1t4/is_there_a_way_to_terminate_a_running_ml/,datascience,Is there a way to terminate a running ML algorithm in python?,"I have a set of ML algorithms to be fit to the same data on a df. Some of them takes days to run while others usually take minutes. What I'd like to do is to set up a max model fitting timer, so once the fitting/training of an algorithm exceeds that, it will forgot that algo and move onto the next one. Is there way to terminate the model.fit() after it is initiated based on a prespecified time? Here are my code excerpts.

    ml_model_param_for_price_model_simple = {
                'Linear Regression': {
                    'model': LinearRegression(),
                    'params': {
                        'fit_intercept': [True, False],
                        'copy_X': [True, False],
                        'n_jobs': [None, -1]
                    }
                },
                'XGBoost Regressor': {
                    'model': XGBRegressor(objective='reg:squarederror', random_state=random_state),
                    'params': {
                        'n_estimators': [100, 200, 300],
                        'learning_rate': [0.01, 0.1, 0.2],
                        'max_depth': [3, 5, 7],
                        'subsample': [0.7, 0.8, 1.0],
                        'colsample_bytree': [0.7, 0.8, 1.0]
                    }
                },
                'Lasso Regression': {
                    'model': Lasso(random_state=random_state),
                    'params': {
                        'alpha': [0.01, 0.1, 1.0, 10.0],  # Lasso regularization strength
                        'fit_intercept': [True, False],
                        'max_iter': [1000, 2000]  # Maximum number of iterations
                    }
                },        }

The looping and fitting of data below:

    X = df[list_of_predictors]
    y = df['outcome_var']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=self.random_state)
    
    # Hyperparameter tuning and model training
    tuned_models = {}
    
    for model_name, current_param in self.param_grids.items():
        model = current_param['model']
        params = current_param['params']
    
        if params:  # Check if there are parameters to tune
            if model_name == 'XGBoost Regressor':
                model = RandomizedSearchCV(
                    model, params, n_iter=10, cv=5, scoring='r2', random_state=self.random_state
                )
            else:
                model = GridSearchCV(model, params, cv=5, scoring='r2')
    
            start_time = datetime.now()  # Start timing
            model.fit(X_train, y_train) # NOTE: I want this to break out when a timer is done!!
            end_time = datetime.now()  # End timing
    
            tuned_models[model_name] = model.best_estimator_  # Store the best fitted model
            logger.info(f""\n{model_name} best estimator: {model.best_estimator_}"")
            logger.info(f""{model_name} fitting time: {end_time - start_time}"")  # Print the fitting time
    
        else:
            start_time = datetime.now()  # Start timing
            model.fit(X_train, y_train)  # Fit model directly if no params to tune
            end_time = datetime.now()  # End timing
    
            tuned_models[model_name] = model  # Save the trained model
            logger.info(f""{model_name} fitting time: {end_time - start_time}"")  # Print the fitting time",13,0.78,https://www.reddit.com/r/datascience/comments/1ibm1t4/is_there_a_way_to_terminate_a_running_ml/,False,True,False
1ibkw2h,JobIsAss,1738013919.0,34,/r/datascience/comments/1ibkw2h/would_you_rather_be_comfortable_or_take_risks/,datascience,Would you rather be comfortable or take risks moving around?,"I recently received a job offer from a mid-to-large tech company in the gig economy space. The role comes with a competitive salary, offering a 15-20k increase over my current compensation. While the pay bump is nice, the job itself will be challenging as it focuses on logistics and pricing. However, I do have experience in pricing and have demonstrated my ability to handle optimization work. This role would also provide greater exposure to areas like causal inference, optimization, and real-time analytics, which are areas I’d like to grow in.

That said, I’m concerned about my career trajectory. I’ve moved around frequently in the past—for example, I spent 1.5 years at a big bank in my first role but left due to a toxic team. While I’m currently happy and comfortable in my role, I haven’t been here for a full year yet.

My current total compensation is $102k. While the work-life balance is great, my team is lacking in technical skills, and I’ve essentially been responsible for upskilling the entire practice. Another area of concern is that technically we are not able to keep up with bigger companies and the work is highly regulated so innovation isnt as easy.

Given the frequency move what would you do in my shoes? Take it and try to improve career opportunities for big tech?",25,0.82,https://www.reddit.com/r/datascience/comments/1ibkw2h/would_you_rather_be_comfortable_or_take_risks/,False,True,False
1ibedfy,Emotional-Rhubarb725,1737998200.0,79,/r/datascience/comments/1ibedfy/as_someone_who_aims_to_be_a_ml_engineer_how_much/,datascience,"as someone who aims to be a ML engineer, How much OOP and programming skills do i need ?","When to stop on the developer track ? 

how much do I need to master to help me being a good MLE 

",123,0.87,https://www.reddit.com/r/datascience/comments/1ibedfy/as_someone_who_aims_to_be_a_ml_engineer_how_much/,False,True,False
1ib8lg1,vastava_viz,1737983418.0,7,/r/datascience/comments/1ib8lg1/sample_size_calculator_with_live_data/,datascience,Sample size calculator with live data visualization as parameters change,"[Demo of live updating chart on samplesizecalc.com](https://i.redd.it/44ow6kyqcjfe1.gif)

It's been a while since I've worked on my sample size calculator tool ([last post here](https://www.reddit.com/r/datascience/comments/1e9f41u/easiest_way_to_calculate_required_sample_size_for/)). But I had a lot of fun adding an interactive chart to visualize required sample size, and thought you all would appreciate it! Made with d3.js

Check it out here: [https://www.samplesizecalc.com/calculator?metricType=proportion](https://www.samplesizecalc.com/calculator?metricType=proportion)

What I love about this is that it helps me understand the relationship between each of the variables, statistical power and sample size. Hope it's a nice explainer for you all too.

I also have plans to add a line chart to show how the statistical power increases over time (ie. the longer the experiment runs, the more samples you collect and the greater the power!)

As always, let me know if you run into any bugs.",27,0.92,https://www.reddit.com/r/datascience/comments/1ib8lg1/sample_size_calculator_with_live_data/,False,True,False
1ib0dfb,AutoModerator,1737954098.0,27,/r/datascience/comments/1ib0dfb/weekly_entering_transitioning_thread_27_jan_2025/,datascience,"Weekly Entering & Transitioning - Thread 27 Jan, 2025 - 03 Feb, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",7,1.0,https://www.reddit.com/r/datascience/comments/1ib0dfb/weekly_entering_transitioning_thread_27_jan_2025/,False,True,False
1iawkau,productanalyst9,1737942221.0,15,/r/datascience/comments/1iawkau/free_product_analytics_product_data_scientist/,datascience,Free Product Analytics / Product Data Scientist Case Interview (with answers!),"If you are interviewing for Product Analyst, Product Data Scientist, or Data Scientist Analytics roles at tech companies, you are probably aware that you will most likely be asked an analytics case interview question. It can be difficult to find real examples of these types of questions. I wrote an example of this type of question and included sample answers. Please note that you don’t have to get everything in the sample answers to pass the interview. If you would like to learn more about passing the Product Analytics Interviews, check out my [blog post here](https://futureproductanalyst.substack.com/p/how-to-pass-the-product-analytics). If you want to learn more about passing the A/B test interview, check out [this blog post](https://futureproductanalyst.substack.com/p/how-to-passing-the-ab-test-interview).

If you struggled with this case interview, I highly recommend these two books: [Trustworthy Online Controlled Experiments](https://amzn.to/3EbBybQ) and [Ace the Data Science Interview](https://amzn.to/4jLGbd5) (these are affiliate links, but I bought and used these books myself and vouch for their quality).

Without further ado, here is the sample case interview. If you found this helpful, please subscribe to [my blog](https://substack.com/@futureproductanalyst/p-148705045) because I plan to create more samples interview questions.

\_\_\_

**Prompt:** Customers who subscribe to Amazon Prime get free access to certain shows and movies. They can also buy or rent shows, as not all content is available for free to Prime customers. Additionally, they can pay to subscribe to channels such as Showtime, Starz or Paramount+, all accessible through their Amazon Prime account.

In case you are not familiar with Amazon Prime Video, the homepage typically has one large feature such as “Watch the Seahawks vs. the 49ers tomorrow!”. If you scroll past that, there are many rows of video content such as “Movies we think you’ll like”, “Trending Now”, and “Top Picks for You”. Assume that each row is either all free content, or all paid content. [Here is an example screenshot](https://imgur.com/a/PoWux79).

# Question 1: What are the benefits to Amazon of focusing on optimizing what is shown to each user on the Prime Video home page?

Potential answers:

>!(looking for pros/cons, candidate should list at least 3 good answers)!<

>!Showing the right content to the right customer on the Prime Video homepage has lots of potential benefits. It is important for Amazon to decide how to prioritize because the right prioritization could:!<

* >!**Drive engagement:** Highlighting free content ensures customers derive value from their Prime subscription.!<
* >!**Increase revenue:** Promoting paid content or paid channels can drive additional purchases or subscriptions.!<
* >!**Customer satisfaction:** Ensuring users find relevant and engaging content quickly leads to a better browsing experience.!<
* >!**Content discovery:** Showcasing a mix of content encourages customers to explore beyond free offerings.!<
* >!**But keep in mind potential challenges:** Overemphasis on paid content may alienate customers who want free content. They could think “I’m paying for Prime to get access to free content, why is Amazon pushing all this paid content”!<

# Question 2: What key considerations should Amazon take into account when deciding how to prioritize content types on the Prime Video homepage?

Potential answers:

>!(Again the candidate should list at least 3 good answers)!<

* >!**Free vs. paid balance:** Ensure users see value in their Prime subscription while exposing them to paid options. This is a delicate balance - Amazon wants to upsell customers on paid content without increasing Prime subscription churn. Keep in mind that paid content is usually newer and more in demand (e.g. new releases)!<
* >!**User engagement:** Consider the user’s watch history and preferences (e.g., genres, actors, shows vs. movies).!<
* >!**Revenue impact:** Assess how prominently displaying paid content or channels influences rental, purchase, and subscription revenue.!<
* >!**Content availability:** Prioritize content that is currently trending, newly released, or exclusive to Amazon Prime Video.!<
* >!**Geo and licensing restrictions:** Adapt recommendations based on the content available in the user’s region.!<

# Question 3: Let’s say you hypothesize that prioritizing free Prime content will increase user engagement. How would you measure whether this hypothesis is true?

Potential answer:

>!I would design an experiment where the treatment is that free Prime content is prioritized on row one of the homepage. The control group will see whatever the existing strategy is for row one (it would be fair for the candidate to ask what the existing strategy is. If asked, respond that the current strategy is to equally prioritize free and paid content in row one).!<

>!To measure whether prioritizing free Prime content in row one would increase user engagement, I would use the following metrics:!<

* >!**Primary metric:** Average hours watched per user per week.!<
* >!**Secondary metrics:** Click-through rate (CTR) on row one.!<
* >!**Guardrail metric:** Revenue from paid content and channels!<

# Question 4: How would you design an A/B test to evaluate which prioritization strategy is most effective? Be detailed about the experiment design.

Potential answer:

>!1. Clearly State the Hypothesis:!<

>!Prioritizing free Prime content on the homepage will increase engagement (e.g., hours watched) compared to equal prioritization of paid content and free content because free content is perceived as an immediate value of the Prime subscription, reducing friction of watching and encouraging users to explore and watch content without additional costs or decisions.!<

>!2. Success Metrics:!<

* >!Primary Metric: Average hours watched per user per week.!<
* >!Secondary Metric: Click-through rate (CTR) on row one.!<

>!3. Guardrail Metrics:!<

* >!Revenue from paid content and channels, per user: Ensure prioritizing free content does not drastically reduce purchases or subscriptions.!<
   * >!Numerator: Total revenue generated from each experiment group from paid rentals, purchases, and channel subscriptions during the experiment.!<
   * >!Denominator: Total number of users in the experiment group.!<
* >!Bounce rate: Ensure the experiment does not unintentionally make the homepage less engaging overall.!<
   * >!Numerator: Number of users who log in to Prime Video but leave without clicking on or interacting with any content.!<
   * >!Denominator: Total number of users who log in to Prime Video, per experiment group!<
* >!Churn rate: Monitor for any long-term negative impact on overall customer retention.!<
   * >!Numerator: Number of Prime members who cancel their subscription during the experiment!<
   * >!Denominator: Total number of Prime members in the experiment.!<

>!4. Tracking Metrics:!<

* >!CTR on free, paid, and channel-specific recommendations. This will help us evaluate how well users respond to different types of content being highlighted.!<
   * >!Numerator: Number of clicks on free/paid/channel content cards on the homepage.!<
   * >!Denominator: Total number of impressions of free/paid/channel content cards on the homepage.!<
* >!Adoption rate of paid channels (percentage of users subscribing to a promoted channel).!<

>!5. Randomization:!<

* >!Randomization Unit: Users (Prime subscribers).!<
* >!Why this will work: User-level randomization ensures independent exposure to different homepage designs without contamination from other users.!<
* >!Point of Incorporation to the experiment: Users are assigned to treatment (free content prioritized) or control (equal prioritization of free and paid content) upon logging in to Prime Video, or landing on the Prime Video homepage if they are already logged in.!<
* >!Randomization Strategy: Assign users to treatment or control groups in a 50/50 split.!<

>!6. Statistical Test to Analyze Metrics:!<

* >!For continuous metrics (e.g., hours watched): t-test!<
* >!For proportions (e.g., CTR): Z-test of proportions!<
* >!Also, using regression is an appropriate answer, as long as they state what the dependent and independent variables are.!<
* >!Bonus points if candidate mentions CUPED for variance reduction, but not necessary!<

>!7. Power Analysis:!<

* >!Candidate should mention conducting a power analysis to estimate the required sample size and experiment duration. Don’t have to go too deep into this, but candidate should at least mention these key components of power analysis:!<
   * >!Alpha (e.g. 0.05), power (e.g. 0.8), MDE (minimum detectable effect) and how they would decide the MDE (e.g. prior experiments, discuss with stakeholders), and variance in the metrics!<
   * >!Do not have to discuss the formulas for calculating sample size!<

# Question 5: Suppose the new prioritization strategy won the experiment, and is fully launched. Leadership wants a dashboard to monitor its performance. What metrics would you include in this dashboard?

Potential answers:

* >!**Engagement metrics:**!<
   * >!Average hours watched per user per week.!<
   * >!CTR on homepage recommendations (broken down by free, paid, and channel content).!<
   * >!CTR on by row!<
* >!**Revenue metrics:**!<
   * >!Revenue from paid content rentals and purchases.!<
   * >!Subscriptions to paid channels.!<
* >!**Retention metrics:**!<
   * >!Weekly active users (WAU).!<
   * >!Monthly active users (MAU).!<
   * >!Churn rate of Prime subscribers.!<
* >!**Operational metrics:**!<
   * >!Latency or errors in the recommendation algorithm.!<
   * >!User satisfaction scores (e.g., via feedback or surveys).!<",195,0.95,https://www.reddit.com/r/datascience/comments/1iawkau/free_product_analytics_product_data_scientist/,False,True,False
1iahg6i,furioncruz,1737905585.0,11,/r/datascience/comments/1iahg6i/warantly_period_and_coverage_after_resignation/,datascience,Warantly period and coverage after resignation,"I am leaving my current job. I have built tooling to automate ML processes, document everything, and transfer knowledge. Nevertheless, these systems are not battle-hardened yet, and those I am transferring to are either DevOps who know little ML or DS who have poor SWE skills. I suppose they would need my help later down the road. I already offered that I would be available for quick chats if they needed me. 

I was wondering what the norm is in handling these scenarios. Do people usually offer free consultation as a warranty, and for how long?",9,0.67,https://www.reddit.com/r/datascience/comments/1iahg6i/warantly_period_and_coverage_after_resignation/,False,True,False
1iadgz9,mehul_gupta1997,1737895019.0,2,/r/datascience/comments/1iadgz9/why_ai_agents_will_be_a_disaster/,datascience,Why AI Agents will be a disaster,,0,0.4,/r/ArtificialInteligence/comments/1iadfd2/why_ai_agents_will_be_a_disaster/,False,False,False
1ia175l,Omega037,1737850714.0,347,/r/datascience/comments/1ia175l/official_2024_end_of_year_salary_sharing_thread/,datascience,[Official] 2024 End of Year Salary Sharing thread,"This is the official thread for sharing your current salaries (or recent offers).

See [last year's Salary Sharing thread here](https://www.reddit.com/r/datascience/comments/18tevwk/official_2023_end_of_year_salary_sharing_thread/). There was also [an unofficial one from an hour ago here](https://www.reddit.com/r/datascience/comments/1i9zcgm/unofficial_2024_salary_thread/).

Please only post salaries/offers if you're including hard numbers, but feel free to use a throwaway account if you're concerned about anonymity. You can also generalize some of your answers (e.g. ""Large biotech company""), or add fields if you feel something is particularly relevant.

**Title:**

* **Tenure length:**
* **Location:**
   * **$Remote:**
* **Salary:**
* **Company/Industry:**
* **Education:**
* **Prior Experience:**
   * **$Internship**
   * **$Coop**
* **Relocation/Signing Bonus:**
* **Stock and/or recurring bonuses:**
* **Total comp:**

Note that while the primary purpose of these threads is obviously to share compensation info, discussion is also encouraged.",412,0.96,https://www.reddit.com/r/datascience/comments/1ia175l/official_2024_end_of_year_salary_sharing_thread/,False,True,False
1i9shbm,Proof_Wrap_2150,1737827205.0,59,/r/datascience/comments/1i9shbm/seeking_advice_on_organizing_a_sprawling_jupyter/,datascience,Seeking advice on organizing a sprawling Jupyter Notebook in VS Code,"
I’ve been using a single Jupyter Notebook for quite some time, and it’s evolved into a massive file that contains everything from data loading to final analysis. My typical process starts with importing data, cleaning it up, and saving the results for reuse in pickle files. When I revisit the notebook, I load these intermediate files and build on them with transformations, followed by exploratory analysis, visualizations, and insights.

While this workflow gets the job done, it’s becoming increasingly chaotic. Some parts are clearly meant to be reusable steps, while others are just me testing ideas or exploring possibilities. It all lives in one place, which is convenient in some ways but a headache in others. I often wonder if there’s a better way to organize this while keeping the flexibility that makes Jupyter such a great tool for exploration.

If this were your project, how would you structure it? ",117,0.93,https://www.reddit.com/r/datascience/comments/1i9shbm/seeking_advice_on_organizing_a_sprawling_jupyter/,False,True,False
1i9n64r,DataPastor,1737812495.0,32,/r/datascience/comments/1i9n64r/do_you_implement_own_high_performance_python/,datascience,Do you implement own high performance Python algorithms and in which language?,"I want to implement some numerical algorithms as a Python library in a low level (compiled) language like C/Cython/Zig; C++/nanobind/pybind11; Rust/PyO3 – and want to listen to some experiences from this field. If you have some hands-on experience, which language and library have you used and what is your recommendation? I also have some experience with R/C++/Rcpp, but also want to learn to do this in Python.",51,0.89,https://www.reddit.com/r/datascience/comments/1i9n64r/do_you_implement_own_high_performance_python/,False,True,False
1i9gcxu,mehul_gupta1997,1737784103.0,1,/r/datascience/comments/1i9gcxu/what_gpu_config_to_choose_for_ai_usecases/,datascience,What GPU config to choose for AI usecases?,,0,0.27,/r/ArtificialInteligence/comments/1i9gc92/what_gpu_config_to_choose_for_ai_usecases/,False,False,False
1i9ar5b,one_more_throwaway12,1737765610.0,26,/r/datascience/comments/1i9ar5b/what_to_expect_from_this_technical_test/,datascience,What to expect from this Technical Test?,"I applied for a SQL data analytics role and have a technical test with the following components

* Multiple choice SQL questions (up to 10 mins)
* Multiple choice general data science questions (15 mins)
* SQL questions where you will write the code (20 mins)

I can code well so Im not really worried about the coding part but do not know what to expect of the multiple choice ones as ive never had this experience before. I do not know much of the like infrastructure of sql of theory so dont know how to prepare, especially for the general data science questions which I have no idea what that could be. Any advice? ",52,0.92,https://www.reddit.com/r/datascience/comments/1i9ar5b/what_to_expect_from_this_technical_test/,False,True,False
1i98tom,Emuthusiast,1737760171.0,10,/r/datascience/comments/1i98tom/data_imbalance_monitoring_metrics/,datascience,Data Imbalance Monitoring Metrics?,"Hello all,

I am consulting a business problem from a colleague with a dataset that has 0.3% of the class of interest.  The dataset  70k+ has observations, and we were debating on what  thresholds were selected for metrics robust to data imbalance , like PRAUC, Brier, and maybe MCC.

Do you have any thoughts from your domains on how to deal with data imbalance problems and what performance metrics and thresholds to monitor them with ? As a an FYI, sampling was ruled out due to leading to models in need of strong calibration. Thank you all in advance.",6,0.72,https://www.reddit.com/r/datascience/comments/1i98tom/data_imbalance_monitoring_metrics/,False,True,False
1i90imp,AdFew4357,1737738942.0,4,/r/datascience/comments/1i90imp/dml_researchers_want_to_help_me_out_here/,datascience,DML researchers want to help me out here?,"

Hey guys, I’m a MS statistician by background who has been doing my masters thesis in DML for about 6 months now. 

One of the things that I have a question about is, does the functional form of the propensity and outcome model really not matter that much? 

My advisor isn’t trained in this either, but we have just been exploring by fitting different models to the propensity and outcome model. 

What we have noticed is no matter you use xgboost, lasso, or random forests, the ATE estimate is damn close to the truth most of the time, and any bias is like not that much.

So I hate to say that my work thus far feels anti-climactic, but it feels kinda weird to done all this work to then just realize, ah well it seems the type of ML model doesn’t really impact the results.

In statistics I have been trained to just think about the functional form of the model and how it impacts predictive accuracy. 

But what I’m finding is in the case of causality, none of that even matters.


I guess I’m kinda wondering if I’m on the right track here 


Edit: DML = double machine learning ",0,0.33,https://www.reddit.com/r/datascience/comments/1i90imp/dml_researchers_want_to_help_me_out_here/,False,True,False
1i8t59p,phicreative1997,1737717457.0,17,/r/datascience/comments/1i8t59p/building_a_reliable_texttosql_pipeline_a/,datascience,Building a Reliable Text-to-SQL Pipeline: A Step-by-Step Guide pt.1,,32,0.81,https://www.firebird-technologies.com/p/building-a-reliable-text-to-sql-pipeline,False,False,False
1i8nauq,M0shka,1737692941.0,30,/r/datascience/comments/1i8nauq/i_made_a_guide_to_help_people_understand_docker/,datascience,I made a guide to help people understand Docker,"When I first started out using Docker it was really confusing. I made a guide to help people understand what Docker is used for. Please let me know what you think and if you have any feedback 

https://youtu.be/QtH-RqFcDFc?si=PtQe7z7kZ2vlF_3Q",378,0.98,https://www.reddit.com/r/datascience/comments/1i8nauq/i_made_a_guide_to_help_people_understand_docker/,False,True,False
1i8hb9n,Franzese,1737674877.0,84,/r/datascience/comments/1i8hb9n/where_is_the_standard_mldl_are_we_all_shifting_to/,datascience,Where is the standard ML/DL? Are we all shifting to prompting ChatGPT?,"I am working at a consulting company and while so far all the focus has been on cool projects involving setting up ML\DL models, lately all the focus has been shifted on GenAI. As a data scientist/maching learning engineer who tackled difficult problems of data and modles, for the past 3 months I have been editing the same prompt file, saying things differently to make ChatGPT understand me. Is this the new reality? or should I change my environment? Please tell me there are standard ML projects.",239,0.93,https://www.reddit.com/r/datascience/comments/1i8hb9n/where_is_the_standard_mldl_are_we_all_shifting_to/,False,True,False
1i89r5d,Tamalelulu,1737655783.0,123,/r/datascience/comments/1i89r5d/the_most_in_demand_ds_skills_via_901_adzuna/,datascience,The most in demand DS skills via 901 Adzuna listings,,700,0.96,https://i.redd.it/o2fq74qlasee1.png,False,False,False
1i894sd,bweber,1737654262.0,0,/r/datascience/comments/1i894sd/deep_learning_in_adtech_a_handson_example_with/,datascience,"Deep Learning in AdTech, a hands-on example with Kaggle",,0,0.5,https://bgweber.medium.com/deep-learning-for-click-prediction-in-mobile-adtech-9739fe3f52de,False,False,False
1i86l83,chomoloc0,1737647935.0,1,/r/datascience/comments/1i86l83/call_for_input_regression_discontinuity_design/,datascience,"Call for input: Regression discontinuity design, and interrupted time series",,3,0.8,/r/CausalInference/comments/1i801e0/call_for_input_regression_discontinuity_design/,False,False,False
1i7eaj9,Tamalelulu,1737561336.0,2,/r/datascience/comments/1i7eaj9/scrapy_mro_error_without_any_references_to/,datascience,Scrapy MRO error without any references to conflicting packages,"Hi all,

I'm working on a little personal project, quantifying what technologies are most asked for in Data Science JDs. Really I'm more using it to work on my Python chops. I'm hitting a slightly perplexing error and I think ChatGPT has taken me as far as it possibly can on this one. 

When I attempt to crawl my spider I get this error:   
TypeError: Cannot create a consistent method resolution order (MRO) for bases Injectable, Generic

Previously the code was attempting to import Injectable from scrap\_poet until I eventually inspected the package and saw that Injectable doesn't exist. So I attempted to avoid using that entirely and omitted all references to Injectable in my code. Yet I'm still getting this error. Any thoughts?

Here's what the spider looks like: 

    import scrapy
    import csv
    from scrapy_autoextract import request_raw
    
    class JobSpider(scrapy.Spider):
        name = ""job_spider""
        custom_settings = {
            ""DOWNLOADER_MIDDLEWARES"": {
                ""scrapy_autoextract.AutoExtractMiddleware"": 543,
            },
        }
    
        # Read URLs from links.csv and start requests
        def start_requests(self):
            with open(""/adzuna_links.csv"", ""r"") as file:
                reader = csv.reader(file)
                for row in reader:
                    url = row[0] 
                    yield request_raw(url=url, page_type=""jobposting"", callback=self.parse)
    
        def parse(self, response):
            try:
                # Extract job details directly from the response JSON data returned by AutoExtract
                job_data = response.json().get(""job_posting"", {})
    
                if job_data:
                    yield {
                        ""title"": job_data.get(""title""),
                        ""description"": job_data.get(""description""),
                        ""company"": job_data.get(""hiringOrganization"", {}).get(""name""),
                        ""location"": job_data.get(""jobLocation"", {}).get(""address""),
                        ""datePosted"": job_data.get(""datePosted""),
                    }
                else:
                    self.logger.error(f""No job data extracted from {response.url}"")
    
            except Exception as e:
                self.logger.error(f""Error parsing job data from {response.url}: {e}"")",1,0.6,https://www.reddit.com/r/datascience/comments/1i7eaj9/scrapy_mro_error_without_any_references_to/,False,True,False
1i7bnpd,meevis_kahuna,1737554248.0,45,/r/datascience/comments/1i7bnpd/meta_career_advice_vs_data_science/,datascience,Meta: Career Advice vs Data Science,"I joined the thread to learn about Data Science.  Something like 75 percent of the posts are peoples resumes and requests for career advice. I thought these were supposed to go into a weekly thread or something - I'm getting a warning about the weekly thread even as I'm posting this comment. 

Can anyone suggest alternative subs with more educational content?",153,0.91,https://www.reddit.com/r/datascience/comments/1i7bnpd/meta_career_advice_vs_data_science/,False,True,False
1i7b8id,GiovannaDio,1737552944.0,87,/r/datascience/comments/1i7b8id/graduated_september_2024_and_i_am_now_looking_for/,datascience,"Graduated september 2024 and i am now looking for an entry level data engineering position , what do you think about my cv ?",,224,0.86,https://i.redd.it/lyi4ifsysjee1.jpeg,False,False,False
1i7a8e0,NoteClassic,1737549714.0,18,/r/datascience/comments/1i7a8e0/ds_interested_in_lower_level_languages/,datascience,DS interested in Lower level languages,"Hi community,

I’m primarily DS with quite a number of years in DS and DE. I’ve mostly worked with on-site infrastructure. 

My stack is currently Python, Julia, R… and my field of interest is numerical computing, OpenMP, MPI and GPU parallel computing (down the line)

I’m curious as to how best to align my current work with high level languages with my interest in lower level languages. 

If I were deciding based on work alone, Fortran will be the best language for me to learn as there’s a lot of legacy code we’d have to port in the next years. 

However, I’d like to develop in a language that’ll complement the skill set of a DS.

My current view is Julia, C and Fortran. However, I’m not completely sure of how useful these are outside of my very-specific field. 

Are there any other DS that have gone through this? How did you decide? What would you recommend? What factors did you consider.",13,0.78,https://www.reddit.com/r/datascience/comments/1i7a8e0/ds_interested_in_lower_level_languages/,False,True,False
1i6qa6u,Proof_Wrap_2150,1737486130.0,5,/r/datascience/comments/1i6qa6u/analyzing_changes_to_gravel_height_along_a_road/,datascience,Analyzing changes to gravel height along a road,"I’m working with a dataset that measures the height of gravel along a 50 km stretch of road at 10-meter intervals. I have two measurements:

Baseline height: The original height of the gravel.

New height: A more recent measurement showing how the gravel has decreased over time.

This gives me the difference in height at various points along the road. I’d like to model this data to understand and predict gravel depletion. 

Here’s what I’m considering:Identifying trends or patterns in gravel loss (e.g., areas with more significant depletion).

Using interpolation to estimate gravel heights at points where measurements are missing.

Exploring possible environmental factors that could influence depletion (e.g., road curvature, slope, or proximity to towns).

However, I’m not entirely sure how to approach this analysis. Some questions I have:

What are the best methods to visualize and analyze this type of spatial data?

Are there statistical or machine learning models particularly suited for this?

If I want to predict future gravel heights based on the current trend, what techniques should I look into? Any advice, suggestions, or resources would be greatly appreciated!",5,0.73,https://www.reddit.com/r/datascience/comments/1i6qa6u/analyzing_changes_to_gravel_height_along_a_road/,False,True,False
1i6pu2t,Guyserbun007,1737485051.0,7,/r/datascience/comments/1i6pu2t/how_to_get_individual_restaurant_review_data/,datascience,How to get individual restaurant review data?,,0,0.25,/r/webscraping/comments/1i6pthr/how_to_get_individual_restaurant_review_data/,False,False,False
1i658fp,VolunteerEdge56,1737417755.0,14,/r/datascience/comments/1i658fp/what_should_i_do_to_build_a_strong_foundation_in/,datascience,What should I do to build a strong foundation in developing?,"I’m interested in becoming a developer. I’m currently proficient in Tableau, Alteryx, Power BI etc. 

I feel like there’s 1 million different avenues. I’m not sure which route to take. 

I want to get around a community, where I can connect and get exposed to more. I’m in the Miami area. 

I’ve checked out YouTube videos on Java script

What do you all recommend? ",10,0.6,https://www.reddit.com/r/datascience/comments/1i658fp/what_should_i_do_to_build_a_strong_foundation_in/,False,True,False
1i60m31,Proof_Wrap_2150,1737406032.0,16,/r/datascience/comments/1i60m31/question_about_using_geographic_data_for_soil/,datascience,Question about Using Geographic Data for Soil Analysis and Erosion Studies,"I’m working on a project involving a dataset of latitude and longitude points, and I’m curious about how these can be used to index or connect to meaningful data for soil analysis and erosion studies. Are there specific datasets, tools, or techniques that can help link these geographic coordinates to soil quality, erosion risk, or other environmental factors?

I’m interested in learning about how farmers or agricultural researchers typically approach soil analysis and erosion management. Are there common practices, technologies, or methodologies they rely on that could provide insights into working with geographic data like this?

If anyone has experience in this field or recommendations on where to start, I’d appreciate your advice!",13,0.81,https://www.reddit.com/r/datascience/comments/1i60m31/question_about_using_geographic_data_for_soil/,False,True,False
1i5inrb,AutoModerator,1737349304.0,47,/r/datascience/comments/1i5inrb/weekly_entering_transitioning_thread_20_jan_2025/,datascience,"Weekly Entering & Transitioning - Thread 20 Jan, 2025 - 27 Jan, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",13,1.0,https://www.reddit.com/r/datascience/comments/1i5inrb/weekly_entering_transitioning_thread_20_jan_2025/,False,True,False
1i5d77u,AdFew4357,1737331692.0,140,/r/datascience/comments/1i5d77u/anyone_ever_feel_like_working_as_a_data_scientist/,datascience,Anyone ever feel like working as a data scientist at hinge?,"Need to figure out what that damn algorithm is doing to keep me from getting matches lol. On a serious note I have read about some interesting algorithmic work at dating app companies. Any data scientists here ever worked for a dating app company? 

Edit: gale-shapely algorithm

https://reservations.substack.com/p/hinge-review-how-does-it-work#:~:text=It%20turns%20out%20that%20the,among%20those%20who%20prefer%20them.",448,0.9,https://www.reddit.com/r/datascience/comments/1i5d77u/anyone_ever_feel_like_working_as_a_data_scientist/,False,True,False
1i57vx1,usernamehere93,1737317875.0,7,/r/datascience/comments/1i57vx1/where_to_start_when_data_is_limited_a_guide/,datascience,Where to Start when Data is Limited: A Guide,"
Hey, I’ve put together an article on my thoughts and some research around how to get the most out of small datasets when performance requirements mean conventional analysis isn’t enough. 

It’s aimed at helping people get started with new projects who have already started with the more traditional statistical methods.

Would love to hear some feedback and thoughts.",73,0.9,https://towardsdatascience.com/effective-ml-with-limited-data-where-to-start-194492e7a6f8,False,False,False
1i5576y,FellowZellow,1737311263.0,86,/r/datascience/comments/1i5576y/should_i_try_to_postpone_my_faang_interview/,datascience,Should I Try to postpone my FAANG Interview?,"So I got contacted by a FAANG Recruiter for a Data Scientist Role I applied for a month and a half ago. But as I have started to prep, I realize I am not ready and need 1 to 2 months before I would be able to do well on all the technical interviews (there are 4 of them). My SQL is rusty because I have been using Pyspark so much that I didn't really need to do medium to hard SQL queries at work (We're also not allowed in most cases since SQL is slower). So I would just do everything in Pyspark. But now, as I start practicing my SQL I realize it's very basic, and it's going to take some time before I can get it on the level my pyspark is at.

I've noticed that I feel like there is no chance of me performing well enough on this interview, and it sucks because the recruiter said that the hiring manager was looking at my resume and really wants to interview me as soon as possible since he thinks I have strong experience for the role (They made me bypass the phone screens because of it). I have no doubt I would be able to do the role, but interviews are another beast. According to the prep guide, my Stats, ML Theory, SQL, and Python all have to be perfect. Since I joined my current company as an intern, I didn't have to do as many in-depth technicals as I have to do here. I've interviewed at a couple other big companies last year and didn't make it to the final round for one simply because I needed more time to prepare. The FAANG recruiter wants me to do the first 2 interviews within the next two weeks, and I'm worried about what it would do to my confidence if I failed this interview since this is pretty much my dream Data Scientist role. My mind is already telling me just to make the best of this and use it as a learning experience, but another part of me is wondering if I should just cancel it altogether or try to delay it as much as possible. I have a mock interview with a Company Data Scientist they set up for me in a few days, but part of me feels defeated already and it sucks...

I honestly am not sure what to do as I need a lot more time. I've heard others say it took them as long as 2-6 months before they were ready to crush their FAANG interview and I know I am not there yet...",214,0.88,https://www.reddit.com/r/datascience/comments/1i5576y/should_i_try_to_postpone_my_faang_interview/,False,True,False
1i4yyoe,nkafr,1737294977.0,32,/r/datascience/comments/1i4yyoe/influential_timeseries_forecasting_papers_of/,datascience,Influential Time-Series Forecasting Papers of 2023-2024: Part 1,"This article explores some of the latest advancements in time-series forecasting.

You can find the article [here](https://aihorizonforecast.substack.com/p/influential-time-series-forecasting).

Edit: If you know of any other interesting papers, please share them in the comments.",191,0.95,https://www.reddit.com/r/datascience/comments/1i4yyoe/influential_timeseries_forecasting_papers_of/,False,True,False
1i4f1go,takuonline,1737227935.0,42,/r/datascience/comments/1i4f1go/ai_is_difficult_to_get_right_apple_intelligence/,datascience,AI is difficult to get right: Apple Intelligence rolled back(Mostly the summary feature),"Source: https://edition.cnn.com/2025/01/16/media/apple-ai-news-fake-headlines/index.html#:\~:text=Apple%20is%20temporarily%20pulling%20its,organization%20and%20press%20freedom%20groups.

Seems like even Apple is struggling to deploy AI and deliver real-world value.  
Yes, companies can make mistakes, but Apple rarely does, and even so, it seems like most of Apple Intelligence is not very popular with IOS users and has led to the creation of r/AppleIntelligenceFail.

It's difficult to get right in contrast to application development which was the era before the ai boom. ",312,0.99,https://www.reddit.com/r/datascience/comments/1i4f1go/ai_is_difficult_to_get_right_apple_intelligence/,False,True,False
1i40izz,Tamalelulu,1737177777.0,36,/r/datascience/comments/1i40izz/do_these_recruiters_sound_like_a_scam/,datascience,Do these recruiters sound like a scam?,"Hi all, unsure of where else to ask this so asking here. 

I had a recruiter (heavy Indian accent) call/email me with an interesting proposition. They work for the candidate rather than the company. If they place you in a job within 45 days they ask for 9% of your first year's salary.

They claim their value add is in a couple of things. First they promise that they have advanced ATS software that will help tweak professional qualifications. Second, they say they will apply to approximately 50 JDs per day (I am skeptical this many relevant jobs are even being posted).

I have never had luck with Indian recruiters before but I have had good experiences professionally in offshoring some repetitive tasks for cheap. This process sounds like it fits the bill. The part where it gets sketchy is they want either access to my LinkedIn/Gmail or they want me to create second LinkedIn/Gmail accounts that they would have control over. Access to my gmail is a nonstarter obviously. But creating spoof LinkedIn/Gmails feels a little sketchy. 

If we're living in a universe where these guys are simply trying to provide the service they've described, I'm all in. I just don't want to get soft-rolled into some sort of scam.",15,0.8,https://www.reddit.com/r/datascience/comments/1i40izz/do_these_recruiters_sound_like_a_scam/,False,True,False
1i3zajz,mehul_gupta1997,1737173285.0,2,/r/datascience/comments/1i3zajz/huggingface_smolagents_code_centric_agent/,datascience,Huggingface smolagents : Code centric Agent framework. Is it the best AI Agent framework? I don't think so,,2,0.67,/r/ArtificialInteligence/comments/1i3z9ni/huggingface_smolagents_code_centric_agent/,False,False,False
1i3x2cf,jmhimara,1737165883.0,39,/r/datascience/comments/1i3x2cf/are_there_any_ways_to_earn_a_little_extra_money/,datascience,Are there any ways to earn a little extra money on the side as a data scientist?,"Using data science skills (otherwise I'm sure there are plenty). 

I know there is data annotation, but I'm not sure that qualifies as data science.",101,0.91,https://www.reddit.com/r/datascience/comments/1i3x2cf/are_there_any_ways_to_earn_a_little_extra_money/,False,True,False
1i3clrk,mehul_gupta1997,1737105866.0,1,/r/datascience/comments/1i3clrk/microsoft_mattergen_genai_model_for_material/,datascience,Microsoft MatterGen: GenAI model for Material design and discovery ,,2,0.67,/r/ArtificialInteligence/comments/1i3clk7/microsoft_mattergen_genai_model_for_material/,False,False,False
1i3cgo0,1_plate_parcel,1737105217.0,53,/r/datascience/comments/1i3cgo0/guys_is_web_crawling_and_scraping_1_for_data/,datascience,guys is web crawling and scraping +1 for data science or it doesn't matter. ,"by web crawling and scraping i mean advanced scraping with multiple websites for prices and products then building further things around it like strategic planning and buisness analytics. 

edit: is it a necessary skill or not. +1 it means its a great add on to ur skill stack",39,0.69,https://www.reddit.com/r/datascience/comments/1i3cgo0/guys_is_web_crawling_and_scraping_1_for_data/,False,True,False
1i3bwdj,meis_xry,1737102536.0,2,/r/datascience/comments/1i3bwdj/can_someone_help_me_understand_what_is_the_issue/,datascience,Can someone help me understand what is the issue exactly?,,0,0.25,/r/aws/comments/1i3bvho/can_someone_help_me_understand_what_is_the_issue/,False,False,False
1i3a45a,Relative_Practice_93,1737094521.0,48,/r/datascience/comments/1i3a45a/how_long_did_it_take_you_to_get_a_new_role_when/,datascience,How long did it take you to get a new role when looking for a new job?,"I'm feeling very miserable at my job as well as feeling uneasy with the ethics of my company so I desperately am looking for a new role, but this job market is concerning. I have a BS in Math and MS in DS, been at my job as a data scientist for 1.5 years, worked for 3 years between BS and MS in analyst roles. Is there hope to have something new soon? How many apps per day should I be sending? ",50,0.9,https://www.reddit.com/r/datascience/comments/1i3a45a/how_long_did_it_take_you_to_get_a_new_role_when/,False,True,False
1i3a227,mehul_gupta1997,1737094288.0,0,/r/datascience/comments/1i3a227/google_titans_new_llm_architecture_with_better/,datascience,Google Titans : New LLM architecture with better long term memory,,7,0.71,/r/ArtificialInteligence/comments/1i3a1ub/google_titans_new_llm_architecture_with_better/,False,False,False
1i34tao,Think_Huckleberry299,1737076708.0,5,/r/datascience/comments/1i34tao/looking_for_arts_sales_data_to_understand_arts/,datascience,looking for arts sales data to understand arts pricing dynamics or madness,"I would like to explore datasets of arts sale and auctions, please if anyone has a good source please post below in the link. Just curious to explore if there are any patterns in art prices or just maddness which data science can't understand why a banana and tape would sell for 6 million or perhaps I can learn more about arts from this dataset. 

thanks in advance



Thanks ",1,0.57,https://www.reddit.com/r/datascience/comments/1i34tao/looking_for_arts_sales_data_to_understand_arts/,False,True,False
1i33mt0,UnsafeBaton1041,1737073226.0,83,/r/datascience/comments/1i33mt0/ive_been_given_the_choice_between_being_a_data/,datascience,I've been given the choice between being a Data Scientist or an Analytics Manager. Which would you choose and why?,"I'm coming from a Data Analyst position, and I've essentially been given the choice between being a Data Scientist and or an Analytics Manager. I thought Data Scientist was my dream job, but the Manager position would pay more, and I've been dreaming about working my way up to Director or CDO... Does Analytics Manager make the most sense in this case?

Update for context: I'm 25, have a master's in data analytics, and have been working in the same industry for 7 years but in different roles. I've been an Analyst for 1.5+ years, and previously was a Data Manager, and a Researcher.",200,0.91,https://www.reddit.com/r/datascience/comments/1i33mt0/ive_been_given_the_choice_between_being_a_data/,False,True,False
1i2vmuv,turingincarnate,1737052050.0,11,/r/datascience/comments/1i2vmuv/introducing_mlsynth/,datascience,Introducing mlsynth.,"Hi DS Reddit. For those of who you work in causal inference, you may be interested in a Python library I developed called ""machine learning synthetic control"", or ""mlsynth"" for short.

As I write in its [documentation](https://mlsynth.readthedocs.io), mlsynth is a one-stop shop of sorts for implementing some of the most recent synthetic control based estimators, many of which use machine learning methodologies. Currently, the software is hosted from my GitHub, and it is still undergoing developments (i.e., for computing inference for point-estinates/user friendliness).

mlsynth implements the following methods: [Augmented Difference-in-Differences](https://doi.org/10.1287/mksc.2022.1406), CLUSTERSCM, [Debiased Convex Regression](https://doi.org/10.1287/inte.2023.0028)  (undocumented at present), the [Factor Model Approach](https://doi.org/10.1177/00222437221137533), [Forward Difference-in-Differences](https://doi.org/10.1287/mksc.2022.1406), [Forward Selected Panel Data Approach](https://doi.org/10.1016/j.jeconom.2021.04.009), the [L1PDA](https://doi.org/10.1002/jae.1230), the [L2-relaxation PDA](https://doi.org/10.13140/RG.2.2.11670.97609), [Principal Component Regression](https://doi.org/10.1080/01621459.2021.1928513), [Robust PCA Synthetic Control](https://academicworks.cuny.edu/gc_etds/4984), [Synthetic Control Method (Vanilla SCM)](https://doi.org/10.1198/jasa.2009.ap08746), [Two Step Synthetic Control](https://doi.org/10.1287/mnsc.2023.4878)  and finally the two newest methods which are not yet fully documented, [Proximal Inference-SCM](https://arxiv.org/abs/2108.13935) and [Proximal Inference with Surrogates-SCM](https://arxiv.org/abs/2308.09527)  

While each method has their own options (e.g., Bayesian or not, l2 relaxer versus L1), all methods have a common syntax which allows us to switch seamlessly between methods without needing to switch softwares or learn a new syntax for a different library/command. It also brings forth methods which either had no public documentation yet, or were written mostly for/in MATLAB.

The documentation that currently exists explains installation as well as the basic methodology of each method. I also provide worked examples from the academic literature to serve as a reference point for how one may use the code to estimate causal effects.

So, to anybody who uses Python and causal methods on a regular basis, this is an option that may suit your needs better than standard techniques.",22,0.89,https://www.reddit.com/r/datascience/comments/1i2vmuv/introducing_mlsynth/,False,True,False
1i2vj0x,Ryan_3555,1737051786.0,29,/r/datascience/comments/1i2vj0x/free_learning_paths_for_data_analysts_data/,datascience,"Free Learning Paths for Data Analysts, Data Scientists, and Data Engineers – Using 100% Open Resources ","Hey, I’m Ryan, and I’ve created 

https://www.datasciencehive.com/learning-paths 

a platform offering free, structured learning paths for data enthusiasts and professionals alike.

The current paths cover:

	•	Data Analyst: Learn essential skills like SQL, data visualization, and predictive modeling.
	•	Data Scientist: Master Python, machine learning, and real-world model deployment.
	•	Data Engineer: Dive into cloud platforms, big data frameworks, and pipeline design.

The learning paths use 100% free open resources and don’t require sign-up. Each path includes practical skills and a capstone project to showcase your learning.

I see this as a work in progress and want to grow it based on community feedback. Suggestions for content, resources, or structure would be incredibly helpful.

I’ve also launched a Discord community (https://discord.gg/Z3wVwMtGrw) with over 150 members where you can:

	•	Collaborate on data projects
	•	Share ideas and resources
	•	Join future live hangouts for project work or Q&A sessions

If you’re interested, check out the site or join the Discord to help shape this platform into something truly valuable for the data community.

Let’s build something great together.

Website: https://www.datasciencehive.com/learning-paths
Discord: https://discord.gg/Z3wVwMtGrw ",281,0.95,https://i.redd.it/ustzy4gseede1.jpeg,False,False,False
1i2qj4j,geotheory,1737038919.0,18,/r/datascience/comments/1i2qj4j/books_on_machine_learning_in_r/,datascience,Books on Machine Learning + in R,"I'm interested in everyone's experience of books based specifically in R on machine learning, deep learning, and more recently LLM modelling, etc.  If you have particular experience to share it would really useful to hear about it.

As a sub-question it would be great to hear about books intended for relative beginners, by which I mean those familiar with R and statistical analysis but with no formal training in AI. There is obviously the well-known *""Introduction to Machine Learning with R""* by Scott V Burger, available as a [free pdf](https://edisciplinas.usp.br/pluginfile.php/8527271/mod_resource/content/0/Burger%2C%20Scott%20V%20-%20Introduction%20to%20machine%20learning%20with%20R_%20rigorous%20mathematical%20analysis-OReilly%20%282018%29.pdf).  But it hasn't been updated in nearly 7 years now, and a quick [scan of Google](https://www.google.co.uk/search?tbm=shop&hl=en-GB&psb=1&ved=2ahUKEwi6jeS9vPqKAxXdQ0ECHRvSDwAQu-kFegQIABAK&q=Machine+Learning+in+R&oq=Machine+Learning+in+R&gs_lp=Egtwcm9kdWN0cy1jYyIVTWFjaGluZSBMZWFybmluZyBpbiBSSABQAFgAcAB4AJABAJgBAKABAKoBALgBA8gBAJgCAKACAJgDAJIHAKAHAA&sclient=products-cc) shows quite a number of others.  Suggestions much appreciated.",31,0.9,https://www.reddit.com/r/datascience/comments/1i2qj4j/books_on_machine_learning_in_r/,False,True,False
1i2mh17,Tarneks,1737025307.0,14,/r/datascience/comments/1i2mh17/solution_completeness_and_take_home_assignments/,datascience,Solution completeness and take home assignments for interviews?,"What is the general consensus about take home interviews and then completeness of solution.

I have around a week and it took me already 2 days just to work with with the data just so I can
1) clean it
2) enhance it with external data
3) feature engineer it
4) establish baselines to capture lift

The whole thing is supposed to be finished around the span of a week. As i was scoping it out the whole thing is essentially potentially 3-4 models in a framework given the complex nature of the work.

How critical is the completeness and assumptions being made regarding these take home assignments. I didnt get a take home that large in scope. Its difficult task but very doable just laborious in the sense that it requires to be well thought out. ",5,1.0,https://www.reddit.com/r/datascience/comments/1i2mh17/solution_completeness_and_take_home_assignments/,False,True,False
1i2m3mv,jameslee2295,1737023704.0,21,/r/datascience/comments/1i2m3mv/what_challenges_do_businesses_face_when/,datascience,What Challenges Do Businesses Face When Developing AI Solutions?,"Hello everyone,

I’m currently working on providing cloud services and looking to better understand the challenges businesses face when developing AI. As a cloud provider, I’m keen to learn about the real-world obstacles organizations encounter when scaling their AI solutions.

For those in the AI industry, what specific issues or limitations have you faced in terms of infrastructure, platform flexibility, or integration challenges? Are there any key challenges in AI development that remain unresolved? What specific support or solutions do AI developers need from cloud providers to overcome current limitations?

Looking forward to hearing your thoughts and learning from your experiences. Thanks in advance!",0,0.44,https://www.reddit.com/r/datascience/comments/1i2m3mv/what_challenges_do_businesses_face_when/,False,True,False
1i2jytl,tropianhs,1737013947.0,78,/r/datascience/comments/1i2jytl/start_freelancing_with_0_experience/,datascience,Start freelancing with 0 experience?,"I hear many people have the ambition to start freelancing as soon as they can, ideally before having significant job experience. 
I like the attitude, but I tried myself a few years ago and got burned. So I wanna share my experience. 
   
I am a Data Scientist and tried to start freelancing with just one year job experience in 2017. Did the usual stuff. Set up an Upwork profile, applied to jobs at nights and during weekends and waited for a reply. 
Crickets. I **applied to 11 jobs** and didn't get any. Looking back at that experience I see a few mistakes
1 I didn't have a portfolio of projects that matched the jobs I applied to. 
2 I only used Upwork, without leveraging LInkedIn, Catalant, Fiverr and others. 
3 I gave up too early. Just 11 applications over one month is not enough. I recommend applying to 20-30 jobs per week if possible.
4 I set an unreasonable hourly rate. I set my hourly rate same as my daily job, Freelancing is a market where you are the product. When there is no demand for you (because nobody knows you) it's a smart move to set the price low. Once demand picks up, increase the price accordingly. 

Overall, I think experience is not the number one factor that a client looks for when hiring a freelancer. It's way more important to give the client confidence that you can do the job. So you should always work with that goal in mind, from the way you build your profile, to all the communication with your client. 
Last bit of advice. I found success in my local market at first. In Italy there is not many Data professionals that are also freelancers, and that helped me. People like to work with familiar faces and speaking the same language, sharing the same culture, goes a long way building confidence.

Curious to know your point of view too. ",49,0.77,https://www.reddit.com/r/datascience/comments/1i2jytl/start_freelancing_with_0_experience/,False,True,False
1i28x7i,imberttt,1736978111.0,23,/r/datascience/comments/1i28x7i/what_do_you_think_about_building_the_pipeline/,datascience,What do you think about building the pipeline first with bad models to start refining quickly?,"we have to build a computer vision application, I detect 4 main problems, 



get the highest quality training set, it is requiring lots of code and it may require lots of manual work to generate the ground truth

train a classification model, two main orthogonal approaches are being considered and will be tested

train a segmentation model

connect the dots and build the end to end pipeline

  
one teammate is working in the highest quality training set, and three other teammates in the classification models. I think it would be incredibly beneficial to have the pipeline as soon as possible integrated with the extremely simple models, and then iterate taking into account error metrics, as it gives us goals and this lets them test their module/section of the work also taking into account variation of the final metrics.

  
this would also help the other teams that depend on our output, web development can use a model, it is just a bad model, but we'll improve the results, the deployment work could also start now.

  
what do you guys think about this approach? for me it looks like its all benefits and zero problems but I see some teammates are reluctant on building something that definitely fails at the beginning and I'm not definitely the most experienced data scientist.",39,0.93,https://www.reddit.com/r/datascience/comments/1i28x7i/what_do_you_think_about_building_the_pipeline/,False,True,False
1i275yh,mmmmmmyles,1736973521.0,0,/r/datascience/comments/1i275yh/wasmpowered_codespaces_for_python_notebooks_on/,datascience,WASM-powered codespaces for Python notebooks on GitHub,"During a hackweek, we built this project that allows you to run [marimo](https://github.com/marimo-team/marimo) and Jupyter notebooks directly from GitHub in a Wasm-powered, codespace-like environment. What makes this powerful is that we mount the GitHub repository's contents as a filesystem in the notebook, making it really easy to share notebooks with data.

**All you need to do is prepend** [`https://marimo.app`](https://marimo.app) **to any Python notebook on GitHub.** Some examples:

* Jupyter Notebook: [https://marimo.app/github.com/jakevdp/PythonDataScienceHandb...](https://marimo.app/github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.08-Sorting.ipynb)
* marimo notebook: [https://marimo.app/github.com/marimo-team/marimo/blob/07e8d1...](https://marimo.app/github.com/marimo-team/marimo/blob/07e8d14109f7312f19916fd13e4046a561a740f8/examples/third_party/polars/polars_example.py)

Jupyter notebooks are automatically converted into marimo notebooks using basic static analysis and source code transformations. Our conversion logic assumes the notebook was meant to be run top-down, which is usually but not always true \[2\]. It can convert many notebooks, but there are still some edge cases.

We implemented the filesystem mount using our own FUSE-like adapter that links the GitHub repository’s contents to the Python filesystem, leveraging Emscripten’s filesystem API. The file tree is loaded on startup to avoid waterfall requests when reading many directories deep, but loading the file contents is lazy. For example, when you write Python that looks like

    with open(""./data/cars.csv"") as f:
        print(f.read())
    
    # or
    
    import pandas as pd
    pd.read_csv(""./data/cars.csv"")

behind the scenes, you make a request \[3\] to *https://raw.githubusercontent.com/<org>/<repo>/main/data/cars.csv*

Docs: [https://docs.marimo.io/guides/publishing/playground/#open-notebooks-hosted-on-github](https://docs.marimo.io/guides/publishing/playground/#open-notebooks-hosted-on-github)

\[2\] [https://blog.jetbrains.com/datalore/2020/12/17/we-downloaded-10-000-000-jupyter-notebooks-from-github-this-is-what-we-learned/](https://blog.jetbrains.com/datalore/2020/12/17/we-downloaded-10-000-000-jupyter-notebooks-from-github-this-is-what-we-learned/)

\[3\] We technically proxy it through the playground [https://marimo.app](https://marimo.app) to fix CORS issues and GitHub rate-limiting.

**Why is this useful?**

Vieiwng notebooks on GitHub pages is limiting. They don't allow external css or scripts so charts and advanced widgets can fail. They also aren't itneractive so you can't tweek a value or pan/zoom a chart. It is also difficult to share your notebook with code - you either need to host it somehwere or embed it inside your notebook. Just append `https://marimo.app/<github_url>`",11,1.0,https://www.reddit.com/r/datascience/comments/1i275yh/wasmpowered_codespaces_for_python_notebooks_on/,False,True,False
1i20otn,AdFew4357,1736957009.0,43,/r/datascience/comments/1i20otn/aspirations_of_starting_a_data_science_consultancy/,datascience,aspirations of starting a data science consultancy ,"Has anyone ever here thought of how to use their skills to start their own consultancy or some kind of business? Lately ive been kinda feeling that it would be really nice to have something of my own to work one involving analytics. Working for a company is great experience, but part of me would really like to have a business that I own where I help small businesses who have data make sense of it with low hanging fruit solutions.

Just a thought, but I’ve always thought of some sort of consultancy where clients are some sort of local business that collects data but doesn’t use it effectively or does not have the expertise on how to turn their data into insights that can be used. 

For example, suppose you had three clients:

1. Local gyms which have lots of membership data - my consultancy could offer services to measure engagement, etc and use demographic information to further understand gym goers - don’t know what “action” they could take but a thought 

2. Local shop has expenses they track and right now it’s all over the place. A dashboard that can help them view everything in one place

Something where, it’s tasks which are trivial for the average data scientist, but generate a lot of value for local businesses.

But maybe you can go deeper? I’m not sure how genAI works and haven’t played around with like any of these tools, but I’ve thought of ways these can be incorporated too.

Idk, I just find working in the industry sole draining and I just want to be able to have something that I can call my own, work on my own schedule, and it lead to a lot more revenue than working for a company. 

If anyone has any thoughts on what they have done, or how they have tried to do something, please let me know. Ideally I’d try and start this after 3-4 years of experience where I’ve built some niche industry experience. ",36,0.83,https://www.reddit.com/r/datascience/comments/1i20otn/aspirations_of_starting_a_data_science_consultancy/,False,True,False
1i1z6pj,pg860,1736953054.0,37,/r/datascience/comments/1i1z6pj/who_is_the_most_hungry_for_ai_ml_talent_right_now/,datascience,Who is the most hungry for AI / ML talent right now,"I run a job search engine for Data Scientists. This week we added monitoring of the highest paid job openings in the last week. This is what I saw. It seems one company in particular wants to outbid everyone else. And this is not because of lack of competition - we monitor more than 30.000 companies including all of Fortune 100 and most of Fortune 1000. We index more than 60k data science jobs every month. 

Source: [jobs-in-data.com](https://jobs-in-data.com)

https://preview.redd.it/sqxgf9u786de1.png?width=2438&format=png&auto=webp&s=476af7f8ec1456a3d3f0e27f2fea61d4519daa9c

",128,0.86,https://www.reddit.com/r/datascience/comments/1i1z6pj/who_is_the_most_hungry_for_ai_ml_talent_right_now/,False,True,False
1i1wnxj,Illustrious-Mind9435,1736945403.0,20,/r/datascience/comments/1i1wnxj/leaving_public_sector_for_private/,datascience,Leaving Public Sector for Private,"Posting for a friend:

Currently in a an ostensibly manager level DS position in local government. They are in the final stages of interviewing for a Director level role at a private firm. Is the compensation change worth it (posted below) and are there any DS specific aspects they should consider? 

Right now they are an IC who occasionally manages, but it seems this new role might be 80-90% managing. Is that common for the private sector? I told them it doesn't seem worth it (I'm biased as I am also in the public sector), but they said the compensation combined with more interesting work might be worth it.

Public Sector:
Manager
135k
Pension (secure but only okay payout)
Student Loan Forgiveness

Private Sector:
Director
165k
10-15% Bonus
401k 4% Match

",20,0.83,https://www.reddit.com/r/datascience/comments/1i1wnxj/leaving_public_sector_for_private/,False,True,False
1i1bjhi,Stochastic_berserker,1736876152.0,63,/r/datascience/comments/1i1bjhi/evalues_a_modern_alternative_to_pvalues/,datascience,E-values: A modern alternative to p-values,"In many modern applications - A/B testing, clinical trials, quality monitoring - we need to analyze data as it arrives. Traditional statistical tools weren't designed with this sequential analysis in mind, which has led to the development of new approaches.

E-values are one such tool, specifically designed for sequential testing. They provide a natural way to measure evidence that accumulates over time. An e-value of 20 represents 20-to-1 evidence against your null hypothesis - a direct and intuitive interpretation. They're particularly useful when you need to:

- Monitor results in real-time
- Add more samples to ongoing experiments
- Combine evidence from multiple analyses
- Make decisions based on continuous data streams

While p-values remain valuable for fixed-sample scenarios, e-values offer complementary strengths for sequential analysis. They're increasingly used in tech companies for A/B testing and in clinical trials for interim analyses.

If you work with sequential data or continuous monitoring, e-values might be a useful addition to your statistical toolkit. Happy to discuss specific applications or mathematical details in the comments.​​​​​​​​​​​​​​​​

P.S: Above was summarized by an LLM.

Paper: Hypothesis testing with e-values - https://arxiv.org/pdf/2410.23614

Current code libraries:

Python:

- expectation: New library implementing e-values, sequential testing and confidence sequences (https://github.com/jakorostami/expectation)

- confseq: Core library by Howard et al for confidence sequences and uniform bounds (https://github.com/gostevehoward/confseq)


R: 

- confseq: The original R implementation, same authors as above

- safestats: Core library by one of the researchers in this field of Statistics, Alexander Ly. (https://cran.r-project.org/web/packages/safestats/readme/README.html)

",105,0.78,https://www.reddit.com/r/datascience/comments/1i1bjhi/evalues_a_modern_alternative_to_pvalues/,False,True,False
1i1951j,Due-Duty961,1736870071.0,2,/r/datascience/comments/1i1951j/exit_cmdexe_from_r_or_python_without_admin/,datascience,exit cmd.exe from R (or python) without admin privilege,"I run:

system(""TASKKILL /F /IM cmd.exe"")

I get

Erreur�: le processus ""cmd.exe"" de PID 10333 n'a pas pu être arrêté.

Raison�: Accès denied.

Erreur�: le processus ""cmd.exe"" de PID 11444 n'a pas pu être arrêté.

Raison�: Accès denied.


I execute a batch file> a cmd open>a shiny open (I do my calculations)> a button on shiny should allow the cmd closing (and the shiny of course)

I can close the cmd from command line but I get access denied when I try to execute it from R. Is there hope? I am on the pc company so I don't have admin privilege",0,0.36,https://www.reddit.com/r/datascience/comments/1i1951j/exit_cmdexe_from_r_or_python_without_admin/,False,True,False
1i18xcv,OxheadGreg123,1736869506.0,4,/r/datascience/comments/1i18xcv/dash_python_incosistence_performance/,datascience,Dash Python Incosistence Performance,"I'm currently working on a project using Dash Python. It was light and breezy in the beginning. I changed a few codes while maintaining the error at 0, test-running it once in a while just to check if the code change affected the website, and nothing bad happened. But after I left it for a few hours without changing anything, the website wouldn't run anymore and showed me an ""Internal Server Error"". This happened way too many times, and it stresses me out, as I have to update most of the backend ASAP. Does anyone has any similar experience and manage to solve it? I'd like to know how.",7,0.82,https://www.reddit.com/r/datascience/comments/1i18xcv/dash_python_incosistence_performance/,False,True,False
1i13e03,jameslee2295,1736851459.0,5,/r/datascience/comments/1i13e03/seeking_advice_on_amazon_bedrock_and_azure/,datascience,Seeking Advice on Amazon Bedrock and Azure,"Hello everyone. I’m currently exploring AI infrastructure and platform for a new project and I’m trying to decide between Amazon Bedrock and Azure (AI Infrastructure & AI Studio). I’ve been considering both but would love to hear about your real-world experiences with them.

Has anyone used Amazon Bedrock or Azure AI Infrastructure and Azure AI Studio? How would you compare the two in terms of ease of use, performance, and overall flexibility? Are there specific features from either platform that stood out to you, or particular use cases where one was clearly better than the other?

Any advice or insights would be greatly appreciated. Thanks in advance! ",9,0.76,https://www.reddit.com/r/datascience/comments/1i13e03/seeking_advice_on_amazon_bedrock_and_azure/,False,True,False
1i0x2pm,SnooLobsters8778,1736825814.0,328,/r/datascience/comments/1i0x2pm/fuck_pandas_rant/,datascience,Fuck pandas!!! [Rant],"I have been a heavy R user for 9 years and absolutely love R. I can write love letters about the R data.table package. It is fast. It is efficient. it is beautiful. A coder’s dream.
 
But of course all good things must come to an end and given the steady decline of R users decided to switch to python to keep myself relevant.

And let me tell you I have never seen a stinking hot pile of mess than pandas. Everything is 10 layers of stupid? The syntax makes me scream!!!!!! There is no coherence or pattern ? Oh use [] here but no use ({}) here.
Want to do a if else ooops better download numpy. 
Want to filter ooops use loc and then iloc and write 10 lines of code.

It is unfortunate there is no getting rid of this unintuitive maddening, mess of a library, given that every interviewer out there expects it!!! There are much better libraries and it is time the pandas reign ends!!!!! (Python data table even creates pandas data frame faster than pandas!)

Thank you for coming to my Ted talk
I leave you with this datatable comparison article while I sob about learning pandas 

",494,0.72,https://www.kaggle.com/code/sudalairajkumar/getting-started-with-python-datatable,False,False,False
1i0wxxt,mehul_gupta1997,1736825403.0,6,/r/datascience/comments/1i0wxxt/mistral_released_codestral_2501_free_to_use_with/,datascience,Mistral released Codestral 25.01 : Free to use with VS Code and Jet brains,,0,0.4,/r/OpenAI/comments/1i0wwxm/mistral_released_codestral_2501_ranks_1_on_lmsys/,False,False,False
1i0m1ts,empirical-sadboy,1736795972.0,1,/r/datascience/comments/1i0m1ts/advice_on_stabilizing_an_autoencoders/,datascience,Advice on stabilizing an autoencoder's representation?,,3,0.71,/r/learnmachinelearning/comments/1haqmu6/advice_on_stabilizing_an_autoencoders/,False,False,False
1i0dbaj,chomoloc0,1736772966.0,17,/r/datascience/comments/1i0dbaj/mastering_the_poisson_distribution_intuition_and/,datascience,Mastering The Poisson Distribution: Intuition and Foundations,,149,0.94,https://medium.com/@alejandroalvarezprez/mastering-the-poisson-distribution-intuition-and-foundations-d96bae3de61d,False,False,False
1i0czn6,mehul_gupta1997,1736771828.0,0,/r/datascience/comments/1i0czn6/skyt132b_opensourced_reasoning_model_outperforms/,datascience,Sky-T1-32B: Open-sourced reasoning model outperforms OpenAI-o1 on coding and maths benchmarks ,,1,0.54,/r/ArtificialInteligence/comments/1i0cyyw/skyt132b_opensourced_reasoning_model_outperforms/,False,False,False
1i0c3x8,tinkinc,1736768520.0,7,/r/datascience/comments/1i0c3x8/humana_senior_ds_position_merrygoround/,datascience,Humana Senior DS Position merry-go-round,Anyone in the US apply to the Humana revolving Senior DS position over the last 5 months? They continuously post this position and never seem to fill it. Wondering if anyone has gotten an actual interview. I make it to the prescreen rounds  every single time I apply and then it just gets reposted.  ,26,0.86,https://www.reddit.com/r/datascience/comments/1i0c3x8/humana_senior_ds_position_merrygoround/,False,True,False
1i0bhi3,jameslee2295,1736765919.0,1,/r/datascience/comments/1i0bhi3/seeking_advice_on_gpu_comparison_greennode_vs_fpt/,datascience,Seeking Advice on GPU Comparison: GreenNode vs FPT,"I’m currently exploring GPU options for my projects and I’m curious if anyone here has experience using GPUs from GreenNode or FPT. I’m looking for real feedback on how they compare in terms of performance, pricing, and overall experience.

Has anyone used GPUs from either of these providers? How do they stack up against each other in terms of power efficiency, speed, and reliability? Are there any specific use cases where one outperforms the other?

I’d love to hear your thoughts, personal experiences, or any suggestions you might have on which GPU might be better for intensive workloads. Thanks in advance!",0,0.5,https://www.reddit.com/r/datascience/comments/1i0bhi3/seeking_advice_on_gpu_comparison_greennode_vs_fpt/,False,True,False
1i06k3y,AutoModerator,1736744505.0,43,/r/datascience/comments/1i06k3y/weekly_entering_transitioning_thread_13_jan_2025/,datascience,"Weekly Entering & Transitioning - Thread 13 Jan, 2025 - 20 Jan, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",6,0.88,https://www.reddit.com/r/datascience/comments/1i06k3y/weekly_entering_transitioning_thread_13_jan_2025/,False,True,False
1i03pk7,lowkeyripper,1736735034.0,47,/r/datascience/comments/1i03pk7/where_do_you_go_to_stay_up_to_date_on_data/,datascience,Where do you go to stay up to date on data analytics/science?,"Are there any people or organizations you follow on Youtube, Twitter, Medium, LinkedIn, or some other website/blog/podcast that you always tend to keep going back to? 

My previous career absolutely lacked all the professional ""content creators"" that data analytics have, so I was wondering what content you guys tend to consume, if any. Previously I'd go to two sources: one to stay up to date on semi-relevant news, and the other was a source that'd do high level summaries of interesting research papers. 

Really, the kind of stuff would be talking about new tools/products that might be of use, tips and tricks, some re-learning of knowledge you might have learned 10+ years ago, deep dives of random but pertinent topics, or someone that consistently puts out unique visualizations and how to recreate them. You can probably see what I'm getting at: sources for stellar information.",308,0.98,https://www.reddit.com/r/datascience/comments/1i03pk7/where_do_you_go_to_stay_up_to_date_on_data/,False,True,False
1hzpcuv,chomoloc0,1736696534.0,8,/r/datascience/comments/1hzpcuv/how_we_matured_fisher_our_ab_testing_library/,datascience,"How we matured Fisher, our A/B testing library",,65,0.88,https://medium.com/@alejandroalvarezprez/how-we-matured-fisher-our-a-b-testing-package-6f2294746a56,False,False,False
1hyxec6,takuonline,1736606856.0,8,/r/datascience/comments/1hyxec6/simple_full_stack_agentic_ai_project_to_please/,datascience,Simple Full stack Agentic AI project to please your Business stakeholders,"Since you all refused to share how you are applying gen ai in the real world, I figured I would just share mine.

  
So here it is:  [https://adhoc-insights.takuonline.com/](https://adhoc-insights.takuonline.com/)  
There is a rate limiter, but we will see how it goes.



Tech Stack:

Frontend: Next.js, Tailwind, shadcn

Backend: Django (DRF), langgraph

LLM: Claude 3.5 Sonnet

I am still unsure if l should sell it as a tool for data analysts that makes them more productive or for quick and easy data analysis for business stakeholders to self-serve on low-impact metrics.

So what do you all think?",0,0.38,https://www.reddit.com/r/datascience/comments/1hyxec6/simple_full_stack_agentic_ai_project_to_please/,False,True,False
1hyte5x,SemperZero,1736592284.0,34,/r/datascience/comments/1hyte5x/feeling_stuck_in_my_career_please_help/,datascience,Feeling stuck in my career. Please help,"I'm in a weird position, where I feel like I'm stuck in my career. I really enjoy mathematics, ML/AI, implemented a lot of algorithms from scratch in C, developed new models for business purposes, presented at some internal/small conferences, and developed entire ML infrastructures for startups, but having no real opportunities to grow more.

At the moment I'm making over 100k$ working remotely from eastern Europe for a FAANG in the US (they have an office here, but my entire data science team is based in the US and I'm working on the same things as them).

When applying to companies in the US/UK I'm receiving zero callbacks (willing to relocate), although companies from the same areas are reaching out with remote offers of \~100k$/year. Those don't have the benefits of my current company, and are not attractive opportunities. I'm looking to relocate and get 200k$+. Current internal transfers to the US are closed, as they are looking to expand in east Europe. I've also asked for more difficult projects, but those are only available for US, not for my region.

The projects that are open to me at the moment offer zero satisfaction and I want to solve more complex problems and continue to expand my skills, but I'm stuck for the only thing that my studies are in eastern Europe and that I don't hold a PhD, even though I've already worked on novel models in industry, and speaking with friends and colleagues that hold a PhD, my skills are on par.

I'm at a point where I feel like skills and projects don't mean absolutely anything, and the only thing that has any weight for getting a job are diplomas and people you know... Maybe I'm exaggerating, but from all of my experiences I'm starting to feel like people from my region without studies abroad are seen only as cheap labor that should never be given the chance to work on real problems and be paid accordingly (a shitty company directly told me that, while another told me explicitly that my skills don't matter and they're only offering bad projects with bad pay in my region). It's like, there's a limit to the level of difficulty I can work on and the pay I can receive, regardless of how much I outcompete others...

At the moment, I'm working on a side research project that I'll be sending to some top tier conferences, and then try getting a PhD in the west... but that will take years, and if I already have the skills it's so frustrating to be stuck for so long just for a diploma and a title...

Or maybe my skills are really not on par, and I'm only good compared to the people in my region? Here's my resume if anyone would be willing to offer me some feedback.",57,0.86,https://www.reddit.com/r/datascience/comments/1hyte5x/feeling_stuck_in_my_career_please_help/,False,True,False
1hyploh,Sad_Campaign713,1736575525.0,168,/r/datascience/comments/1hyploh/200_applications_no_response_please_help_i_have/,datascience,"200 applications - no response, please help. I have applied for data science (associate or mid-level) positions. Thank you ",,430,0.91,https://www.reddit.com/gallery/1hyploh,False,False,False
1hy9am1,clashofphish,1736529004.0,25,/r/datascience/comments/1hy9am1/spreadsheet_first_cell_debate/,datascience,Spreadsheet first cell debate ,"Settle this debate I'm having with a coworker. 

I say that spreadsheets should always start in row 1, column A. They say row 2, column B, [edit] so that there is an empty row and column before the table starts.

What's your take?",0,0.36,https://www.reddit.com/r/datascience/comments/1hy9am1/spreadsheet_first_cell_debate/,False,True,False
1hy8jhq,berserk539,1736527127.0,3,/r/datascience/comments/1hy8jhq/sas_sql_question_inobs_vs_outobs/,datascience,SAS - SQL question: inobs= vs outobs=,"Just a quick question here regarding PROC SQL in SAS.  Let's say I'm just writing some code and I want to test it.  Since the database I'm querying has over a million records, I don't want it to process my code for all the records.  

My understanding is that I would want to use the inobs= option to limit how much of the table is queried and processed on the server.  Is this correct?

The outobs= option will return however many records I set, but it process every record on the table in the server.  Is this correct?",5,0.86,https://www.reddit.com/r/datascience/comments/1hy8jhq/sas_sql_question_inobs_vs_outobs/,False,True,False
1hy7g0m,NickSinghTechCareers,1736524298.0,39,/r/datascience/comments/1hy7g0m/sql_squid_game_imagine_you_were_a_data_scientist/,datascience,SQL Squid Game: Imagine you were a Data Scientist for Squid Games (9 Levels),,525,0.96,https://datalemur.com/sql-game,False,False,False
1hxxjz6,mehul_gupta1997,1736487671.0,0,/r/datascience/comments/1hxxjz6/microsofts_rstarmath_7b_llms_matches_openai_o1s/,datascience,Microsoft's rStar-Math: 7B LLMs matches OpenAI o1's performance on maths,,3,0.6,/r/OpenAI/comments/1hxxjcc/microsofts_rstarmath_7b_llms_matches_openai_o1s/,False,False,False
1hxt0wl,officialcrimsonchin,1736472663.0,40,/r/datascience/comments/1hxt0wl/how_good_are_your_linear_algebra_skills/,datascience,How good are your linear algebra skills?,"Started my masters in computer science in August. Bachelors was in chemistry so I took up to diff eq but never a full linear algebra class. I’m still familiar with a lot of the concepts as they are used in higher level science classes, but in my machine learning class I’m kind of having to teach myself a decent bit as I go. Maybe it’s me over analyzing and wanting to know the deep concepts behind everything I learn, and I’m sure in the real world these pure mathematical ideas are rarely talked about, but I know having a strong understanding of core concepts of a field help you succeed in that field more naturally as it begins becoming second nature.

Should I lighten my course load to take a linear algebra class or do you think my basic understanding (although not knowing how basic that is) will likely be good enough?",87,0.94,https://www.reddit.com/r/datascience/comments/1hxt0wl/how_good_are_your_linear_algebra_skills/,False,True,False
1hxplq8,Corpulos,1736462904.0,13,/r/datascience/comments/1hxplq8/best_resources_for_co2_emissions_modeling/,datascience,Best resources for CO2 emissions modeling forecasting,"I'm looking for a good textbook or resource to learn about air emissions data modeling and forecasting using statistical methods and especially machine learning. Also, can you discuss your work in the field; id like tonlearn more.",8,0.79,https://www.reddit.com/r/datascience/comments/1hxplq8/best_resources_for_co2_emissions_modeling/,False,True,False
1hxnq3t,Deray22,1736457945.0,13,/r/datascience/comments/1hxnq3t/question_on_quasiexperimental_approach_for/,datascience,Question on quasi-experimental approach for product feature change measurement,"I work in ecommerce analytics and my team runs dozens of traditional, ""clean"" online A/B tests each year. That said, I'm far from an expert in the domain - I'm still working through a part-time master's degree and I've only been doing experimentation (without any real training) for the last 2.5 years. 

One of my product partners wants to run a learning test to help with user flow optimization. But because of some engineering architecture limitations, we can't do a normal experiment. Here are some details:

* Desired outcome is to understand the impact of removing the (outdated) new user onboarding flow in our app. 
* Proposed approach is to release a new app version without the onboarding flow and compare certain engagement, purchase, and retention outcomes.
* ""Control"" group: users in the previous app version who did experience the new user flow
* ""Treatment"" group: users in the new app version who *would have* gotten the new user flow had it not been removed

One major thing throwing me off is how to handle the shifted time series; the 4 weeks of data I'll look at for each group will be different time periods. Another thing is the lack of randomization, but that can't be helped.

Given these parameters, curious what might be the best way to approach this type of ""test""? My initial thought was to use difference-in-difference but I don't think it applies given the specific lack of 'before' for each group. ",5,0.78,https://www.reddit.com/r/datascience/comments/1hxnq3t/question_on_quasiexperimental_approach_for/,False,True,False
1hxi5em,Mysterious-Rent7233,1736443855.0,2,/r/datascience/comments/1hxi5em/rn_tabpfn_v2_accurate_predictions_on_small_data/,datascience,[R][N] TabPFN v2: Accurate predictions on small data with a tabular foundation model,,5,0.78,/r/MachineLearning/comments/1hwvk9x/rn_tabpfn_v2_accurate_predictions_on_small_data/,False,False,False
1hxalxo,mediocrity4,1736421422.0,132,/r/datascience/comments/1hxalxo/companies_are_finally_hiring/,datascience,Companies are finally hiring,"I applied to 80+ jobs before the new year and got rejected or didn’t hear back from most of them. A few positions were a level or two lower than my currently level. I got only 1 interview and I did accept the offer. 

In the last week, 4 companies reached out for interviews. Just want to put this out there for those who are still looking. Keep going at it. 

Edit - thank you all for the congratulations and I’m sorry I can’t respond to DMs. Here are answers to some common questions. 

1. The technical coding challenge was only SQL. Frankly in my 8 years of analytics, none of my peers use Python regularly unless their role is to automate or data engineering. You’re better off mastering SQL by using leetcode and DataLemur

2. Interviews at all the FAANGs are similar. Call with HR rep, first round is with 1 person and might be technical. Then a final round with a bunch of individual interviews on the same day. Most of the questions will be STAR format. 

3. As for my skillsets, I advertise myself as someone who can build strategy, project manage, and can do deep dive analyses. I’m never going to compete against the recent grads and experts in ML/LLM/AI on technical skills, that’s just an endless grind to stay at the top. I would strongly recommend others to sharpen their soft skills. A video I watched recently is from The Diary of a CEO with Body Language Expert with Vanessa Edwards. I legit used a few tips during my interviews and I thought that helped ",1585,0.98,https://www.reddit.com/r/datascience/comments/1hxalxo/companies_are_finally_hiring/,False,True,False
1hx305z,Stauce52,1736391735.0,123,/r/datascience/comments/1hx305z/i_was_penalized_in_a_ds_interview_for_answering/,datascience,I was penalized in a DS interview for answering that I would use a Generalized Linear Model for an A/B test with an outcome of time on an app... But a linear model with a binary predictor is equivalent to a t-test. Has anyone had occasions where the interviewer was wrong?,"Hi,

I underwent a technical interview for a DS role at a company. The company was nice enough to provide feedback. This reason was not only reason I was rejected, but I wanted to share because it was very surprising to me. 

They said I aced the programming. However, hey gave me feedback that my statistics performance was mixed. I was surprised. The question was what type of model would I use for an A/B test with time spent on an app as an outcome. I suspect many would use a t-test but I believe that would be inappropriate since time is a skewed outcome, with only positive values, so a t-test would not fit the data well (i.e., Gaussian outcome). I suggested a log-normal or log-gamma generalized linear model instead.

  
I later received feedback that I was penalized for suggesting a linear model for the A/B test. However, a linear model with a binary predictor *is equivalent to a t-test*. I don't want to be arrogant or presumptuous that I think the interviewer is wrong and I am right, but I am struggling to have any other interpretation than the interviewer did not realize a linear model with a binary predictor is equivalent to a t-test.

Has anyone else had occasions in DS interviewers where the interviewer may have misunderstood or been wrong in their assessment?",266,0.94,https://www.reddit.com/r/datascience/comments/1hx305z/i_was_penalized_in_a_ds_interview_for_answering/,False,True,False
1hx286f,UnsafeBaton1041,1736389552.0,57,/r/datascience/comments/1hx286f/am_i_underpaidunderemployed_at_65k_for_a_data/,datascience,Am I underpaid/underemployed at $65k for a Data Analyst position in a MCOL city?,"I'm in a mcol city. I have a master's in Data Analytics that I finished in October 2024, and I've been working as a Data Analyst for 1.5 years. Before that, I was a study lead Clinical Data Manager for over a year (and before that I was a tax researcher and worked in HR). Currently, I make $65k base salary, but $85k total compensation. 

I keep getting interviews for Data Scientist positions that are well into the $100k+ base salary range, but I haven't landed an offer yet (it's really disheartening). Am I underpaid?

P.S. I'm open to job suggestions lol",69,0.82,https://www.reddit.com/r/datascience/comments/1hx286f/am_i_underpaidunderemployed_at_65k_for_a_data/,False,True,False
1hwmsd2,Due-Duty961,1736350202.0,4,/r/datascience/comments/1hwmsd2/absolute_path_to_image_in_shiny_ui/,datascience,absolute path to image in shiny ui,"Hello,
Is there a way to get an image from an absolute path in shiny ui, I have my shiny app in a .R and I havn t created any R project or formal shiny app file so I don t want to use a relative paths
for now 
ui <- fluidPage(  tags$div( tags$img(src= absolute path to image).....
doesn t work",3,0.64,https://www.reddit.com/r/datascience/comments/1hwmsd2/absolute_path_to_image_in_shiny_ui/,False,True,False
1hwcayh,mehul_gupta1997,1736313595.0,5,/r/datascience/comments/1hwcayh/cag_improved_rag_framework_using_cache/,datascience,CAG : Improved RAG framework using cache,,6,0.81,/r/OpenAI/comments/1hwc8xp/cag_improved_rag_framework_using_cache/,False,False,False
1hw5s76,SmartPercent177,1736293697.0,77,/r/datascience/comments/1hw5s76/as_of_2025_which_one_would_you_install_miniforge/,datascience,As of 2025 which one would you install? Miniforge or Miniconda? ,"As the title says, which one would you install today if having a new computer for Data Science purposes. Miniforge or Miniconda and why?

For TensorFlow, PyTorch, etc.

Used to have both, but used Miniforge more since I got used to it (since 2021). But I am formatting my machine and would like to know what you guys think would be more relevant now.

I will try UV soon but want to install miniforge or miniconda at the moment.",39,0.86,https://www.reddit.com/r/datascience/comments/1hw5s76/as_of_2025_which_one_would_you_install_miniforge/,False,True,False
1hvzskd,Any-Fig-921,1736278466.0,43,/r/datascience/comments/1hvzskd/change_my_mind_feature_stores_are_needless/,datascience,Change my mind: feature stores are needless complexity.,"I started last year at my second full-time data science role. The company I am at uses DBT extensively to transform data. And I mean very extensively. 

The last company I was at the data scientist did not use DBT or any sort of feature store. We just hit the raw data and write sql for our project.

The argument for our extensive feature store seems to be that it allows for reusability of complex logic across projects. And yes, this is occasionally true. But it is just as often true that there is a Table that is used for exactly one project. 

Now that I'm starting to get comfortable with the company, I'm starting to see the crack in all of this; complex tables built on top of complex tables built in to of complex tables built on raw data. Leakage and ambiguity everywhere. Onboarding is a beast. 

I understand there are times when it might be computationally important to pre-compute some calculation when doing real-time inference. But this is, in most cases, the exception, not the rule. Most models can be run on a schedule. 

TLDR; The amount of infrastructure, abstraction, and systems in place to make it so I don't have to copy and paste a few dozen lines of SQL is n or even close to a net positive. It's a huge drag.

Change my mind. ",116,0.94,https://www.reddit.com/r/datascience/comments/1hvzskd/change_my_mind_feature_stores_are_needless/,False,True,False
1hvy3ld,RobertWF_47,1736274260.0,45,/r/datascience/comments/1hvy3ld/gradient_boosting_machine_still_running_after_13/,datascience,Gradient boosting machine still running after 13 hours - should I terminate?,"I'm running a gradient boosting machine with the caret package in RStudio on a fairly large healthcare dataset, \~700k records, 600+ variables (most are sparse binary) predicting a binary outcome. It's running very slow on my work laptop, over 13 hours.

Given the dimensions of my data, was I too ambitious choosing hyperparameters of 5,000 iterations and a shrinkage parameter of .001? 

  
My code:  
\### Partition into Training and Testing data sets ###

set.seed(123)

inTrain <- createDataPartition(asd\_data2$K\_ASD\_char, p = .80, list = FALSE)

train <- asd\_data2\[ inTrain,\]

test  <- asd\_data2\[-inTrain,\]



\### Fitting Gradient Boosting Machine ###

set.seed(345)

gbmGrid <- expand.grid(interaction.depth=c(1,2,4), n.trees=5000, shrinkage=0.001, n.minobsinnode=c(5,10,15))

gbm\_fit\_brier\_2 <- train(as.factor(K\_ASD\_char) \~ .,

tuneGrid = gbmGrid,

data=train,

trControl=trainControl(method=""cv"", number=5, summaryFunction=BigSummary, classProbs=TRUE, savePredictions=TRUE),

train.fraction = 0.5,

method=""gbm"",

metric=""Brier"", maximize = FALSE,

preProcess=c(""center"",""scale""))

",24,0.72,https://www.reddit.com/r/datascience/comments/1hvy3ld/gradient_boosting_machine_still_running_after_13/,False,True,False
1hvwxzv,maverick_css,1736271429.0,33,/r/datascience/comments/1hvwxzv/people_who_do_dsanalytics_as_freelancing_any/,datascience,People who do DS/Analytics as freelancing any suggestions ,"Hi all

I've been in DS and aligned fields in corporate for 5+ years now. I'm thinking of trying DS freelance to earn additional income as well as learn whatever new things I can by doing more projects. I have few questions for people who have done it or tried it. 

Does it pay well? Do you do it fulltime or along with your job? Is it very difficult with a job?

What are some good platforms?

How do you get started? How much time does it take? How to get your first project? How to build your brand?

If you do it with your current job how much time does it take? Did you take permission from your manager about this?

Other than freelancing are there better options to make additional income?

Thanks!",79,0.95,https://www.reddit.com/r/datascience/comments/1hvwxzv/people_who_do_dsanalytics_as_freelancing_any/,False,True,False
1hvnkbl,mehul_gupta1997,1736241165.0,2,/r/datascience/comments/1hvnkbl/tried_leetcode_problems_using_deepseekv3_solved/,datascience,"Tried Leetcode problems using DeepSeek-V3, solved 3/4 hard problems in 1st attempt",,0,0.33,/r/OpenAI/comments/1hvnjf6/tried_leetcode_problems_using_deepseekv3_solved/,False,False,False
1hvk25m,mehul_gupta1997,1736226461.0,4,/r/datascience/comments/1hvk25m/best_llms_to_use/,datascience,Best LLMs to use ,"So I tried to compile a list of top LLMs (according to me) in different categories like ""Best Open-sourced"", ""Best Coder"", ""Best Audio Cloning"", etc. Check out the full list and the reasons here : https://youtu.be/K_AwlH5iMa0?si=gBcy2a1E3e6CHYCS",0,0.19,https://www.reddit.com/r/datascience/comments/1hvk25m/best_llms_to_use/,False,True,False
1hvfuwa,Tamalelulu,1736213326.0,23,/r/datascience/comments/1hvfuwa/what_technology_should_i_acquaint_myself_with_next/,datascience,What technology should I acquaint myself with next?,"Hey all. First, I'd like to thank everyone for your immense help on my last question. I'm a DS with about ten years experience and had been struggling with learning Python (I've managed to always work at R-shops, never needed it on the job and I'm profoundly lazy). With your suggestions, I've been putting in lots of time and think I'm solidly on the right path to being proficient after just a few days. Just need to keep hammering on different projects. 

At any rate, while hammering away at Python I figure it would be beneficial to try and acquaint myself with another technology so as to broaden my resume and the pool of applicable JDs. My criteria for deciding on what to go with is essentially: 

1. Has as broad of an appeal as possible, particularly for higher paying gigs
2. Isn't a total B to pick up and I can plausibly claim it as within my skillset within a month or two if I'm diligent about learning it

I was leaning towards some sort of big data technology like Spark but I'm curious what you fine folks think. Alternatively I could brush up on a visualization tool like Tableau.",14,0.68,https://www.reddit.com/r/datascience/comments/1hvfuwa/what_technology_should_i_acquaint_myself_with_next/,False,True,False
1hv5720,jinstronda,1736186285.0,34,/r/datascience/comments/1hv5720/swe_ds_is_learning_both_good/,datascience,SWE + DS? Is learning both good,"I am doing a bachelor in DS but honestly i been doing full stack on the side (studying 4-5 hours per day and developing) and i think its way cooler.

Can i combine both? Will it give me better skills?",4,0.58,https://www.reddit.com/r/datascience/comments/1hv5720/swe_ds_is_learning_both_good/,False,True,False
1hv3gn4,FullStackAI-Alta,1736182086.0,43,/r/datascience/comments/1hv3gn4/are_medium_articles_helpful/,datascience,Are Medium Articles helpful?,"I read almost every day something from Medium (I do write stuff myself too) though I kind of feel some of the articles even though highly rated are not properly written and to some extent loses its flow from the title to the content.

I want to know your thoughts and how have you found articles helpful on Medium or TDS.",22,0.73,https://www.reddit.com/r/datascience/comments/1hv3gn4/are_medium_articles_helpful/,False,True,False
1huz0m1,mehul_gupta1997,1736170208.0,0,/r/datascience/comments/1huz0m1/metas_large_concept_models_lcms_llms_to_output/,datascience,Meta's Large Concept Models (LCMs) : LLMs to output concepts ,,4,0.75,/r/OpenAI/comments/1huyy4h/metas_large_concept_models_lcms_llms_to_output/,False,False,False
1hurpgg,fool126,1736140819.0,31,/r/datascience/comments/1hurpgg/data_experience/,datascience,data experience,,476,0.97,https://i.redd.it/aun922n06bbe1.jpeg,False,False,False
1hurdd1,AutoModerator,1736139681.0,87,/r/datascience/comments/1hurdd1/weekly_entering_transitioning_thread_06_jan_2025/,datascience,"Weekly Entering & Transitioning - Thread 06 Jan, 2025 - 13 Jan, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",7,1.0,https://www.reddit.com/r/datascience/comments/1hurdd1/weekly_entering_transitioning_thread_06_jan_2025/,False,True,False
1huoyaf,yorevodkas0a,1736131935.0,5,/r/datascience/comments/1huoyaf/what_schema_or_data_model_are_you_using_for_your/,datascience,What schema or data model are you using for your LLM / RAG prototyping?,"How are you organizing your data for your RAG applications? I've searched all over and have found tons of tutorials about how the tech stack works, but very little about how the data is actually stored. I don't want to just create an application that can give an answer, I want something I can use to evaluate my progress as I improve my prompts and retrievals.

This is the kind of stuff that I think needs to be stored:

* Prompt templates (i.e., versioning my prompts)
* Final inputs to and outputs from the LLM provider (and associated metadata)
* Chunks of all my documents to be used in RAG
* The chunks that were retrieved for a given prompt, so that I can evaluate the performance of the retrieval step
* Conversations (or chains?) for when there might be multiple requests sent to an LLM for a given ""question""
* Experiments. This is for the purposes of evaluation. It would associate an experiment ID with a series of inputs/outputs for an evaluation set of questions.

I can't be the first person to hit this issue. I started off with a simple SQLite database with a handful of tables, and now that I'm going to be incorporating RAG into the application (and probably agentic stuff soon), I really want to leverage someone else's learning so I don't rediscover all the same mistakes.",9,0.91,https://www.reddit.com/r/datascience/comments/1huoyaf/what_schema_or_data_model_are_you_using_for_your/,False,True,False
1huloe0,conlake,1736122444.0,2,/r/datascience/comments/1huloe0/how_are_these_companies_building_videoimage/,datascience,"How are these companies building video/image generation tools? From scratch, fine-tuning Llama, or something else?
","There’s an enormous amount of LLM-based tools popping up lately, especially in video/image generation, each tied to a different company. Meanwhile, we only see a handful of really good open-source LLM models available.

So, my question is: How are these companies creating their video/image/avatar-generation tools? Are they building these models entirely from scratch, or are they leveraging existing LLMs like Llama, GPT, or something else?

If they are leveraging a model, are they simply using an API to interact with it, or are they actually fine-tuning those models with new data these companies collected for their specific use case?

If you’re guessing the answer, please let me know you’re guessing, as I’d like to hear from those with first-hand experience as well.

Here are some companies I’m referring to:

* **Video/image generation**:
   * [heygen.com](https://heygen.com)
   * [invideo.io](https://invideo.io)
   * [character.ai](https://character.ai)
   * [kindroid.ai](https://kindroid.ai)
   * [runwayml.com](https://runwayml.com)",20,0.92,https://www.reddit.com/r/datascience/comments/1huloe0/how_are_these_companies_building_videoimage/,False,True,False
1huk9gq,Any-Fig-921,1736118652.0,100,/r/datascience/comments/1huk9gq/whats_your_biggest_time_sink_as_a_data_scientist/,datascience,What's your biggest time sink as a data scientist?,"I've got a few ideas for DS tooling I was thinking of taking on as a side project, so this is a bit of a market research post. I'm curious what data-scientist specific task/problem is the biggest time suck for you at work. I feel like we're often building a new class of software in companies and systems that were designed for web 2.0 (or even 1.0). ",185,0.96,https://www.reddit.com/r/datascience/comments/1huk9gq/whats_your_biggest_time_sink_as_a_data_scientist/,False,True,False
1hudtrj,Lamp_Shade_Head,1736102433.0,46,/r/datascience/comments/1hudtrj/do_you_prepare_for_interviews_first_or_apply_for/,datascience,Do you prepare for interviews first or apply for jobs first?,"I’ve started looking for a new job and find myself in a bit of a dilemma that I’m hoping you might have some experience with. Every day, I come across roles that seem like a great fit, but I hesitate to apply because I feel like I’m not fully prepared for an interview. While I know there’s no guarantee I’ll even get an interview, I worry about wasting an opportunity if I’m not ready.

On the other hand, preparing for an interview when you have one lined up seems like the most effective approach, but I’m not sure how to balance it all.

How do you usually handle this?",188,0.95,https://www.reddit.com/r/datascience/comments/1hudtrj/do_you_prepare_for_interviews_first_or_apply_for/,False,True,False
1hu86xb,Dorshalsfta,1736087336.0,0,/r/datascience/comments/1hu86xb/optimizing_advent_of_code_d9p2_with/,datascience,Optimizing Advent of Code D9P2 with High-Performance Rust,,12,1.0,https://cprimozic.net/blog/optimizing-advent-of-code-2024/,False,False,False
1htxzrg,PraiseChrist420,1736052777.0,6,/r/datascience/comments/1htxzrg/looking_for_some_advice_on_my_career_path/,datascience,Looking for some advice on my career path,,6,0.69,/r/datasciencecareers/comments/1hpdycn/please_help_me_figure_out_what_to_do_with_my_life/,False,False,False
1htlb8y,reddit_browsers,1736016656.0,21,/r/datascience/comments/1htlb8y/i_dont_like_my_current_subfield_of_ds/,datascience,I don't like my current subfield of DS,"I have been in Data Science for 5 years and working as Senior Data Scientist for a big company.

In my DS journey most of my work are Applied Data Science where I was working on creating and training models, improving models and analysing features and make improvements so on (I worked on both ML, DL models) which I loved. 

Recently I have been moved to marketing data science where it feels like it is not appealing to me as I'm doing Product Data science with designing Experiment, analysing causal impact, Media mix modeling so on (also I'm somewhat not well experienced in Bayesian models or causal inference still learning). 

But in this field what I feel is you do buch of stuff to answer to business stakeholder in 1 or 2 slides and move on to next business question . Also even if you come up with something business always work based on traditional way with their past experience. I'm not feeling motivated and not seeing any of my solution is creating an impact.

Is this common with product data science/ causal inference world or I'm not seeing with correct picture?",90,0.93,https://www.reddit.com/r/datascience/comments/1htlb8y/i_dont_like_my_current_subfield_of_ds/,False,True,False
1htjd17,wodkaholic,1736011551.0,7,/r/datascience/comments/1htjd17/is_there_a_similar_career_outperformance_todo/,datascience,"Is there a similar career outperformance to-do list for a DS/DA, given some of the options/approaches aren’t available?",,10,0.78,/r/ProductManagement/comments/1hti01u/i_outperform_other_pms_that_are_smarter_than_me/,False,False,False
1hti98t,_deepskyblue,1736008688.0,14,/r/datascience/comments/1hti98t/do_you_have_any_tips_to_keep_up_to_date_with_all/,datascience,Do you have any tips to keep up to date with all the ML implementations?,"I work as a data scientist, but sometimes i feel so left-behind in the field. do you guys have some tips to keep up to date with the latest breakthrough ML implementations?",37,0.95,https://www.reddit.com/r/datascience/comments/1hti98t/do_you_have_any_tips_to_keep_up_to_date_with_all/,False,True,False
1htfjez,django_free,1736001067.0,28,/r/datascience/comments/1htfjez/whats_the_best_resources_to_be_better_at_eda/,datascience,Whats the best resources to be better at EDA,"While I understand the math about ML, The one thing I lack is understanding and interpreting the data better.  
What resources could help me understand them?",86,0.96,https://www.reddit.com/r/datascience/comments/1htfjez/whats_the_best_resources_to_be_better_at_eda/,False,True,False
1htcyqo,TechNerd10191,1735992024.0,15,/r/datascience/comments/1htcyqo/how_do_you_find_data_science_internships/,datascience,How do you find data science internships?,"I am a high school student (grade 12) in a EU country, and if I do well on the national entrance exams, I'll get to the best university in the country which is in the top 200-250 for CS - according to QS. 

My experience with programming/data science is with Kaggle (for the last 2 years), having participated in 10+ competitions (1 bronze medal), and having \~4000 forks for my notebooks/codebases. 

Starting with university, how and when should I look for internships (preferably overseas because my country is lackluster when it comes to tech, let alone AI). Is there anything I can use to my advantage?

What did you guys do when you got your internships? Is it networking/nepotism that makes the difference?",18,0.75,https://www.reddit.com/r/datascience/comments/1htcyqo/how_do_you_find_data_science_internships/,False,True,False
1ht6ztm,Tenet_Bull,1735967040.0,44,/r/datascience/comments/1ht6ztm/i_feel_useless/,datascience,I feel useless ,I’m an intern deploying models to google cloud. Everyday I work 9-10 hours debugging GCP crap that has little to no documentation. I feel like I work my ass off and have nothing to show for it because some weeks I make 0 progress because I’m stuck on a google cloud related issue. GCP support is useless and knows even less than me. Our own IT is super inefficient and takes weeks for me to get anything I need and that’s with me having to harass them. I feel like this work is above my pay grade. It’s so frustrating to give my manager the same updates every week and having to push back every deadline and blame it on GCP. I feel lazy sometimes because i’ll sleep in and start work at 10am but then work till 8-9pm to make up for it. I hate logging on to work now besides I know GCP is just going to crash my pipeline again with little to no explanation and documentation to help. Every time I debug a data engineering error I have to wait an hour for the pipeline to run so I just feel very inefficient. I feel like the company is wasting money hiring me. Is this normal when starting out? ,348,0.94,https://www.reddit.com/r/datascience/comments/1ht6ztm/i_feel_useless/,False,True,False
1ht2bbg,physicsguy21,1735952305.0,58,/r/datascience/comments/1ht2bbg/moving_to_germany/,datascience,Moving to Germany,"Hi, I am a data scientist in Australia with about two years experience building ML models, doing data mining and predictive analysis for a big company. For personal reasons, I am moving to Munich at the end of the year, but am a bit worried about finding a data job abroad. 

I am wondering how difficult it might be to find a job in Germany, and what can I do to make myself competitive in an international market. What skillsets are in demand these days that I can learn and market?

Any advice would be greatly appreciated! ",33,0.82,https://www.reddit.com/r/datascience/comments/1ht2bbg/moving_to_germany/,False,True,False
1hsyiwl,PostponeIdiocracy,1735942121.0,15,/r/datascience/comments/1hsyiwl/dicts_vs_classes_which_do_you_tend_to_use/,datascience,Dicts vs classes: which do you tend to use?,"I’ve been thinking about the trade-offs between using plain Python dicts and more structured options like dataclasses or Pydantic’s BaseModel in my data science work.

On one hand, dicts are super flexible and easy to use, especially when dealing with JSON data or quick prototypes. On the other hand, dataclasses and BaseModels offer structure, type validation, and readability, which can make debugging and scaling more manageable.

I’m curious—what do you all use most often in your projects? Do you prefer the simplicity of dicts, or do you lean towards dataclasses/BaseModels for the added structure?

Would love to hear the community's thoughts!",27,0.85,https://www.reddit.com/r/datascience/comments/1hsyiwl/dicts_vs_classes_which_do_you_tend_to_use/,False,True,False
1hsxfrd,Excellent_Common8528,1735939373.0,52,/r/datascience/comments/1hsxfrd/data_science_job_market_in_uk_vs_usa/,datascience,Data Science Job Market in UK vs. USA,"I've seen a worrying number of posts on social media over the past year describing how bad the job market is for recent computer science graduates, particularly in the US. Obviously there are differences between CS grads and those who pursue DS (though the general consensus (as far as I am aware) is that a CS could do a data scientist role but not vice versa).

Firstly, why do you think this is occurring? I've seen a lot of people mention the H-1B visa is a key issue surrounding this though I personally haven't a clue.

Secondly, is there a vast difference in the UK and USA job markets surrounding data science roles and is the market just as bad in the UK as it is in the USA?

Thirdly, are these CS graduates who are unable to get tech jobs migrating to more DS-centred jobs? This will obviously saturate the DS job market significantly.

Finally, as someone who is just starting to transition into the DS field, how worried should I be about job market saturation in the UK?",36,0.76,https://www.reddit.com/r/datascience/comments/1hsxfrd/data_science_job_market_in_uk_vs_usa/,False,True,False
1hsv9ql,crom5805,1735933833.0,6,/r/datascience/comments/1hsv9ql/professor_looking_for_college_basketball_data/,datascience,Professor looking for college basketball data similar to Kaggles March Madness,"The last 2 years we have had students enter the March Madness Kaggle comp and the data is amazing,  I even did it myself against the students and within my company (I'm an adjunct professor).  In preparation for this year I think it'd be cool to test with regular season games.  After web scraping and searching, Kenpom, NCAA website etc .. I cannot find anything as in depth as the Kaggle comp as far as just regular season stats, and matchup dataset. Any ideas?  Thanks in advance!",4,0.63,https://www.reddit.com/r/datascience/comments/1hsv9ql/professor_looking_for_college_basketball_data/,False,True,False
1hsn3e4,takenorinvalid,1735912872.0,11,/r/datascience/comments/1hsn3e4/why_doesnt_changepoint_detection_work_the_way_i/,datascience,Why doesn't changepoint detection work the way I expect it to?,"I've been experimenting with changepoint detection packages and keep getting results that look like this:

  
[https://www.reddit.com/media?url=https%3A%2F%2Fi.redd.it%2Fonitdxu7ylae1.png](https://www.reddit.com/media?url=https%3A%2F%2Fi.redd.it%2Fonitdxu7ylae1.png)

  
If you look at 2024-05-26 in that picture, you'll what -- to me -- looks like an obvious changepoint. The line has been going down for a while and has suddenly started going up.

However, the model I'm using here is using the red and blue bands to show where it identified changepoints, and it's putting the changepoint just a little bit after the obvious one.

This particular visualization was made using the Ruptures package in Python, but I'm seeing pretty consistent results with every built-in changepoint model I can find. 

Does anyone know why these models, by default, aren't picking up significant changes in direction and how I need to update the calibration to change their behavior?",6,0.88,https://www.reddit.com/r/datascience/comments/1hsn3e4/why_doesnt_changepoint_detection_work_the_way_i/,False,True,False
1hsm94k,oihjoe,1735910239.0,8,/r/datascience/comments/1hsm94k/data_scientist_for_schools_chain_of_schools/,datascience,Data Scientist for Schools/ Chain of Schools,"Hi All,

I’m currently a data manager in a school but my job is mostly just MIS upkeep, data returns and using very basic built in analytics tools to view data. 

I am currently doing a MSc in Data Science and will probably be looking for a career step up upon completion but given the state of the market at the moment I am very aware that I need to be making the most of my current position and getting as much valuable experience as possible (my work are very flexible and they would support me by supplying any data I need). 

I have looked online and apparently there are jobs as data scientists within schools but there are so many prebuilt analytics tools and government performance measures for things like student progress that I am not sure there is any value in trying to build a tool that predicts student performance etc. 

Does anyone work as a data scientist in a school/ chain of schools? If so, what does your job usually entail? Does anyone have any suggestions on the type of project I can undertake, I have access to student performance data (and maybe financial data) across 4 secondary schools (and maybe 2/3 primary schools). 

I’m aware that I should probably be able to plan some projects that create value but I need some inspiration and for someone more experienced to help with whether this is actually viable. 

Thanks in advance. Sorry for the meandering post…",16,0.87,https://www.reddit.com/r/datascience/comments/1hsm94k/data_scientist_for_schools_chain_of_schools/,False,True,False
1hsgfvp,mehul_gupta1997,1735886939.0,4,/r/datascience/comments/1hsgfvp/finetuning_modernbert_for_classification/,datascience,Fine-Tuning ModernBERT for Classification ,,9,0.85,/r/learnmachinelearning/comments/1hsgegf/finetuning_modernbert_for_classification/,False,False,False
1hrpb9q,DieselZRebel,1735806871.0,80,/r/datascience/comments/1hrpb9q/how_do_you_selfidentify_in_this_field_and_what_is/,datascience,How do you self-identify in this field and what is your justification?,"I've been in this field for many years, holding various titles, and connecting with peers who are unfathomably dissimilar in their roles, education, and skills, despite sharing titles.

I am curious to learn how folks view themselves and the various titles in this field. Assuming Data Science is the umbrella that encompasses computer science, machine learning, statistics, maths, etc., and there is a spectrum of roles within this field, how would you self-identify? The rules are:

1. It doesn't have to be your actual title from your employer or degree major.
2. It doesn't have to be a formally known identity. For example, you can identify as a ""number cruncher"", a ""tableau manager"", a ""deep learning developer"", make up your own, or just use a formal identity, such as ""Data Scientist"" or ""Machine Learning Engineer"".
3. You have to also add your justification. i.e. why do you believe such identity justly represents you/your role?
4. It should be self-explainable, technical, maturely and reasonably justified. So avoid the likes of ""Ninja"", ""Unicorn"", ""Guru"", unless you can maturely make a compelling argument.
5. You must be open to criticism and being challenged. Other redditors are not compelled to agree with your self-identity.

I'll also add my own response in the comments because I do not want it to be the center focus of the discussion. ",37,0.8,https://www.reddit.com/r/datascience/comments/1hrpb9q/how_do_you_selfidentify_in_this_field_and_what_is/,False,True,False
1hr8ifj,MrLongJeans,1735754916.0,69,/r/datascience/comments/1hr8ifj/what_was_your_favorite_workproject_of_2024_and/,datascience,What was your favorite work/project of 2024 and why was it personally fulfilling?,"I'm curious what the state of data science was in 2024, and what 2025 may bring, based on what data scientists prefer to be working on.

So let us know what project or type of work you most enjoyed last year that you may want to do more of in 2025 :)",103,0.96,https://www.reddit.com/r/datascience/comments/1hr8ifj/what_was_your_favorite_workproject_of_2024_and/,False,True,False
1hqkw4y,alpha_centauri9889,1735669948.0,29,/r/datascience/comments/1hqkw4y/any_help_for_advanced_numpy/,datascience,Any help for advanced numpy,"I am working on something where I need to process data using numpy. It's a tabular data and I need to convert it to multi dimensional arrays and then perform operations efficiently. 

Can anyone suggest some resources for advanced numpy so that I can understand and visualise numpy arrays, concept of axis, broadcasting etc.? I need to convert my data in such a way that I can do efficient operations on them. For that I need to understand multi dimensional numpy arrays and axis well enough. 
",24,0.77,https://www.reddit.com/r/datascience/comments/1hqkw4y/any_help_for_advanced_numpy/,False,True,False
1hq0s6q,Tamalelulu,1735601417.0,96,/r/datascience/comments/1hq0s6q/what_would_be_the_fastest_way_for_me_to_get_from/,datascience,What would be the fastest way for me to get from novice to advanced level Python?,"I'm a data scientist with ten years experience. I've always worked at R shops and haven't been forced to learn Python on the job so my knowledge of the language is just from piddling around with it on my own and distinctly novice. If I was prepared to sink 5+ hours a day into it, what would be my best bet in terms of fastest way to hone my skills?",126,0.89,https://www.reddit.com/r/datascience/comments/1hq0s6q/what_would_be_the_fastest_way_for_me_to_get_from/,False,True,False
1hpqjrk,ergodym,1735574950.0,125,/r/datascience/comments/1hpqjrk/how_did_you_learn_git/,datascience,How did you learn Git?,"What resources did you find most helpful when learning to use Git? 

I'm playing with it for a project right now by asking everything to ChatGPT, but still wanted to get a better understanding of it (especially how it's used in combination with GitHub to collaborate with other people).

I'm also reading at the same time the book Git Pocket Guide but it seems written in a foreign language lol",318,0.93,https://www.reddit.com/r/datascience/comments/1hpqjrk/how_did_you_learn_git/,False,True,False
1hpfkyr,AutoModerator,1735534882.0,63,/r/datascience/comments/1hpfkyr/weekly_entering_transitioning_thread_30_dec_2024/,datascience,"Weekly Entering & Transitioning - Thread 30 Dec, 2024 - 06 Jan, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",3,1.0,https://www.reddit.com/r/datascience/comments/1hpfkyr/weekly_entering_transitioning_thread_30_dec_2024/,False,True,False
1hpaa3c,variab1e_J,1735518438.0,13,/r/datascience/comments/1hpaa3c/looking_for_some_senior_ds_advice/,datascience,Looking for some Senior DS Advice,"Hello everyone,

I think this is okay to be a post since it's not about entering/transitioning, but if I need to repost in the weekly threads please let me know! 

TLDR:

* I started working as a Data Scientist at a medium to large company almost 3 years ago.
* I spent the majority of my time doing more Software Engineering/Data Engineering related tasks with DS projects sprinkled in.
* A reorg changed the entire landscape of my company and potential growth at the company.
* I don't know what to do because I don't know if I got solid enough experience to leave for another DS job, but my current situation is very uncomfortable.
* Looking for any seasoned perspective/advice on the situation to help anchor me since I'm in a bit of a doom spiral. 



I am looking for some career advice. I don't want to write a novel about my journey to this point, but it was a hell of a lot of work. A snippet of my relevant work experience is I worked at various tech startups doing Data Analyst/Engineering work before I found my way to DS. I graduated with my MS in Data Science back in 2021, and I landed a job at a medium/large global business in the retail space. To my surprise, it was the common meme situation where they had no infrastructure put in place for DS work, and on top of that, a former IBM DS had built a Python ""application"" being used by an internal team that was barely hanging on.

**Year 1** 

My boss asked if I'd be able to modernize the application, and since I have a bit of a programming background, I told them I'd be happy to do that to get my feet wet with the org. I am going to way oversimplify the work I did for the sake of time. The important part is this project took around 6 months as the org had everything on-prem, so I had to go through approvals to get the more ""modern"" tech. I refactored a large portion of it, containerized it, and deployed it via an OpenShift (RedHat's Kubernetes product) cluster. The bulk of the program was a massive Jupyter Notebook (5000 lines of code with some custom-built math libraries) that an analyst would execute each cell after a request was made. This notebook housed all the business logic, so I just wrapped all that up to be executed automatically when the internal team interacted with the new app. By the end of it, I had a firm grasp on various business processes and was already talking to my boss about possibilities. Additionally, I found out that I was the only ""Data Scientist"" on staff, and I was a little bummed because I had chosen to work for a larger org in hopes of getting some sort of mentor/learn-by-osmosis going on. However, since my background is in startups I wasn't overly concerned because I knew I could utilize this environment to grow by trailblazing.

The conversation then shifted to the logic in the notebook, and the fact that no one really knew what was happening inside it. This notebook was driving a fairly important piece of the business by analyzing various datapoints, applying business rules, and spitting out results to be used day to day. They asked if I could dissect it, and I readily agreed – really wish LLMs were as commercialized as they are now. I spent the next 2-3 months working out bugs in the newly deployed app, and flow charting out all the business logic inside the notebook into nice Confluence pages. It was fairly spaghettified, so making changes to it was going to prove challenging. I put my ""Product Manager"" hat on and asked what their goals were with this application, the logic, measuring success, etc. I was asked to start a rewrite so that the laundry list of changes they had wanted to make could be done. It was also at this time my boss was super happy with the ideas/work I had done (I had several other smaller projects I did during this time), so they began speaking to me about being promoted up. How we'd get an actual software engineer on my team so I could focus on more of the ""Data Science"" stuff. I was super excited/anxious because I was hoping to get more hands-on DS experience before leading a team. However, once again, I come from startups so sort of par for the course.



**Year 2** 

The IT department announces a ""reorg"" a month before my promotion. By this point I had job descriptions for a few new positions, and we had made plans for who would be shifting to my team. All of this gets put on hold, and there's tons of uncertainty. I spend the next year doing the rewrite by myself. I build a few classification models in the process to help a few other internal teams operate more efficiently.

Basically they come through with a domain-driven design philosophy so that the Software teams can build more efficiently by having more autonomy. They establish practices across the domains, and they had a Data/ML practice initially. That gave me some confidence that I'd at least have ""peers"" when it was all said and done.



**Year 3 – Current year** 

I get moved into a domain, and they establish a separate BI & Analytics domain. They decentralized everything else but anything to do with ""Data Work"". I am given a promotion to DS Manager with a single employee – a Data Engineer. It has been super confusing all year with things taking much longer as the org adjusts for the new bureaucratic processes that have been introduced – tooling now has to be approved, Business analyst, delivery leads, PMO offices, etc. I meet with the head of engineering to ask how I go about getting tools approved (Sage Maker endpoints), and to get a sense of our overall data strategy. I'm basically told there isn't one in place, but they hope to get one together soonish. A lot has happened and it all feels very confusing. Basically no one is empowered to make decisions, the BI domain is leading the charge for their stuff, and me and my team are sort of this island that exists outside of everything else going on.



I tried to keep that as short as possible, and happy to give further detail if you believe it'd help.



**Here's my main issue:** I spent these years doing what needed to be done, but there really isn't a path of ""growth"" because they aren't really accounting for Data Scientists yet – though they say they hope to hire them. It was clear in the first year what the path would probably look like, but with everything becoming more corporate it feels like I could easily get shafted in one way or another. However, because I spent these years being the ""good employee"" and doing what needed to be done instead of what was best for my own experience I think it may be hard for me to get a DS job at another org. I'm hoping to get some perspective from all of you more seasoned professionals.",15,0.82,https://www.reddit.com/r/datascience/comments/1hpaa3c/looking_for_some_senior_ds_advice/,False,True,False
1hp7pim,irndk10,1735511285.0,177,/r/datascience/comments/1hp7pim/my_data_science_manifesto_from_a_self_taught_data/,datascience,My Data Science Manifesto from a Self Taught Data Scientist,"**Background**

I’m a self-taught data scientist, with about 5 years of data analyst experience and now about 5 years as a Data Scientist. I’m more math minded than the average person, but I’m not special. I have a bachelor’s degree in mechanical engineering, and have worked alongside 6 data scientists, 4 of which have PHDs and the other 2 have a masters. Despite being probably, the 6th out of 7 in natural ability, I have been the 2nd most productive data scientist out of the group.


**Gatekeeping**

Every day someone on this subreddit asks some derivative of “what do I need to know to get started in ML/DS?” The answers are always smug and give some insane list of courses and topics one must master. As someone who’s been on both sides, this is attitude extremely annoying and rampart in the industry. I don’t think you can be bad at math and have no pre-requisite knowledge, and be successful, but the levels needed are greatly exaggerated. Most of the people telling you these things are just posturing due to insecurity.



As a mechanical engineering student, I had at least 3 calculus courses, a linear algebra course, and a probability course, but it was 10+ years before I attempted to become a DS, and I didn’t remember much at all. This sub, and others like it, made me think I had to be an expert in all these topics and many more to even think about trying to become a data scientist. 



When I started my journey, I would take coding, calculus, stats, linear algebra, etc. courses. I’d take a course, do OK in it, and move onto the next thing. However, eventually I’d get defeated because I realized I couldn’t remember much from the courses I took 3 months prior. It just felt like too much information for me to hold at a single time while working a full-time job. I never got started on actually solving problems because the internet and industry told me I needed to be an expert in all these things.


**What you actually need**

The reality is, 95% of the time you only need a basic understanding of these topics. Projects often require a deeper dive into something else, but that's a case by case basis, and you figure that out as you go.


For calculus, you don't need to know how to integrate multivariable functions by hand. You need to know that derivatives create a function that represents the slope of the original function, and that where the derivative = 0 is a local min/max. You need to know integrals are area under the curve.



For stats, you need to understand what a p value represents. You don't need to know all the different tests, and when to use them. You need to know that they exist and why you need them. When it's time to use one, just google it, and figure out which one best suits your use case.



For linear algebra, you don't need to know how to solve for eigenvectors by hand, or whatever other specific things you do in that class. You need to know how to ‘read’ it. It is also helpful to know properties of linear algebra. Like the cross product of 2 vectors yields a vector perpendicular to both.



For probability, you need to understand basic things, but again, just google your specific problem.



You don't need to be an expert software dev. You need to write ok code, and be able to use chatGPT to help you improve it little by little.



You don't need to know how to build all the algorithms by hand. A general understanding of how they work is enough in 95% of cases.



Of all of those things, the only thing you absolutely NEED to get started is basic coding ability. 



By far the number one technical ability needed to 'master' is understanding how to ""frame"" your problem, and how to test and evaluate and interpret performance. If you can ensure that you're accurately framing the problem and evaluating the model or alogithm, with metrics that correctly align with the use case, that's enough to start providing some real value. I often see people asking things like ""should I do this feature engineering technique for this problem?"" or “which of these algorithms will perform best?”. The answer should usually be, ""I don't know, try it, measure it, and see"". Understanding how the algorithms work can give you clues into what you should try, but at the end of the day, you should just try it and see.   



Despite the posturing in the industry, very few people are actually experts in all these domains. Some people are better at talking the talk than others, but at the end of the day, you WILL have to constantly research and learn on a project by project basis. That’s what makes it fun and interesting. As you gain PRACTICAL experience, you will grow, you will learn, you will improve beyond what you could've ever imagined. Just get the basics down and get started, don't spin your wheels trying and failing to nail all these disciplines before ever applying anything.



The reason I’m near the top in productivity while being near the bottom in natural and technical ability is my 5 years of experience as a data analyst at my company. During this time, I got really good at exploring my companies’ data. When you are stumped on problem, intelligently visualizing the data often reveals the solution. I’ve also had the luxury of analyzing our data from all different perspectives. I’d have assignments from marketing, product, tech support, customer service, software, firmware, and other technical teams. I understand the complete company better than the other data scientists. I’m also just aware of more ‘tips and tricks’ than anyone else.  



Good domain knowledge and data exploration skills with average technical skills will outperform good technical skills with average domain knowledge and data exploration almost every time. 


**Advice for those self taught**


I’ve been on the hiring side of things a few times now, and the market is certainly difficult. I think it would be very difficult for someone to online course and side project themselves directly into a DS job. The side project would have to be EXTREMELY impressive to be considered. However, I think my path is repeatable.



I taught myself basic SQL and Tableau and completed a few side projects. I accepted a job as a data analyst, in a medium sized (100-200 total employees) on a team where DS and DA shared the same boss. The barrier to DA is likely higher than it was ~10 years ago, but it's definitely something achievable. My advice would be to find roles that you have some sort of unique experience with, and tailor your resume to that connection. No connection is too small. For example, my DA role required working with a lot of accelerometer data. In my previous job as a test engineer, I sometimes helped set up accelerometers to record data from the tests. This experience barely helped me at all when actually on the job, but it helped my resume actually get looked at. For entry level jobs employers are looking for ANY connection, because most entry level resumes all look the same.


The first year or two I excelled at my role as a DA. I made my boss aware that I wanted to become a DS eventually. He started to make me a small part of some DS projects, running queries, building dashboards to track performance and things like that. I was also a part of some of the meetings, so I got some insight into how certain problems were approached. 



My boss made me aware that I would need to teach myself to code and machine learning. My role in the data science projects grew over time, but I was ultimately blocked from becoming a DS because I kept trying and failing to learn to code and the 25 areas of expertise reddit tells you that you need by taking MOOCs. 

  

Eventually, I paid up for DataQuest. I naively thought the course would teach me everything I needed to know. While you will not be proficient in anything DS upon completing, the interactive format made it easy to jump into 30-60 minutes of structured coding every day. Like a real language consistency is vital. 



Once I got to the point where I could do some basic coding, I began my own side project. THIS IS THE MOST IMPORTANT THING. ONCE YOU GET THE BASELINE KNOWLEDGE, JUST GET STARTED WORKING ON THINGS. This is where the real learning began. You'll screw things up, and that's ok. Titanic problem is fine for day 1, but you really need a project of your own. I picked a project that I was interested in and had a function that I would personally use (I'm on V3 of this project and it's grown to a level that I never could've dreamed of at the time). This was crucial in ensuring that I stuck with the project, and had real investment in doing it correctly. When I didn’t know how to do something in the project, I would research it and figure it out. This is how it works in the real world.



After 3 months of Dataquest and another 3 of a project (along with 4 years of being a data analyst) I convinced my boss to assign me DS project. I worked alongside another data scientist, but I owned the project, and they were mostly there for guidance, and coded some of the more complex things. I excelled at that project, and was promoted to data scientist, and began getting projects of my own, with less and less oversight. We have a very collaborative work environment, and the data scientists are truly out to help each other. We present our progress to each other often which allows us all to learn and improve. I have been promoted twice since I began DS work.



I'd like to add that you can almost certainly do all this in less time than it took me. I wasted a lot of time spinning my wheels. ChatGPT is also a great resource that could also increase your learning speed. Don't blindly use it, but it's a great resource.


**Tldr:** Sir this is Wendy’s.

**Edit:** I’m not saying to never go deeper into things, I’m literally always learning. I go deeper into things all the time. Often in very niche domains, but you don't need to be a master in all things get started or even excel. Be able to understand generalities of those domains, and dig deeper when the problem calls for it. Learning a concept when you have a direct application is much more likely to stick.


I thought it went without saying, but I’m not saying those things I listed are literally the only things you need to know about those topics, I was just giving examples of where relatively simple concepts were way more important than specifics.

**Edit #2:** I'm not saying schooling is bad. Yes obviously having a masters and/or PhD is better than not. I'm directing this to those who are working a full time job who want to break into the field, but taking years getting a masters while working full time and going another 50K into debt is unrealistic",2074,0.97,https://www.reddit.com/r/datascience/comments/1hp7pim/my_data_science_manifesto_from_a_self_taught_data/,False,True,False
1hp65ll,MrLongJeans,1735507199.0,3,/r/datascience/comments/1hp65ll/iye_how_does_the_computational_infrastructure_for/,datascience,"IYE, how does the computational infrastructure for AI models and their cost impact developers and users? Has your org ever bottlenecked development by cost to deploy the AI solution, either for you or in their pricing for clients?","I'm curious how the expense of AI factors into business. It seems like an individual could write code that impacts their cost of employment, and that LLM training algorithms and other AI work would be more expensive. 

 I'm wondering how businesses are governing the cost of a data scientist/software developer's choices with AI.",7,1.0,https://www.reddit.com/r/datascience/comments/1hp65ll/iye_how_does_the_computational_infrastructure_for/,False,True,False
1hp0cbx,phicreative1997,1735491867.0,4,/r/datascience/comments/1hp0cbx/building_productionready_ai_agents_llm_programs/,datascience,Building Production-Ready AI Agents & LLM programs with DSPy: Tips and Code Snippets,,12,0.87,https://medium.com/firebird-technologies/building-production-ready-ai-agents-llm-programs-with-dspy-tips-and-code-snippets-05d80ffc3933,False,False,False
1hoy3dm,Emotional-Rhubarb725,1735485620.0,53,/r/datascience/comments/1hoy3dm/recommend_me_the_best_statistics_textbook_for/,datascience,recommend me the best statistics textbook for data science ,"I am intermediate level student who already studied stats , But i want to revisit it from DS and ML perspective  ",126,0.94,https://www.reddit.com/r/datascience/comments/1hoy3dm/recommend_me_the_best_statistics_textbook_for/,False,True,False
1houdgh,takuonline,1735472512.0,20,/r/datascience/comments/1houdgh/what_are_some_of_the_most_interesting_applied_ml/,datascience,What are some of the most interesting applied ml papers/blogs you read in 2024 or projects you worked on,"I am looking for some interesting successful/unsuccessful real-world machine learning applications. You are also free to share experiences building applications with machine learning that have actually had some real world impact.

Something of this type: 

1. LinkedIn has developed a new family of domain-adapted foundation models called Economic Opportunity Network (EON) to enhance their platform's AI capabilities.

https://www.linkedin.com/blog/engineering/generative-ai/how-we-built-domain-adapted-foundation-genai-models-to-power-our-platform


Edit: Just to encourage this conversation here is my own personal SAAS app - this is how l have been applying machine learning in the real world as a machine learning engineer. It's not much, but it's something.
This is a side project(built during weekends and evenings) which flopped and has no users
[Clipbard](https://clipbard.com). I mostly keep it around to enhance my resume.
My main audience were educators would like to improve engagement with the younger 'tiktok' generation. I assumed this would be a better way of sharing things like history in a more memorable way as opposed to a wall of text. I also targeted groups like churches (Sunday school/ Children's church) who want to bring bible stories to life or tell stories with lessons or parents who want to bring bedtime stories to life every evening.


",53,0.97,https://www.reddit.com/r/datascience/comments/1houdgh/what_are_some_of_the_most_interesting_applied_ml/,False,True,False
1hoq8yb,mehul_gupta1997,1735454485.0,2,/r/datascience/comments/1hoq8yb/modernbert_vs_bert/,datascience,ModernBERT vs BERT ,,11,0.79,/r/learnmachinelearning/comments/1hoq8ss/modernbert_vs_bert/,False,False,False
1ho91f8,BlackPlasmaX,1735402904.0,19,/r/datascience/comments/1ho91f8/will_the_official_year_end_salary_thread_be/,datascience,Will the official Year End Salary thread be posted for 2024? ,I tried searching for it with the “salary” as the keyword. Usually that thread is up by now. Was just curious as I was looking for comparisons to my own salary. ,50,0.88,https://www.reddit.com/r/datascience/comments/1ho91f8/will_the_official_year_end_salary_thread_be/,False,True,False
1ho4com,mehul_gupta1997,1735387439.0,2,/r/datascience/comments/1ho4com/metas_byte_latent_transformer_new_llm/,datascience,Meta's Byte Latent Transformer: new LLM architecture (improved Transformer),Byte Latent Transformer is a new improvised Transformer architecture introduced by Meta which doesn't uses tokenization and can work on raw bytes directly. It introduces the concept of entropy based patches. Understand the full architecture and how it works with example here : https://youtu.be/iWmsYztkdSg,39,0.88,https://www.reddit.com/r/datascience/comments/1ho4com/metas_byte_latent_transformer_new_llm/,False,True,False
1hnnwf5,Typical-Macaron-1646,1735330775.0,4,/r/datascience/comments/1hnnwf5/euchre_simulation_and_winning_chances/,datascience,Euchre Simulation and Winning Chances,"I tried posting this to r/euchre but it got removed immediately.

I’ve been working on a project that calculates the odds of winning a round of Euchre based on the hand you’re dealt. For example, I used the program to calculate this scenario:

If you in the first seat to the left of the dealer, a hand with the right and left bower, along with the three non-trump 9s wins results in a win 61% of the time. (Based on 1000 simulations)

For the euchre players here:

Would knowing the winning chances for specific hands change how you approach the game?
Could this kind of information improve strategy, or would it take away from the fun of figuring it out on the fly?
What other scenarios or patterns would you find valuable to analyze?
I’m excited about the potential applications of this, but I’d love to hear from any Euchre players. Do you think this kind of data would add to the game, or do you prefer to rely purely on instinct and experience? Here is the github link:

https://github.com/jamesterrell/Euchre_Calculator",24,0.97,https://www.reddit.com/r/datascience/comments/1hnnwf5/euchre_simulation_and_winning_chances/,False,True,False
1hnlbhw,Professional_Ball_58,1735323961.0,2,/r/datascience/comments/1hnlbhw/prepost_implementation_analysis_interpretation/,datascience,Pre/Post Implementation Analysis Interpretation,"I am using an interrupted time series to understand whether a certain implementation affected the behavior of the users. We can't do a proper A/B testing since we introduced the feature to all the users.

Lets say we were able to create a model and predict the post implementation daily usage to create the ""counterfactual"" which would be ""What would be the usage look like if there was no implementation?""

Since I have the actual post-implementation usage, now I can use it to find the cumulative difference/residual.

But my question is, since the model is trained on the pre-implementation data doesn't it make sense for the residual error to be high against the counter factual?

The data points in pre-implementation are mostly even across the lower and higher boundary and Its clear that there are more data points in the lower boundaries in the post-implementation but not sure how I would correctly test this. I want to understand the direction so was thinking about using MBE (Mean Bias Deviation)

Any thoughts?",3,0.8,https://www.reddit.com/r/datascience/comments/1hnlbhw/prepost_implementation_analysis_interpretation/,False,True,False
1hnl48d,Fit-Employee-4393,1735323441.0,53,/r/datascience/comments/1hnl48d/imputation_use_cases/,datascience,Imputation Use Cases,"I’m wondering how and why people use this technique. I learned about it early on in my career and have avoided it entirely after trying it a few times. If people could provide examples of how they’ve used this in a real life situation it would be very helpful.

I personally think it’s highly problematic in nearly every situation for a variety of reasons. The most important reason for me is that nulls are often very meaningful. Also I think it introduces unnecessary bias into the data itself. So why and when do people use this?",28,0.8,https://www.reddit.com/r/datascience/comments/1hnl48d/imputation_use_cases/,False,True,False
1hng96m,LiqC,1735310424.0,12,/r/datascience/comments/1hng96m/puppy_organize_your_2025_python_projects/,datascience,Puppy: organize your 2025 python projects,"# TLDR

[https://github.com/liquidcarbon/puppy](https://github.com/liquidcarbon/puppy) is a transparent wrapper around pixi and uv, with simple APIs and recipes for using them to help write reproducible, future-proof scripts and notebooks.

## From 0 to rich toolset in one command:

Start in an empty folder.

```
curl -fsSL ""https://pup-py-fetch.hf.space?python=3.12&pixi=jupyter&env1=duckdb,pandas"" | bash
```

installs python and dependencies, in complete isolation from any existing python on your system.  Mix and match URL [query params](https://github.com/liquidcarbon/puppy?tab=readme-ov-file#one-installer-to-rule-them-all) to specify python version, tools, and venvs to create.

The above also installs puppy's CLI (`pup --help`):

## CLI - kind of like ""uv-lite""
- `pup add myenv pkg1 pkg2` (install packages to ""myenv"" folder using uv)
- `pup list` view what's installed across all projects
- `pup clone` and `pup sync` clone and build external repos (must have buildable `pyproject.toml` files)

## Pup as a Module - no more notebook kernels

The original motivation for writing puppy was to simplify handling kernels, but you might just not need them at all.  Activate/create/modify ""kernels"" interactively with:

```
import pup
pup.fetch(""myenv"")  # ""activate"" - packages in ""myenv"" are now importable
pup.fetch(""myenv"", ""pkg1"", ""pkg2"")  # ""install and activate"" - equivalent to `pup add myenv pkg1 pkg2`  
```

Of course you're welcome to use `!uv pip install`, but after 10 times it's liable to get messy.


## Target Audience

Loosely defining 2 personas:

1. Getting Started with Python (or herding folks who are):
   1. puppy is the easiest way to go from 0 to modern python - one-command installer that lets you specify python version, venvs to build, repos to clone - getting everyone from 0 to 1 in an easy and standardized way
   2. if you're confused about virtual environments and notebook kernels and install full jupyter into every project

2. Competent - check out [Multi-Puppy-Verse](https://github.com/liquidcarbon/puppy?tab=readme-ov-file#multi-puppy-verse) and [Where Pixi Shines](https://github.com/liquidcarbon/puppy?tab=readme-ov-file#where-pixi-shines-) sections:
   1. you have 10 work and hobby projects going at the same time and need a better way to organize them for packaging, deployment, or even to find stuff 6 months later
   2. you need support for conda and non-python stuff - you have many fast-moving external and internal dependencies - check out `pup clone` and `pup sync` workflows and [dockerized examples](https://github.com/liquidcarbon/puppy/tree/main/examples)


## Filesystem is your friend

Puppy recommends a sensible folder structure where each outer folder houses one and only one python executable - in isolation from each other and any other python on your system. Pup is tied to a python executable that is installed by Pixi, along with project-level tools like Jupyter, conda packages, and non-python tools (NodeJS, make, etc.) Puppy commands work the same from anywhere within this folder.

The inner folders are git-ready projects, defined by pyproject.toml, with project-specific packages handled by uv.



```
# ├── puphome/  # python 3.12 lives here
# │   ├── public-project/
# │   │   ├── .git  # this folder may be a git repo (see pup clone)
# │   │   ├── .venv
# │   │   └── pyproject.toml
# │   ├── env2/
# │   │   ├── .venv/  # this one is in pre-git development
# │   │   └── pyproject.toml
# │   ├── pixi.toml
# │   └── pup.py
# ├── pup311torch/  # python 3.11 here
# │   ├── env3/
# │   ├── env4/
# │   ├── pixi.toml
# │   └── pup.py
# └── pup313beta/  # 3.13 here
#     ├── env5/
#     ├── pixi.toml
#     └── pup.py
```

Puppy embraces ""explicit is better than implicit"" from the Zen of python; it logs what it's doing, with absolute paths, so that you always know where you are and how you got there.



PS I've benefited a great deal from the many people's OSS work - now trying to pay it forward. The ideas laid out in puppy's README and implementation have come together after many years of working in different orgs, where average ""how do you rate yourself in python"" ranged from zero (Excel 4ever) to highly sophisticated. The matter of ""how do we build stuff"" is kind of never settled, and this is my take.

Thanks for checking this out! Suggestions and feedback are welcome!",0,0.41,https://www.reddit.com/r/datascience/comments/1hng96m/puppy_organize_your_2025_python_projects/,False,True,False
1hn1eqn,ergodym,1735256582.0,115,/r/datascience/comments/1hn1eqn/whats_your_2025_resolution_as_a_ds/,datascience,What's your 2025 resolution as a DS?,"As 2024 wraps up, it’s time to reflect and plan ahead. What’s your new year resolution as a data scientist? Are you aiming for a promotion, a pay bump, or a new job? Maybe you’re planning to dive into learning a new skill, step into a people manager role, or pivot to a different field.

Curious to hear what's on your radar for 2025 (of course coasting counts too).",82,0.86,https://www.reddit.com/r/datascience/comments/1hn1eqn/whats_your_2025_resolution_as_a_ds/,False,True,False
1hn0k9f,Ok_Bonus_2760,1735254124.0,43,/r/datascience/comments/1hn0k9f/i_analyzed_you_guys/,datascience,I analyzed you guys ,"In my quest for finding an internship and figuring what I want to do with my life workwise I decided to analyze how y'all feel about jobs in data science. One of the fields I am interested in is machine learning/data science so I decided to do a project that would help me see what other people think about this field. 

The project is available here: [Sentiment analysis part 1 | Ted’s cave](https://tedthecaver.github.io/2024/11/29/sentiment_analysis.html)

I would really appreciate any advice on the project itself if anyone bothers to read through it or on the problem of how I'm supposed to figure out what my passions are, and how do i commit to one thing (and how do i land an internship lol). 

Anyways I thought I would share with my dataset the project I did. Thanks y'all. ",145,0.92,https://www.reddit.com/r/datascience/comments/1hn0k9f/i_analyzed_you_guys/,False,True,False
1hmuob8,Daamm1,1735238072.0,16,/r/datascience/comments/1hmuob8/regression_on_multiple_independent_variable/,datascience,Regression on multiple independent variable,"Hello everyone,

I've come across a use case that's got me stumped, and I'd like your opinion.

I have around 1 million pieces of data representing the profit of various projects over a period of time. Each project has its ID, its profits at the date, the date, and a few other independent variables such as the project manager, city, etc...

So I have projects over years, with monthly granularity. Several projects can be running simultaneously.

I'd like to be able to predict a project's performance at a specific date. (based on profits)

The problem I've encountered is that each project only lasts 1 year on average, which means we have 12 data points per project, so it's impossible to do LSTM per project. As far as I know, you can't generalise LSTM for a case like mine (similar periods of time for different projects).

How do you build a model that could generalise the prediction of the benefits of a project over its lifecycle?

What I've done for the moment is classic regression (xgboost, decision tree) with variables such as the age of the project (in months), the date, the benefits over M-1, M-6, M-12. I've chosen 1 or 0 as the target variable (positive or negative margin at the current month).

I'm afraid that regression won't be enough to capture more complex trends (lagged trend especially). Which kind of model would you advise me to go ? Am I on a good direction ?",31,1.0,https://www.reddit.com/r/datascience/comments/1hmuob8/regression_on_multiple_independent_variable/,False,True,False
1hmrwcw,mehul_gupta1997,1735230550.0,0,/r/datascience/comments/1hmrwcw/deepseekv3_looks_the_best_opensourced_llm_released/,datascience,DeepSeek-v3 looks the best open-sourced LLM released,,6,0.88,/r/OpenAI/comments/1hmrucw/deepseekv3_looks_the_best_opensourced_llm_released/,False,False,False
1hm9he8,Potential_Front_1492,1735162224.0,19,/r/datascience/comments/1hm9he8/am_i_cooked_or_is_it_this_job_market/,datascience,Am I cooked or is it this job market? ,,0,0.31,/r/datasciencequestions/comments/1hm9fz6/am_i_cooked_or_is_it_this_job_market/,False,False,False
1hm7es6,Potential_Front_1492,1735155586.0,10,/r/datascience/comments/1hm7es6/updated_with_250_questions_ds_questions/,datascience,Updated with 250+ Questions - DS Questions,"Hi everyone, 

Just wanted to give a heads up we updated our list of data science interview questions to now have almost 250 questions for you guys to try out and access for yourselves. Again with a free plan you can access most of the content on the site.

Hope this helps you guys in your interview prep - merry christmas.

[https://www.dsquestions.com/problems](https://www.dsquestions.com/problems)",20,0.58,https://www.reddit.com/r/datascience/comments/1hm7es6/updated_with_250_questions_ds_questions/,False,True,False
1hlz38v,No-Brilliant6770,1735126029.0,15,/r/datascience/comments/1hlz38v/where_can_i_find_realworld_mlds_experience/,datascience,Where can I find real-world ML/DS experience? Volunteering works too!,"Hey everyone,

So, I’m trying to get some hands-on experience in machine learning and data science—not just the “do more projects” advice (I’ve already done a bunch), but actual real-world stuff where I can work on meaningful problems. Paid or unpaid, doesn’t really matter to me—I’d even love to volunteer if it means I get to learn and grow.

I recently applied for an Omdena project, and I’m wondering if anyone here has done something with them? What’s it like? Did it actually help you gain valuable experience, or was it just another “group project” kind of thing?

Also, are there other platforms or places where I could jump into something similar? I’m trying to avoid the whole “chasing certifications” rabbit hole. I just want to get better at solving real problems, not stacking credentials.

Would love to hear your thoughts or any experiences you’ve had. Thanks in advance!

bit about me: I’m a 3rd-year undergrad in Computer Science with a minor in Statistics, and I just got an internship for a data role at a pretty big company. Super excited about it, but I want to keep building my skills and exploring different opportunities in ML/DS.",35,0.86,https://www.reddit.com/r/datascience/comments/1hlz38v/where_can_i_find_realworld_mlds_experience/,False,True,False
1hlup8w,mehul_gupta1997,1735105238.0,1,/r/datascience/comments/1hlup8w/langchain_in_your_pocket_generative_ai_book_packt/,datascience,"LangChain In Your Pocket (Generative AI Book, Packt published) : Free Audiobook","Hi everyone,

It's been almost a year now since I published my debut book

>“LangChain In Your Pocket : Beginner’s Guide to Building Generative AI Applications using LLMs”

https://preview.redd.it/lgtj9570ix8e1.png?width=934&format=png&auto=webp&s=8b2a0e87914072d5125551adf830b731afcb293e

And what a journey it has been. The book saw major milestones becoming a **National and even International Bestseller in the AI category**. So to celebrate its success, I’ve released the Free Audiobook version of “LangChain In Your Pocket” making it accessible to all users free of cost. I hope this is useful. The book is currently rated at 4.6 on amazon India and 4.2 on amazon com, making it amongst the top-rated books on LangChain and is published by Packt as well

More details : [https://medium.com/data-science-in-your-pocket/langchain-in-your-pocket-free-audiobook-dad1d1704775](https://medium.com/data-science-in-your-pocket/langchain-in-your-pocket-free-audiobook-dad1d1704775)

# Table of Contents

* Introduction
* Hello World
* Different LangChain Modules
* Models & Prompts
* Chains
* Agents
* OutputParsers & Memory
* Callbacks
* RAG Framework & Vector Databases
* LangChain for NLP problems
* Handling LLM Hallucinations
* Evaluating LLMs
* Advanced Prompt Engineering
* Autonomous AI agents
* LangSmith & LangServe
* Additional Features

**Edit :** Unable to post direct link (maybe Reddit Guidelines), hence posted medium post with the link.",0,0.31,https://www.reddit.com/r/datascience/comments/1hlup8w/langchain_in_your_pocket_generative_ai_book_packt/,False,True,False
1hl9xdo,mehul_gupta1997,1735031798.0,5,/r/datascience/comments/1hl9xdo/12_days_of_openai_summarized/,datascience,12 days of OpenAI summarized ,,0,0.15,/r/OpenAI/comments/1hl9x42/12_days_of_openai_summarized/,False,False,False
