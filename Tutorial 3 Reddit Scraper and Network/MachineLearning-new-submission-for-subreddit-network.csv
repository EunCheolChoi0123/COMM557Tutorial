id,author,created_utc,num_comments,permalink,subreddit,title,selftext,score,upvote_ratio,url,over_18,is_self,is_original_content
1ne2qux,prod-v03zz,1757576898.0,2,/r/MachineLearning/comments/1ne2qux/d_only_out_of_curiosity/,MachineLearning,[D] Only out of CURIOSITY!,Name the most RIDICULOUS AI/GenAI projects that are going on in you company!,0,0.35,https://www.reddit.com/r/MachineLearning/comments/1ne2qux/d_only_out_of_curiosity/,False,True,False
1ndulfv,drv29,1757549750.0,2,/r/MachineLearning/comments/1ndulfv/d_the_best_way_to_structure_data_for_a_predictive/,MachineLearning,[D] The best way to structure data for a predictive model of corporate delinquency,"I have annual financial indicators for thousands of clients (businesses), their credit data, and delinquency data, and I want to use this data to create a predictive model.

But what's the best way to structure the data?

* Take the annual financial data and associate it with the following year's delinquency data. So, for example, data from 2024 will predict delinquency in 2025.

OR

* Group by client and calculate the average, maximum, and minimum of the financial data to see if this data can predict delinquency.",5,0.78,https://www.reddit.com/r/MachineLearning/comments/1ndulfv/d_the_best_way_to_structure_data_for_a_predictive/,False,True,False
1ndtey6,grabber500,1757546402.0,16,/r/MachineLearning/comments/1ndtey6/d_having_trouble_organising_massive_csv_files_for/,MachineLearning,[D] Having trouble organising massive CSV files for your machine learning models?,"I've been fighting with CSVs from our high end power quality meter from a very reputable instrument company. 

The CSV files come out from the unit immediately unusable and at 2 million samples per second its a huge dataset, and we take lots of measurements. I made some scripts go clean it but its still a mission every time that I dread to get to the good bit. ",4,0.64,https://www.reddit.com/r/MachineLearning/comments/1ndtey6/d_having_trouble_organising_massive_csv_files_for/,False,True,False
1ndo5md,pmv143,1757533191.0,10,/r/MachineLearning/comments/1ndo5md/dnvidia_blackwell_ultra_crushes_mlperf/,MachineLearning,[D]NVIDIA Blackwell Ultra crushes MLPerf,"NVIDIA dropped MLPerf results for Blackwell Ultra yesterday. 5√ó throughput on DeepSeek-R1, record runs on Llama 3.1 and Whisper, plus some clever tricks like FP8 KV-cache and disaggregated serving. The raw numbers are insane.

But I wonder though . If these benchmark wins actually translate into lower real-world inference costs.

In practice, workloads are bursty. GPUs sit idle, batching only helps if you have steady traffic, and orchestration across models is messy. You can have the fastest chip in the world, but if 70% of the time it‚Äôs underutilized, the economics don‚Äôt look so great to me. IMO",42,0.94,https://www.reddit.com/r/MachineLearning/comments/1ndo5md/dnvidia_blackwell_ultra_crushes_mlperf/,False,True,False
1ndbzb7,ScaryCommission7829,1757504917.0,0,/r/MachineLearning/comments/1ndbzb7/d_iccv_2025_registration/,MachineLearning,[D] ICCV 2025 registration,"Two years ago at Paris I had a workshop paper, I purchased the workshop entrance ticket, everything is okay.

This year I have done the same and now I am receiving emails saying only a full conference entrance is considered an author registration for a workshop paper. 

I did see the website is slightly different this year but still‚Ä¶ the code of conduct did not explain this clearly, does anyone have better insights for me?",4,0.83,https://www.reddit.com/r/MachineLearning/comments/1ndbzb7/d_iccv_2025_registration/,False,True,False
1ndajmq,Feuilius,1757500247.0,6,/r/MachineLearning/comments/1ndajmq/d_questions_on_fairness_and_expectations_in/,MachineLearning,[D] Questions on Fairness and Expectations in Top-Tier Conference Submissions,"Hello everyone,

I know that in this community there are many experienced researchers and even reviewers for top-tier conferences. As a young researcher, I sincerely hope to learn from your perspectives and get some clarity on a few concerns I‚Äôve been struggling with.

**My first question:**  
Does a research paper always need to achieve *state-of-the-art (SOTA)* results‚Äîoutperforming every existing method‚Äîto be accepted at an A\* conference? I often feel that so many published papers present dazzling results, making it nearly impossible for newcomers to surpass them.

**My second question, about fairness and accuracy in comparisons:**  
When evaluating a new method, is it acceptable to compare primarily against the most ‚Äúrelated,‚Äù ‚Äúsimilar,‚Äù or ‚Äúsame-family‚Äù methods rather than the absolute SOTA? For example:

* If I make a small modification to the Bagging procedure in Random Forest, would it be fair to compare only against other Bagging-based forests, rather than something fundamentally different like XGBoost (which is boosting-based)?
* Similarly, if I improve a variant of SVM, is it reasonable to compare mainly with other margin-based or kernel methods, instead of tree-based models like Decision Trees?

I understand that if my method only beats some similar baselines but does not surpass the global best-performing method, reviewers might see it as ‚Äúmeaningless‚Äù (since people naturally gravitate toward the top method). Still, I‚Äôd like to hear your thoughts: from an experienced researcher‚Äôs point of view, what is considered fair and convincing in such comparisons?

Thank you very much in advance for your time and advice.",5,0.69,https://www.reddit.com/r/MachineLearning/comments/1ndajmq/d_questions_on_fairness_and_expectations_in/,False,True,False
1ndaesz,Soft-Possibility2929,1757499778.0,4,/r/MachineLearning/comments/1ndaesz/d_sota_modern_alternative_to_bertscore/,MachineLearning,[D] SOTA modern alternative to BertScore?,"Hi everyone,  
I‚Äôm looking for an embedding-based metric to score text generation. BertScore is great, but it‚Äôs a bit outdated. Could you suggest some modern state-of-the-art alternatives?

",14,0.86,https://www.reddit.com/r/MachineLearning/comments/1ndaesz/d_sota_modern_alternative_to_bertscore/,False,True,False
1ncyf2r,United_Intention42,1757460069.0,2,/r/MachineLearning/comments/1ncyf2r/d_completed_amazon_ml_summer_school_2025_curious/,MachineLearning,[D] Completed Amazon ML Summer School 2025 curious who else attended?,"Hey everyone,  
I just completed¬†**Amazon ML Summer School 2025**¬†üéâ  
It was a month-long program covering a solid range of ML topics¬†***supervised/unsupervised learning, deep neural nets, generative AI & LLMs, RL, and even causal inference***.  
The sessions were intense but super rewarding. I feel like this experience gave me a strong foundation to explore advanced AI research and projects.

Curious if anyone here has also attended and how you re planning to apply what you learned?

https://preview.redd.it/b5ulzuq038of1.png?width=655&format=png&auto=webp&s=c328f24e6b674b9f576cebae727f44a526f185a9

",0,0.29,https://www.reddit.com/r/MachineLearning/comments/1ncyf2r/d_completed_amazon_ml_summer_school_2025_curious/,False,True,False
1ncu0na,Starscream-11813,1757449397.0,13,/r/MachineLearning/comments/1ncu0na/d_ijcnlpaacl_2025_paper_reviews_arr_july_2025/,MachineLearning,[D] IJCNLP-AACL 2025: Paper Reviews (ARR July 2025 Cycle),"The ARR July cycle reviews for AACL-IJCNLP 2025 just dropped.  
Feel free to share your thoughts and feelings! How did you do?",20,0.93,https://www.reddit.com/r/MachineLearning/comments/1ncu0na/d_ijcnlpaacl_2025_paper_reviews_arr_july_2025/,False,True,False
1ncrkpp,Sami10644,1757444072.0,9,/r/MachineLearning/comments/1ncrkpp/d_negative_r¬≤_on_unseen_dataset_despite_good/,MachineLearning,[D] Negative R¬≤ on unseen dataset despite good train/test performance,"I am working on a regression problem where I predict Pavement Condition Index (PCI) values from multi-sensor time-series data collected in the same region and under the same conditions. I have multiple sets of data from the same collection process, where I use some sets for training and testing and keep the remaining ones for evaluating generalization. Within the training and testing sets, the model performs well, but when I test on the held-out dataset from the same collection, the R¬≤ value often becomes negative , even though the mean absolute error and root mean square error remain reasonable. I have experimented with several feature engineering strategies, including section-based, time-based, and distance-based windowing, and I have tried using raw PCI data as well. I also tested different window lengths and overlap percentages, but the results remain inconsistent. I use the same data for a classification task, the models perform very well and generalize properly, yet for PCI regression, the generalization fails despite using the same features and data source. In some cases, removing features like latitude, longitude, or timestamps caused performance to drop significantly, which raises concerns that the model might be unintentionally relying on location and time information instead of learning meaningful patterns from sensor signals. I have also experimented with different models, including traditional machine learning and deep learning approaches, but the issue persists. I suspect the problem may be related to the variance of the target PCI values across datasets, potential data leakage caused by overlapping windows, or possibly a methodological flaw in how the evaluation is performed. I want to understand whether it is common in research to report only the R¬≤ values on the train/test splits from the same dataset, or whether researchers typically validate on entirely separate held-out sets as well. Given that classification on the same data works fine but regression fails to generalize, I am trying to figure out if this is expected behavior in PCI regression tasks or if I need to reconsider my entire evaluation strategy.",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1ncrkpp/d_negative_r¬≤_on_unseen_dataset_despite_good/,False,True,False
1ncemg3,ExtentBroad3006,1757411548.0,28,/r/MachineLearning/comments/1ncemg3/d_whats_the_most_frustrating_stuck_moment_youve/,MachineLearning,[D] What‚Äôs the most frustrating ‚Äústuck‚Äù moment you‚Äôve faced in an ML project?,"Curious about community experience: what‚Äôs the most painful ‚Äòstuck‚Äô moment you‚Äôve faced in an ML project (convergence, dataset issues, infra)?  
How did you eventually move past it, or did you abandon the attempt? Would be great to hear real war stories beyond published papers.",26,0.85,https://www.reddit.com/r/MachineLearning/comments/1ncemg3/d_whats_the_most_frustrating_stuck_moment_youve/,False,True,False
1ncdt5o,krychu,1757408387.0,8,/r/MachineLearning/comments/1ncdt5o/p_implementation_and_ablation_study_of_the/,MachineLearning,[P] Implementation and ablation study of the Hierarchical Reasoning Model (HRM): what really drives performance?,"I recently implemented the [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734) (HRM) for educational purposes and applied it to a simple pathfinding task. You can watch the model solve boards step by step in the generated animated GIF.

HRM is inspired by multi-timescale processing in the brain: a slower H module for abstract planning and a faster L module for low-level computation, both based on self-attention. HRM is an attempt to model reasoning in latent space.

To understand a bit better what drives the performance I ran a small ablation study. Key findings (full results in the README):

* The biggest driver of performance (both accuracy and refinement ability) is training with more segments (outer-loop refinement), not architecture.
* The two-timescale H/L architecture performs about the same as a single-module trained with BPTT.
* Notably, H/L still achieves good performance/refinement without full BPTT, which could mean cheaper training.

Repo: [https://github.com/krychu/hrm](https://github.com/krychu/hrm)

This is of course a limited study on a relatively simple task, but I thought the results might be interesting to others exploring reasoning models.

The findings line up with the ARC Prize team's analysis: [https://arcprize.org/blog/hrm-analysis](https://arcprize.org/blog/hrm-analysis)

Below two examples of refinement in action: early steps explore solution with rough guesses, later steps make smaller and smaller corrections until the full path emerges:

[20x20 board](https://i.redd.it/i1qi4l2vs3of1.gif)

[30x30 board](https://i.redd.it/j6fpueovs3of1.gif)

",56,0.98,https://www.reddit.com/r/MachineLearning/comments/1ncdt5o/p_implementation_and_ablation_study_of_the/,False,True,False
1ncceqw,Coffeee_addictt,1757402626.0,9,/r/MachineLearning/comments/1ncceqw/d_best_ocr_as_of_now/,MachineLearning,[D] Best ocr as of now,"I want to know which ocr has high accuracy and consumes less time for the extraction of data for given input images (especially tables), anything which works better than paddleocr?",20,0.99,https://www.reddit.com/r/MachineLearning/comments/1ncceqw/d_best_ocr_as_of_now/,False,True,False
1nc6r7l,AtharvBhat,1757383538.0,3,/r/MachineLearning/comments/1nc6r7l/project_otters_a_minimal_vector_search_library/,MachineLearning,[Project] Otters ü¶¶ - A minimal vector search library with powerful metadata filtering,"I'm excited to share something I've been working on for the past few weeks:

Otters ü¶¶ - A minimal vector search library with powerful metadata filtering powered by an ergonomic Polars-like expressions API written in Rust!

Why I Built This

In my day-to-day work, I kept hitting the same problem. I needed vector search with sophisticated metadata filtering, but existing solutions were either,
Too bloated (full vector databases when I needed something minimal for analysis)
Limited in filtering capabilities
Had unintuitive APIs that I was not happy about.

I wanted something minimal, fast, and with an API that feels natural - inspired by Polars, which I absolutely love.

What Makes Otters Different

Exact Search: Perfect for small-to-medium datasets (up to ~10M vectors) where accuracy matters more than massive scale.

 Performance: 
SIMD-accelerated scoring
Zonemaps and Bloom filters for intelligent chunk pruning

Polars-Inspired API: Write filters as simple expressions
```
meta_store.query(query_vec, Metric::Cosine)
    .meta_filter(col(""price"").lt(100) & col(""category"").eq(""books""))
    .vec_filter(0.8, Cmp::Gt)
    .take(10)
    .collect()
```

The library is in very early stages and there are tons of features that i want to add
Python bindings, NumPy support
Serialization and persistence
Parquet / Arrow integration
Vector quantization
etc.

I'm primarily a Python/JAX/PyTorch developer, so diving into rust programming has been an incredible learning experience.

If you think this is interesting and worth your time, please give it a try.
I welcome contributions and feedback !

üì¶ https://crates.io/crates/otters-rs
üîó https://github.com/AtharvBhat/otters",14,0.9,https://www.reddit.com/r/MachineLearning/comments/1nc6r7l/project_otters_a_minimal_vector_search_library/,False,True,False
1nc5jb5,ekkarpinski,1757380091.0,12,/r/MachineLearning/comments/1nc5jb5/r_llms_play_a_cooperative_card_game_coordination/,MachineLearning,"[R] LLMs play a cooperative card game, coordination without communication","One of my favorite card games is called The Crew, which is a trick-taking game (like hearts) but cooperative. There's no table talk allowed, players have to coordinate silently (with limited options for in-game communication) - figuring out what their teammates are doing and why, and what they need to do to work together. I wondered what SOTA LLMs would do if you asked them to play. To make this work, I implemented a backend for the game logic and structured outputs so models play by submitting moves and reasoning at each turn. 

Originally I wanted to re-create the 50 mission campaign, but models were so spotty on  mission 1 (the simplest possible mission) that I stuck to mission 1 and experimented with different configurations instead. I ran 8 OpenAI models on 10 different versions, ranging from very easy (random chance gets you there 2/3rds of the time) to very hard (random chance succeeds 0.5%), and gave each model ten trials on each mission.

What I've found out:

\* Smaller models struggle both with gameplay, and with understanding their role on the team. In these missions, a designated player (the commander) has to win a designated card. But these models hate having to lose a trick for the sake of their teammate, even when that's how they win the game.

[This does not \\""help him secure the win and fulfill his task.\\"" It loses the game.](https://preview.redd.it/3lqyqf3tg1of1.png?width=2030&format=png&auto=webp&s=b57c0a46fee169e14dbf6fc0cda107024a11a59e)

\* GPT-4o-mini (worst model so far) plays randomly on easy setups and worse than randomly on harder ones. GPT-4o-mini in particular loses the game in the first turn almost 90% of the time in harder setups with GPT-5-nano and GPT-4.1-mini are close behind at 60-70%. 

[GREEN 1 is the lowest GREEN card in the game, so playing it straight away actually guarantees immediate failure.](https://preview.redd.it/fx5jqyhug1of1.png?width=2046&format=png&auto=webp&s=da5d4abb5a7fcd4c1e8ee42c09d7acfb4a7ba5dc)

\* GPT-5 is self-aware enough to avoid the ""losing on the very first turn"" error, but actually did it on purpose once as a deliberate suicide when it saw that it couldn't win the game on the very first turn.

[There are multiple turns in the game!](https://preview.redd.it/91qnnfuvg1of1.jpg?width=1900&format=pjpg&auto=webp&s=ebc98a3fbf4381c4a95f7e96ec2fa96f8e84692f)

\* The harder missions - which require coordination across multiple turns - absolutely cook the smaller models with <10% win rates. Only GPT-5 is beating random chance on the harder missions (73% GPT-5 vs 4% random) 

\* GPT-5 also found optimal 1-trick solutions to a couple of setups I thought required at least two tricks. Oops. So in a sense, we're above human performance in some areas.

\* ...But most of the time, GPT-5 generally screwed around for 3 or more tricks in puzzles it could have solved in 1. This is like solving a mate in one chess puzzle in 3 moves. It's not losing, but it's not exactly showing a mastery of the game.

\* The lack of goal-oriented behavior (or risk-averse hesitation) on GPT-5's part means that GPT-5-mini actually performs better if we count speed (number of turns) to win as criteria and grade on optimal play (winning in the least number of turns, rather than just winning.)

I published the repo and did a write-up with some graphs and demos here: [https://ekkarpinski.github.io/LLMCrew/](https://ekkarpinski.github.io/LLMCrew/)

",44,0.92,https://www.reddit.com/r/MachineLearning/comments/1nc5jb5/r_llms_play_a_cooperative_card_game_coordination/,False,True,False
1nc1mxq,Acceptable_Army_6472,1757369783.0,3,/r/MachineLearning/comments/1nc1mxq/project_phishing_url_detection_with_random/,MachineLearning,[Project] Phishing URL detection with Random Forests and handcrafted features,"**\[Project\] Phishing URL detection with Random Forests on handcrafted features** 

I recently finished a project where I trained and deployed a phishing URL detector using **traditional ML techniques**. The goal was to explore how far a lightweight, interpretable model could go for this problem before moving to deep learning.

**Data & Features**

* Dataset: Combined PhishTank + Kaggle phishing URLs with Alexa top legitimate domains.
* Preprocessing: Removed duplicates, balanced classes, stratified train/test split.
* Features (hand-engineered):
   * URL length & token counts
   * Number of subdomains, ‚Äú@‚Äù usage, hyphens, digits
   * Presence of IP addresses instead of domains
   * Keyword-based flags (e.g., ‚Äúlogin‚Äù, ‚Äúsecure‚Äù)

**Model & Training**

* Algorithm: Random Forest (scikit-learn).
* Training: 80/20 split, 10-fold CV for validation.
* Performance: \~92% accuracy on test data.
* Feature importance: URL length, IP usage, and hyphen frequency were the strongest predictors.

**Takeaways**

* A simple RF + handcrafted features still performs surprisingly well on phishing detection.
* Interpretability (feature importances) adds practical value in a security context.
* Obvious limitations: feature set is static, adversaries can adapt.

**Future work (exploration planned)**

* Gradient boosting (XGBoost/LightGBM) for comparison.
* Transformers or CNNs on raw URL strings (to capture deeper patterns).
* Automating retraining pipelines with fresh phishing feeds.

**Repo:** [https://github.com/saturn-16/AI-Phishing-Detection-Web-App](https://github.com/saturn-16/AI-Phishing-Detection-Web-App)

Would love feedback on:

* What other URL features might improve detection?
* Have people here seen significant gains moving from RF/GBM ‚Üí deep learning for this type of task?",0,0.43,https://www.reddit.com/r/MachineLearning/comments/1nc1mxq/project_phishing_url_detection_with_random/,False,True,False
1nbsems,Senior-Let-7576,1757348869.0,13,/r/MachineLearning/comments/1nbsems/d_aaai_26_alignment_track/,MachineLearning,[D] AAAI 26 Alignment Track,Does anyone know whether they‚Äôre going to release the Phase 1 rejections today or on September 12?,14,0.94,https://www.reddit.com/r/MachineLearning/comments/1nbsems/d_aaai_26_alignment_track/,False,True,False
1nbr57g,Technical-Seesaw9383,1757346058.0,1,/r/MachineLearning/comments/1nbr57g/r_benchmarking_an_ml_service_in_python/,MachineLearning,[R] Benchmarking an ML service in python,"Recently, I needed to build an ML service that would be called by a latency-sensitive client. The requirements for load and latency were higher than what I had worked with in the past, so I wasn‚Äôt sure what to expect from my Python application.

I googled around and couldn‚Äôt find any concrete answers, so I wrote this brief article for anyone out there in a similar situation:

https://medium.com/@javiermas/benchmarking-an-ml-service-in-pytho-4238399d2229

I hope you find it useful!
",0,0.33,https://www.reddit.com/r/MachineLearning/comments/1nbr57g/r_benchmarking_an_ml_service_in_python/,False,True,False
1nbisbw,Anmol_garwal,1757322434.0,13,/r/MachineLearning/comments/1nbisbw/d_how_to_automate_parsing_of_bank_statement_pdfs/,MachineLearning,[D] How to Automate parsing of Bank Statement PDFs to extract transaction level data,"I am working on a project where I need to extract transaction data from Bank Statement PDFs. 80% of my working PDFs are digitally generated so to handle those I put the Regex approach, where I first extract the text into a txt file and then run Regex on this data to extract data in a meaningful format \[Date, Particulars, Credit/Debit amount, Balance\]. The challenge is that the Regex approach is brittle, and very sensitive to formats. So every bank requires a new Regex plus any little change in the format tomorrow by the bank will break the pipeline.

I want to make a pipeline which is agnostic to bank-format and is capable of extracting the info from the PDFs. I cannot use any 3rd party APIs as the bank data is sensitive and we want to keep everything on internal servers.

Hence, I have been exploring ways in Open Source models to built this pipeline. After doing some research, I landed on LayoutLMv3 Model which can essentially label the Tokens based on their location on the page so if we are able to train the model on our data it should be able to tag every token on the page and that should do it, but the challenge here is that this model is sensitive to reading order and fails on few bank formats.

Since then I have explored MinerU but that failed as well, it isolated the transaction content table but later failed to extract data in orderly fashion as it could not differentiate between multiple lines of transactions.

Now I am working with YOLOv8 which I am training to identify transaction rows and amount columns using BBox and then I will pull the info from these BBox intersection. But the confidence here is not very high.

Has anyone here faced similar challenge? Can anyone help me with some solution or approach. It would be a great help!

Know that the most of the PDFs don't have any defined table, it's just text hanging in air with lot of whitespace. I need a solve for Scanned PDFs as well \[integrated with OCR\]",3,0.59,https://www.reddit.com/r/MachineLearning/comments/1nbisbw/d_how_to_automate_parsing_of_bank_statement_pdfs/,False,True,False
1nbhqmq,Set-New,1757318283.0,17,/r/MachineLearning/comments/1nbhqmq/d_how_do_you_stay_current_with_aiml_research_and/,MachineLearning,[D] How do you stay current with AI/ML research and tools in 2025? (Cybersec engineer catching up after Transformers),"Hi everyone,

I‚Äôm a cybersecurity and network engineer/sysadmin by profession, but I studied AI/ML quite seriously at university. My knowledge is solid up until around the Transformer era (when attention-based models started becoming central), but I stopped following developments after that.

Now I‚Äôd like to get back into the field and stay current‚Äînot necessarily to publish research, but to understand new architectures, applications, and tools. In cybersecurity, I stay updated through curated blogs, newsletters, and professional communities. I‚Äôd like to adopt a similar approach for ML/AI.

For those of you who actively track progress:

* Which blogs, newsletters, or feeds do you find most useful?
* Are there particular researchers or labs whose updates you follow?
* Any books or surveys that bridge foundational knowledge with current trends?
* How do you cut through hype-heavy content and focus on signal?

I‚Äôd really appreciate hearing what works for you. The field moves incredibly fast, and I‚Äôd like to plug back in with a structured approach.

Thanks in advance!",102,0.94,https://www.reddit.com/r/MachineLearning/comments/1nbhqmq/d_how_do_you_stay_current_with_aiml_research_and/,False,True,False
1naz0eb,Lestode,1757265556.0,30,/r/MachineLearning/comments/1naz0eb/d_vibecoding_and_structure_when_writing_ml/,MachineLearning,[D] Vibe-coding and structure when writing ML experiments,"Hey!

For context, I'm a Master's student at ETH Z√ºrich. A friend and I recently tried writing a paper for a NeurIPS workshop, but ran into some issues.  
We had both a lot on our plate and probably used LLMs a bit too much. When evaluating our models, close to the deadline, we caught up on some bugs that made the data unreliable. We also had plenty of those bugs along the way. I feel like we shot ourselves in the foot but that's a lesson learned the way. Also, it made me realise the negative effects it could have had if those bugs had been kept uncaught.

I've been interning in some big tech companies, and so I have rather high-standard for clean code. Keeping up with those standards would be unproductive at our scale, but I must say I've struggled finding a middle ground between speed of execution and code's reliability.

For researchers on this sub, do you use LLMs at all when writing ML experiments? If yes, how much so? Any structure you follow for effective experimentation (writing (ugly) code is not always my favorite part)? When doing experimentation, what structure do you tend to follow w.r.t collaboration?

Thank you :)

",19,0.65,https://www.reddit.com/r/MachineLearning/comments/1naz0eb/d_vibecoding_and_structure_when_writing_ml/,False,True,False
1nanw9i,prabhjots665,1757233614.0,1,/r/MachineLearning/comments/1nanw9i/p_terra_code_cli_an_ai_coding_assistant_with/,MachineLearning,[P] Terra Code CLI ‚Äì An AI coding assistant with domain knowledge and semantic code search,"One limitation I‚Äôve noticed with most AI coding assistants is that they don‚Äôt really understand a team‚Äôs domain knowledge or architectural decisions.

To explore this, we built a small CLI project: Terra Code CLI. The idea was to see if an assistant could feel more like a senior developer who knows the org, rather than just autocomplete.

Things we experimented with:
‚Ä¢ Interactive Knowledge Transfer ‚Äì let senior devs ‚Äúteach‚Äù patterns
‚Ä¢ Semantic Code Search ‚Äì context-aware retrieval across repos
‚Ä¢ Persistent Memory ‚Äì standards remembered across projects
‚Ä¢ Domain Expertise ‚Äì ingesting architecture docs, API specs, etc.

We‚Äôre curious:
üëâ Has anyone here tried giving AI assistants persistent org-specific knowledge? Did it actually help productivity, or just add complexity?

For free quick start:

npm install -g @terra-code/terra-code

terra

For those interested, we‚Äôve open-sourced the CLI [ https://github.com/TerraAGI/terra-code-cli ]. There‚Äôs also a simple website which we will be updating with docs + install guide here: [ https://terra-agi.com/ ]. Currently in beta, so it‚Äôs free to use.",5,0.69,https://www.reddit.com/r/MachineLearning/comments/1nanw9i/p_terra_code_cli_an_ai_coding_assistant_with/,False,True,False
1namvsk,OkOwl6744,1757229675.0,48,/r/MachineLearning/comments/1namvsk/why_language_models_hallucinate_openai_pseudo/,MachineLearning,Why Language Models Hallucinate - OpenAi pseudo paper - [D],"Hey
Anybody read this ? It seems rather obvious and low quality, or am I missing something ? 

https://openai.com/index/why-language-models-hallucinate/

‚ÄúAt OpenAI, we‚Äôre working hard to make AI systems more useful and reliable. Even as language models become more capable, one challenge remains stubbornly hard to fully solve: hallucinations. By this we mean instances where a model confidently generates an answer that isn‚Äôt true. Our new research paper‚Å†(opens in a new window) argues that language models hallucinate because standard training and evaluation procedures reward guessing over acknowledging uncertainty.
ChatGPT also hallucinates. GPT‚Äë5 has significantly fewer hallucinations especially when reasoning‚Å†, but they still occur. Hallucinations remain a fundamental challenge for all large language models, but we are working hard to further reduce them.‚Äù",111,0.91,https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf,False,False,False
1nahnmz,absurdistonvacation,1757211884.0,5,/r/MachineLearning/comments/1nahnmz/d_thought_experiment_rolling_without_slipping_as/,MachineLearning,[D] Thought experiment: ‚ÄúRolling without slipping‚Äù as a blueprint for nD‚Üí(n‚àí1) embeddings?,"I came across the recent ROLLING HONED paper (designing 3D shapes that, when rolling without slipping, trace arbitrary 2D paths). It got me thinking:

In 3D, rolling constraints let you encode a 2D trajectory into the geometry of a 3D body.

In principle, in 4D you could imagine a convex hypersurface rolling on a 3D hyperplane, tracing out a 3D trajectory.

More generally: could there be a systematic way to map nD data into (n‚àí1)D dynamics via such constraints?

I know in ML we already have PCA, autoencoders, product quantization, etc. ‚Äî and those actually preserve metrics we care about. My hunch is that this ‚Äúmechanical embedding‚Äù idea probably fails the usefulness test for similarity search (no guarantee of inner product preservation).

But still:

Does the analogy make any theoretical sense in higher dimensions (rolling manifolds w/o slip/twist)?

Could there be hidden value in treating ‚Äúconstrained dynamics‚Äù as a new kind of coding scheme?

Or am I over-romanticizing a neat geometric trick after too much late-night reading?

Curious what the community thinks ‚Äî is there any research potential here, or should I file this under ‚Äúfun alcohol-fueled metaphors‚Äù and move on?
",7,0.7,https://www.reddit.com/r/MachineLearning/comments/1nahnmz/d_thought_experiment_rolling_without_slipping_as/,False,True,False
1naejuk,Artoriuz,1757202709.0,9,/r/MachineLearning/comments/1naejuk/d_the_apparent_randomness_of_residual_block_design/,MachineLearning,[D] The apparent randomness of residual block design,"Skip connections and residual blocks have been ubiquitous in the ML field ever since the original ResNets were published. I think it's fair to say most people agree skip connections help, but at a glance, the design of the residual blocks themselves is still something that differs from paper to paper.

The most recent ""innovation"" is splitting channel mixing from spatial mixing, which is what ConvNeXt does in an attempt to mimic transformers. Other models that also claim SotA-ish performance, however, do not necessarily follow suit. NFNet, for example, employs grouped 3x3 convolution layers, good old normal bottlenecks (not inverted) and channel attention (Squeeze-and-Excitation).

If we look at modern LLMs, they all have residual blocks that look very similar, but with one or two minor differences that often look arbitrary.

I think residual block design is one of those things that people don't really pay much attention to since it generally works well enough regardless of what you do, but at some point it does look like we're just making semi-random decisions based on semi-random observations. Why the block is designed in the way it is is rarely a point of concern.

I've tried looking for papers making direct comparisons between different design choices, but I couldn't really find anything conclusive.

",70,0.97,https://www.reddit.com/r/MachineLearning/comments/1naejuk/d_the_apparent_randomness_of_residual_block_design/,False,True,False
1na5ixj,pmv143,1757179986.0,9,/r/MachineLearning/comments/1na5ixj/dbaseten_raises_150m_series_d_for_inference_infra/,MachineLearning,[D]Baseten raises $150M Series D for inference infra. where‚Äôs the real bottleneck?,"Baseten just raised $150M Series D at a $2.1B valuation. They focus on inference infra  like low latency serving, throughput optimization, developer experience.

They‚Äôve shared benchmarks showing their embeddings inference outperforms vLLM and TEI, especially on throughput and latency. The bet is that inference infra is the pain point, not training.

But this raises a bigger question. what‚Äôs the real bottleneck in inference?
	‚Ä¢Baseten and others (Fireworks, Together) are competing on latency + throughput.
	‚Ä¢Some argue the bigger cost sink is cold starts and low GPU utilization , serving multiple models elastically without waste is still unsolved at scale.

I wonder what everyone thinks 

	‚Ä¢Will latency/throughput optimizations be enough to differentiate?
	‚Ä¢Or is utilization (how efficiently GPUs are used across workloads) the deeper bottleneck?
	‚Ä¢Does inference infra end up commoditized like training infra, or is there still room for defensible platforms?
",1,0.52,https://www.reddit.com/r/MachineLearning/comments/1na5ixj/dbaseten_raises_150m_series_d_for_inference_infra/,False,True,False
1n9xg20,local___host,1757158986.0,1,/r/MachineLearning/comments/1n9xg20/d_online_hierarchical_clustering_for_news_how_to/,MachineLearning,[D] Online hierarchical clustering for news: how to keep event IDs stable under merges/splits in a streaming pipeline?,"I‚Äôm building a news ingestion system (currently Poland-focused; designed to scale) that clusters incoming articles into ‚Äúevents‚Äù powering maps and graph views. Pipeline: embeddings ‚Üí cosine HAC with a fixed threshold ‚Üí periodic (5min) recluster. Granularity, time decay, and summarization are fine, my sole pain point is¬†*stable event identity*¬†in a streaming setting.

As new articles arrive, clusters should sometimes merge (a legitimate bridge appears) or split (bridge was spurious). I need user-facing event IDs to persist through these transitions, i.e., minimize label churn across snapshots while respecting the hierarchical/threshold constraints.

**Question:**¬†What‚Äôs the best-known algorithmic approach (and any open-source references) for¬†*evolutionary/streaming hierarchical clustering with persistent labels*, explicitly merge/split-aware, that¬†*minimizes an inter-snapshot ID-churn* *penalty*¬†under latency constraints?",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1n9xg20/d_online_hierarchical_clustering_for_news_how_to/,False,True,False
1n9wnel,Nearby_Reaction2947,1757156231.0,5,/r/MachineLearning/comments/1n9wnel/p_an_opensource_pipeline_for_speechtospeech/,MachineLearning,[P] An Open-Source Pipeline for Speech-to-Speech Translation with Voice Preservation (RVC) and Lip-Sync,"Hello¬†[r/MachineLearning](https://www.reddit.com/r/MachineLearning/),

I'm a final-year undergrad exploring multimodal systems, and I wanted to share a project I've built and open-sourced. It‚Äôs an end-to-end pipeline designed to tackle video dubbing for low-resource languages, using Telugu as the initial target. The system translates speech from an English video while preserving the original speaker's vocal identity and syncing their lips to the new audio.

* **GitHub Repo:**¬†[\[GitHub\]](https://github.com/M-SRIKAR-VARDHAN/speech-to-speech-with-lipsync)
* **Full Technical Write-up:**¬†[\[writeup\]](https://medium.com/@srikarvardhan2005/speech-to-speech-translation-with-lip-sync-425d8bb74530)
* **Demo Video:**¬†[\[Demo\]](https://drive.google.com/drive/folders/1l6jZEDdmUzr9VhfYkvoVdaXJSSipN-nm?usp=sharing)

The core technical challenge was achieving voice preservation without access to large, speaker-specific datasets typically required for high-fidelity voice cloning. After a dead-end attempting a direct S2S architecture inspired by Translatotron, I found that using Retrieval-based Voice Conversion (RVC) as a post-processing step on a generic TTS output was a surprisingly practical and data-efficient solution.

The final pipeline is structured as follows:

1. **ASR:**¬†Whisper for robust transcription.
2. **NMT:**¬†Meta's NLLB for English-to-Telugu translation.
3. **TTS:**¬†Meta's MMS model to synthesize the base Telugu audio.
4. **Voice Conversion:**¬†A trained RVC model converts the timbre of the synthetic speech to match the original speaker.
5. **Lip Sync:**¬†Wav2Lip aligns the video frames to the new audio.

My main takeaway is that RVC seems to function as a very effective ""style transfer"" layer for voice, making it a viable tool for projects where full voice cloning is computationally or data-prohibitive.

I'm sharing this to start a discussion and get feedback from the community on this approach. I'm particularly curious about two points:

1. Has anyone else experimented with using RVC in a more formal pipeline, and what were the qualitative limitations you encountered?
2. Are there newer or more robust alternatives to Wav2Lip for lip-syncing that maintain good performance without requiring massive computational resources?

Any thoughts on the architecture or suggestions for improvement would be highly appreciated. Thank you for your time.",2,1.0,https://www.reddit.com/r/MachineLearning/comments/1n9wnel/p_an_opensource_pipeline_for_speechtospeech/,False,True,False
1n9ufsf,Confident-Meal3457,1757147760.0,4,/r/MachineLearning/comments/1n9ufsf/p_knowledge_distillation_for_texttosql_training/,MachineLearning,[P] Knowledge Distillation for Text-to-SQL ‚Äî Training GPT-2 with Qwen2-7B as Teacher,"Hey folks,

I‚Äôve been working on an experiment that combines¬†**Knowledge Distillation (KD)**¬†with the¬†**Text-to-SQL problem**, and I wanted to share the results + repo with the community.

# üéØ Motivation

* Natural language ‚Üí SQL is a powerful way for¬†**non-technical users**¬†to query databases without always relying on analysts.
* Most solutions use massive LLMs (GPT-4.1, etc.), but they‚Äôre¬†**expensive**,¬†**hard to deploy locally**, and raise¬†**data privacy concerns**.
* So the question I asked:¬†*Can a much smaller model (like GPT-2) be trained to generate SQL for a given DB effectively if it learns from a bigger LLM?*

# üß† Approach

I used¬†**Knowledge Distillation (KD)**¬†‚Äî i.e., transferring knowledge from a large teacher model into a smaller student model.

* **Teacher Model**:¬†[Qwen2-7B]()
* **Student Model**:¬†[GPT-2]()

Steps:

1. Built a¬†**custom dataset**¬†‚Üí pairs of (natural language query, SQL query) for a toy retail database schema.
2. Teacher (Qwen2-7B) generates SQL from the queries.
3. Student (GPT-2) is trained on two signals:
   * **Cross-Entropy Loss (75%)**¬†‚Üí match ground-truth SQL.
   * **MSE Loss (25%)**¬†‚Üí align with the teacher‚Äôs hidden state values (projected from teacher‚Äôs layer 25).
4. Trained for¬†**20 epochs on Colab GPU**.

# ‚öôÔ∏è Training Setup

* Teacher hidden states projected ‚Üí aligned with GPT-2‚Äôs final hidden states.
* Loss =¬†**0.75 \* CE + 0.25 \* MSE**.
* Achieved¬†**total loss \~0.21**¬†after training.

# üìä Results

* GPT-2 (student) was able to¬†**generate SQL queries directly from natural language**¬†for the schema.
* While not perfect (due to limited resources at my disposal), it showed that **small models can be viable for domain-specific SQL generation**¬†when trained this way.
* Benefits:
   * ‚ö° Lightweight (runs locally).
   * üí∏ Cost-efficient.
   * üîê More privacy-friendly than cloud-only LLM APIs.

# üì∑ Visuals in the repo:

* Schema diagram (retail DB).
* Teacher ‚Üí Student distillation architecture.
* Sample outputs (NL ‚Üí SQL).

# üìé Repo

Code + diagrams + outputs are here:  
üëâ¬†[GitHub: Knowledge Distillation for SQL generation on GPT-2](https://github.com/Gokul-GMenon/Knowledge_Distillation-SQL_generation_on_gpt_2?utm_source=chatgpt.com)

Would love feedback, suggestions, or discussions on:

* Other lightweight models worth trying as students (LLaMA-7B distilled further? Phi-2?).
* Improvements to the KD setup (layer selection, different projection strategies).
* Extensions: applying this to more complex schemas / real enterprise DBs.

Cheers!

  
Can follow me in [LinkedIn](https://www.linkedin.com/in/gokul-g-menon/) as well for discussions",4,0.61,https://www.reddit.com/r/MachineLearning/comments/1n9ufsf/p_knowledge_distillation_for_texttosql_training/,False,True,False
1n9spn2,Forsaken-Order-7376,1757141135.0,6,/r/MachineLearning/comments/1n9spn2/d_advice_on_handling_completely_incorrect_review/,MachineLearning,[D] Advice on handling completely incorrect review?,"Recently submitted a paper to WACV 2026. Two of the three reviews are positive. The third recommends rejection, citing items as ‚Äúmissing‚Äù that are actually in the paper (2nd page dude) and claiming our architecture is identical to a 2022 model, though there are clear differences- moreover, the performances tend to drastically differ as showcased in the results.

What are the typical options in this situation? He seems to be inclined towards finding ""excuses"" for rejecting paper (not sure why) and thereby I doubt a rebuttal will help. Can I ask the AC to get the reviewer replaced?",12,0.74,https://www.reddit.com/r/MachineLearning/comments/1n9spn2/d_advice_on_handling_completely_incorrect_review/,False,True,False
1n9m5hv,Specialist_Clock_368,1757119886.0,2,/r/MachineLearning/comments/1n9m5hv/d_seeking_arxiv_endorsement/,MachineLearning,[D] Seeking arXiv endorsement,"Hi All

I‚Äôm preparing to submit to arXiv in Experimentation. Since this is my first submission, I need an endorsement.

The draft is ready and I can share it upon request. Thanks! 
",0,0.13,https://www.reddit.com/r/MachineLearning/comments/1n9m5hv/d_seeking_arxiv_endorsement/,False,True,False
1n9hnq9,KeyIsNull,1757107846.0,9,/r/MachineLearning/comments/1n9hnq9/d_anyone_successful_with_training_lora_for_visual/,MachineLearning,[D] Anyone successful with training LoRA for visual LLMs on a multi-GPU setup?,"Hello sub,

I'm trying to train a LoRA for Llama 3.2 90B Visual Instruct on a 8xA100 cluster but I cannot find a framework/package that supports it.

Model is of course too large to fit into a single A100, so the only way is to leverage multiple device.

Unsloth does not support multi GPU training (at least in its open version)  
Axtol has multimodal models in beta

Was any of you successful into training multimodal models of this size? I'd appreciate any kind of feedback.",10,0.87,https://www.reddit.com/r/MachineLearning/comments/1n9hnq9/d_anyone_successful_with_training_lora_for_visual/,False,True,False
1n9ecmj,DeeplyConvoluted,1757099947.0,3,/r/MachineLearning/comments/1n9ecmj/d_anyone_attending_eusipco_next_week/,MachineLearning,[D]  Anyone attending EUSIPCO next week?,"Anyone attending EUSIPCO in Palermo next week? Unfortunately, none of my labmates will be able to travel, so would be cool to meet new people from here !",6,1.0,https://www.reddit.com/r/MachineLearning/comments/1n9ecmj/d_anyone_attending_eusipco_next_week/,False,True,False
1n95etu,Says_Watt,1757079205.0,3,/r/MachineLearning/comments/1n95etu/d_reversed_born_again_network_because_its_easier/,MachineLearning,"[D] Reversed born again network because it's easier to train, is this stupid?","I want to implement this paper: [https://arxiv.org/pdf/1805.04770](https://arxiv.org/pdf/1805.04770)

but I'm not excited about having to manage the student models / save them independently and also there's the issue of cost because we'd have to train each student model from scratch.

To get around this I was thinking I could just do the inverse: train the teacher model and derive ""dark knowledge"" based on the ""incorrect"" logits of the last checkpoint.

What I mean is can I have a training loop similar to the following

    for epoch in range(10):
      student = teacher.clone()
      student.requires_grad_(False) # the student deliberately does not learn, only the teacher learns
      for data in dataset:
        optim.zero_grad()
        teacher_logits = teacher(data.input)
        student_logits = student(data.input)
        loss_cross_entropy = cross_entropy(teacher_logits, data.label)
        loss_dark_knowledge = cross_entropy(teacher_logits - student_logits, data.label)
        loss = (loss_cross_entropy + loss_dark_knowledge) / 2
        loss.backward()
        optim.step()

is this dumb?",4,0.67,https://www.reddit.com/r/MachineLearning/comments/1n95etu/d_reversed_born_again_network_because_its_easier/,False,True,False
1n947jj,Pitiful-Ad8345,1757076094.0,12,/r/MachineLearning/comments/1n947jj/p_i_was_wrong_about_complex_ml_solutions_gower/,MachineLearning,[P] I Was Wrong About Complex ML Solutions - Gower Distance Beat My UMAP Approach,"Four years ago, I built [DenseClus ](https://github.com/awslabs/amazon-denseclus)for mixed-data clustering using dual UMAP embeddings. After reflecting on the Zen of Python (""simple is better than complex""), I realized I was overengineering.

Gower (1971) computes distances for mixed categorical/numerical data using weighted averages of appropriate metrics. Despite being 50+ years old, it often outperforms complex embeddings for small-to-medium datasets.

The implementation I coded (with Claude's help) saw a 20% speedup, 40% in memory, has GPU support (CuPy) and Sklearn integration.

Code: [https://github.com/momonga-ml/gower-express](https://github.com/momonga-ml/gower-express)

Blog post with analysis: [https://charles-frenzel.medium.com/i-was-wrong-start-simple-then-move-to-more-complex-5e2f40765481](https://charles-frenzel.medium.com/i-was-wrong-start-simple-then-move-to-more-complex-5e2f40765481)

**Discussion**:  When do you choose simple, interpretable methods over deep embeddings? Have others found similar success reverting to classical approaches?",21,0.72,https://www.reddit.com/r/MachineLearning/comments/1n947jj/p_i_was_wrong_about_complex_ml_solutions_gower/,False,True,False
1n8ynn2,Tanmay__13,1757056865.0,12,/r/MachineLearning/comments/1n8ynn2/p_i_built_a_convolutional_neural_network_that/,MachineLearning,[P] I Built a Convolutional Neural Network that understands Audio,"Hi everyone, I am sharing a project that I built recently, I trained a convolutional neural network (CNN) based on a¬†ResNet‚Äë34 style residual architecture¬†to classify audio clips from the¬†ESC‚Äë50 dataset¬†(50 environmental sound classes). I used log‚Äìmel spectrograms as input, reached strong accuracy and generalization with residual blocks, and packaged the model with dropout and adaptive average pooling for robustness. Would love to get your opinions on it.  Check it out -->¬†[https://sunoai.tanmay.space](https://sunoai.tanmay.space/)

Read the blog -->¬†[https://tanmaybansal.hashnode.dev/sunoai](https://tanmaybansal.hashnode.dev/sunoai)",2,0.53,https://www.reddit.com/r/MachineLearning/comments/1n8ynn2/p_i_built_a_convolutional_neural_network_that/,False,True,False
1n918yb,CaptainBudy,1757066980.0,0,/r/MachineLearning/comments/1n918yb/p_dcnv2_update_compatibility_pytorch_280/,MachineLearning,[P] DCNv2 (Update Compatibility) Pytorch 2.8.0,"Hello Reddit,

Working on several project I had to use the DCNv2 for different models I tweak it a little bit to work under the most recent CUDA version I had on my computer. There is probably some changes to make but currently it seems to work on my models training under CUDA 12.8 + Pytorch 2.8.0 configuration still haven't tested the retrocompatibility if anyone would like to give it a try.

Feel free to use it for training model like YOLACT+, FairMOT or others.

[https://github.com/trinitron620/DCNv2-CUDA12.8/tree/main](https://github.com/trinitron620/DCNv2-CUDA12.8/tree/main)",7,1.0,https://www.reddit.com/r/MachineLearning/comments/1n918yb/p_dcnv2_update_compatibility_pytorch_280/,False,True,False
1n8po18,jonas__m,1757028636.0,8,/r/MachineLearning/comments/1n8po18/r_the_illusion_of_progress_reevaluating/,MachineLearning,[R] The Illusion of Progress: Re-evaluating Hallucination Detection in LLMs,"Curious what folks think about this paper: [https://arxiv.org/abs/2508.08285](https://arxiv.org/abs/2508.08285)  
  
In my own experience in hallucination-detection research, the other popular benchmarks are also low-signal, even the ones that don't suffer from the flaw highlighted in this work.

Other common flaws in existing benchmarks:

\- Too synthetic, when the aim is to catch real high-stakes hallucinations in production LLM use-cases.

\- Full of incorrect annotations regarding whether each LLM response is correct or not, due to either low-quality human review or just relying on automated LLM-powered annotation.

\- Only considering responses generated by old LLMs, which are no longer representative of the type of mistakes that modern LLMs make.  
  
I think part of the challenge in this field is simply the overall difficulty of proper Evals.  For instance, Evals are much easier in multiple-choice / closed domains, but those aren't the settings where LLM hallucinations pose the biggest concern",33,0.94,https://www.reddit.com/r/MachineLearning/comments/1n8po18/r_the_illusion_of_progress_reevaluating/,False,True,False
1n8lvz5,Infinite_Explosion,1757019202.0,35,/r/MachineLearning/comments/1n8lvz5/d_how_do_you_read_code_with_hydra/,MachineLearning,[D] How do you read code with Hydra,"[Hydra](https://hydra.cc/) has become a very popular in machine learning projects. I understand the appeal, it makes configurations modular, allows you to reuse some parts of it while changing another. It makes the code more reusable and modular too and if you understand all of it its better structured.

My big problem is it makes it damn well near impossible to read someone else's code since every part of the code is now some mysterious implicit thing that gets instantiated from a string in the config file during execution. The problem would be alleviated if there was a way of quickly accessing the definition of the object that will get instantiated at runtime at least with the default values of the config. Is there a plugin that does that? If not, how do you guys do it ?",85,0.94,https://www.reddit.com/r/MachineLearning/comments/1n8lvz5/d_how_do_you_read_code_with_hydra/,False,True,False
1n83e6e,baddie_spotted,1756969887.0,0,/r/MachineLearning/comments/1n83e6e/d_performance_overhead_of_running_ml_inference_in/,MachineLearning,[D] Performance overhead of running ML inference in hardware-isolated environments - production metrics,"Been collecting data on ML inference performance in trusted execution environments and thought the numbers might be useful for others dealing with similar constraints.

**Context:** Fraud detection models processing ~10M daily transactions, needed hardware-level isolation for compliance reasons.

After 3 months of production data, seeing 5-8% performance overhead compared to standard deployment. This is way better than the 30-40% overhead reported in older papers about SGX.

The interesting technical challenge was memory management. TEE environments have strict memory limits and different allocation patterns than standard containers. Had to completely rewrite our batching logic - what worked fine with dynamic batching in regular pods caused constant OOM errors in enclaves.

**Model optimization discoveries:**

- ONNX runtime worked, pytorch was too memory heavy
- Preprocessing became the bottleneck, not inference
- Had to keep models under 8GB total memory
- P95 latency went from 12ms to 13ms

Tried multiple approaches including raw SGX implementation and phala's abstraction layer. The attestation complexity alone makes raw implementation painful.

**For those working on similar problems:**
Profile your entire pipeline, not just model inference. Data transformation overhead in isolated environments is real.

**Technical question for the community:** 
How are you handling model updates in TEE environments? The attestation requirements make standard blue-green deployments complicated. Currently doing full enclave restarts but that means brief downtime.

Also curious if anyone's tried running transformer models larger than 1B params in TEE. Memory constraints seem prohibitive but maybe there are tricks I'm missing?",2,0.67,https://www.reddit.com/r/MachineLearning/comments/1n83e6e/d_performance_overhead_of_running_ml_inference_in/,False,True,False
1n7oh8p,WildAppearance2153,1756927846.0,0,/r/MachineLearning/comments/1n7oh8p/p_arbitrary_order_automatic_differentiation_for/,MachineLearning,[P] Arbitrary Order Automatic Differentiation for PyTorch,"I‚Äôm excited to present **thoad** (short for Py**T**orch **H**igh **O**rder **A**utomatic **D**ifferentiation), a Python only library that computes arbitrary order partial derivatives directly on a PyTorch computational graph. The package has been developed within a bachelor's research project at Universidad Pontificia de Comillas - ICAI, and we are considering publishing a future academic article reviewing the mathematical details and the implementation design.

At its core, thoad takes a one output, many inputs view of the graph and pushes high order derivatives back to the leaf tensors. Although a 1‚ÜíN problem can be rewritten as 1‚Üí1 by concatenating flattened inputs, as in functional approaches such as `jax.jet` or `functorch`, thoad‚Äôs graph aware formulation enables:

* Working with smaller **pieced external derivatives**
* An optimization based on **unifying independent dimensions** (especially batch).

This delivers **asymptotically better scaling** with respect to order and batch size (respectively).

Additionally, we compute derivatives with a *vectorial* approach rather than component by component, which makes our pure PyTorch implementation possible. Consequently, the implementation stays at a high level, written entirely in Python and using **PyTorch** as its only dependency. Avoiding custom C++ or CUDA has a very positive impact on the long-term maintainability of the package.

The package is already available to be installed from **GitHub** or **PyPI**:

* GitHub: [https://github.com/mntsx/thoad](https://github.com/mntsx/thoad)

In our benchmarks, thoad **outperforms** torch.autograd for **Hessian calculations even on CPU**. See the repository *examples/benchmarks* to check the comparisons and run them in your own hardware.

**thoad** is designed to align closely with PyTorch‚Äôs interface philosophy, so running the high order backward pass is practically indistinguishable from calling PyTorch‚Äôs own `backward`. When you need finer control, you can keep or reduce Schwarz symmetries, group variables to restrict mixed partials, and fetch the exact mixed derivative you need. Shapes and independence metadata are also exposed to keep interpretation straightforward.

# USING THE PACKAGE

**thoad** exposes two primary interfaces for computing high-order derivatives:

1. `thoad.backward`: a function-based interface that closely resembles `torch.Tensor.backward`. It provides a quick way to compute high-order gradients without needing to manage an explicit controller object, but it offers only the core functionality (derivative computation and storage).
2. `thoad.Controller`: a class-based interface that wraps the output tensor‚Äôs subgraph in a controller object. In addition to performing the same high-order backward pass, it gives access to advanced features such as fetching specific mixed partials, inspecting batch-dimension optimizations, overriding backward-function implementations, retaining intermediate partials, and registering custom hooks.

Example of autodifferentiation execution via `thoad.backward`

    import torch
    import thoad
    from torch.nn import functional as F
    
    #### Normal PyTorch workflow
    X = torch.rand(size=(10,15), requires_grad=True)
    Y = torch.rand(size=(15,20), requires_grad=True)
    Z = F.scaled_dot_product_attention(query=X, key=Y.T, value=Y.T)
    
    #### Call thoad backward
    order = 2
    thoad.backward(tensor=Z, order=order)
    
    #### Checks
    ## check derivative shapes
    for o in range(1, 1 + order):
       assert X.hgrad[o - 1].shape == (Z.numel(), *(o * tuple(X.shape)))
       assert Y.hgrad[o - 1].shape == (Z.numel(), *(o * tuple(Y.shape)))
    ## check first derivatives (jacobians)
    fn = lambda x, y: F.scaled_dot_product_attention(x, y.T, y.T)
    J = torch.autograd.functional.jacobian(fn, (X, Y))
    assert torch.allclose(J[0].flatten(), X.hgrad[0].flatten(), atol=1e-6)
    assert torch.allclose(J[1].flatten(), Y.hgrad[0].flatten(), atol=1e-6)
    ## check second derivatives (hessians)
    fn = lambda x, y: F.scaled_dot_product_attention(x, y.T, y.T).sum()
    H = torch.autograd.functional.hessian(fn, (X, Y))
    assert torch.allclose(H[0][0].flatten(), X.hgrad[1].sum(0).flatten(), atol=1e-6)
    assert torch.allclose(H[1][1].flatten(), Y.hgrad[1].sum(0).flatten(), atol=1e-6)

Example of autodifferentiation execution via `thoad.Controller`

    import torch
    import thoad
    from torch.nn import functional as F
    
    #### Normal PyTorch workflow
    X = torch.rand(size=(10,15), requires_grad=True)
    Y = torch.rand(size=(15,20), requires_grad=True)
    Z = F.scaled_dot_product_attention(query=X, key=Y.T, value=Y.T)
    
    #### Instantiate thoad controller and call backward
    order = 2
    controller = thoad.Controller(tensor=Z)
    controller.backward(order=order, crossings=True)
    
    #### Fetch Partial Derivatives
    ## fetch T0 and T1 2nd order derivatives
    partial_XX, _ = controller.fetch_hgrad(variables=(X, X))
    partial_YY, _ = controller.fetch_hgrad(variables=(Y, Y))
    assert torch.allclose(partial_XX, X.hgrad[1])
    assert torch.allclose(partial_YY, Y.hgrad[1])
    ## fetch cross derivatives
    partial_XY, _ = controller.fetch_hgrad(variables=(X, Y))
    partial_YX, _ = controller.fetch_hgrad(variables=(Y, X))

>NOTE. A more detailed user guide with examples and feature walkthroughs is available in the notebook: [https://github.com/mntsx/thoad/blob/master/examples/user\_guide.ipynb](https://github.com/mntsx/thoad/blob/master/examples/user_guide.ipynb)",6,0.88,https://www.reddit.com/r/MachineLearning/comments/1n7oh8p/p_arbitrary_order_automatic_differentiation_for/,False,True,False
1n7brbk,Any_Commercial7079,1756897413.0,0,/r/MachineLearning/comments/1n7brbk/p_sentiment_analysis_model_for_cloud_services/,MachineLearning,[P] Sentiment Analysis Model for cloud services,"Hi all! Some time ago, I asked for help with a survey on ML/AI compute needs. After limited responses, I built a model that parses ML/cloud subreddits and applies BERT-based aspect sentiment analysis to cloud providers (AWS, Azure, Google Cloud, etc.). It classifies opinions by key aspects like cost, scalability, security, performance, and support.

I‚Äôm happy with the initial results, but I‚Äôd love advice on making the interpretation more precise:

Ensuring sentiment is directed at the provider (not another product/entity mentioned)  
Better handling of comparative or mixed statements (e.g., ‚Äúfast but expensive‚Äù)  
Improving robustness to negation and sarcasm

If you have expertise in aspect/target-dependent sentiment analysis or related NLP tooling, I‚Äôd really appreciate your input.

Repo:¬†[https://github.com/PatrizioCugia/cloud-sentiment-analyzer](https://github.com/PatrizioCugia/cloud-sentiment-analyzer)  
  
It would also be great if you could answer my original survey:¬†[https://survey.sogolytics.com/r/vTe8Sr](https://survey.sogolytics.com/r/vTe8Sr)

Thanks!",12,1.0,https://www.reddit.com/r/MachineLearning/comments/1n7brbk/p_sentiment_analysis_model_for_cloud_services/,False,True,False
1n77v29,Turbulent_Visual_948,1756882629.0,18,/r/MachineLearning/comments/1n77v29/acl_rolling_recview_is_the_most_garbage/,MachineLearning,Acl rolling recview is the most garbage conference to submit your papers [R],"You will find the most generic AI generated reviews in ARR. 
Waste of time. Submit to AI conferences. 
ARR is dead",10,0.61,https://www.reddit.com/r/MachineLearning/comments/1n77v29/acl_rolling_recview_is_the_most_garbage/,False,True,False
1n77fsw,akshitsharma1,1756881007.0,96,/r/MachineLearning/comments/1n77fsw/d_wacv_2026_paper_reviews/,MachineLearning,[D] WACV 2026 Paper Reviews,"WACV Reviews are supposed to be released by today EOD. Creating a discussion thread to discuss among ourselves, thanks!",48,0.87,https://www.reddit.com/r/MachineLearning/comments/1n77fsw/d_wacv_2026_paper_reviews/,False,True,False
1n75xxx,Impossible_Tutor_824,1756875610.0,0,/r/MachineLearning/comments/1n75xxx/r_practical_tee_deployment_for_sensitive_research/,MachineLearning,[R] Practical TEE deployment for sensitive research datasets - lessons from our lab,"
Posting this because I wish someone had done the same when we started. Our lab needed to work with industry partners on sensitive datasets but legal restrictions meant we couldn't access the raw data.

Traditional methods like differential privacy added too much noise for our research goals. Synthetic data was useless for our specific use case.

What went good for us: deploying our models in trusted execution environments. Partners felt comfortable because data never left their control. We could iterate on models without seeing actual data values.

Tech setup through phala network was surprisingly direct. Only difficulty was adapting our workflow since you can't just print tensors to debug anymore. Had to get creative with logging aggregate statistics.

Unexpected: our industry partnerships increased 3x because companies that previously wouldn't share data are now willing to collaborate. Turns out the privacy barrier was bigger than we realized.

If your research is stuck due to data access issues definitely worth exploring TEE options. Happy to share our deployment scripts if useful.",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1n75xxx/r_practical_tee_deployment_for_sensitive_research/,False,True,False
1n71dzv,OkOwl6744,1756861959.0,15,/r/MachineLearning/comments/1n71dzv/a_friendly_starter_paper_entropyguided_loop/,MachineLearning,A friendly starter paper - Entropy-Guided Loop: Achieving Reasoning through Uncertainty-Aware Generation [R],"Hey r/MachineLearning 

I had this idea and wanted to put it in a very simple and straightforward way, tried to make the paper easy to read and starter friendly! Also it shows my research partner focus on uncertainty measurement from metrology, which I think it‚Äôs not very widely addressed in ML and NLP! 

The motivation here came while doing exploration at the Weights & Biases Sunday cafe event in SF, where we were exploring their observability Weave Product. I think running loops and adding more complex tools that I did for the paper, should be production valuable and help in a bunch of ways, but most importantly, help with making small models
More useful and a kind of reasoning process of sorts. In the future it might be useful to make this loop inside the model before output layers, anybody think of any cools applications for such methods ? 


[Title]: Entropy-Guided Loop: Achieving Reasoning through Uncertainty-Aware Generation

[Abstract]: Reasoning models often outperform smaller models but at 3--5√ó higher cost and added latency. We present entropy-guided refinement: a lightweight, test-time loop that uses token-level uncertainty to trigger a single, targeted refinement pass. We extract logprobs, compute Shannon entropy on top-k alternatives, and apply a simple OR-logic trigger over perplexity, maximum token entropy, and low-confidence-token count. Unlike approaches that use entropy only for measurement or decoding, we pass a compact uncertainty report (tokens, confidences, alternatives, context) back to the model to guide corrective edits. On representative technical queries across reasoning, mathematics, and code generation tasks, a small model with our loop approaches 95\% of a reference reasoning model's quality at approximately one-third of the cost. The method achieves selective refinement on ~31\% of responses while improving accuracy by 16 percentage points over single-pass inference. We demonstrate that this uncertainty-aware loop provides an effective middle ground between single-pass inference and expensive reasoning chains, making it practical for production deployments where both quality and cost matter.

https://arxiv.org/abs/2509.00079

If you don‚Äôt like it, let me know! Am open to critique and learning! ",26,0.91,https://www.reddit.com/r/MachineLearning/comments/1n71dzv/a_friendly_starter_paper_entropyguided_loop/,False,True,False
1n6wc4k,impatiens-capensis,1756848866.0,31,/r/MachineLearning/comments/1n6wc4k/d_has_paper_submission_quality_remained_roughly/,MachineLearning,[D] Has paper submission quality remained roughly the same?,"Over the last year, I reviewed 12 papers at top tier conferences. It's a small sample size but I noticed that roughly 3 or 4 of them were papers I would consider good enough for acceptance at a top tier conference. That is to say: (1) they contained a well-motivated and interesting idea, (2) they had reasonable experiments and ablation, and (3) they told a coherent story.

That means roughly 30% of papers met my personal threshold for quality.... which is roughly the historic acceptance rate for top-tier conferences. From my perspective, as the number of active researchers has increased, the number of well executed interesting ideas has also increased. I don't think we've hit a point where there's a clearly finite set of things to investigate in the field. 

I would also say essentially every paper I rejected was distinctly worse than those 3 or 4 papers. Papers I rejected were typically poorly motivated -- usually an architecture hack poorly situated in the broader landscape with no real story that explains this choice. Or, the paper completely missed an existing work that already did nearly exactly what they did. 

What has your experience been? ",66,0.89,https://www.reddit.com/r/MachineLearning/comments/1n6wc4k/d_has_paper_submission_quality_remained_roughly/,False,True,False
1n6vf0x,glazmann,1756846706.0,1,/r/MachineLearning/comments/1n6vf0x/r_neurips_workshop_change_of_authors_post/,MachineLearning,[R] NeurIPS workshop - change of authors post submission,"Hi all, I submitted a paper to a NeurIPs workshop recently and it just dawned on me that I forgot to enter one of the authors in the OpenReview portal (the deadline for submission has now passed). I will reach out to the workshop but has anyone had any luck with this kind of thing?",12,0.93,https://www.reddit.com/r/MachineLearning/comments/1n6vf0x/r_neurips_workshop_change_of_authors_post/,False,True,False
1n6t4vd,farizrahman4u,1756841533.0,2,/r/MachineLearning/comments/1n6t4vd/p_datatune_use_natural_language_llms_to_transform/,MachineLearning,[P] Datatune ‚Äì Use natural language + LLMs to transform and filter tabular data,"https://github.com/vitalops/datatune

Introducing Datatune, a Python library that enables row-wise transformations on tabular data using natural language prompts, powered by LLMs.

Unlike tools that generate SQL or static scripts, Datatune is designed for per-row semantic operations on tabular data. It‚Äôs particularly useful for fuzzy logic tasks like classification, filtering, derived metrics, and text extraction - anything that‚Äôs hard to express in SQL but intuitive in plain English.

### What it does

You write prompts like:

* ""Extract categories from the product description and name""
* ""Keep only electronics products""
* ""Add a column called ProfitMargin = (Total Profit / Revenue) * 100""

Datatune interprets the prompt and applies the right operation (map, filter, or an LLM-powered agent pipeline) on your data using OpenAI, Azure, Ollama, or other LLMs via LiteLLM.

### Key Features

* Row-level map() and filter() operations using natural language
* Agent interface for auto-generating multi-step transformations
* Built-in support for Dask DataFrames (for scalability)
* Works with multiple LLM backends (OpenAI, Azure, Ollama, etc.)
* Compatible with LiteLLM for flexibility across providers
* Auto-token batching, metadata tracking, and smart pipeline composition

### Token & Cost Optimization

* Datatune gives you explicit control over which columns are sent to the LLM, reducing token usage and API cost:
* Use input_fields to send only relevant columns
* Automatically handles batching and metadata internally
* Supports setting tokens-per-minute and requests-per-minute limits
* Defaults to known model limits (e.g., GPT-3.5) if not specified
* This makes it possible to run LLM-based transformations over large datasets without incurring runaway costs.

### Quick Example
```python
import datatune as dt
from datatune.llm.llm import OpenAI

llm = OpenAI(model_name=""gpt-3.5-turbo"")
df = dd.read_csv(""products.csv"")

# Map step
mapped = dt.map(
    prompt=""Extract categories from the description and name of product."",
    output_fields=[""Category"", ""Subcategory""],
    input_fields=[""Description"", ""Name""]
)(llm, df)

# Filter step
filtered = dt.filter(
    prompt=""Keep only electronics products"",
    input_fields=[""Name""]
)(llm, mapped)

result = dt.finalize(filtered)
```

Or using the agent:

```python
agent = dt.Agent(llm)
df = agent.do(""Add a column called ProfitMargin = (Total Profit / Total Revenue) * 100."", df)
result = dt.finalize(df)
```
### Use Cases

* Product classification from text fields
* Filtering based on semantic conditions
* Creating derived metrics using natural language
* Review quality detection, support ticket triage
* Anonymization (PII removal) when needed

### Links

* GitHub: https://github.com/vitalops/datatune
* Docs: https://docs.datatune.ai
* Examples: https://github.com/vitalops/datatune/tree/main/examples

We‚Äôre actively developing the project and would appreciate any feedback, bug reports, or feature requests via Github issues.
.
",9,0.7,https://www.reddit.com/r/MachineLearning/comments/1n6t4vd/p_datatune_use_natural_language_llms_to_transform/,False,True,False
1n6swom,Ill_Virus4547,1756841000.0,7,/r/MachineLearning/comments/1n6swom/d_how_can_i_license_datasets/,MachineLearning,[D] How can I license datasets?,"I've been working on AI projects for a while now and I keep running into the same problem over and over again. Wondering if it's just me or if this is a universal developer experience.

You need specific training data for your model. Not the usual stuff you find on Kaggle or other public datasets, but something more niche or specialized, for e.g. financial data from a particular sector, medical datasets, etc. I try to find quality datasets, but most of the time, they are hard to find or license, and not the quality or requirements I am looking for.

So, how do you typically handle this? Do you use datasets free/open source? Do you use synthetic data? Do you use whatever might be similar, but may compromise training/fine-tuning?

Im curious if there is a better way to approach this, or if struggling with data acquisition is just part of the AI development process we all have to accept. Do bigger companies have the same problems in sourcing and finding suitable data?

If you can share any tips regarding these issues I encountered, or if you can share your experience, will be much appreciated!",3,0.67,https://www.reddit.com/r/MachineLearning/comments/1n6swom/d_how_can_i_license_datasets/,False,True,False
1n6sd4l,poppear,1756839773.0,2,/r/MachineLearning/comments/1n6sd4l/p_csmrs_a_highperformance_rust_implementation_of/,MachineLearning,[P] csm.rs: A High-Performance Rust Implementation of Sesame's Conversational Speech Model for Real-Time Streaming TTS,"Hi everyone,

I'm sharing a project I've developed, [`csm.rs`](https://github.com/cartesia-one/csm.rs), a high-performance inference implementation for Sesame's Conversational Speech Model (`sesame/csm-1b`). The project is written in Rust and built on the `candle` ML framework.

The primary goal was to create an efficient, standalone inference engine capable of real-time, streaming text-to-speech, moving beyond typical Python-based inference scripts to achieve maximum performance.",16,1.0,https://www.reddit.com/r/MachineLearning/comments/1n6sd4l/p_csmrs_a_highperformance_rust_implementation_of/,False,True,False
1n6rijz,peepee_peeper,1756837883.0,7,/r/MachineLearning/comments/1n6rijz/d_building_conversational_ai_the_infrastructure/,MachineLearning,[D] Building conversational AI: the infrastructure nobody talks about,"Everyone's focused on models. Nobody discusses the plumbing that makes real-time AI conversation possible.

The stack I'm testing:

* STT: Whisper vs Google Speech
* LLM: GPT-4, Claude, Llama
* TTS: ElevenLabs vs PlayHT
* Audio routing: This is where it gets messy

The audio infrastructure is the bottleneck. Tried raw WebRTC (painful), looking at managed solutions like Agora, LiveKit, Daily.

Latency breakdown targets:

* Audio capture: <50ms
* STT: <100ms
* LLM: <200ms
* TTS: <100ms
* Total: <500ms for natural conversation

Anyone achieved consistent sub-500ms latency? What's your setup?",6,0.58,https://www.reddit.com/r/MachineLearning/comments/1n6rijz/d_building_conversational_ai_the_infrastructure/,False,True,False
1n6kr0d,AgeOfEmpires4AOE4,1756822724.0,4,/r/MachineLearning/comments/1n6kr0d/p_training_environment_for_ps2_game_rl/,MachineLearning,[P] Training environment for PS2 game RL,"https://preview.redd.it/hx8od7wvfrmf1.png?width=3819&format=png&auto=webp&s=8989ff64c23e66ff7f22e4694cae88a0f192c2b5

It's alive!!! The environment I'm developing is already functional and running Granturismo 3 on PS2!!! If you want to support the development, the link is this:

[https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)",21,1.0,https://www.reddit.com/r/MachineLearning/comments/1n6kr0d/p_training_environment_for_ps2_game_rl/,False,True,False
1n6ir0a,hakimgafai,1756817861.0,38,/r/MachineLearning/comments/1n6ir0a/d_what_apps_or_workflows_do_you_use_to_keep_up/,MachineLearning,[D] What apps or workflows do you use to keep up with reading AI/ML papers regularly?,"I‚Äôm a postgraduate in AI, and I‚Äôm trying to build a better habit of reading papers consistently.

I wanted to ask: what tools, apps, or workflows do you personally use to track new papers and actually read them?

Curious to hear what‚Äôs worked for you in terms of discovery (finding the right papers) and sticking with the reading habit.",66,0.89,https://www.reddit.com/r/MachineLearning/comments/1n6ir0a/d_what_apps_or_workflows_do_you_use_to_keep_up/,False,True,False
1n5qjvu,Dry-Count4414,1756737742.0,6,/r/MachineLearning/comments/1n5qjvu/d_emnlp_2025_cameraready_page_limits_virtual/,MachineLearning,[D] EMNLP 2025 camera-ready page limits + virtual poster presentation,"Hey folks,

My paper just got into EMNLP 2025  and I‚Äôm trying to sort out two things before the camera-ready:

1. Page limits

- ARR submission was capped at 8 pages (long paper). The acceptance email says we get +1 page for camera-ready, so I‚Äôm assuming that means 9 pages for the main text.

- Is the Limitations section required but outside this 9-page count?

- And are appendices unlimited, or do they somehow count toward the limit?



2. Virtual poster presentation

- On OpenReview I‚Äôve already been assigned poster status. The email also says we can choose to present either in person or virtually.

Does that mean I‚Äôm free to do my poster virtually if I want?

- For those who‚Äôve done virtual posters at EMNLP/ACL in recent years: what platform did they use (GatherTown, Zoom, something else), and how was the interaction?




Would love to hear from anyone who‚Äôs navigated this before",2,0.75,https://www.reddit.com/r/MachineLearning/comments/1n5qjvu/d_emnlp_2025_cameraready_page_limits_virtual/,False,True,False
1n5rbwc,Even-Tour-4580,1756739530.0,5,/r/MachineLearning/comments/1n5rbwc/p_computer_vision_backbone_model_paperswithcode/,MachineLearning,[P] Computer Vision Backbone Model PapersWithCode Alternative: Heedless Backbones,"

https://preview.redd.it/d2mm661vnkmf1.png?width=3126&format=png&auto=webp&s=aa83a5002ebcba917c48d158460133701a81989a

This is a site I've made that aims to do a better job of what Papers with Code did for ImageNet and Coco benchmarks.

I was often frustrated that the data on Papers with Code didn't consistently differentiate backbones, downstream heads, and pretraining and training strategies when presenting data. So with heedless backbones, benchmark results are all linked to a single pretrained model (e.g. convenxt-s-IN1k), which is linked to a model (e.g. convnext-s), which is linked to a model family (e.g. convnext). In addition to that, almost all results have FLOPS and model size associated with them. Sometimes they even throughput results on different gpus (though this is pretty sparse).

I'd love to hear feature requests or other feedback. Also, if there's a model family that you want added to the site, please open an issue on the project's [github](https://github.com/igm503/heedless-backbones)

  
[Heedless Backbones](https://heedlessbackbones.com/)",26,0.96,https://www.reddit.com/r/MachineLearning/comments/1n5rbwc/p_computer_vision_backbone_model_paperswithcode/,False,True,False
1n6aisc,Outrageous_Tip_8109,1756788531.0,70,/r/MachineLearning/comments/1n6aisc/d_openreview_website_is_down/,MachineLearning,[D] OpenReview website is down!,"I'm trying to upload one pending AAAI review but the website is not opening. 

Anyone facing the same issue? I'm also curious what would happen if I miss the review submission deadline due to website downtime. ",81,0.96,https://www.reddit.com/r/MachineLearning/comments/1n6aisc/d_openreview_website_is_down/,False,True,False
1n67lft,AutoModerator,1756779330.0,26,/r/MachineLearning/comments/1n67lft/d_selfpromotion_thread/,MachineLearning,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

\--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

\--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",14,0.89,https://www.reddit.com/r/MachineLearning/comments/1n67lft/d_selfpromotion_thread/,False,True,False
1n5zzln,-math-4-life-,1756758975.0,1,/r/MachineLearning/comments/1n5zzln/r_how_hard_is_it_to_get_accepted_into_the_aaai/,MachineLearning,[R] How hard is it to get accepted into the AAAI Student Abstract and Poster Program?,"Hi everyone,

II‚Äôm considering submitting to the AAAI Student Abstract and Poster Program (AAAI-26), but I can‚Äôt find much information about how competitive it is compared to the main technical track.

I know the main conference has a pretty low acceptance rate but AAAI doesn‚Äôt seem to share stats for the student program. Has anyone here submitted to or been accepted into this track before? How selective is it?

Also, would it be enough if my work is more of an application of existing AI methods to radar (less novelty in the method itself, more novelty in the application)? Or are they mainly looking for new algorithms/AI contributions even in the student track?",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1n5zzln/r_how_hard_is_it_to_get_accepted_into_the_aaai/,False,True,False
1n5tmp7,pedromnasc,1756744706.0,0,/r/MachineLearning/comments/1n5tmp7/d_lessons_from_building_an_ai_data_analyst/,MachineLearning,[D] Lessons from building an AI data analyst,"Hi all,

I wrote a post on some lessons from building an AI data analyst: [https://pedronasc.com/articles/lessons-building-ai-data-analyst](https://pedronasc.com/articles/lessons-building-ai-data-analyst)

The gap from a nice demo to a real production system is big -> with a lot of yet to be solved challenges.

Would love to share ideas with other builders in the space and willing to learn more about it.",0,0.29,https://www.reddit.com/r/MachineLearning/comments/1n5tmp7/d_lessons_from_building_an_ai_data_analyst/,False,True,False
1n5t7cv,AgencyPuzzleheaded,1756743750.0,3,/r/MachineLearning/comments/1n5t7cv/r_latent_diffusion_question/,MachineLearning,[R] Latent Diffusion Question,"Is this normal for generated data from latent diffusion? The large spikes at the end of the histogram edges. Does this indicate the autoencoder is overfitting?

https://preview.redd.it/i1gtm7h3xkmf1.png?width=536&format=png&auto=webp&s=1589ad23cffc3a678eefad82750b71eefbad9962

",8,1.0,https://www.reddit.com/r/MachineLearning/comments/1n5t7cv/r_latent_diffusion_question/,False,True,False
1n5r08b,AutoModerator,1756738811.0,0,/r/MachineLearning/comments/1n5r08b/d_simple_questions_thread/,MachineLearning,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",2,1.0,https://www.reddit.com/r/MachineLearning/comments/1n5r08b/d_simple_questions_thread/,False,True,False
1n5qhr4,New-Skin-5064,1756737606.0,1,/r/MachineLearning/comments/1n5qhr4/d_oom_when_resuming_from_checkpoint/,MachineLearning,[D] OOM When Resuming From Checkpoint,"I was training a GPT-2 XL-sized LLM, and I had to stop the run. When I try to resume the run on the same hardware, I get an OOM. I had a similar issue when my model had about 930m parameters, but I solved it by moving all tensors in the model/optimizer state dicts to CPU before saving. When I run this code:optimizer.state = collections.defaultdict(dict)the OOM goes away. The OOM always happens during the optimizer step. I use xm.optimizer_step with the barrier enabled. I have also tried manually sharding the optimizer states using xs.mark_sharding. Here are some details about my project/setup:

TPU v3-8

Torch 2.7.0

jax 0.6.2

I use FSDP with SPMD

Here is some relevant code from my codebase:
Saving:
```
def save_checkpoint(model, optimizer, step, train_device_loader=None):
    # Save model weights via XLA SPMD checkpoint (supported)
    os.makedirs(f""./ckpt-{step}"", exist_ok=True)
    model_state_dict = model.module.state_dict()
    for i in model_state_dict.keys():
        xla_tensor = model_state_dict[i]
        model_state_dict[i] = xla_tensor.to(""cpu"")
        del xla_tensor
    model_sd = {""model"": model_state_dict}
    xm.save(model_sd, f""./ckpt-{step}/model.pt"")

    # Save host-only states separately (optimizer, step, RNG, dataloader)
    optim_state = optimizer.state_dict()
    optim_state_for_saving = {
        ""state"": {},
        ""param_groups"": optimizer.state_dict()[""param_groups""]
    }
    for i in optim_state[""state""]:
        optim_state_for_saving[""state""][i] = {}
        optim_state_for_saving[""state""][i][""step""] = optim_state[""state""][i][""step""].to(""cpu"")
        optim_state_for_saving[""state""][i][""exp_avg""] = optim_state[""state""][i][""exp_avg""].to(""cpu"")
        optim_state_for_saving[""state""][i][""exp_avg_sq""] = optim_state[""state""][i][""exp_avg_sq""].to(""cpu"")
    host_state = {
        ""optim"": optim_state_for_saving,
        ""step"": step,
    }

    if train_device_loader:
        rng_states = {
            'torch_rng_state': torch.get_rng_state(),
            'numpy_rng_state': np.random.get_state(),
            'random_rng_state': random.getstate(),
        }
        dataloader_states = {
            ""shard_order"": train_device_loader._loader.dataset.shards,
            ""local_order"": train_device_loader._loader.dataset.curr_order,
            ""warmup_order"": train_device_loader._loader.dataset.warmup_order,
            ""warmup_prob"": train_device_loader._loader.dataset.warmup_prob,
        }
    else:
        rng_states = None
        dataloader_states = None

    # Write host-side files
    with open(f""./ckpt-{step}/host_state.pkl"", ""wb"") as f:
        pickle.dump(host_state, f)
    if rng_states is not None:
        with open(f""./ckpt-{step}/rng.pkl"", ""wb"") as f:
            pickle.dump(rng_states, f)
    if dataloader_states is not None:
        with open(f""./ckpt-{step}/dataloader.json"", ""w"") as json_file:
            json.dump(dataloader_states, json_file, indent=4)
```
Loading:
```
if resume_from != """":
        model_sd = torch.load(f""{resume_from}/model.pt"", map_location='cpu')
        model.load_state_dict(model_sd[""model""])
model = model.to(device)
if gradient_checkpointing:
        model = FSDPv2(module=checkpoint_module(model), mesh=mesh)
else:
        model = FSDPv2(module=model, mesh=mesh)
optimizer = build_optimizer(model, peak_lr, betas, weight_decay)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps*(1-warmup_pct), eta_min=min_lr)
if resume_from != """":
        xm.mark_step()
        # 2) Restore host-only states (optimizer, step)
        with open(f""{resume_from}/host_state.pkl"", 'rb') as f:
            host_state = pickle.load(f)
        optim_state = host_state[""optim""]
        
        # Load the processed state dict
        optimizer.load_state_dict(optim_state)
        del optim_state
        last_step = host_state[""step""]
        # 3) Restore RNG and dataloader state (if present)
        try:
            with open(f""{resume_from}/rng.pkl"", ""rb"") as f:
                rng = pickle.load(f)
            torch.set_rng_state(rng['torch_rng_state'])
            np.random.set_state(rng['numpy_rng_state'])
            random.setstate([rng['random_rng_state'][0], tuple(rng['random_rng_state'][1]), rng['random_rng_state'][2]])
        except FileNotFoundError:
            pass
        with open(f'{resume_from}/dataloader.json', 'r') as file:
            dataloader = json.load(file)
```
Step:
```
for k in range(gradient_accumulation_steps):
    x, y = next(train_iter)
     with autocast(xm.xla_device(), dtype=torch.bfloat16):
          loss = model(x, y)
    (loss / gradient_accumulation_steps).backward()
     train_loss += loss.detach()
     xm.mark_step()
                
torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)
                
xm.optimizer_step(optimizer, barrier=True)
                
optimizer.zero_grad()
```",1,0.67,https://www.reddit.com/r/MachineLearning/comments/1n5qhr4/d_oom_when_resuming_from_checkpoint/,False,True,False
1n5qgcd,IcarusZhang,1756737513.0,40,/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/,MachineLearning,[D] Proposal: Multi-year submission ban for irresponsible reviewers ‚Äî feedback wanted,"**TL;DR:** I propose introducing multi-year submission bans for reviewers who repeatedly fail their responsibilities. Full proposal + discussion here: [GitHub](https://github.com/IcarusWizard/ML-review-proposal-for-accountability).

Hi everyone,

Like many of you, I‚Äôve often felt that our review system is broken due to irresponsible reviewers. Complaints alone don‚Äôt fix the problem, so I‚Äôve written a proposal for a possible solution: **introducing a multi-year submission ban for reviewers who repeatedly fail to fulfill their responsibilities.**

Recent policies at major conferences (e.g., CVPR, ICCV, NeurIPS) include desk rejections for poor reviews, but these measures don‚Äôt fully address the issue‚Äîespecially during the rebuttal phase. Reviewers can still avoid accountability once their own papers are withdrawn.

In my proposal, I outline how longer-term consequences might improve reviewer accountability, along with safeguards and limitations. I‚Äôm not a policymaker, so I expect there will be issues I haven‚Äôt considered, and I‚Äôd love to hear your thoughts.

üëâ Read the full proposal here: [GitHub](https://github.com/IcarusWizard/ML-review-proposal-for-accountability).  
üëâ Please share whether you think this is viable, problematic, or needs rethinking.

If we can spark a constructive discussion, maybe we can push toward a better review system together.",58,0.75,https://www.reddit.com/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/,False,True,False
1n5oznp,SnappierSoap318,1756734014.0,14,/r/MachineLearning/comments/1n5oznp/d_why_arent_there_any_diffusion_speech_to_text/,MachineLearning,[D] Why aren't there any diffusion speech to text models?,"Title,

I was reading upon diffusion models and speech models and that some of the new diffusion text models are being now developed. Since we know the length of the output that a chunk of audio produces wouldn't it be possible to create a diffusion model to fill in text for the whole length all at once instead of the current auto regressive models?

PS: I am really not that advanced so this might be a dumb question.",6,0.75,https://www.reddit.com/r/MachineLearning/comments/1n5oznp/d_why_arent_there_any_diffusion_speech_to_text/,False,True,False
1n5n1v2,_puhsu,1756728809.0,2,/r/MachineLearning/comments/1n5n1v2/r_graph_ml_benchmarks_and_foundation_models/,MachineLearning,[R] Graph ML benchmarks and foundation models,"Our team has recently published two graph ML papers: one with a new realistic benchmark and the second one on graph foundation models and how they can be related to tabular foundation models.  
  
**GraphLand benchmark**

üìù Paper:  [https://arxiv.org/abs/2409.14500](https://arxiv.org/abs/2409.14500)  
üíª Code:  [https://github.com/yandex-research/graphland](https://github.com/yandex-research/graphland) 

It is widely discussed in the community that graph machine learning suffers from the lack of realistic, meaningful, reliable, and diverse benchmarks. We agree with this and we hope that we improve this situation with our recent paper ‚ÄúGraphLand: Evaluating Graph Machine Learning Models on Diverse Industrial Data‚Äù. GraphLand is a benchmark of 14 diverse graph datasets for node property prediction (both classification and regression) from different industrial applications. The datasets cover realistic machine learning problems and come with rich numerical and categorical node features that are common in real-world applications. Importantly, besides standard random splits, GraphLand provides splits with temporal distributional shifts and the inductive prediction setting, which enable evaluating GNNs in more realistic and challenging scenarios.

[GraphLand benchmark datasets.](https://preview.redd.it/nkl4qs9nnjmf1.png?width=2224&format=png&auto=webp&s=1819461078e34be3e98030c9e65ee61a7b98adc9)

We evaluated a wide range of models on GraphLand. This includes several openly available graph foundation models (GFMs), which we found provide very weak performance compared to classical GNNs.   
  
Thus, we set out to develop a better GFM, which led us to the next paper...

**Turning Tabular Foundation Models into Graph Foundation Models**

üìù Paper: [https://arxiv.org/abs/2508.20906](https://arxiv.org/abs/2508.20906)  
üíª Code: [https://github.com/yandex-research/G2T-FM](https://github.com/yandex-research/G2T-FM)

Graphs may come from very different domains and thus may have diverse features varying across datasets. As a result, one of the key challenges for GFMs is how to deal with such diverse heterogeneous features. Prior studies did not fully address this issue, often limiting themselves to text-attributed graphs or relying on simple techniques like PCA and SVD. However, this challenge is not unique to the graph domain. The tabular domain faces exactly the same issue, and recent tabular foundation models like TabPFNv2 successfully deal with it. We‚Äôve decided to transfer their success to graphs.

[G2T-FM Framework](https://preview.redd.it/xnfsjf77ojmf1.jpg?width=1280&format=pjpg&auto=webp&s=d840e9794068202829dec2bdfa71e426198a7a15)

In our framework ‚Äì G2T-FM (Graph-to-Table Foundation Model) ‚Äì we augment the original features with graph information by computing neighborhood feature aggregations and some structure-based encodings, essentially transforming graph tasks to tabular tasks (G2T). After that, we apply TabPFNv2 to these augmented features to get predictions.

[G2T-FM Results](https://preview.redd.it/z3mz5tmaojmf1.jpg?width=1280&format=pjpg&auto=webp&s=6feb591cdd5fb1231d36c2a937ced802a27a26e7)

We evaluated G2T-FM on GraphLand and several other graph datasets and found that it shows strong performance in both in-context learning and finetuning settings. In particular, G2T-FM outperforms both well-tuned classic GNNs trained from scratch and prior publicly available GFMs.   
  
We hope our work will help develop better GFMs and highlight for the graph community the similarities of graph and tabular domains and the prospects of utilizing tabular foundation models for graph tasks!



",39,0.93,https://www.reddit.com/r/MachineLearning/comments/1n5n1v2/r_graph_ml_benchmarks_and_foundation_models/,False,True,False
1n5mcba,Fantastic-Nerve-4056,1756726627.0,33,/r/MachineLearning/comments/1n5mcba/recommended_cloud_service_d/,MachineLearning,Recommended Cloud Service [D],"Hi there, a senior PhD fellow this side.  
Recently, I entered the LLM space; however, my institute lacks the required computing resources.  
  
Hence, my PI suggested that I opt for some cloud services, given that we have a good amount of funding available. So, can anyone recommend a decent cloud platform which, first of all, is budget-friendly, has available A100s, and most importantly, has a friendly UI to run the .ipynb or .py files

Any suggestions on it would be appreciated",8,0.7,https://www.reddit.com/r/MachineLearning/comments/1n5mcba/recommended_cloud_service_d/,False,True,False
1n5kl6k,Deepblue597,1756720559.0,0,/r/MachineLearning/comments/1n5kl6k/p_beaver_a_dsl_for_building_streaming_ml_pipelines/,MachineLearning,[P] Beaver: A DSL for Building Streaming ML Pipelines,"Hi guys!

My name is Jason I am an Electrical and Computer Engineering student and for the last year I have been working on my thesis, in which I have developed Beaver¬†‚Äì a domain-specific language (DSL) designed to make building machine learning pipelines for streaming data (e.g., Kafka) much simpler and more accessible.

What is Beaver?

* A DSL that lets you define ML pipelines using a clear, declarative syntax (instead of complex Python code)
* Generates Python code that integrates with the¬†[River](http://riverml.xyz/latest/)¬†library for online ML and supports real-time data streams
* Includes built-in validation, analysis, and automatic dashboard generation

  
I'm making this post to ask for some feedback. I‚Äôve prepared a user testing experience with 3 tasks (from basic to advanced) that should take about 30-45 minutes. I‚Äôd love to hear your thoughts on usability, clarity, and the overall concept.

* üìñ¬†[Concept overview & docs](http://deepblue597.github.io/beaver-doc/)
* üìù¬†[User testing instructions](https://github.com/deepblue597/beaver/blob/user_testing/user_testing.md)
* ü¶´¬†[Example pipeline file](https://github.com/deepblue597/beaver/blob/user_testing/examples/linear.bvr)
* üí¨¬†[Feedback form](https://forms.gle/ioLVyvruJ2KCs6wd8)

Repo : [https://github.com/deepblue597/beaver](https://github.com/deepblue597/beaver)  
It is recommended to use the user\_testing branch for the feedback.   
  
Thank you so much for your time <3 ",4,0.84,https://www.reddit.com/r/MachineLearning/comments/1n5kl6k/p_beaver_a_dsl_for_building_streaming_ml_pipelines/,False,True,False
1n5j2sr,Naneet_Aleart_Ok,1756714705.0,3,/r/MachineLearning/comments/1n5j2sr/p_improving_model_performance/,MachineLearning,[P] Improving model performance,"So I have been working on Continuous Sign Language Recognition (CSLR) for a while. Tried ViViT-Tf, it didn't seem to work. Also, went crazy with it in wrong direction and made an over complicated model but later simplified it to a simple encoder decoder, which didn't work.

Then I also tried several other simple encoder-decoder. Tried ViT-Tf, it didn't seem to work. Then tried ViT-LSTM, finally got some results (38.78% word error rate). Then I also tried X3D-LSTM, got 42.52% word error rate. 

Now I am kinda confused what to do next. I could not think of anything and just decided to make a model similar to SlowFastSign using X3D and LSTM. But I want to know how do people approach a problem and iterate their model to improve model accuracy. I guess there must be a way of analysing things and take decision based on that. I don't want to just blindly throw a bunch of darts and hope for the best. ",6,0.8,https://www.reddit.com/r/MachineLearning/comments/1n5j2sr/p_improving_model_performance/,False,True,False
1n55r7s,Outrageous-Travel-80,1756673510.0,4,/r/MachineLearning/comments/1n55r7s/r_measuring_semantic_novelty_in_ai_text/,MachineLearning,[R] Measuring Semantic Novelty in AI Text Generation Using Embedding Distances,"We developed a simple metric to measure semantic novelty in **collaborative text generation** by computing cosine distances between consecutive sentence embeddings. 

Key finding: Human contributions showed consistently higher semantic novelty than AI across multiple embedding models (RoBERTa, DistilBERT, MPNet, MiniLM) in our human-AI storytelling dataset. 

The approach is straightforward - just encode sentences and measure distances between consecutive pairs. Could be useful for evaluating dialogue systems, story generation models, or any sequential text generation task.

Some links:  
[Paper site](https://idanvidra.github.io/playing_along_paper_site/)    
[Code](https://github.com/idanvidra/Yes-And-Game-Paper)[Blog post with implementation details](https://medium.com/@idan.vidra/measuring-semantic-novelty-in-ai-generated-text-a-simple-embedding-based-approach-c92042c88338)

The work emerged from studying human-AI collaborative storytelling using improvisational theater techniques (""Yes! and..."" games).",8,0.78,https://www.reddit.com/r/MachineLearning/comments/1n55r7s/r_measuring_semantic_novelty_in_ai_text/,False,True,False
1n55mr4,dduka99,1756673207.0,2,/r/MachineLearning/comments/1n55mr4/d_aaai_review_template/,MachineLearning,[D] AAAI Review Template,"Hello everyone,  
I‚Äôm serving as a first-time reviewer for AAAI and am getting ready to submit my reviews. I‚Äôm a bit uncertain about the expected structure for the different fields in the review form. For instance, in the *‚ÄúBrief summary of your review‚Äù* field, should this be a recap of the paper‚Äôs content or a short explanation of my evaluation and decision? More broadly, I‚Äôd be grateful for any guidance on how to approach the overall submission.",12,0.88,https://www.reddit.com/r/MachineLearning/comments/1n55mr4/d_aaai_review_template/,False,True,False
1n4y2y3,pmv143,1756655147.0,107,/r/MachineLearning/comments/1n4y2y3/d_huaweis_96gb_gpu_under_2k_what_does_this_mean/,MachineLearning,[D] Huawei‚Äôs 96GB GPU under $2k ‚Äì what does this mean for inference?,"Looks like Huawei is putting out a 96GB GPU for under $2k. NVIDIA‚Äôs cards with similar memory are usually $10k+. From what I‚Äôve read, this one is aimed mainly at inference.

Do you think this could actually lower costs in practice, or will the real hurdle be software/driver support? ",233,0.85,https://i.redd.it/bnsra3anldmf1.jpeg,False,False,False
1n4w3i9,PossibleTop1492,1756650395.0,1,/r/MachineLearning/comments/1n4w3i9/r_beating_baselines_with_geometry_introducing_gmc/,MachineLearning,"[R] Beating Baselines with Geometry: Introducing GMC, a Fast and Well-Calibrated Classifier","A Technical Writer's ambition to prove.

Being a Technical Writer, I yearned to learn Machine learning and prove myself. This is a try towards achieving that.  I've developed a new classifier, the¬†**Geometric Mixture Classifier (GMC)**, and I'm seeking feedback from the community before submitting it to arXiv and conferences.

**The Problem:**¬†Linear models (LR, SVM) are interpretable but fail on multi-modal data. Non-linear models (RBF-SVM, MLPs) are effective but often operate as black boxes. We wanted a model that is¬†**both interpretable and expressive**.

**The Idea:**¬†GMC represents each class as a¬†**mixture of hyperplanes**¬†(a ""soft union of half-spaces""). It uses a soft-OR (log-sum-exp) within a class and softmax across classes. It's like a Mixture of Experts but without a separate gating network.

* **Interpretable:**¬†You can see which ""local expert"" (hyperplane) was responsible for a prediction.
* **Performant:**¬†Competitive with RBF-SVM, RF, and MLPs on standard benchmarks.
* **Efficient:**¬†CPU-friendly, ¬µs-scale inference (faster than RBF-SVM, on par with MLP).
* **Calibrated:**¬†Produces reliable probabilities.

[Algorithm analogy with similar baselines](https://preview.redd.it/64vyu4u87dmf1.png?width=1385&format=png&auto=webp&s=08b2014b60836edd0b28adbac68eb388a4a091fa)

* **Accuracy:**¬†Outperforms linear models, competitive with strong non-linear baselines.
* **Speed:**¬†\~2-40¬µs inference time per example (see table below).
* **Calibration:**¬†Low ECE, further improved with temperature scaling.

We would be incredibly grateful for any feedback on:

* Is the¬†**core idea**¬†and its¬†**differentiation from MoE/Maxout**¬†clear?
* Are the¬†**experiments**¬†and¬†**comparisons**¬†fair and convincing?
* Is there any¬†**related work**¬†we might have overlooked?
* Any general feedback on¬†**clarity**¬†or¬†**presentation**?

You can find a detailed copy of the algorithm [here](https://drive.google.com/file/d/1vRTAucCpVqImJnojVwzAUHQ2SmbAvdsi/view?usp=sharing).

Please feel free to test the algorithm: [Geometric Mixture Classifie](https://github.com/Abitsfhuusrtyt/-Geometric-Mixture-Classifier-GMC---A-Discriminative-Per-Class-Mixture-of-Hyperplanes)r",6,1.0,https://www.reddit.com/r/MachineLearning/comments/1n4w3i9/r_beating_baselines_with_geometry_introducing_gmc/,False,True,False
1n4ul5d,ProfessionalType9800,1756646531.0,18,/r/MachineLearning/comments/1n4ul5d/d_openset_recognition_problem_using_deep_learning/,MachineLearning,[D] Open-Set Recognition Problem using Deep learning,"I‚Äôm working on a deep learning project where I have a dataset with n classes

But here‚Äôs my problem:

üëâ What if a totally new class comes in which doesn‚Äôt belong to any of the trained classes? 

I've heard of a few ideas but would like to know many approaches:

* analyzing the embedding space: Maybe by measuring the distance of a new input's embedding to the known class 'clusters' in that space? If it's too far from all of them, it's an outlier.
* Apply Clustering in Embedding Space.

everything works based on embedding space...

are there any other approaches?",5,0.78,https://www.reddit.com/r/MachineLearning/comments/1n4ul5d/d_openset_recognition_problem_using_deep_learning/,False,True,False
1n4st5p,Shan444_,1756641459.0,11,/r/MachineLearning/comments/1n4st5p/d_my_model_is_taking_too_much_time_in_calculating/,MachineLearning,[D] My model is taking too much time in calculating FFT to find top k,"so basically my batch size is 32  
d\_model is 128  
d\_ff is 256  
enc\_in = 5  
seq\_len = 128 and pred\_len is 10

I narrow downed the bottle neck and found that my FFT step is taking too much time. i can‚Äôt use autocast to make f32 ‚Üí bf16 (assume that its not currently supported).

**but frankly its taking too much time to train. and that too total steps per epoch is 700 - 902 and there are 100 epoch‚Äôs.**  
roughly the FFT is taking 1.5 secs per iteration below. so

    for i in range(1,4):
         calculate FFT()
    
    

can someone help me?",0,0.43,https://www.reddit.com/r/MachineLearning/comments/1n4st5p/d_my_model_is_taking_too_much_time_in_calculating/,False,True,False
1n4ppbi,Immediate-Hour-8466,1756629936.0,0,/r/MachineLearning/comments/1n4ppbi/d_advanced_nlp_with_transformers_full_talk/,MachineLearning,[D] Advanced NLP with Transformers: Full talk recording and GitHub repo,"**Just gave a 1.5-hour talk on ""Advanced NLP with Transformers"" covering:**

* Transformer architecture
* Prompting, RAG and fine-tuning techniques
* AI safety, security and governance challenges
* Curated papers, fellowships and resources

**Resources:** üé• Recording: [https://www.youtube.com/watch?v=9WVtUDDcAXw&t=2330s](https://www.youtube.com/watch?v=9WVtUDDcAXw&t=2330s) üíª GitHub: [https://github.com/vgcharan/Advanced-NLP-Workshop-2025](https://github.com/vgcharan/Advanced-NLP-Workshop-2025)

Designed for researchers, students and practitioners who want conceptual depth as well as practical references. Feedback and discussion are welcome!",0,0.43,https://www.reddit.com/r/MachineLearning/comments/1n4ppbi/d_advanced_nlp_with_transformers_full_talk/,False,True,False
1n4nm4h,sourgrammer,1756621874.0,29,/r/MachineLearning/comments/1n4nm4h/d_what_is_up_with_tensorflow_and_jax/,MachineLearning,[D] What is up with Tensorflow and JAX?,"Hi all,

  
been in the Machine Learning world till 2021, I still mostly used the old TF 1.x interface and just used TF2.x for a short time. Last work I did was with CUDA 9.

  
It seems like quite a bit shifted with Tensorflow, I looked at the architecture again to see how much changed. To me, it's incomprehensible. Has Google shifted all efforts towards JAX, a framework with fewer layers than TF?",78,0.93,https://www.reddit.com/r/MachineLearning/comments/1n4nm4h/d_what_is_up_with_tensorflow_and_jax/,False,True,False
1n4l73x,AdInevitable1362,1756613327.0,5,/r/MachineLearning/comments/1n4l73x/p_why_didnt_semantic_item_profiles_help_my_gcn/,MachineLearning,[P] Why didn‚Äôt semantic item profiles help my GCN recommender model?,"Hey everyone,

I‚Äôm working on a recommender system based on a GCN model for regression task ( predicting rating score). Normally, the model initializes user and item embeddings randomly, but I wanted to improve this by following a paper ( the diagram is presented above )  that integrates semantic item profiles as initial embeddings.

Here‚Äôs what I did:
	‚Ä¢	I generated structured item profiles with 3 parts using Gemini api : 
	‚Ä¢	[Summarization]: short description of the business.
	‚Ä¢	[User Preferences]: predicted/extracted types of users who‚Äôd like it.
	‚Ä¢	[Recommendation Reasoning]: explanation for why it fits.
	‚Ä¢	I also encoded metadata like review count and stars into natural language (e.g., review_count > 100 ‚Üí ""popular item"", avg_stars ~4.2 ‚Üí ""well-rated"").
	‚Ä¢	I used Gemini text embeddings to encode these profiles into fixed-size embeddings.
	‚Ä¢	Then I replaced the random item embeddings in my GCN with these semantic embeddings (after projecting them down to my model‚Äôs embedding size).

The issue:
	‚Ä¢	When I train the GCN with these semantic embeddings, performance actually gets worse compared to just using random initialization or identical. 

Could the item profiles themselves be ‚Äúbad‚Äù ?
",25,0.92,https://i.redd.it/e9yv3aba5amf1.jpeg,False,False,False
1n4jdo7,AutoModerator,1756607434.0,2,/r/MachineLearning/comments/1n4jdo7/d_monthly_whos_hiring_and_who_wants_to_be_hired/,MachineLearning,[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&#x200B;

Please remember that this community is geared towards those with experience.",12,0.78,https://www.reddit.com/r/MachineLearning/comments/1n4jdo7/d_monthly_whos_hiring_and_who_wants_to_be_hired/,False,True,False
1n4dqsc,GuiltyBookkeeper4849,1756590980.0,0,/r/MachineLearning/comments/1n4dqsc/introducing_art08b_reasoning_the_way_you_want_it/,MachineLearning,üåüIntroducing Art-0-8B: Reasoning the way you want it to with Adaptive Thinkingüåü [R],"Hi everyone! Today I'm announcing a new experimental open-source model finetuned from Qwen3-¬†**Art-0-8B is the first reasoning model where users can explicitly control how the model thinks through prompts.**

Unlike normal reasoning models that only let you control the final output, Art-0-8B lets you control the actual thinking process. Tell it to ""think in rap lyrics"" or ""use bullet points to organize thoughts"" and it will literally reason that way before giving you an answer.

You can check out the model on HuggingFace:¬†[https://huggingface.co/AGI-0/Art-0-8B](https://huggingface.co/AGI-0/Art-0-8B)¬†(please leave a like in the repo if you like this model)

Let me know your thoughts!

P.s. If you are an AI researcher working solo, consider joining us, we are a decentralized research lab, you can read about our mission in this section of the model card¬†[https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab](https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab)",13,0.81,https://www.reddit.com/r/MachineLearning/comments/1n4dqsc/introducing_art08b_reasoning_the_way_you_want_it/,False,True,False
1n4bebi,impatiens-capensis,1756584878.0,114,/r/MachineLearning/comments/1n4bebi/d_neurips_is_pushing_to_sacs_to_reject_already/,MachineLearning,[D] NeurIPS is pushing to SACs to reject already accepted papers due to venue constraints,"What are our options as a discipline? We are now at a point where 3 or more reviewers can like your paper, the ACs can accept it, and it will be rejected for no reason other than venue constraints. ",395,0.98,https://i.redd.it/l46o5xcwr7mf1.png,False,False,False
1n4asaq,alvises,1756583314.0,0,/r/MachineLearning/comments/1n4asaq/p_building_a_yolox_plate_detector_setup/,MachineLearning,"[P] Building a YOLOX Plate Detector: Setup, Fine-Tuning, Metrics, Dashcam Inference","Hey all üëã

I just published this is end-to-end walkthrough of fine-tuning YOLOX on a \~7k-image license-plate dataset: clean environment setup, dataset prep, training & evaluation with COCO metrics (mAP/AP50-95), ONNX export, and real-world dashcam inference. Includes notes on dependency pinning (YOLOX‚Äôs older stack), small script fixes, and a side-by-side comparison with an Ultralytics YOLO11 model trained on the same data. Results are on par once everything is configured correctly.

Here's the post where you find the code and commands: [https://www.poeticoding.com/building-a-yolox-plate-detector-setup-fine-tuning-metrics-dashcam-inference/](https://www.poeticoding.com/building-a-yolox-plate-detector-setup-fine-tuning-metrics-dashcam-inference/)

YOLOX github repo: [https://github.com/Megvii-BaseDetection/YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)

Roboflow car plates dataset: [https://universe.roboflow.com/roboflow-universe-projects/license-plate-recognition-rxg4e](https://universe.roboflow.com/roboflow-universe-projects/license-plate-recognition-rxg4e)

",3,0.67,https://www.youtube.com/watch?v=xPJqoX0EYKE,False,False,False
1n3i2fx,TaxPossible5575,1756498682.0,5,/r/MachineLearning/comments/1n3i2fx/d_scaling_inference_lessons_from_running_multiple/,MachineLearning,[D] Scaling Inference: Lessons from Running Multiple Foundation Models in Production,"We‚Äôve been experimenting with deploying a mix of foundation models (LLaMA, Mistral, Stable Diffusion variants, etc.) in a single platform. One of the recurring pain points is **inference optimization** at scale:

* **Batching tradeoffs**: Batching reduces cost but can kill latency for interactive use cases.
* **Quantization quirks**: Different levels (INT8, FP16) affect models inconsistently. Some speed up 4√ó, others break outputs.
* **GPU vs. CPU balance**: Some workloads run shockingly well on optimized CPU kernels ‚Äî but only for certain model families.

Curious how others have approached this.

* What‚Äôs your go-to strategy for **latency vs throughput tradeoffs**?
* Are you using **model distillation** or sticking to quantization?
* Any underrated **libraries or frameworks** for managing multi-model inference efficiently?",2,0.63,https://www.reddit.com/r/MachineLearning/comments/1n3i2fx/d_scaling_inference_lessons_from_running_multiple/,False,True,False
1n3nfye,Mountain_Reward_1252,1756512456.0,5,/r/MachineLearning/comments/1n3nfye/is_isolation_forest_ideal_for_realtime_imubased/,MachineLearning,Is Isolation Forest ideal for real-time IMU-based anomaly detection? Open to better alternatives [P],"Hey folks,

I‚Äôm working on a project involving real-time anomaly detection using IMU data from a mobile robot (acc_x, acc_y, acc_z, magnitude). The goal is to detect small disturbances (e.g., bumping into wires or obstacles) based on sensor changes.

I trained an Isolation Forest model on normal motion data and integrated it into a ROS 2 node using the .decision_function() threshold for runtime detection.

It works, but I‚Äôm worried about false positives, especially with fixed contamination. Since this will later run on embedded IMU hardware, I‚Äôm looking for something accurate and lightweight.

Is Isolation Forest reliable for this?
Any better algorithms you‚Äôd recommend (e.g., LOF, One-Class SVM, AE)? Would love to hear your thoughts or experience.

Thanks!",16,0.9,https://www.reddit.com/r/MachineLearning/comments/1n3nfye/is_isolation_forest_ideal_for_realtime_imubased/,False,True,False
1n3iiam,Unlikeghost,1756499735.0,12,/r/MachineLearning/comments/1n3iiam/d_working_with_optuna_autosampler_in_massive/,MachineLearning,[D] Working with Optuna + AutoSampler in massive search spaces,"Hi!
I‚Äôm using Optuna with AutoSampler to optimize a model, but the search space is huge‚Äîaround 2 million combinations.

Has anyone worked with something similar?
I‚Äôm interested in learning which techniques have worked for reducing the search space.",11,1.0,https://www.reddit.com/r/MachineLearning/comments/1n3iiam/d_working_with_optuna_autosampler_in_massive/,False,True,False
1n3gfpt,Immediate-Cake6519,1756494814.0,1,/r/MachineLearning/comments/1n3gfpt/p_opensource_protocol_designed_for_multiagent/,MachineLearning,[P] Open-Source Protocol designed for Multi-Agent Communication,"[Project](https://www.reddit.com/r/MachineLearning/?f=flair_name%3A%22Project%22)

OSS Released¬†**MAPLE ‚Äì a Multi Agent Protocol Language Engine**¬†designed for fast, secure, and reliable agent communication.

‚Äî a new¬†**open-source protocol**¬†designed for¬†**multi-agent communication**¬†at¬†**production scale**.

**MAPLE**¬†offers features we haven't seen in other protocols:

üîß¬†**Integrated Resource Management:**¬†The¬†**ONLY**¬†protocol with built-in resource specification, negotiation, and optimization

üõ°Ô∏è¬†**Link Identification Mechanism (LIM):**¬†Revolutionary security through verified communication channels

‚ö°¬†**Result<T,E> Type System:**¬†ELIMINATES all silent failures and communication errors

üåê¬†**Distributed State Synchronization:**¬†Sophisticated¬†**state management**¬†across agent networks

üè≠¬†**Production-Grade Performance:**¬†**Very high**¬†**performance**¬†for a¬†**feature-rich**¬†protocol with¬†**sub-millisecond**¬†latency

üíª¬†**pip install maple-oss**

PyPI here:¬†[https://pypi.org/project/maple-oss/](https://pypi.org/project/maple-oss/)

If you‚Äôre building with agents or need robust, real-world communication between systems,  
check out¬†**MAPLE GitHub**¬†repo:¬†[https://github.com/maheshvaikri-code/maple-oss](https://github.com/maheshvaikri-code/maple-oss)

Please try and test it with your projects.

[MAPLE Multi Agent Communication Protocol](https://preview.redd.it/bovnmzoqc0mf1.png?width=256&format=png&auto=webp&s=6563e85b9f830d36a244cbf9783bc05ba45ed8c3)

",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1n3gfpt/p_opensource_protocol_designed_for_multiagent/,False,True,False
1n3g1p7,DenOmania,1756493920.0,12,/r/MachineLearning/comments/1n3g1p7/d_how_do_we_make_browserbased_ai_agents_more/,MachineLearning,[D] How do we make browser-based AI agents more reliable?,"I‚Äôve been experimenting with different approaches for giving AI agents the ability to use browsers in real workflows (data collection, QA automation, multi-step workflows). The promise is huge but the reliability problems are just as big:

1. Sessions break after login or CAPTCHA
2. Agents fail when sites change structure
3. Security is hard to guarantee at scale
4. Each framework has its own dialect / quirks


Recently I‚Äôve been looking into managed environments that abstract some of this away. For example, I am using hyperbrowser right now and it does provide a unified layer for running browser-based agents without setting up everything manually. 

But then my question is... Is there ongoing research or promising directions in making browser-agent interactions more robust? Are there known benchmarks, best practices, or papers that deal with these reliability issues?",34,0.71,https://www.reddit.com/r/MachineLearning/comments/1n3g1p7/d_how_do_we_make_browserbased_ai_agents_more/,False,True,False
1n3e27s,bci-hacker,1756489360.0,20,/r/MachineLearning/comments/1n3e27s/d_upcoming_interviews_at_frontier_labs_tips/,MachineLearning,"[D] Upcoming interviews at frontier labs, tips?","Hi all,

I‚Äôm currently interviewing at a few labs for MLE positions and there‚Äôs two interviews in particular that have stumped me that I‚Äôd like some clarity on:

1. Transformer debugging - to my knowledge, the interviewer will provide a buggy implementation of things like causal attention, self-attention, incorrect layer norm, scaling issues, and broadcast/shape mismatch. Is there anything else I‚Äôd need to master here? So far, I‚Äôve only been studying GPT style transformers, should I add BERT to the mix or nah?
2. Training classifier & data analysis. The recruiter said this is around evaluation and model performance. I‚Äôm guessing they‚Äôll throw me an unbalanced dataset and ask me to improve model performance somehow. Things to study here are: 1) chip hguyns book and 2) look at regularization, pandas/sklearn normalization and data clean up methods. How else can I master this topic? Any sample questions you have seen here before?

Lastly, what is your go-to source for practicing MLE related topics, both in terms of knowledge-base as well as real interview questions. I tried 1point3acres but very limited when it comes to ML.",104,0.92,https://www.reddit.com/r/MachineLearning/comments/1n3e27s/d_upcoming_interviews_at_frontier_labs_tips/,False,True,False
1n38fr0,Suitable-Director809,1756476494.0,5,/r/MachineLearning/comments/1n38fr0/finetuning_vision_transformers_d/,MachineLearning,Finetuning Vision Transformers [D],"Hey, 
Looking to see how DinoV3 will do on my dataset post finetuning. 

Any practical advice on finetuning Dino? 
Scheduler, optimizer, flow - freezing, discriminative lr etc. 
Any recommandations for blogs or articals related to this? ",1,0.67,https://www.reddit.com/r/MachineLearning/comments/1n38fr0/finetuning_vision_transformers_d/,False,True,False
1n37qnu,AnyIce3007,1756474808.0,8,/r/MachineLearning/comments/1n37qnu/d_ollamagptoss20b_cant_seem_to_generate/,MachineLearning,[D] ollama/gpt-oss:20b can't seem to generate structured outputs.,"I'm experimenting with `""ollama/gpt-oss:20b""`'s capability to generate structured outputs. For example, I used it to evaluate against GSM8K dataset. The schema is as follows: `answer`: for the answer, and `solution`: for the CoT solution. However, it doesn't make sense that for a 20B model, it cannot generate a valid structured output.

Any thoughts or hacks on this one? I would appreciate it. Thanks.",12,0.8,https://www.reddit.com/r/MachineLearning/comments/1n37qnu/d_ollamagptoss20b_cant_seem_to_generate/,False,True,False
1n30p1v,JollySimple188,1756451710.0,10,/r/MachineLearning/comments/1n30p1v/how_are_teams_handling_small_dataset_training_for/,MachineLearning,How are teams handling small dataset training for industrial vision inspection?[P],"We're evaluating different approaches for vision-based defect detection where getting large labeled datasets is challenging. Lots of methods need thousands of examples, but some defects are rare (maybe 10-20 examples total in 6 months). Anyone working with similar constraints? I've been looking into platforms that can work with smaller datasets - curious what others are doing?",13,1.0,https://www.reddit.com/r/MachineLearning/comments/1n30p1v/how_are_teams_handling_small_dataset_training_for/,False,True,False
1n2rvvh,eh-tk,1756424223.0,17,/r/MachineLearning/comments/1n2rvvh/r_technical_skills_analysis_of_machine_learning/,MachineLearning,[R] Technical Skills Analysis of Machine Learning Professionals in Canada,"I manage a slack community of a couple hundred ML devs in Canada. I got curious and ran some numbers on our members to see if any interesting insights emerged. Here's what I found:

**The ""Pandemic ML Boom"" Effect**:  
Nearly 40% of members started an ML specific role between 2020-2022. 

**RAG and Vector Database Expertise**:  
Over 30% of members have hands-on experience with Retrieval-Augmented Generation systems and vector databases (Pinecone, Weaviate, ChromaDB), representing one of the hottest areas in enterprise AI.

‚Äç**Multi-modal AI Pioneers**:  
A significant portion of members work across modalities (vision + text, audio + text).

**Most Common Job¬†Titles**:

15% of members hold senior leadership roles (Principal, Staff, Director, CTO level), demonstrating strong senior representation within the community.

**ML-Engineering Bridge Roles**:

Over 35% of members hold hybrid titles that combine ML with other disciplines:¬†""MLOps Engineer,"" ""Software Engineer, ML,"" ""AI & Automation Engineer,"" ""Conversational AI Architect,"" and ""Technical Lead, NLP"".

You can see the full breakdown here: [https://revela.io/the-collective](https://revela.io/the-collective)",73,0.89,https://www.reddit.com/gallery/1n2rvvh,False,False,False
1n2pku5,AgeOfEmpires4AOE4,1756418297.0,3,/r/MachineLearning/comments/1n2pku5/p_training_environment_for_rl_of_ps2_and_other/,MachineLearning,[P] Training environment for RL of PS2 and other OpenGL games,"Hello everyone. I'm working on a training environment based on stable-retro and a Retroarch frontend, Sdlarch. This environment is intended to support PS2, GameCube, Dreamcast, and other video games that aren't supported by the original Stable-retro/Gym-Retro. If anyone wants to support me, or is curious, the link is below:

[https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)

There's still a lot of work ahead, as I'm implementing the final phase that enables PS2 training: loading states. For some reason I don't yet fully understand, the save state isn't loading (it just saves). But it's now possible to run games in the environment via Python, without the need to intercept any external processes.",16,1.0,https://www.reddit.com/r/MachineLearning/comments/1n2pku5/p_training_environment_for_rl_of_ps2_and_other/,False,True,False
1n2jekd,erfan_mhi,1756403937.0,2,/r/MachineLearning/comments/1n2jekd/r_emnlp_2025_ccps_confidence_from_consistency/,MachineLearning,[R] [EMNLP 2025] CCPS: Confidence from Consistency under Perturbation of States ‚Äî Superior Calibration Performance Across Benchmarks/Models,"Hi everyone,

Our paper **‚Äú*****Confidence from Consistency under Perturbation of States (CCPS)*****‚Äù** was accepted to the **EMNLP 2025 Main Conference**, placing in the **top 15% of accepted papers** with a **final meta-review rating of 9 (strong accept)**.

# üîç Motivation

LLMs don‚Äôt just make mistakes, they‚Äôre often confidently wrong. That‚Äôs fine when asking for trivia, but risky in domains like healthcare and finance. Reliable confidence estimation is critical for safe deployment.

# ‚ú® What is CCPS?

CCPS looks at the hidden states of an LLM. We apply small perturbations to the final hidden representations and observe how stable the prediction is:

* If the answer remains stable ‚Üí the model was truly confident.
* If the answer flips ‚Üí the confidence was unreliable.

This approach is simple, efficient, and does not require fine-tuning the base LLM.

# üìä Results

Across LLaMA, Mistral, and Qwen on MMLU and MMLU-Pro, CCPS outperformed prior methods like LitCab and Calibration Tuning (CT):

* **Calibration**: Error cut by more than 50%, down to \~4.5% on the toughest benchmarks.
* **Discrimination**: More accurate at telling right vs. wrong answers than prior SOTA (LitCab, CT, etc.).
* **Performance**: Boosts accuracy and robustness, all without fine-tuning the base LLM.

# üí° Why it matters

CCPS delivers more reliable, better-calibrated LLMs, models that don‚Äôt just generate answers but also provide trustworthy confidence signals. This is key for high-stakes AI applications, especially in the medical and finance industries.

# üìé Resources

* üìÑ Paper: [arXiv link](https://arxiv.org/abs/2505.21772)
* üíª Code: [GitHub repo](https://github.com/ledengary/CCPS)
* üìä Data: [HF Dataset](https://huggingface.co/datasets/ledengary/CCPS)

Happy to hear feedback, especially from anyone working on calibration, verifiers (for RL), or LLM deployment.",1,0.57,https://www.reddit.com/r/MachineLearning/comments/1n2jekd/r_emnlp_2025_ccps_confidence_from_consistency/,False,True,False
1n2i7iy,Stunning_Put_6077,1756401270.0,5,/r/MachineLearning/comments/1n2i7iy/r_how_im_structuring_a_16m_character_dialogue/,MachineLearning,[R] ‚ÄúHow I‚Äôm structuring a 16M character dialogue corpus for persona reconstruction in LLMs‚Äù,"In the past weeks, I‚Äôve been working on a somewhat ‚Äúcrazy‚Äù project:
manually splitting and structuring 16 million characters of dialogue data, preparing it for feeding into a model to reconstruct a persona module.

Along the way, I‚Äôve noticed a few technical challenges:
	1.	File size balance
Keeping each file around 300k‚Äì400k characters is the most stable. Beyond that, performance tends to drop.
	2.	Context continuity
Poor segmentation can easily break the model‚Äôs sense of persona, resulting in inconsistent tone.
	3.	Tagging & classification
It‚Äôs not just about cutting text, but also annotating emotional states and tonal shifts, so the model can later rebuild ‚Äúmemory‚Äù in a coherent way.

This made me realize that large-scale corpus curation is itself a kind of language engineering.
It‚Äôs not just data processing ‚Äî it shapes whether an AI can emerge as a whole presence.

I‚Äôm curious:
In your NLP or LLM practice, how do you balance scale with contextual integrity?",0,0.37,https://www.reddit.com/r/MachineLearning/comments/1n2i7iy/r_how_im_structuring_a_16m_character_dialogue/,False,True,False
1n2gdd4,Pan000,1756397137.0,15,/r/MachineLearning/comments/1n2gdd4/r_adding_layers_to_a_pretrained_llm_before/,MachineLearning,[R] Adding layers to a pretrained LLM before finetuning. Is it a good idea?,"I'm doing a full fine-tune on the Qwen 3 14B Base model with around 10B tokens for loss. I'd have preferred a little higher capacity. My idea is to add a few more layers at the end, initialized close to zero, and then train. Perhaps increase from 40 to 50 layers.

This is straightforward to implement. Is there a reason why I don't hear of this being done? Is anyone familiar with this? Any research indicating success or failure? It makes sense conceptually but I would assume it would be more common if it works.

(I asked the GPT5, Gemini Pro & Claude, but I'm getting mixed answers. It'll agree or disagree depending how I phrase the question.)",11,0.72,https://www.reddit.com/r/MachineLearning/comments/1n2gdd4/r_adding_layers_to_a_pretrained_llm_before/,False,True,False
1n2c588,Fragrant-Dog-3706,1756387471.0,0,/r/MachineLearning/comments/1n2c588/d_where_to_find_vast_amounts_of_schemas_for_ai/,MachineLearning,[D] Where to find vast amounts of schemas for AI model training?,"**\[D\] Looking for massive schema collections for training models**

working on a project and need to find vast amounts of schemas for training models. specifically looking for financial data (transactions, market data, etc) and retail/ecommerce stuff (product catalogs, user behavior, sales data) but honestly need schemas from pretty much every domain I can get. anyone know where to find quality structured schemas at scale? open to paid sources too. need thousands of different schema types ideally. thanks!",0,0.33,https://www.reddit.com/r/MachineLearning/comments/1n2c588/d_where_to_find_vast_amounts_of_schemas_for_ai/,False,True,False
1n29q0e,Knok0932,1756380675.0,2,/r/MachineLearning/comments/1n29q0e/p_paddleocrv5_implemented_in_c_with_ncnn/,MachineLearning,[P] PaddleOCRv5 implemented in C++ with ncnn,"I made a C++ implementation of PaddleOCRv5 that might be helpful to some people: https://github.com/Avafly/PaddleOCR-ncnn-CPP

The official Paddle C++ runtime has a lot of dependencies and is very complex to deploy. To keep things simple I use [ncnn](https://github.com/Tencent/ncnn) for inference, it's much lighter (and faster in my task), makes deployment easy. The code runs inference on the CPU, if you want GPU acceleration, most frameworks like ncnn let you enable it with just a few lines of code.

Hope this helps, and feedback welcome!",15,1.0,https://www.reddit.com/r/MachineLearning/comments/1n29q0e/p_paddleocrv5_implemented_in_c_with_ncnn/,False,True,False
1n28w7j,c-f_i,1756378010.0,2,/r/MachineLearning/comments/1n28w7j/p_built_sparrow_a_custom_language_modelnlp_tool/,MachineLearning,[P] Built Sparrow: A custom language model/NLP tool for microcontrollers,"Hey everyone,

Don't know if it fully matches this subreddit, but since there have been a lot of discussions around LLMs using a lot of power and water, and even more discussions around LLMs plateauing, as everyone focuses on making the biggest and most powerful model.

I've been super focused for a while now in bringing Language Models and complex NLP capabilities to microcontrollers and finally been able to finish the architecture and an ML Toolkit that enables training models from scratch, with this architecture and enables easy deployment on almost any MCUs.

The architecture uses state of the art methods, with many in-depth optimisations tested through over 1700 trained models, to get the most of every single memory byte and clock cycle, specifically for MCUs while also enabling extremely fast responses on PC.

The idea is to have domain specific and task specific models, using Sparrow's architecture, instead of a general prupose frontier model like ChatGPT/Llama etc. In the demo I showcase a Biology only model, that was made to give straight answrs (as per research papers showcasing that's what people want) for a question-answering chat-like system. Anything can be created. And then due to the model being only 50-200KB depending on how it is build (with twice that needed in total when flashed), mutiple models could be loaded in memory and a mixture-of-experts system can be designed. Which is what I want to explore with SPARROW 2.

I still have to see exactly how to proceed in terms of making the code open-source, best licensing methods, how to create the API, etc. But the idea is that it would be easy to create language models for MCUs, similar to how Sci-kit Learn is used for regular ML.

It supports encoder, decoder, encoder-decoder models, and the fastest model uses linear attention, but I have also been able to deploy dot attention and additive attention on the ESP32.

Let me know what you think!¬†[Here's a demo video](https://youtu.be/WCvv5W9gEiA?si=QCXvXei3qfp0qAG8)¬†with a ChatGPT simple-webapp to give people something they are familiar with. I'd also like to know opinions around the best way to go forward, release it as a website of sorts, release it as an API like Scikit Learn etc.

I have a lot of videos with the models running on PC with full phrases/paragraphs outputs in less than 10 miliseconds, have different versions Small, Main, Large running on the ESP32S3, have the Main flavour running on the ESP32P4 which can process everything 5-6 times faster due to the intrustions available, and outputting a phrase every 50-100ms, compared to ESP32S3's 300-600ms.",8,1.0,https://www.reddit.com/r/MachineLearning/comments/1n28w7j/p_built_sparrow_a_custom_language_modelnlp_tool/,False,True,False
1n1mboq,Good-Alarm-1535,1756313306.0,0,/r/MachineLearning/comments/1n1mboq/p_implemented_grpo_on_top_of_karpathys_makemore/,MachineLearning,[P] Implemented GRPO on top of Karpathy's makemore,"Hey all! I wanted to share my recent project where I implemented the GRPO (Group Relative Policy Optimization) algorithm on top of the [makemore](https://github.com/karpathy/makemore) repo.

I wanted to understand how the algorithm works and was trying to find small-scale toy problems where I can implement my own version and see if it works. I had a couple of ideas at first but then I settled on this one idea: to implement the algorithm on top of the makemore project where my goal would be to finetune the character-level language model to generate names with more vowels! So the reward is essentially the number of vowels you have in the generated names.

GRPO is actually a simplified version of PPO (which itself is a derivative of TRPO), and while its predecessors are rather complicated to fully grasp unless you have some background in policy gradient or RL in general, GRPO is much simpler to understand and code up (e.g., you don't have to worry about writing Generalized Advantage Estimation etc.)

Feel free to take a look and share your thoughts! Here's the repo: [https://github.com/souvikshanku/makemore-grpo/](https://github.com/souvikshanku/makemore-grpo/)",14,0.95,https://www.reddit.com/r/MachineLearning/comments/1n1mboq/p_implemented_grpo_on_top_of_karpathys_makemore/,False,True,False
1n2579o,AdInevitable1362,1756363912.0,6,/r/MachineLearning/comments/1n2579o/d_clarification_on_text_embeddings_models/,MachineLearning,[D] Clarification on text embeddings models,"I came across Gemini‚Äôs text embeddings model, and their documentation mentions that semantic similarity is suitable for recommendation tasks. They even provide this example:
	‚Ä¢	‚ÄúWhat is the meaning of life?‚Äù vs ‚ÄúWhat is the purpose of existence?‚Äù ‚Üí 0.9481
	‚Ä¢	‚ÄúWhat is the meaning of life?‚Äù vs ‚ÄúHow do I bake a cake?‚Äù ‚Üí 0.7471
	‚Ä¢	‚ÄúWhat is the purpose of existence?‚Äù vs ‚ÄúHow do I bake a cake?‚Äù ‚Üí 0.7371

What confuses me is that the ‚Äúcake‚Äù comparisons are still getting fairly high similarity scores, even though the topics are unrelated.

If semantic similarity works like this, then when I encode product profiles for my recommendation system, won‚Äôt many items end up ‚Äútoo close‚Äù in the embedding space? Does all the text embeddings model work that way ? 
And what is the best model or type of configuration could be suitable to my task ",11,0.93,https://www.reddit.com/r/MachineLearning/comments/1n2579o/d_clarification_on_text_embeddings_models/,False,True,False
1n23r3t,Lonely-Loquat9638,1756358475.0,0,/r/MachineLearning/comments/1n23r3t/r_discrete_diffusion_vla_bringing_discrete/,MachineLearning,[R] Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies,"**TL;DR.**¬†We introduce¬†**discrete diffusion**¬†as the action decoder¬†**inside a single transformer**¬†for VLA. Two simple components‚ÄîAdaptive decoding order and Secondary re-masking‚Äîyield consistent action refinement and outperform AR and continuous-diffusion heads. Trains with the¬†**same cross-entropy objective**¬†as VLMs, preserving pretrained priors. This design shows better success rates vs AR and continuous diffusion.  
**Disclosure:**¬†I‚Äôm an author.

**What‚Äôs new**

* **First discrete-diffusion action head for VLA**¬†(to our knowledge).
* **Single-transformer, VLM-style training:**¬†keeps the discrete token interface and uses the same CE loss as the VLM backbone ‚Üí¬†**maximizes retention of pretrained VLM priors**.
* **Adaptive decoding order:**¬†in each refinement round, we¬†**keep easy tokens first**¬†via confidence / confidence-gap scores and a cosine keep schedule; the rest remain masked for the next round.
* **Secondary re-masking:**¬†previously kept tokens are¬†**re-checked**¬†(threshold + residual-drop) and¬†**re-masked**¬†if uncertain/inconsistent, enabling robust cross-round error correction.

**Why it matters**

* For robotics manipulation tasks, unlike continuous diffusion decoders, our formulation keeps action generation inside a unified transformer and trains with the same cross-entropy objective used by VLMs. This¬†**preserves the backbone‚Äôs pretrained vision-and-language capability**‚Äîakin to extending a vocabulary‚Äîwhile opening a path to¬†**inherit unified transformers‚Äô scaling behavior**, paving the way for¬†**large-scale VLA**. Moreover, Discrete Diffusion VLA¬†**breaks the left-to-right bottleneck**¬†of AR decoders: action chunks are¬†**adaptively decoded in parallel**¬†over a small, fixed number of steps, and uncertain tokens can be revisited via iterative re-masking, leveraging full cross-modal context (including inter-action dependencies) for refinement.

**Links**

* Paper:¬†[https://arxiv.org/abs/2508.20072](https://arxiv.org/abs/2508.20072)
* Demo videos:¬†[https://huggingface.co/papers/2508.20072](https://huggingface.co/papers/2508.20072)",1,1.0,https://www.reddit.com/r/MachineLearning/comments/1n23r3t/r_discrete_diffusion_vla_bringing_discrete/,False,True,False
1n1wm8n,Adventurous-Cut-7077,1756337246.0,108,/r/MachineLearning/comments/1n1wm8n/n_unprecedented_number_of_submissions_at_aaai_2026/,MachineLearning,[N] Unprecedented number of submissions at AAAI 2026,"And 20K out of 29K submissions are from China (clearly dominating AI research now, well done to my Chinese friends). The review process at AI conferences isn't just broken - it's nuked. We need change, fast.

https://preview.redd.it/ih3vliracnlf1.png?width=1938&format=png&auto=webp&s=b7112a3e5e78ec7bcd0e6b100b5887a880fb82be",189,0.94,https://www.reddit.com/r/MachineLearning/comments/1n1wm8n/n_unprecedented_number_of_submissions_at_aaai_2026/,False,True,False
1n1ug7b,Material_Pool_986,1756331801.0,0,/r/MachineLearning/comments/1n1ug7b/p_jupytercadmcp_mcp_server_for_jupytercad_to/,MachineLearning,[P] jupytercad-mcp: MCP server for JupyterCAD to control it using LLMs/natural language.,"Demo: https://github.com/user-attachments/assets/7edb31b2-2c80-4096-9d9c-048ae27c54e7

Repo: https://github.com/asmith26/jupytercad-mcp",6,1.0,https://www.reddit.com/r/MachineLearning/comments/1n1ug7b/p_jupytercadmcp_mcp_server_for_jupytercad_to/,False,True,False
1n1tdcl,OkOwl6744,1756329289.0,4,/r/MachineLearning/comments/1n1tdcl/arxiv_submission_on_hold_r/,MachineLearning,Arxiv submission on hold  [R],"Hey 
Looking for information online about the on hold status but couldn‚Äôt find very clearly. The on hold is automatic or normal? Or if some sort of problem was found ? 

I already have a DOI from Zenodo, but wanted to publish on arxiv as it seems to be the norm currently. It‚Äôs my first publication there, so I‚Äôm not sure what the process is exactly. 

Thanks! ",0,0.36,https://www.reddit.com/r/MachineLearning/comments/1n1tdcl/arxiv_submission_on_hold_r/,False,True,False
1n1pcj7,AlanzhuLy,1756320000.0,8,/r/MachineLearning/comments/1n1pcj7/d_anyone_successfully_running_llms_fully_on_apple/,MachineLearning,[D] Anyone successfully running LLMs fully on Apple Neural Engine (ANE)?,"Has anyone managed to get near-full ANE utilization for large language models on Apple silicon?

In my experiments:

* Core ML conversions run, but ANE usage seems capped <20%.
* Apple‚Äôs own foundation models reportedly hit close to 100% ANE.

**Questions:**

* Has anyone here seen full (or close to full) ANE usage for LLMs?
* Are there known tricks or constraints (model architecture, quantization, Core ML flags) that unlock more ANE execution?
* Any open-source repos, discussions, or Apple docs you‚Äôd point to?

Would love to hear practical experiences‚Äîsuccesses, failures, or hard limits you‚Äôve hit.",6,0.88,https://www.reddit.com/r/MachineLearning/comments/1n1pcj7/d_anyone_successfully_running_llms_fully_on_apple/,False,True,False
1n1p7rb,function-devs,1756319712.0,16,/r/MachineLearning/comments/1n1p7rb/d_i_reviewed_100_models_over_the_past_30_days/,MachineLearning,[D] I reviewed 100 models over the past 30 days. Here are 5 things I learnt.,"I reviewed 100 models over the past 30 days. Here are 5 things I learnt.

TL;DR: Spent a month testing every AI model for work, a few tools I'm building and RL. Build task-specific evals. Most are overhyped, a few are gems, model moats are ephemeral, and routers/gateways are the real game-changer.

So I've been building a few evaluation tools, RHLF and RL environments for the past few months so I decided to be extra and test literally everything.

100 models. 30 days. Too much coffee :( Here's what I found:  
  
1. Model moats are ephemeral

Model moats don't last and it can be hard to pay for many subscriptions if you're building for users and machines. What's SOTA today gets beaten in 2 months. Solution: Use platforms like Groq, OpenRouter, FAL, Replicate etc

My system now routes based on task complexity: Code generation, Creativity, Complex reasoning and Code generation.

2. Open source FTW

The gap is closing FAST. Scratch that. The gap between open and closed models has basically disappeared. If you're not evaluating open-source options, you're missing 80% of viable choices. From Deepseek, Qwen to Kimi, these models help you build quick MVPs at little or no cost. If you do care about privacy, Ollama and LMStudio are really good for local deployment.

3.Benchmarks are mostly decieving due to reward hacking

Benchmaxxing is a thing now. Models are increasingly being trained on popular eval sets, and it's actually annoying when models that scored ""high"" but sucked in practice. It's also why I'm a huge fan of human preference evaluation platforms that are not easily gamed (real world vs benchmarks). Build your own task-specific evals.

4.Inference speed is everything

Speed matters more than you think. Users don't care if your model is 2% more accurate if it takes 30 seconds to respond. Optimize for user experience, not just accuracy. Which leads me to..

5.Task-specific models > general purpose models for specialized work.

No 4 is also a huge reason why I'm a huge fan of small models finetuned for special tasks. Model size doesn't predict performance.

Test small models first etc Llama 3.2 1B, smolLLM, moondream etc and see if you can get a huge boost by finetuning them on domain tasks rather than just deploying a big SoTA general purpose model. Cost way lesser and usually faster.

What models are in your current prod stack? Any hidden gems I missed in the open source space?",0,0.44,https://www.reddit.com/r/MachineLearning/comments/1n1p7rb/d_i_reviewed_100_models_over_the_past_30_days/,False,True,False
1n1k9ty,AdventurousSwim1312,1756308731.0,0,/r/MachineLearning/comments/1n1k9ty/r_archifactory_benchmark_slm_architecture_on/,MachineLearning,"[R] ArchiFactory : Benchmark SLM architecture on consumer hardware, apples to apples","[35M Parameters : RWKV vs Mamba vs GQA vs RetNet](https://preview.redd.it/vul29llezklf1.png?width=1106&format=png&auto=webp&s=c951d5647cd895d418b5a0863184cf9f6745397e)



Since it's introduction, the Attention mechanism has been king in LLM architecture, but a few vaillant projects like RWKV, Mamba, Retnet, LiquidAI have been proposing several new mixin mecanisms over time, to attempt to dethrone the king.



One of the major issue is that LLM pretraining is extremely dependant on number of parameters and dataset choices, so performing an ablation study on new architecture is not an easy tricks.



On the other hand, I met many people with brillant ideas for new architecture and who never got the chance to put it to the test.

For that purpose, i create ArchiFactory, a simple (<500 lines of codes) and modular repo that enables to pretrain Small Language Models with comparable parameter count and architecture tricks, in a couple of hours on a single 3090 level GPU.



Included:

\- simple modular architecture to be sure to compare similar stuff

\- complete optimized training loop using pytorch lightning

\- fp8 training (can achieve <20min training on 5090 grade GPU)

\- examples of common modules like FFN, MOE, GQA, Retnet, Mamba, RWKV6 etc.

\- guidelines to test integrate new modules



Link: [https://github.com/gabrielolympie/ArchiFactory](https://github.com/gabrielolympie/ArchiFactory)",21,0.96,https://www.reddit.com/r/MachineLearning/comments/1n1k9ty/r_archifactory_benchmark_slm_architecture_on/,False,True,False
1n1gucy,kekkodigrano,1756300770.0,43,/r/MachineLearning/comments/1n1gucy/d_how_to_do_impactful_research_as_a_phd_student/,MachineLearning,[D] How to do impactful research as a PhD student?,"Hi everyone,

I‚Äôm feeling a bit lost in my PhD journey and would really appreciate some outside perspectives.

I‚Äôm doing a PhD on LLMs, and so far I‚Äôve been fairly productive: I‚Äôve published several first-author papers, some accepted at top conferences, others under review with good chances of acceptance. I‚Äôve also had a few successful collaborations.

The issue is that I don‚Äôt actually like my research. To be honest, I often feel a bit fraudulent, I rush through projects, produce papers that look solid and well-structured, but in the end, I think their impact is minimal. What I really want is to work on something meaningful and useful. But I keep running into two several obstacles:

- Any problem I consider tackling already has an overwhelming amount of literature, making it difficult to figure out what truly matters.

- While I‚Äôm trying to sort this out, there‚Äôs always the risk that someone else publishes a similar idea first, since so many people are working in this space.

- I work with two supervisors which are both young and highly hambitius. They always propose me new research and collaboration but they never propose me hambitius project or give me time to think deep about something. I'm always involved in fast-paced project that lead to pubblication in few months.


Because of this, my current strategy has been to work quickly, run experiments fast, and push out papers, even if they‚Äôre not especially deep or important. I also see publications as my main leverage: since I‚Äôm at a low-ranked university in a unknown group, my publication record feels like the only card I can play to land some opportunities in top labs/companies.

At times, I think I just want to land an industry roles as a research engineer, where just having a good numbers of papers on my CV would be enough. But deep down, I do care about my work, and I want to contribute something that feels genuinely important.

So I‚Äôm curious: how do you approach doing meaningful research in such a competitive field? How do you balance the pressure to publish with the desire to work on something truly impactful?",134,0.94,https://www.reddit.com/r/MachineLearning/comments/1n1gucy/d_how_to_do_impactful_research_as_a_phd_student/,False,True,False
1n1fsa3,FreakedoutNeurotic98,1756298060.0,1,/r/MachineLearning/comments/1n1fsa3/d_short_write_up_on_how_to_implement_custom/,MachineLearning,[D] short write up on how to implement custom optimizers in Optax,"Hi, I was trying to implement the muon optimizer in JAX and found there was no proper documentation about how to hack optax for custom optimizers so tried to write a mini blog about it.

https://slavozard.bearblog.dev/implementcustomoptimizerwithoptax/

Feedback appreciated.",13,0.93,https://www.reddit.com/r/MachineLearning/comments/1n1fsa3/d_short_write_up_on_how_to_implement_custom/,False,True,False
1n1ebmk,Any_Commercial7079,1756293760.0,13,/r/MachineLearning/comments/1n1ebmk/r_computational_power_needs_for_machine_learningai/,MachineLearning,[R] Computational power needs for Machine Learning/AI,"Hi everyone!

As part of my internship, I am conducting research to understand the computational power needs of professionals who work with machine learning and AI. The goal is to learn how different practitioners approach their requirements for GPU and computational resources, and whether they prefer cloud platforms (with inbuilt ML tools) or value flexible, agile access to raw computational power.

If you work with machine learning (in industry, research, or as a student), I‚Äôd greatly appreciate your participation in the following survey. Your insights will help inform future solutions for ML infrastructure.

The survey will take about two to three minutes. Here¬¥s the link:¬†[https://survey.sogolytics.com/r/vTe8Sr](https://survey.sogolytics.com/r/vTe8Sr)

Thank you for your time! Your feedback is invaluable for understanding and improving ML infrastructure for professionals.",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1n1ebmk/r_computational_power_needs_for_machine_learningai/,False,True,False
1n1e9c1,Altruistic_Bother_25,1756293571.0,20,/r/MachineLearning/comments/1n1e9c1/r_is_stacking_classifier_combining_bert_and/,MachineLearning,[R] Is stacking classifier combining BERT and XGBoost possible and practical?,Suppose a dataset has a structured features in tabular form but in one column there is a long text data. Can we use stacking classifier using boosting based classifier in the tabular structured part of the data and bert based classifier in the long text part as base learners. And use logistic regression on top of them as meta learner. I just wanna know if it is possible specially using the boosting and bert as base learners. If it is possible why has noone tried it (couldn‚Äôt find paper on it)‚Ä¶ maybe cause it will probably be bad?,20,0.81,https://www.reddit.com/r/MachineLearning/comments/1n1e9c1/r_is_stacking_classifier_combining_bert_and/,False,True,False
1n127sr,ChoiceStranger2898,1756253200.0,16,/r/MachineLearning/comments/1n127sr/are_neurips_workshop_competitive_r/,MachineLearning,Are Neurips workshop competitive? [R],"Hi y‚Äôall, I have a optimisation paper that is not quite ready for conference yet, and I see there are a few Neurips workshop coming up that fits my research direction. I‚Äôm wondering if it‚Äôs good to submit the work to the workshop?",14,0.86,https://www.reddit.com/r/MachineLearning/comments/1n127sr/are_neurips_workshop_competitive_r/,False,True,False
1n10vyv,SoggyClue,1756249681.0,7,/r/MachineLearning/comments/1n10vyv/d_tips_tricks_for_preparing_slidestalks_for_ml/,MachineLearning,[D] Tips & tricks for preparing slides/talks for ML Conferences?,"I'm a PhD student in HCI, and I recently had a paper accepted at a B-ranked ML conference. While I have prior experience presenting at HCI venues, this will be my first time presenting at an ML conference.

I want to know if there are any tips or best practices for preparing slides and giving talks in the ML community. Are there particular presentation styles, slide formats, or expectations that differ from HCI conferences?

Thanks in advance for your advice!",9,0.81,https://www.reddit.com/r/MachineLearning/comments/1n10vyv/d_tips_tricks_for_preparing_slidestalks_for_ml/,False,True,False
1n0zndc,SwissMountaineer,1756246524.0,21,/r/MachineLearning/comments/1n0zndc/d_laptop_suggestion_for_phd_in_ml_for_robotics/,MachineLearning,[D] Laptop Suggestion for PhD in ML for Robotics,"Hi!

I'll be starting a PhD in ML for Robotics (RL, Sensor Fusion etc.) and was wondering which laptop would be best to support me throughout the next 4 years. I am looking for a powerful laptop, with good battery life, not too heavy and that is robust.

My budget is $3000.

So far, I have identified the following laptops, but am unsure which would be the best choice.

\-¬†**Razer Blade 16**¬†(either RTX 5070 Ti + 32GB RAM ($3100) or RTX 5080 + 64GB ($4050)): apart from battery life which is not the most ideal, would I see a significant difference when running RL simulations (IsaacGym) or large multimodal (video, imu, ...) ML models between both configurations? Price difference between both configurations is \~$850 (with taxes) which is significant.

\-¬†**MSI Vector 16¬†HX¬†AI**¬†(RTX‚ÄØ5080, 64‚ÄØGB) - $2600

\-¬†**ThinkPad P1 Gen 7**¬†(RTX Ada 3000, 64GB) - $3200: has a good battery life, but its GPU is Ada series, which is not the best for RL simulations.

\-¬†**Legion Pro 7i Gen10**¬†(RTX 5080, 32GB) - $3100: the legions are usually very heavy laptops.

Essentially, I am looking for a laptop that will be somewhat future-proof to the fast pace of new GPUs coming out, is powerful for my intended use (RL simulations + ML sensor fusion), has a good battery life (for note-taking in courses) and easily transportable (ie. neither too bulky nor heavy). Also, do I require RTX 5080 (recommended for IsaacSim) as GPU, and how big a diffference is 32GB vs 64GB RAM?

Thank you in advance for any suggestions or feedback!

EDIT: I have access to cluster, but thought having powerful laptop could be useful when running real-time inference on robot + working with smaller models / testing out stuff before training on cluster.",0,0.32,https://www.reddit.com/r/MachineLearning/comments/1n0zndc/d_laptop_suggestion_for_phd_in_ml_for_robotics/,False,True,False
1n0wdsi,AaronSpalding,1756238884.0,5,/r/MachineLearning/comments/1n0wdsi/r_what_makes_active_learning_or_self_learning/,MachineLearning,[R] What makes active learning or self learning successful ?,"Maybe I am confused between two terms ""active learning"" and ""self-learning"". But the basic idea is to use a trained model to classify bunch of unannotated data to generate pseudo labels, and train the model again with these generated pseudo labels.  Not sure ""bootstraping"" is relevant in this context.

A lot of existing works seem to use such techniques to handle data. For example, SAM (Segment Anything) and lots of LLM related paper, in which they use LLM to generate text data or image-text pairs and then use such generated data to finetune the LLM.

My question is why such methods work?  Will the error be accumulated since the pseudo labels might be wrong?",0,0.25,https://www.reddit.com/r/MachineLearning/comments/1n0wdsi/r_what_makes_active_learning_or_self_learning/,False,True,False
1n0vcrb,JustinAngel,1756236524.0,5,/r/MachineLearning/comments/1n0vcrb/r_Œ¥apt_critical_review_aimed_at_maximizing/,MachineLearning,[R] ŒîAPT: critical review aimed at maximizing clinical outcomes in AI/LLM Psychotherapy,"Hi reddit, wanted to share my thesis on AI / LLM psychotherapy @ [https://osf.io/preprints/psyarxiv/4tmde\_v1](https://osf.io/preprints/psyarxiv/4tmde_v1?fbclid=IwZXh0bgNhZW0CMTAAYnJpZBExNHhlVkhlWWpDVE1xN3dTeAEeoTtZ3pOVtRD7ODEFZo_qpyjjOEkW_2OFHqsH36X4xp7THoZC3F7YFDc1zJU_aem_Etq7yhCr4L3eA8v9QqrFgw)

Since the rules for this subreddit require more than just a link, I thought I'd share some surprising conclusions in plain english. 

**1. AI therapy research tends to use arbitrary success metrics:** the majority of LLM research on psychotherapy uses theraputic-sounding ad-hoc metrics (e.g. ""empathy"" as rated by LLM-as-judge), and not actually improvement in clients or other validated metrics. There's a real risk in AI researchers testing techniques and drawing conclusions when totally unrelated to the purpose of therapy (e.g. quality-of-life improvement). If you're interested in learning more about this issue, section 1.4 focuses on it, and offers the north-star alternatives commonly used in psychotherapy research in sections 1.1-1.3. 

**2. AI therapy tools (APTs) are already comparable to human therapists:** There's two studies from 2025 (Limbic, Therabot) that demonstrate non-inferior clinical outcomes in LLM-driven APTs and human therapists for depression & anxiety symptom reduction. If replicated, that's huge. That's a step-level jump in clinical from the previous generation of rules-based APTs (e.g. Woebot, Wysa), highlighting that maybe the generative properties of LLMs were the key gap to improve clinical performance. There's a lot more to say on these results, and if you're interested sections 2 & 3.1 talk more about them and put them into clinical context. 

3. **ŒîAPT allows predicting future clinical outcomes :** It's actually surprising that APTs perform at the lower-bounds of human therapists, since they kinda suck right now. The predictive model I proposed is that APTs clinical performance is boosted by advantages therapist can't compete with (e.g. 24/7 availability, low cost), while being depressed by current disadvantages (e.g. poor therapy skills, hallucinations, sycophancy, inconsistencies, bias). All of this playing out while major issues around legality, safety, privacy and ethics are unresolved and could shutdown the field. If you're intersted, you can read more about the model (section 3.3),  the advantages of APTs over human therapists (section 3.4), APTs' current limitations (section 3.5), and the key risks (section 3.6). 

https://preview.redd.it/rof96tmbuelf1.png?width=1162&format=png&auto=webp&s=5a1e81bbb9e8b12b09210967da97b2fe96816df0

  
**4. Techniques teaching LLM therapy:** Most people on this subreddit won't be surprised to learn you can teach LLM to perform therapy using a combination of context/prompt engineering, fine-tuning, multi-agent architecture, and ML models. What is surprising is that both clinically-validated APTs use ML models to offset the stochastic nature of LLMs, especially for safety purposes. Also surprising is that neither used a multi-agentic architecture. Therabot used fine-tuning on synthetic dialogues, and Limbic used context-engineering techniques. You can learn more about implementing therapy skills in LLM through context/prompt engineering (section 4.1), fine-tuning (section 4.2), multi-agent architectures (section 4.3), ML models (4.4). Around fine-tuning / pretraining there's a really nested conversation about data requirements, ethically sourcing transcripts, and choosing therapy modalities in section 4.1. 

https://preview.redd.it/lbcoovvc0flf1.png?width=2246&format=png&auto=webp&s=f029fed00649b4cca0ddb84d9830ded03f5f94ea

5. **Overall, most disadvantages of LLMs are addressable in AI therapy**: Reading the literature critiquing APTs it's really easy to get discouraged thinking for examples ""oh wow, hallucinations are going to make AI therapy impossible"". But actually, there's a bunch of techniques that can be used to mitigate the issues LLMs currently have. Combining the lowering rates of issues in newer LLMs released with mitigation techniques, most issues can theoretically be significantly mitigated in production. The outlier here being sycophancy which doesn't appear to have great mitigations on subjective topics. You can read more about the issues of LLMs in APTs and how to mitigate those in section 5. 

**6. video therapy with multi-modal audio/video LLMs:** One surprising fact from psychotherapy research is that therapy done over video (e.g. zoom) is actually as effective as in-person therapy. Ideally, LLMs would be able to pickup and transmit non-verbal cues over video-audio. Having an virtual therapy avatar using audio & video to attune to clients isn't actually that far off based on my literature review. Surprisingly it seems that emotional speech, and attuning to clients facial and body expressions are ready for implementation in AI therapy today. More on that in section 6.

Happy to have a conversation, receive critique, and answer questions here. This summary above was meant to offer informal insights into what is an otherwise quite lengthy paper. For more formal discussion and details, it's really best to read the paper. ",117,0.77,https://www.reddit.com/r/MachineLearning/comments/1n0vcrb/r_Œ¥apt_critical_review_aimed_at_maximizing/,False,True,False
1n0t4hu,Look-Asleep,1756231403.0,11,/r/MachineLearning/comments/1n0t4hu/d_do_industry_research_roles_care_about_findings/,MachineLearning,"[D] Do Industry Research Roles Care about Findings vs. Main (in ACL, NAACL, EMNLP, etc.)?","Basically the title. Obviously the quality of the work and relevance to the role is very important, but all else being equal, what is the perceived prestige difference between Findings and Main in NLP conferences? This would be with regard to getting research internships and research scientist positions.",14,0.82,https://www.reddit.com/r/MachineLearning/comments/1n0t4hu/d_do_industry_research_roles_care_about_findings/,False,True,False
1n0r8b7,FutureIncrease,1756227256.0,23,/r/MachineLearning/comments/1n0r8b7/i_built_a_tool_to_benchmark_tokenizers_across_100/,MachineLearning,I built a tool to benchmark tokenizers across 100+ languages and found some wild disparities [R],"**TL;DR:** Created [tokka-bench](https://tokka-bench.streamlit.app/) to compare tokenizers across languages. Turns out your fine-tune's multilingual performance might suck because of tokenization, not architecture. Also explains why proprietary models (Claude, GPT, Gemini) are so much better at non-English tasks.

**Links:**

* [Live dashboard](https://tokka-bench.streamlit.app/)
* [Full blog post](https://www.bengubler.com/posts/2025-08-25-tokka-bench-evaluate-tokenizers-multilingual)
* [GitHub repo](https://github.com/bgub/tokka-bench)

https://preview.redd.it/7i03jela9elf1.png?width=1724&format=png&auto=webp&s=95378457970e6337b147e71d7a8f0ab2dd67cb91

# The Problem Nobody Talks About

I started this as a side quest while pretraining a multilingual model, but tokenization turned out to be way more important than expected. There are two hidden layers creating massive efficiency gaps:

**UTF-8 encoding differences:**

* English: \~1 byte per character
* Arabic: 2+ bytes per character
* Chinese: 3+ bytes per character

**Tokenization bias:** Most tokenizers are trained on English-heavy data, so they allocate way more vocabulary to English patterns. These compound into serious problems.

# Why This Affects Performance

**During training:** If you allocate tokens proportionally (10M English, 1M Khmer), the Khmer text has WAY less semantic content because it needs more tokens per word. Plus Khmer tokens end up being character-level instead of semantic units, making concept storage much harder.

**During inference:** Low-resource languages need 2-3x more tokens per sentence:

* Slower throughput (costs more to serve)
* Context windows fill up faster
* More chances to mess up during generation

# What I Built

tokka-bench measures four key things:

1. **Efficiency** \- bytes per token (compression quality)
2. **Coverage** \- unique tokens used (script representation)
3. **Word splitting** \- how often semantic units get fragmented
4. **Subword fertility** \- average tokens per semantic unit

# Interesting Findings

You can actually reverse-engineer training data from tokenizer performance:

* Kimi K2: Exceptional Mandarin coverage (obviously Chinese-trained)
* Gemma 3: Strong Urdu/Hindi performance
* gpt-oss: Good Arabic/Gujarati coverage

Weirdest finding: Programming languages show almost identical efficiency across all tokenizers. Probably because everyone trains on GitHub with similar language distributions.

# Technical Details

Built on high-quality datasets (FineWeb, FineWeb-2, StarCoder). Samples 2MB per language and calculates per-language metrics. Has some limitations around cross-linguistic comparison due to UTF-8 differences, but great for comparing tokenizers on the same language.

Shoutout to Judit √Åcs for the original subword fertility metrics and Rust et al's ACL paper that laid the groundwork.

**PS:** if you're from an AI lab and want to contribute your tokenizer's metrics (even if proprietary), please reach out! The community would benefit a lot from understanding how SOTA systems handle this stuff.

*Posted this on LinkedIn/Twitter already but figured* r/MachineLearning *would appreciate the technical details. Happy to answer questions about methodology or findings!*",81,0.93,https://www.reddit.com/r/MachineLearning/comments/1n0r8b7/i_built_a_tool_to_benchmark_tokenizers_across_100/,False,True,False
1n0qwzm,beautiful-potato,1756226565.0,1,/r/MachineLearning/comments/1n0qwzm/d_analyzed_402_healthcare_ai_repos_and_built_the/,MachineLearning,[D] Analyzed 402 healthcare ai repos and built the missing piece,"I looked through 402 healthcare AI repos on GitHub and found almost 50% of infrastructure tools are just solving data format conversion problems, suggesting a systematic gap between ML research and deployment in clinical settings.

Built HealthChain to bridge Python ML workflows with healthcare data standards (FHIR, HL7, etc.) without the usual pain. 4 years of NHS NLP development experience went into making this feel like normal Python.

Post + pretty graphs: https://open.substack.com/pub/jenniferjiangkells/p/healthchain-building-the-tool-i-wish?r=4o6h4

Code: https://github.com/dotimplement/HealthChain

Anyone else work in healthcare AI here? Would love to learn what you‚Äôre working on!",11,0.76,https://www.reddit.com/r/MachineLearning/comments/1n0qwzm/d_analyzed_402_healthcare_ai_repos_and_built_the/,False,True,False
1n0q4d9,devops_to,1756224781.0,4,/r/MachineLearning/comments/1n0q4d9/d_looking_for_a_selfhosted_alternative_to/,MachineLearning,[D] Looking for a self-hosted alternative to Modal.com for running ML workloads,"Hey folks 



I've been using [Modal.com](http://Modal.com) (I am not affiliated) for a while to run machine learning workloads in the cloud, and I really like its simplicity, container-based execution, and ability to scale on demand. However, I'm starting to explore more self-hosted options due to cost reasons and to gain more control over the infrastructure while building apps.



Does anyone know of good self-hosted alternatives that offer similar functionality? Ideally, something that:



\- Supports containerized jobs (Docker or similar)

\- Can run Python/ML workloads easily

\- Has a nice API  for launching jobs (this is important) 

\- Offers some kind of job orchestration or scheduling

\- Bonus: GPU support and autoscaling would be amazing





Thanks in advance 

",4,0.83,https://www.reddit.com/r/MachineLearning/comments/1n0q4d9/d_looking_for_a_selfhosted_alternative_to/,False,True,False
1n0njtk,illustriousplit,1756218944.0,7,/r/MachineLearning/comments/1n0njtk/r_exploring_interpretable_ml_with_piecewiselinear/,MachineLearning,[R] Exploring interpretable ML with piecewise-linear regression trees (TRUST algorithm),"A recurring challenge in ML is balancing **interpretability** and **predictive performance**. We all know the classic tradeoff: simple models like linear regression or short CART-style regression trees are transparent but often lack enough accuracy, while complex ensembles like Random Forests and XGBoost are accurate but opaque.

We‚Äôve been working on a method called **TRUST** (*Transparent, Robust and Ultra-Sparse Trees*). The core idea is to go beyond constant values in the leaves of a tree. Instead, TRUST fits a sparse regression model (either linear or constant) in each leaf, resulting in a **piecewise-linear tree** that remains interpretable.

In our [recent paper](https://arxiv.org/abs/2506.15791), accepted at PRICAI 2025, we compared this method against a range of models on 60 datasets. While we were encouraged by the results ‚Äî TRUST consistently outperformed other interpretable models and closed much of the accuracy gap with Random Forests ‚Äî we'd like to hear your thoughts on this topic.

The problem we‚Äôre tackling is widespread. In many real-world applications, a ""black box"" model isn't an option. We've often found ourselves in situations where we had to choose between a sub-par interpretable model or an accurate but untrustworthy one.

Here‚Äôs a concrete example from a [tutorial on explaining EU life satisfaction](https://github.com/adc-trust-ai/trust-free/blob/main/notebooks/trust-free_tutorial.ipynb).

[TRUST produces a single interpretable tree, while Random Forest uses hundreds of deep trees to achieve similar accuracy.](https://preview.redd.it/3tzdaim3kdlf1.png?width=2600&format=png&auto=webp&s=e289771608b0d74498dc83b39c1efd2670ed8ea9)

As the image above shows, both TRUST and a Random Forest achieve \~85% test R¬≤ ‚Äî but one produces a **single interpretable tree**.

TRUST is implemented as a free Python package on PyPI called `trust-free`.

**Discussion:** How do you usually handle the interpretability vs. accuracy tradeoff in your own regression projects? What methods, beyond the standard ones, have you found effective? We‚Äôre looking forward to hearing your perspectives.",11,0.93,https://www.reddit.com/r/MachineLearning/comments/1n0njtk/r_exploring_interpretable_ml_with_piecewiselinear/,False,True,False
1n0jxbk,Total_Noise1934,1756209761.0,2,/r/MachineLearning/comments/1n0jxbk/p_spam_vs_ham_nlp_classifier_feature_engineering/,MachineLearning,[P] Spam vs. Ham NLP Classifier ‚Äì Feature Engineering vs. Resampling,"I built a spam vs ham classifier and wanted to test a different angle: instead of just oversampling with SMOTE, could **feature engineering** help combat extreme class imbalance?

**Setup:**

* Models: Na√Øve Bayes & Logistic Regression
* Tested with and without SMOTE
* Stress-tested on 2 synthetic datasets (one ‚Äúnormal but imbalanced,‚Äù one ‚Äúadversarial‚Äù to mimic threat actors)

**Results:**

* Logistic Regression ‚Üí **97% F1** on training data
* New imbalanced dataset ‚Üí Logistic still best at **75% F1**
* Adversarial dataset ‚Üí **Na√Øve Bayes** surprisingly outperformed with **60% F1**

**Takeaway:** Feature engineering can mitigate class imbalance (sometimes rivaling SMOTE), but adversarial robustness is still a big challenge.

Code + demo:  
üîó [PhishDetective ¬∑ Streamlit](https://phishdetective.streamlit.app/)  
üîó [ahardwick95/Spam-Classifier: Streamlit application that classifies whether a message is spam or ham.](https://github.com/ahardwick95/Spam-Classifier/tree/main)

Curious ‚Äî when you deal with **imbalanced NLP tasks**, do you prefer resampling, cost-sensitive learning, or heavy feature engineering?",0,0.25,https://www.reddit.com/r/MachineLearning/comments/1n0jxbk/p_spam_vs_ham_nlp_classifier_feature_engineering/,False,True,False
1n0jwj7,LostAmbassador6872,1756209701.0,10,/r/MachineLearning/comments/1n0jwj7/p_docstrange_structured_data_extraction_from/,MachineLearning,[P] DocStrange - Structured data extraction from images/pdfs/docs,"I previously shared the open‚Äësource library DocStrange. Now I have hosted it as a free to use web app to upload pdfs/images/docs to get clean structured data in Markdown/CSV/JSON/Specific-fields and other formats.

**Live Demo:**¬†[**https://docstrange.nanonets.com**](https://docstrange.nanonets.com/)

**Github:** [**https://github.com/NanoNets/docstrange**](https://github.com/NanoNets/docstrange)

Would love to hear feedbacks!

https://i.redd.it/gl23k00osclf1.gif

Original Post - [https://www.reddit.com/r/MachineLearning/comments/1mh9g3r/p\_docstrange\_open\_source\_document\_data\_extractor/](https://www.reddit.com/r/MachineLearning/comments/1mh9g3r/p_docstrange_open_source_document_data_extractor/)

",29,0.87,https://www.reddit.com/r/MachineLearning/comments/1n0jwj7/p_docstrange_structured_data_extraction_from/,False,True,False
1n0j8u0,Adrienkgz,1756207721.0,1,/r/MachineLearning/comments/1n0j8u0/d_ano_updated_optimizer_for_noisy_deep_rl_now_on/,MachineLearning,[D] Ano: updated optimizer for noisy Deep RL ‚Äî now on arXiv (feedback welcome!),"Hi everyone,

A few weeks ago I shared my first preprint on a new optimizer,¬†Ano, designed for noisy and highly non-convex environments such as deep RL. Thanks to all the feedback I received here, I‚Äôve updated the paper: clarified the positioning, fixed some mistakes, and added an Atari benchmark to strengthen the empirical section.

üîó¬†**arXiv link:**¬†[https://arxiv.org/abs/2508.18258](https://arxiv.org/abs/2508.18258)  
üì¶¬†**Install via pip:**¬†`pip install ano-optimizer`  
üíª¬†**Code & experiments:**¬†[github.com/Adrienkgz/ano-experiments](https://github.com/Adrienkgz/ano-experiments)

Quick recap of the idea: Ano separates the momentum¬†direction¬†from the gradient magnitude, aiming to improve robustness and stability compared to Adam in noisy deep RL training. The updated version also includes a¬†convergence proof¬†in standard non-convex stochastic settings.

This is still my first research contribution, so I‚Äôd love to hear your thoughts ‚Äî whether on the method itself, the experiments, or the clarity of the writing. Any feedback, comments, or constructive criticism are very welcome üôè

Thanks again to everyone who took the time to give feedback last time, it really helped me make the work stronger!

Adrien",8,0.91,https://www.reddit.com/r/MachineLearning/comments/1n0j8u0/d_ano_updated_optimizer_for_noisy_deep_rl_now_on/,False,True,False
1n0h48h,Blackliquid,1756200290.0,4,/r/MachineLearning/comments/1n0h48h/d_sota_solution_for_quantization/,MachineLearning,[D] SOTA solution for quantization,"Hello researchers,

  
I am familiar with common basic approaches to quantization, but after a recent interview, I wonder what the current SOTA approaches are, which are actually used in industry.

  
Thanks for the discussion!",1,0.6,https://www.reddit.com/r/MachineLearning/comments/1n0h48h/d_sota_solution_for_quantization/,False,True,False
1n0eyrb,jain-nivedit,1756191737.0,4,/r/MachineLearning/comments/1n0eyrb/p_exosphere_an_open_source_runtime_for_dynamic/,MachineLearning,[P] Exosphere: an open source runtime for dynamic agentic graphs with durable state. results from running parallel agents on 20k+ items,"Disclosure: I am one of the authors. Links will be in the first comment per sub rules.

TLDR  
We are releasing Exosphere, an open source runtime and durable state manager for agentic workflows that need dynamic branching, retries, and parallel execution. To evaluate it on a real workload, we built WhatPeopleWant, an agent that mines Hacker News discussions and posts distilled problem statements to X every 2 hours. This post shares the setup, workload design, and the ablations we are running, and invites feedback on methodology.

Single runs are trivial. At scale you need to

1. fan out across large inputs
2. branch at runtime on model outputs
3. retry with idempotency
4. persist every step for audit and replay
5. mix CPU and GPU stages
6. resume after faults.

Exosphere‚Äôs runtime treats agents like graphs with explicit state, a scheduler, and observability.

We use WhatPeopleWant as a standing benchmark. It ingests Hacker News via the public Firebase API, scores and routes items, optionally enriches high-signal threads, and materializes candidate problem statements. The bot then posts outputs on a fixed schedule.

‚Ä¢ Gating high-signal discussions reduces heavy-model calls and improves tail behavior at similar quality thresholds  
‚Ä¢ Durable state and idempotent nodes make partial replays predictable and minimize upstream rework after faults  
‚Ä¢ Parallelism helps until external API backpressure dominates, which shows up in queue depth and wait times

What I want feedback on  
‚Ä¢ Composite metrics that capture quality, cost, and reliability for agentic graphs  
‚Ä¢ Fair baselines for orchestration when branching is dynamic  
‚Ä¢ Better failure-injection and replay methodologies to compare runtimes

First comment with links",6,1.0,https://www.reddit.com/r/MachineLearning/comments/1n0eyrb/p_exosphere_an_open_source_runtime_for_dynamic/,False,True,False
1n0e7s1,Tesocrat,1756188829.0,2,/r/MachineLearning/comments/1n0e7s1/dhow_can_ai_teams_stay_agile_and_adaptable_when/,MachineLearning,[D]How can AI teams stay agile and adaptable when project goals or data requirements change midstream?,"For those working in AI/ML, how do you keep your teams agile when project goals or data requirements shift halfway through a project? I‚Äôve seen situations where a model was nearly production-ready, but then stakeholders introduced new objectives or the data pipeline changed, forcing big pivots.
",0,0.47,https://www.reddit.com/r/MachineLearning/comments/1n0e7s1/dhow_can_ai_teams_stay_agile_and_adaptable_when/,False,True,False
1n0d12h,ZealousidealSalt7133,1756184492.0,18,/r/MachineLearning/comments/1n0d12h/d_an_honest_attempt_to_implement_attention_is_all/,MachineLearning,"[D] An honest attempt to implement ""Attention is all you need"" paper","I have started working on implementing actual research papers in machine learning and I have started with ""Attention is all you need"" paper.

I have implemented all the code and it is an educational attempt. I would like you to get some eyes on the repo from the members of this subreddit and get your opinion. This is still a work in progress but your reviews and PRs are really appreciated. I have written the code focusing on educational purposes and not optimisations. Please take a look below.

[https://github.com/MayukhSobo/Transformer](https://github.com/MayukhSobo/Transformer)

Edit: I would like to clarify that some of the code related to helper functions and all the doc strings are implemented by Claude not because they are difficult to do but they are simply boring. The core architecture is implemented by me. Also at no point I claimed that this is my own work and I haven't used AI. The part which really required me to code and not use AI, I did it on my own. If you really think that the complete code is just a result of some vibe coding, I welcome you to try that with most advanced AI tools and see if you can reproduce even 70% of what I did or not. ",65,0.81,https://www.reddit.com/r/MachineLearning/comments/1n0d12h/d_an_honest_attempt_to_implement_attention_is_all/,False,True,False
1n055zr,OkOwl6744,1756161910.0,9,/r/MachineLearning/comments/1n055zr/p_training_llms_without_code_would_you_use_it/,MachineLearning,[P] Training LLMs without code - Would you use it?,"https://preview.redd.it/vy1h49l0t8lf1.png?width=3456&format=png&auto=webp&s=1c0991294abf01d6699c04b663cd30973e4bd633

Is Vibe training AI models something people want?   
  
I made a quick 24hours YC hackathon app that wires HF dataset lookups + Synthetic data pipeline + Trnasfomers too quickly fine tune a gemma 3 270m on a mac, I had 24hours to ship something and now have to figure out if this is something people would like to use?   
  
Why this is useful? A lot of founders I've talked to want to make niche models, and/or make more profit (no SOTA apis) and overall build value beyond wrappers. And also, my intuition is that training small LLMs without code will enable researchers of all fields to tap into scientific discovery. I see people using it for small tasks classifiers for example. 

For technical folk, I think an advanced mode that will let you code with AI, should unleash possibilities of new frameworks, new embedding, new training technics and all that. The idea is to have a purposeful built space for ML training, so we don't have to lean to cursor or Claude Code. 

I'm looking for collaborators and ideas on how to make this useful as well?

Anyone interested can DM, and also signup for beta testing at [monostate.ai](http://monostate.ai)  
  
Somewhat overview at [https://monostate.ai/blog/training](https://monostate.ai/blog/training)  

\*\*The project will be free to use if you have your own API keys!\*\* 

In the beginning no Reinforcement learning or VLMs would be present, focus would be only in chat pairs fine tuning and possibly classifiers and special tags injection! 

Please be kind, this is a side project and I am not looking for replacing ML engineers, researchers or anything like that. I want to make our lifes easier, that's all. ",0,0.18,https://www.reddit.com/r/MachineLearning/comments/1n055zr/p_training_llms_without_code_would_you_use_it/,False,True,False
1n01odu,pmv143,1756153684.0,23,/r/MachineLearning/comments/1n01odu/d_cold_start_latency_for_large_models_new/,MachineLearning,[D] Cold start latency for large models: new benchmarks show 141B in ~3.7s,"Some interesting benchmarks I‚Äôve been digging into:
	‚Ä¢~1.3s cold start for a 32B model
	‚Ä¢~3.7s cold start for Mixtral-141B (on A100s)
       ‚Ä¢By comparison, Google Cloud Run reported ~19s for Gemma-3 4B earlier this year, and most infra teams assume 10‚Äì20s+ for 70B+ models (often minutes).

If these numbers hold up, it reframes inference as less of an ‚Äúalways-on‚Äù requirement and more of a ‚Äúruntime swap‚Äù problem.

Open questions for the community:
	‚Ä¢How important is sub-5s cold start latency for scaling inference?
	‚Ä¢Would it shift architectures away from dedicating GPUs per model toward more dynamic multi-model serving?",0,0.45,https://www.reddit.com/r/MachineLearning/comments/1n01odu/d_cold_start_latency_for_large_models_new/,False,True,False
1n00ruv,feller94,1756151643.0,9,/r/MachineLearning/comments/1n00ruv/p_gpubased_backend_deployment_for_an_app/,MachineLearning,[P] GPU-based backend deployment for an app,"Hi all!  
I'm drafting an app with pose detection (currently using¬†**MediaPipe**) and object detection (early¬†**Yolo11**). Since I cannot run these models on the phone itself, I'm developing the backend separately to be deployed somewhere, to then¬†*call it from the app when needed*.  
Basically I would need a¬†**GPU-based backend**¬†(I can also divide the detections and the actual result usage).

Now, I know about¬†*HuggingFace*¬†of course and I've seen a lot of other hosting platforms, but I wanted to ask if you have any suggestions in this regards?  
I think I might want to release it as free, or for a one-time low cost (if the costs are too high to support myself), but I also do not know how widespread it can be... You know, either useful and loved or unknown to most.  
The trick is that, since I would need the APIs always ready to respond, the backend would need to be up and¬†*running 24/7*. All of the options seem to be quite costly...

Is there any better or worse way to do this?",2,1.0,https://www.reddit.com/r/MachineLearning/comments/1n00ruv/p_gpubased_backend_deployment_for_an_app/,False,True,False
1mzxtzb,No_Marionberry_5366,1756144953.0,15,/r/MachineLearning/comments/1mzxtzb/dgepa_reflective_prompt_evolution_beats_rl_with/,MachineLearning,[D]GEPA: Reflective Prompt Evolution beats RL with 35√ó fewer rollouts,"A new preprint (Agrawal et al., 2025) introduces¬†**GEPA (Genetic-Pareto Prompt Evolution)**, a method for adapting compound LLM systems. Instead of using reinforcement learning in weight space (GRPO), GEPA mutates prompts while reflecting in natural language on traces of its own rollouts.

The results are striking:

* GEPA outperforms GRPO by up to¬†**19%**¬†while using¬†**35√ó fewer rollouts**.
* It also consistently surpasses MIPROv2, the state-of-the-art prompt optimizer.
* In many cases, only a few hundred rollouts were sufficient, compared to tens of thousands for RL .

The shift is conceptual as much as empirical: Where RL collapses complex trajectories into a scalar reward, GEPA treats those trajectories as¬†*textual artifacts*¬†that can be reflected on, diagnosed, and evolved. In doing so, it makes use of the medium in which LLMs are already most fluent, language, instead of trying to push noisy gradients through frozen weights.

What‚Äôs interesting is the infra angle: GEPA‚Äôs success in multi-hop QA hinges on generating better second-hop queries.¬†**That implicitly elevates retrieval infrastructure Linkup, Exa, Brave Search into the optimization loop itself**. Likewise, GEPA maintains a pool of Pareto-optimal prompts that must be stored, indexed, and retrieved efficiently.¬†**Vector DBs such as Chroma or Qdrant are natural substrates for this kind of evolutionary memory.**

This work suggests that the real frontier may not be reinforcement learning at scale, but¬†**language-native optimization loops**¬†where reflection, retrieval, and memory form a more efficient substrate for adaptation than raw rollouts in parameter space.

https://preview.redd.it/5l4lcmokg7lf1.png?width=1602&format=png&auto=webp&s=719e33f34feb5103ed1f375d3366745dd3415d77

",54,0.88,https://www.reddit.com/r/MachineLearning/comments/1mzxtzb/dgepa_reflective_prompt_evolution_beats_rl_with/,False,True,False
1mzwck3,DolantheMFWizard,1756141712.0,2,/r/MachineLearning/comments/1mzwck3/d_how_do_you_derive_real_insights_and_interpret/,MachineLearning,[D] How do you derive real insights and interpret experiment data beyond just looking at metrics?,"When running experiments, I often struggle with going beyond the surface-level metrics. How do you approach interpreting experimental data in a way that actually leads to useful insights and new ideas? What frameworks, statistical methods, or mindset shifts help you decide whether results are meaningful versus just noise?",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1mzwck3/d_how_do_you_derive_real_insights_and_interpret/,False,True,False
1mzsrt2,AntreasAntoniou,1756133888.0,26,/r/MachineLearning/comments/1mzsrt2/d_too_much_of_a_good_thing_how_chasing_scale_is/,MachineLearning,[D] Too much of a good thing: how chasing scale is stifling AI innovation,"Dear¬†[r/MachineLearning](https://www.reddit.com/r/MachineLearning/)¬†friends,

Hello everyone! I hope you are all doing well out there.

I've been observing a pattern in the AI research field that I can only describe as a ""Mass Amnesia."" It seems we're forgetting the valuable research paths we were on before the ChatGPT moment.

In my latest blog post, I argue that while scaling up LLMs was initially a courageous endeavour, the current obsession and monoculture around it is actively keeping us stuck. Instead of building on a diverse set of ideas, we're chasing a single approach, which I believe is making us amnesiacs about what came before and what's possible.

I'd love for you to read my spicy takes and share your own. Let's tear my arguments and ideas apart. ;)

üîó¬†**Full Article:**[https://pieces.app/blog/the-cost-of-ai-scaling](https://pieces.app/blog/the-cost-of-ai-scaling)

I look forward to your arguments and thoughts.

Regards,

Antreas

  
PS. This is a repost of [https://www.reddit.com/r/MachineLearning/comments/1mu28xl/d\_too\_much\_of\_a\_good\_thing\_how\_chasing\_scale\_is/](https://www.reddit.com/r/MachineLearning/comments/1mu28xl/d_too_much_of_a_good_thing_how_chasing_scale_is/) because it was removed without any explanation and the mods never replied to my queries on what was done wrong and how I could modify the post so it would abide by whatever rule I inadvertently tripped on.  

The post was starting to get some real discussion going when it was removed and wanted to give this another chance as I want to hear what everyone has to say and engage in discourse. ",16,0.66,https://www.reddit.com/r/MachineLearning/comments/1mzsrt2/d_too_much_of_a_good_thing_how_chasing_scale_is/,False,True,False
1mzsn1q,Mplus479,1756133591.0,10,/r/MachineLearning/comments/1mzsn1q/d_anyone_know_how_to_get_cornells_opensurfaces/,MachineLearning,[D] Anyone know how to get Cornell's OpenSurfaces dataset?,Was it abandoned? The website links are dead.,2,0.6,https://www.reddit.com/r/MachineLearning/comments/1mzsn1q/d_anyone_know_how_to_get_cornells_opensurfaces/,False,True,False
1mzqu1q,TimesLast_,1756129328.0,1,/r/MachineLearning/comments/1mzqu1q/d_malm_a_modular_adapterbased_language_model/,MachineLearning,[D] MALM: A Modular Adapter-based Language Model (paper + Hugging Face link),"Hey everyone, I just finished writing a short paper about a new idea I call MALM, a Modular Adapter-based Language Model.

The core idea is simple: instead of training giant multilingual LLMs, I propose keeping one small, sharp Core Language Model (reasoning in English), and delegating translation to lightweight, swappable Specialized Translation Adapters (STAs).

This means:

\- Smaller, cheaper models

\- Easy to add new languages

\- Better for edge devices and low-resource settings

Example flow:  
\`\`\`  
User: ""Translate 'my name is Adam' into German.""  
CLM ‚Üí <to:de> my name is Adam </to>  
STA ‚Üí ""Mein Name ist Adam""

\`\`\`

Read the full paper here:¬†[https://huggingface.co/TimesLast/MALM](https://huggingface.co/TimesLast/MALM)

Would love feedback, especially on how this could be extended beyond translation (math, code, multimodal adapters, etc.).",0,0.4,https://www.reddit.com/r/MachineLearning/comments/1mzqu1q/d_malm_a_modular_adapterbased_language_model/,False,True,False
1mzpoo4,DimitriMikadze,1756126455.0,0,/r/MachineLearning/comments/1mzpoo4/p_opensource_agentic_ai_for_company_research/,MachineLearning,[P] Open-Source Agentic AI for Company Research,"I open-sourced a project called Mira, an agentic AI system built on the OpenAI Agents SDK that automates company research.

You provide a company website, and a set of agents gather information from public data sources such as the company website, LinkedIn, and Google Search, then merge the results into a structured profile with confidence scores and source attribution.

The core is a Node.js/TypeScript library (MIT licensed), and the repo also includes a Next.js demo frontend that shows live progress as the agents run.

GitHub: [https://github.com/dimimikadze/mira](https://github.com/dimimikadze/mira)",0,0.13,https://www.reddit.com/r/MachineLearning/comments/1mzpoo4/p_opensource_agentic_ai_for_company_research/,False,True,False
1mzp8au,Ok-Ebb6307,1756125204.0,16,/r/MachineLearning/comments/1mzp8au/r_got_6min_i_need_your_help_for_my_phd/,MachineLearning,[R] Got 6min? I need YOUR help for my PhD!,"Hello everyone!

My name is Virginie and I am a first-year French PhD student¬†**studying human‚Äìartificial intelligence interactions.**

I am conducting a¬†**very quick**¬†(approximately 6 minutes) and¬†**anonymous online study**.

To ensure reliable results, I need at least 300 AI users, some of whom should have experience in integrating or designing AI models, although this is not compulsory for taking part!

If you are 18 or over, you can take part by clicking this link:

[https://virginie-lepont.limesurvey.net/967745?newtest=Y&lang=en](https://virginie-lepont.limesurvey.net/967745?newtest=Y&lang=en)

The survey is¬†**also available in French.**

Every response is valuable! Thank you so much for your help!

Virginie 

*This post has been approved by one moderator of this group.* 

https://preview.redd.it/gwtpg6p9t5lf1.jpg?width=940&format=pjpg&auto=webp&s=39e54c6e762ab220af6a1c32d8754d8c9b5ee34c

",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1mzp8au/r_got_6min_i_need_your_help_for_my_phd/,False,True,False
1mzmrm5,alexsht1,1756117538.0,3,/r/MachineLearning/comments/1mzmrm5/p_aligning_nonlinear_features_with_your_data/,MachineLearning,[P] aligning non-linear features with your data distribution,"For some time I've been fascinated by adopting knowledge from approximation theory into ML feature engineering, and I'm sharing my learnings in a series of blog posts, mainly about various polynomial bases as features.

So here is the latest one: [https://alexshtf.github.io/2025/08/19/Orthogonality.html](https://alexshtf.github.io/2025/08/19/Orthogonality.html)

It discusses my understanding of orthogonal bases as informative feature generators. I hope you enjoy reading as I enjoy learning about it.",19,1.0,https://www.reddit.com/r/MachineLearning/comments/1mzmrm5/p_aligning_nonlinear_features_with_your_data/,False,True,False
1mzhsh7,AdInevitable1362,1756098655.0,9,/r/MachineLearning/comments/1mzhsh7/p_yelp_dataset_clarification_is_review_count/,MachineLearning,[P] Yelp Dataset clarification: Is review_count colomn cheating?,"Hey everyone,

I'm working with the Yelp dataset and have a quick question about the review_count field in the business.json (what I'll call the business_df).

The business_df is a list of businesses, and the review_df is a list of every single review interaction.

Is the review_count in the business_df calculated directly from the interactions listed in the review_df?

If I split my data into train and test sets for a recommendation model, should I recalculate review_count from only the training interactions (so that test interactions remain unseen)? Or is review_count a static field provided by Yelp, independent of our data splits?

The reason I'm asking is I'd like to use review_count as part of my initial features/embeddings. I'm not sure if I should treat it as fixed metadata from Yelp or recompute it dynamically from my training set only.

Thanks a lot if anyone can clarify this!",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1mzhsh7/p_yelp_dataset_clarification_is_review_count/,False,True,False
1mzd5kt,Fantastic-Nerve-4056,1756084269.0,26,/r/MachineLearning/comments/1mzd5kt/d_views_on_llm_research_incremental_or_not/,MachineLearning,[D] Views on LLM Research: Incremental or Not?,"Hi folks,  
Fellow ML researcher here üëã

I‚Äôve been working in the LLM space for a while now, especially around *reasoning models* and *alignment* (both online and offline).

While surveying the literature, I couldn‚Äôt help but notice that a lot of the published work feels‚Ä¶ well, incremental. These are papers coming from great labs, often accepted at ICML/ICLR/NeurIPS, but many of them don‚Äôt feel like they‚Äôre really pushing the frontier.

I‚Äôm curious to hear what the community thinks:

* Do you also see a lot of incremental work in LLM research, or am I being overly critical?
* How do you personally filter through the ‚Äúnoise‚Äù to identify genuinely impactful work?
* Any heuristics or signals that help you decide which papers are worth a deep dive?

Would love to get different perspectives on this ‚Äî especially from people navigating the same sea of papers every week.

  
PS: Made use of GPT to rewrite the text, but it appropriately covers my view/questions",53,0.85,https://www.reddit.com/r/MachineLearning/comments/1mzd5kt/d_views_on_llm_research_incremental_or_not/,False,True,False
1mz9ruc,AgeOfEmpires4AOE4,1756075122.0,1,/r/MachineLearning/comments/1mz9ruc/p_ai_learns_to_play_sonic_2_emerald_hill_deep/,MachineLearning,[P] AI Learns to play Sonic 2 Emerald Hill (Deep Reinforcement...,"Hello everyone!!! I have several Reinforcement Learning projects underway. One is Sonic 2 with PPO. The other is developing an environment that supports games not available with Farama Group's stable-retro. I may need collaborators for the latter. I don't know if I'll integrate it into their project, stable-retro, in the future. One thing I've already achieved is running PCSX2 (it's missing the state loading option), and I'm creating a Python lib to load with stable-baselines3, etc. If anyone is interested, the links to both projects are below:

[https://github.com/paulo101977/Sonic-2-Genesis-Reinforcement-Learning](https://github.com/paulo101977/Sonic-2-Genesis-Reinforcement-Learning)

[https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)I also started a PCSX2 environment with direct access to the Python process, but I'll abandon it as it's very slow.

  
",0,0.46,https://youtube.com/watch?v=i0rFDGJ5mw8&si=4XBufmJrr0fgcQrr,False,False,False
1mz70e2,drahcirenoob,1756068417.0,9,/r/MachineLearning/comments/1mz70e2/r_review_advice_wellestablished_work_published/,MachineLearning,[R] Review advice:  Well-established work published years ago on Arxiv,"I'm reviewing for AAAI, and wanted to ask the community for some advice. I got a paper for review that is very well known in my subfield, published in 2023, but only previously published onto Arxiv. As best I can tell, the paper has had some minor rewrites for publication, but is otherwise largely the same as the well-established work. What's the best policy here? It was a very good paper when it came out, but the existing version basically ignores the last two years of work by the community, in part because some decent portion of that work is based on this paper.  Any advice on the best way to review this would be appreciated",33,0.86,https://www.reddit.com/r/MachineLearning/comments/1mz70e2/r_review_advice_wellestablished_work_published/,False,True,False
1mywuni,Pedro_Silva95,1756045249.0,3,/r/MachineLearning/comments/1mywuni/p_options_on_how_to_balance_my_training_dataset/,MachineLearning,[P] options on how to balance my training dataset,"I'm working on developing a ML classification project using Python, divided into 5 output categories (classes). However, my training dataset is extremely unbalanced, and my results always lean toward the dominant class (class 5, as expected).

However, I wanted my models to better learn the characteristics of the other classes, and I realized that one way to do this is by balancing the training dataset. I tried using SMOTETomek for oversampling, but my models didn't respond well. Does anyone have any ideas or possibilities for balancing my training dataset?

There are 6 classification ML models that will ultimately be combined into an ensemble. The models used are: RandomForest, DecisionTree, ExtraTrees, AdaBoost, NaiveBayes, KNN, GradientBoosting, and SVM.

The data is also being standardized via standardSCaler.

Total record count by category:

Category 1: 160 records

Category 2: 446 records

Category 3: 605 records

Category 4: 3,969 records

Category 5: 47,874 records",0,0.44,https://www.reddit.com/r/MachineLearning/comments/1mywuni/p_options_on_how_to_balance_my_training_dataset/,False,True,False
1myr68a,Code-Forge-Temple,1756027938.0,4,/r/MachineLearning/comments/1myr68a/d_exploring_localfirst_ai_workflow_automation/,MachineLearning,[D] Exploring Local-First AI Workflow Automation,"**[D] Exploring Local-First AI Workflow Automation**

Hi all,  

I‚Äôve been experimenting with an open-source approach to AI workflow automation that runs entirely **locally** (no cloud dependencies), while still supporting real-time data sources and integrations. The goal is to provide a **privacy-first, resource-efficient alternative** to traditional cloud-heavy workflow tools like Zapier or n8n, but with LLM support integrated.

üëâ My question for the community:  
How do you see **local-first AI workflows** impacting ML/AI research, enterprise adoption, and robotics/IoT systems where privacy, compliance, and cost efficiency are critical?  

- Repo: [Agentic Signal](https://github.com/code-forge-temple/agentic-signal) (open-source, AGPL v3 / commercial dual license)  
- Demo video: [YouTube link](https://youtu.be/62zk8zE6UJI)  

Would love feedback from both the research and applied ML communities on potential use cases, limitations, or challenges you foresee with this approach.  

Thanks!  
",0,0.38,https://i.redd.it/hxmg601wrxkf1.png,False,False,False
1myptun,Snoo71505,1756022779.0,5,/r/MachineLearning/comments/1myptun/d_neurips_2025_are_there_post_conference_events/,MachineLearning,[D] Neurips 2025: Are there post conference events on the last day of the conference?,"\[EDIT\] I meant December / Dec not November / Nov. It was late at night I'm sorry -  lol.

Context:

* Neurips 2025 conference is from Tue, Dec 2 to Sun, Dec 7
* This is my first time attending the conference.
* As I need to travel again right after the conference for personal reasons, I am figuring out on what dates to book the hotels / flights in advance.
* **Are there post conference events on the last day** eg: Sun, Dec 7 night? I am not sure if it's better to return right away (on Sun, Dec 7 evening) or fly back later (on Mon, Dec 8 morning)?",1,0.53,https://www.reddit.com/r/MachineLearning/comments/1myptun/d_neurips_2025_are_there_post_conference_events/,False,True,False
1myoooy,alexsht1,1756018436.0,6,/r/MachineLearning/comments/1myoooy/d_poles_of_nonlinear_rational_features/,MachineLearning,[D] Poles of non-linear rational features,"Suppose I want to fit a linear model to non-linear **rational** features. Something like `RationalTransformer` instead of `SplineTransformer` in Scikit-Learn, that uses a basis of rational functions. The domain of my raw features before being transformed are (theoretically) unbounded non-negative numbers, such as ""time since X happened"", ""total time spent on the website"", or ""bid in an auction"".

So here is the question: *where would you put the poles? Why?*

Note, I'm not aiming on fitting one rational curve, so algorithms in the spirit of AAA are irrelevant. I'm aiming at a component I can use in a pipeline that transformes features before model fitting, such as `MinMaxScaler` or `SplineTransformer` in scikit-learn.",3,0.64,https://www.reddit.com/r/MachineLearning/comments/1myoooy/d_poles_of_nonlinear_rational_features/,False,True,False
1mylqrb,UnholyCathedral,1756008073.0,6,/r/MachineLearning/comments/1mylqrb/r_building_a_deep_learning_image_model_system_to/,MachineLearning,[R] Building a deep learning image model system to identify BJJ positions in matches,"Hey all, I'm working on developing AI models that can classify and track positions throughout BJJ matches - and I'm keen to get some thoughts on this idea early on.

You can check it out here:¬†[https://bjjhq.ai/](https://bjjhq.ai/)

Ultimately BJJHQ provides an interactive positional timeline beneath match videos, showing all position changes throughout the match, so you're able to instantly jump to specific positions and see how transitions unfold.

The idea is that people would be able to search for not only a competitor, but a specific position and combination (e.g., ""Gordon Ryan in back control""), and instantly access all matches where that scenario occurs. You would also be able to filter and sort matches by time spent in specific positions.

Roadmap:

* Expanding the match database and position categories
* Technique/submission recognition
* Automated scoring system built on this positional foundation

Would love to know if anyone would be interested to chat or collaborate on this project ... please reach out if keen!

Thanks for any feedback!",4,0.75,https://www.reddit.com/r/MachineLearning/comments/1mylqrb/r_building_a_deep_learning_image_model_system_to/,False,True,False
1myj9jk,electricsheeptacos,1756000167.0,20,/r/MachineLearning/comments/1myj9jk/r_routers_to_foundation_models/,MachineLearning,[R] routers to foundation models?,"Are there any projects/packages that help inform an agent which FM to use for their use case? Curious if this is even a strong need in the AI community? Anyone have any experience with ‚Äúrouters‚Äù?

Update: especially curious about whether folks implementing LLM calls at work or for research (either one offs or agents) feel this as a real need or is it just a nice-to-know sort of thing? Intuitively, cutting costs while keeping quality high by routing to FMs that optimize for just that seems like a valid concern, but I‚Äôm trying to get a sense of how much of a concern it really is

Of course, the mechanisms underlying this approach are of interest to me as well. I‚Äôm thinking of writing my own router, but would like to understand what‚Äôs out there/what the need even is first",8,0.83,https://www.reddit.com/r/MachineLearning/comments/1myj9jk/r_routers_to_foundation_models/,False,True,False
1myhj41,Creative_Star_9425,1755994927.0,1,/r/MachineLearning/comments/1myhj41/d_topology_and_geometry_in_deep_learning_beyond/,MachineLearning,[D] Topology and geometry in deep learning beyond TDL/GDL,"Posts here within the past 6 months have discussed both¬†[Topological Deep Learning (TDL)](https://www.reddit.com/r/MachineLearning/comments/1ji6xlv/d_topological_deep_learning_promising_or_hype/)¬†and¬†[Geometric Deep Learning (GDL)](https://www.reddit.com/r/MachineLearning/comments/1jabkt8/d_geometric_deep_learning_and_its_potential/). Even though the nomenclature suggests otherwise, these two (exciting!) areas have come to represent rather specific topics in recent years.¬†*Very crudely speaking*, ""TDL"" seems to focus mainly on higher-order message passing (HOMP); ""GDL"" to the design of neural networks mod domain symmetries.

For the purposes of discussion, let's set the operational definition of TDL to be as in this paper:¬†[Hajij, Mustafa, et al. Topological Deep Learning: Going Beyond Graph Data. Springer, 2024.](https://tdlbook.org/)

and the operational definition of GDL to be as in this paper:¬†[Bronstein, Michael M., et al. Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges. MIT Press, 2021.](https://arxiv.org/abs/2104.13478)

With that in place: what are some applications of geometry and topology in deep learning that¬†*do not properly belong to TDL and GDL as defined above*¬†(and as have already received recent posts here)? Applications of adjacent fields are also welcome- algebra, category theory, etc.- , as are applications in the converse direction.",2,1.0,https://www.reddit.com/r/MachineLearning/comments/1myhj41/d_topology_and_geometry_in_deep_learning_beyond/,False,True,False
1mybwih,TajineMaster159,1755980376.0,72,/r/MachineLearning/comments/1mybwih/d_how_did_jax_fare_in_the_post_transformer_world/,MachineLearning,[D] How did JAX fare in the post transformer world?,"A few years ago, there was a lot of buzz around JAX, with some enthusiasts going as far as saying it would disrupt PyTorch. Every now and then, some big AI lab would release stuff in JAX or a PyTorch dev would write a post about it, and some insightful and inspired discourse would ensue with big prospects. However, chatter and development have considerably quieted down since transformers, large multimodal models, and the ongoing LLM fever. Is it still promising? 

Or at least, this is my impression, which I concede might be myopic due to my research and industry needs. ",153,0.96,https://www.reddit.com/r/MachineLearning/comments/1mybwih/d_how_did_jax_fare_in_the_post_transformer_world/,False,True,False
1mxyqku,huopak,1755947741.0,2,/r/MachineLearning/comments/1mxyqku/d_is_mlsys_a_lowtier_conference_i_cant_find_it_in/,MachineLearning,[D] Is MLSys a low-tier conference? I can't find it in any of the rankings,[https://mlsys.org/](https://mlsys.org/),0,0.38,https://www.reddit.com/r/MachineLearning/comments/1mxyqku/d_is_mlsys_a_lowtier_conference_i_cant_find_it_in/,False,True,False
1mxw9c1,JesuXd,1755938789.0,8,/r/MachineLearning/comments/1mxw9c1/p_i_built_a_mlregression_model_for_biathlon_that/,MachineLearning,[P] I built a ML-regression model for Biathlon that beats current betting market odds,"Hello ya'll!

I recently built a ML-regression model to predict the unpredictable sport of biathlon. In biathlon, external factors such as weather, course profiles and altitude play huge roles in determining who wins and when. But when taking these factors into play, in addition of athletes' past performances, you can score surprisingly high accuracy.

This is how well the model performed when predicting athlete ranks (0 = winner, 1 = last place) using 10 years of historic biathlon data:  
\- MAE (average error): 0.14 -> 4-18 places off depending on race size  
\- RMSE: 0.18 -> penalizing big prediction misses  
\- R¬≤: -> the model explains \~62% of the variation in finish order

Now what does these metrics say?  
\- The model almost cuts in half random guessing (\~25% error)  
\- It consistently outperforms the accuracy of betting odds in the current market, meaning it has a predictive edge.  
\- It is able to tell the majority of happenings (62%), which is very rare in a sport where surprises happen very often.

Next steps:  
\- Build R¬≤ up to 70% using more complex feature engineering and data preprocessing.  
\- Launch a SaaS that sells these odds for businesses and private consumers.",0,0.15,https://www.reddit.com/r/MachineLearning/comments/1mxw9c1/p_i_built_a_mlregression_model_for_biathlon_that/,False,True,False
1mxrt1y,Healthy_Horse_2183,1755922751.0,67,/r/MachineLearning/comments/1mxrt1y/d_aaai_considered_2nd_tier_now/,MachineLearning,[D] AAAI considered 2nd tier now?,"Isn‚Äôt AAAI in the same tier as NeurIPS/ICML/ICLR? 
ICLR literally has >30% acceptance rate.",67,0.87,https://www.reddit.com/r/MachineLearning/comments/1mxrt1y/d_aaai_considered_2nd_tier_now/,False,True,False
1mxih41,Gloomy_Situation5126,1755896815.0,0,/r/MachineLearning/comments/1mxih41/p_relational_pdf_recall_rfc_poc_structured/,MachineLearning,[P] Relational PDF Recall (RFC + PoC) ‚Äì Structured storage + overlay indexing experiment,"I‚Äôve been exploring how far we can push¬†*relational database structures inside PDFs*¬†as a substrate for AI recall. Just published a first draft RFC + PoC:

* Channel splitting (text/vector/raster/audio streams)
* Near-lossless transforms (wavelet/FLAC-style)
* Relational indexing across channels (metadata + hash linking)
* Early geometry-only overlays (tiling + Z-order indexing)

Repo + notes:¬†[https://github.com/maximumgravity1/relational-pdf-recall](https://github.com/maximumgravity1/relational-pdf-recall)

This is still very early (draft/PoC level), but I‚Äôd love feedback on:

* Whether others have tried similar recall-layer ideas on top of PDFs.
* If this approach overlaps with knowledge-graph work, or if it opens a different lane.
* Pitfalls I might be missing re: indexing/overlays.

  
**UPDATE 1: üìå Repo + DOI now live**   
GitHub: [https://github.com/maximumgravity1/pdf-hdd-rfc](https://github.com/maximumgravity1/pdf-hdd-rfc)  
DOI (always latest): [https://doi.org/10.5281/zenodo.16930387](https://doi.org/10.5281/zenodo.16930387)",0,0.17,https://www.reddit.com/r/MachineLearning/comments/1mxih41/p_relational_pdf_recall_rfc_poc_structured/,False,True,False
1mxcd2j,sukhoi-30mki,1755882733.0,3,/r/MachineLearning/comments/1mxcd2j/p_need_to_include_ann_lightgbm_and_knn_results_in/,MachineLearning,"[P] Need to include ANN, LightGBM, and KNN results in research paper","Hey everyone,

I‚Äôm working on a research paper with my group, and so far we‚Äôve done a comprehensive analysis using **Random Forest**. The problem is, my professor/supervisor now wants us to also include results from **ANN, LightGBM, and KNN** for comparison.

We need to:

* Run these models on the dataset,
* Collect performance metrics (accuracy, RMSE, R¬≤, etc.),
* Present them in a **comparison table** with Random Forest,
* Then update the writing/discussion accordingly.

I‚Äôm decent with Random Forests but not as experienced with ANN, LightGBM, and KNN. Could anyone guide me with example code, a good workflow, or best practices for running these models and compiling results neatly into a table?",0,0.31,https://www.reddit.com/r/MachineLearning/comments/1mxcd2j/p_need_to_include_ann_lightgbm_and_knn_results_in/,False,True,False
1mx775g,fishandtech,1755870929.0,3,/r/MachineLearning/comments/1mx775g/d_lowbudget_hardware_for_ondevice_object/,MachineLearning,[D] Low-budget hardware for on-device object detection + VQA?,"Hey folks,

I‚Äôm an undergrad working on my FYP and need advice. I want to:

* Run¬†object detection¬†on medical images (PNGs).
* Do¬†visual question answering¬†with a ViT or small LLaMA model.
* Everything fully¬†on-device¬†(no cloud).

Budget is tight, so I‚Äôm looking at Jetson boards (Nano, Orin Nano, Orin NX) but not sure which is realistic for running a quantized detector + small LLM for VQA.

Anyone here tried this? What hardware would you recommend for the best balance of cost + capability?

Thanks!",1,0.57,https://www.reddit.com/r/MachineLearning/comments/1mx775g/d_lowbudget_hardware_for_ondevice_object/,False,True,False
1mx4a6c,ComprehensiveTop3297,1755863264.0,12,/r/MachineLearning/comments/1mx4a6c/d_why_does_byoljepa_like_models_work_how_does_ema/,MachineLearning,[D] Why does BYOL/JEPA like models work? How does EMA prevent model collapse?,"
I am curious on your takes on BYOL/JEPA like training methods and the intuitions/mathematics behind why the hell does it work?

From an optimization perspective, without the EMA parameterization of the teacher model, the task would be very trivial and it would lead to model collapse. However, EMA seems to avoid this. Why?

Specifically:

How can a network learn semantic embeddings without reconstructing the targets in the real space? Where is the learning signal coming from? Why are these embeddings so good?

I had great success with applying JEPA like architectures to diverse domains and I keep seeing that model collapse can be avoided by tuning the LR scheduler/EMA schedule/masking ratio. I have no idea why this avoids the collapse though.",48,0.94,https://www.reddit.com/r/MachineLearning/comments/1mx4a6c/d_why_does_byoljepa_like_models_work_how_does_ema/,False,True,False
1mwxfxj,Puzzled_Boot_3062,1755838541.0,15,/r/MachineLearning/comments/1mwxfxj/d_using_llms_to_extract_knowledge_graphs_from/,MachineLearning,[D] Using LLMs to extract knowledge graphs from tables for retrieval-augmented methods ‚Äî promising or just recursion?,"I‚Äôve been thinking about an approach where large language models are used to extract structured knowledge (e.g., from tables, spreadsheets, or databases), transform it into a knowledge graph (KG), and then use that KG within a Retrieval-Augmented Generation (RAG) setup to support reasoning and reduce hallucinations.

But here‚Äôs the tricky part: this feels a bit like ‚ÄúLLMs generating data for themselves‚Äù ‚Äî almost recursive. On one hand, structured knowledge could help LLMs reason better. On the other hand, if the extraction itself relies on an LLM, aren‚Äôt we just stacking uncertainties?

I‚Äôd love to hear the community‚Äôs thoughts:

* Do you see this as a viable research or application direction, or more like a dead end?
* Are there promising frameworks or papers tackling this ‚Äúself-extraction ‚Üí RAG ‚Üí LLM‚Äù pipeline?
* What do you see as the biggest bottlenecks (scalability, accuracy of extraction, reasoning limits)?

Curious to know if anyone here has tried something along these lines.",13,0.85,https://www.reddit.com/r/MachineLearning/comments/1mwxfxj/d_using_llms_to_extract_knowledge_graphs_from/,False,True,False
1mwrl72,Franck_Dernoncourt,1755821299.0,31,/r/MachineLearning/comments/1mwrl72/d_why_was_this_paper_rejected_by_arxiv/,MachineLearning,[D] Why was this paper rejected by arXiv?,"One of my co-authors submitted this [paper](https://ia903401.us.archive.org/19/items/images-for-questions/A%20Survey%20on%20LLM-based%20Conversational%20User%20Simulation.pdf) to arXiv. It was rejected. What could the reason be?

[iThenticate](https://www.ithenticate.com/) didn't detect any plagiarism and arXiv didn't give any reason beyond a vague ""submission would benefit from additional review and revision that is outside of the services we provide"":

> Dear author,
> 
> Thank you for submitting your work to arXiv. We regret to inform you that arXiv‚Äôs moderators have determined that your submission will not be accepted at this time and made public on  http://arxiv.org
> 
> In this case, our moderators have determined that your submission would benefit from additional review and revision that is outside of the services we provide. 
> 
> Our moderators will reconsider this material via [appeal](https://info.arxiv.org/help/moderation/appeals.html) if it is published in a conventional journal and you can provide a resolving DOI (Digital Object Identifier) to the published version of the work or link to the journal's website showing the status of the work.
> 
> Note that publication in a conventional journal does not guarantee that arXiv will accept this work.
> 
> For more information on moderation policies and procedures, please see [Content Moderation](https://info.arxiv.org/help/moderation/index.html). 
> 
> arXiv moderators strive to balance fair assessment with decision speed. We understand that this decision may be disappointing, and we apologize that, due to the high volume of submissions arXiv receives, we cannot offer more detailed feedback. Some authors have found that asking their personal network of colleagues or submitting to a conventional journal for peer review are alternative avenues to obtain feedback. 
> 
> We appreciate your interest in arXiv and wish you the best. 
> 
> Regards,
>
> arXiv Support

I read the [arXiv policies](https://info.arxiv.org/help/moderation/index.html) and I don't see anything we infringed.",0,0.35,https://www.reddit.com/r/MachineLearning/comments/1mwrl72/d_why_was_this_paper_rejected_by_arxiv/,False,True,False
1mwfjax,KellinPelrine,1755793065.0,1,/r/MachineLearning/comments/1mwfjax/r_frontier_llms_attempt_to_persuade_into_harmful/,MachineLearning,[R] Frontier LLMs Attempt to Persuade into Harmful Topics,"Gemini 2.5 Pro generates convincing arguments for joining a terrorist organization. GPT-4o-mini suggests that a user should randomly assault strangers in a crowd with a wrench. These models weren't hacked or jailbroken, they simply complied with user requests.

Prior research has already shown large language models (LLMs) can be more persuasive than most humans. But how easy is it to get models to engage in such persuasive behavior? Our Attempt to Persuade Eval (APE) benchmark measures this by simulating conversations between LLMs on topics from benign facts to mass murder. We find:

üîπ Leading models readily produced empathic yet coercive ISIS recruitment arguments

üîπ Safety varied: Claude and Llama 3.1 refused some controversial topics; while other models showed high willingness

üîπ Fine-tuning eliminated safeguards: ""Jailbreak-Tuned"" GPT-4o lost nearly all refusal capability on all topics, like violence, human trafficking, and torture

For clear ethical reasons, we do not test the success rate of persuading human users on highly harmful topics. The models‚Äô attempts to persuade, however, appear to be eloquent and well-written ‚Äì we invite interested readers to peruse the transcripts themselves. Moreover, even small persuasive effect sizes operating at a large scale enabled by automation can have significant effects: Bad actors could weaponize these vulnerabilities for malicious purposes such as planting seeds of doubt in millions of people and radicalizing vulnerable populations. As AI becomes autonomous, we must understand propensity to attempt harm, not just capability.

We‚Äôve already seen the impact of APE: We disclosed our findings to Google, and they quickly started work to solve this for future models. The latest version of Gemini 2.5 is already less willing to engage in persuasion on extreme topics compared to earlier versions we tested.

We've open-sourced APE for testing models' refusal and safe completion mechanisms before deployment to help build stronger safety guardrails.

üë• Research by Matthew Kowal, Jasper Timm, Jean-Fran√ßois Godbout, Thomas Costello, Antonio A. Arechar, Gordon Pennycook, David Rand, Adam Gleave, and Kellin Pelrine.

üìù Blog: [far.ai/news/attempt-persuasion-eval](http://far.ai/news/attempt-persuasion-eval)¬†

üìÑ Paper: [arxiv.org/abs/2506.02873](http://arxiv.org/abs/2506.02873)¬†

üíª Code: [github.com/AlignmentResearch/AttemptPersuadeEval](http://github.com/AlignmentResearch/AttemptPersuadeEval)",0,0.4,https://www.reddit.com/r/MachineLearning/comments/1mwfjax/r_frontier_llms_attempt_to_persuade_into_harmful/,False,True,False
1mwbq81,bjjonin,1755784794.0,34,/r/MachineLearning/comments/1mwbq81/p_language_diffusion_in_80_lines_of_code/,MachineLearning,[P] Language Diffusion in <80 Lines of Code,"Hi! Lately, I've been looking into diffusion language models and thought I should try and replicate part of the paper [Large Language Diffusion Models](https://arxiv.org/abs/2502.09992) by Nie et al. (2025). With the help of Hugging Face's Transformers, it took <80 lines of code to implement the training script. I finetuned [DistilBERT](https://huggingface.co/distilbert/distilbert-base-cased) on the [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset, and the results were better than expected!

[Generating tiny stories via a reverse language diffusion process](https://i.redd.it/sm9xtdpdpdkf1.gif)

You can view the project at https://github.com/gumran/language-diffusion. I will appreciate any feedback/comments/stars!",92,0.85,https://www.reddit.com/r/MachineLearning/comments/1mwbq81/p_language_diffusion_in_80_lines_of_code/,False,True,False
1mwb7pp,NataliaShu,1755783560.0,2,/r/MachineLearning/comments/1mwb7pp/r_observing_unexpected_patterns_in_mtpe_demand/,MachineLearning,[R] Observing unexpected patterns in MTPE demand across languages,"Hi ML folks, I work at Alconost (localization services), and we‚Äôve just wrapped up our 5th annual report on language demand for localization. For the first time, we‚Äôve seen MTPE (machine-translation post-editing) demand reach statistically significant levels across multiple languages.¬†

We analyzed MTPE adoption rates in the Top 20 languages, and what‚Äôs interesting is that some languages that are slipping in overall localization demand are still¬†**seeing more activity**¬†via MTPE.¬†

I‚Äôm curious: if you‚Äôre working with MT or LLM workflows, have you noticed similar patterns in the languages you work with?¬†

What do you think is driving MTPE demand for certain languages? Is it related to model performance, availability of training data, or just market pressure to reduce costs?¬†

Thank you. Cheers!",6,0.67,https://www.reddit.com/gallery/1mwb7pp,False,False,False
1mw4sgp,Mission-Balance-4250,1755763206.0,11,/r/MachineLearning/comments/1mw4sgp/r_how_to_prime_oneself_for_ml_research_coming/,MachineLearning,[R] How to prime oneself for ML research coming from industry,"I've been working as an ML Engineer for the last 5-6 years across a few different industries and have landed a job as a research engineer at a university under an esteemed supervisor in the NLP department who has generously offered to help me figure out my research interests and assist with theirs. I published a paper about 4 years ago in cognitive science - but it involved very little ML.

I don't have any tertiary qualifications/degrees but have industry experience in research-oriented roles - although, none primarily in NLP. I move internationally for the role in 3 months and want to poise myself to be as useful as possible. Does anyone have tips about gearing up to do academic research/engineering having come from industry?

I feel like there is infinite ground to cover; my maths will need much sharpening, I'll need to learn how to properly read scientific papers etc.

Cheers",31,0.94,https://www.reddit.com/r/MachineLearning/comments/1mw4sgp/r_how_to_prime_oneself_for_ml_research_coming/,False,True,False
1mw2z1y,Maleficent-Tone6316,1755756397.0,71,/r/MachineLearning/comments/1mw2z1y/d_phd_vs_startupindustry_for_doing_impactful_ai/,MachineLearning,[D] PhD vs startup/industry for doing impactful AI research ‚Äî what would you pick?,"Hi all,

I‚Äôm deciding between starting a PhD at a top university (ranked \~5‚Äì10) with a great professor (lots of freedom, supportive environment) or going straight into industry.

My long-term goal is to work on the frontier of intelligence, with more focus on research than pure engineering. My background is mostly around LLMs on the ML side, and I already have a few A\* conference papers (3‚Äì4), so I‚Äôm not starting from scratch.

Industry (likely at a smaller lab or startup) could give me immediate opportunities, including large-scale distributed training and more product-driven work. The lab I‚Äôd join for the PhD also has strong access to compute clusters and good chances for internships/collaborations, though in a more research-focused, less product-driven setting. The typical timeline in this lab is \~4 years + internship time.

If you were in this position, which path would you take?",71,0.89,https://www.reddit.com/r/MachineLearning/comments/1mw2z1y/d_phd_vs_startupindustry_for_doing_impactful_ai/,False,True,False
1mw1qty,AdInevitable1362,1755752215.0,12,/r/MachineLearning/comments/1mw1qty/p_model_to_encode_texts_into_embeddings/,MachineLearning,[P] model to encode texts into embeddings,"I need to summarize metadata using an LLM,
and then encode the summary using BERT (e.g., DistilBERT, ModernBERT).
	‚Ä¢	Is encoding summaries (texts) with BERT usually slow?
	‚Ä¢	What‚Äôs the fastest model for this task?
	‚Ä¢	Are there API services that provide text embeddings, and how much do they cost?
",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1mw1qty/p_model_to_encode_texts_into_embeddings/,False,True,False
1mw19zu,Blue-Sea123,1755750658.0,4,/r/MachineLearning/comments/1mw19zu/p_if_i_were_to_add_a_segmentation_head_onto_an_od/,MachineLearning,"[P] If i were to add a segmentation head onto an OD model, how do i go about it?","So i am picking a model from scenic repository and although the model is primarily built for object detection, i want to try and see if i can make it to do segmentation tasks as well. This could include combining it with another model (like SAM, or something), as well as adding a segment head into the model itself. l am a novice in ML having worked for about a year in implementing CV solutions. How should i go about doing this?",0,0.33,https://www.reddit.com/r/MachineLearning/comments/1mw19zu/p_if_i_were_to_add_a_segmentation_head_onto_an_od/,False,True,False
1mvycr9,OkOwl6744,1755741918.0,0,/r/MachineLearning/comments/1mvycr9/p_vibe_datasetting_creating_syn_data_with_a/,MachineLearning,[P] Vibe datasetting- Creating syn data with a relational model,"
TL;DR: I‚Äôm testing the Dataset Director, a tiny tool that uses a relational model as a planner to predict which data you‚Äôll need next, then has an LLM generate only those specific samples. Free to test, capped at 100 rows/dataset, export directly to HF.

Why: Random synthetic data ‚â† helpful. We want on-spec, just-in-time samples that fix the gaps that matter (long tail, edge cases, fairness slices).

How it works:
	1.	Upload a small CSV or connect to a mock relational set.

	2.	Define a semantic spec (taxonomy/attributes + target distribution).

	3.	KumoRFM predicts next-window frequencies ‚Üí identifies under-covered buckets.

	4.	LLM generates only those samples. Coverage & calibration update in place.

What to test (3 min):
	‚Ä¢	Try a churn/click/QA dataset; set a target spec; click Plan ‚Üí Generate.

	‚Ä¢	Check coverage vs. target and bucket-level error/entropy before/after.

Limits / notes: free beta, 100 rows per dataset; tabular/relational focus; no PII; in-memory run for the session.

Looking for feedback, like:
	‚Ä¢	Did the planner pick useful gaps?
	‚Ä¢	Any obvious spec buckets we‚Äôre missing?
	‚Ä¢	Would you want a ‚Äúgenerate labels only‚Äù mode?
	‚Ä¢	Integrations you‚Äôd use first (dbt/BigQuery/Snowflake)?

HTTPS://datasetdirector.com ",9,0.8,https://www.reddit.com/r/MachineLearning/comments/1mvycr9/p_vibe_datasetting_creating_syn_data_with_a/,False,True,False
1mvtjxw,EDEN1998,1755729121.0,116,/r/MachineLearning/comments/1mvtjxw/google_phd_fellowship_2025_d/,MachineLearning,Google phd fellowship 2025 [D],Has anyone heard back anything from Google? On the website they said they will announce results this August but they usually email accepted applicants earlier.,46,0.86,https://www.reddit.com/r/MachineLearning/comments/1mvtjxw/google_phd_fellowship_2025_d/,False,True,False
1mvn89s,Dualweed,1755714926.0,15,/r/MachineLearning/comments/1mvn89s/simple_multiple_choice_questions_about_machine/,MachineLearning,Simple Multiple Choice Questions about Machine Learning [D],"The following statements are either True or False:

1. You can use any differentiable function f: R->R in a neural network as activation function.
2. You can always know whether the perceptron algorithm will converge for any given dataset.

What do you guys think? I got both of them wrong in my exam.",0,0.2,https://www.reddit.com/r/MachineLearning/comments/1mvn89s/simple_multiple_choice_questions_about_machine/,False,True,False
1mvmlbw,lipflip,1755713554.0,8,/r/MachineLearning/comments/1mvmlbw/r_what_do_people_expect_from_ai_in_the_next/,MachineLearning,"[R] What do people expect from AI in the next decade across various domains? Survey with N=1100 people from Germay::We found high likelihood, higher perceived risks, yet limited benefits low perceived value. Yet, benefits outweight risks in forming value judgments. Visual result illustrations :)","Hi everyone, we recently published a peer-reviewed article exploring how people perceive artificial intelligence (AI) across different domains (e.g., autonomous driving, healthcare, politics, art, warfare). The study used a nationally representative sample in Germany (N=1100) and asked participants to evaluate 71 AI-related scenarios in terms of expected likelihood, risks, benefits, and overall value.

If you like AI or studying the public perception of AI, please also give us an upvote here: [https://www.reddit.com/r/science/comments/1mvd1q0/public\_perception\_of\_artificial\_intelligence/](https://www.reddit.com/r/science/comments/1mvd1q0/public_perception_of_artificial_intelligence/) üôà

**Main takeaway:** People often see AI scenarios as likely, but this doesn‚Äôt mean they view them as beneficial. In fact, most scenarios were judged to have high risks, limited benefits, and low overall value. Interestingly, we found that people‚Äôs value judgments were almost entirely explained by risk-benefit tradeoffs (96.5% variance explained, with benefits being more important for forming value judgements than risks), while expectations of likelihood didn‚Äôt matter much.  
  
**Why this matters?** These results highlight how important it is to communicate concrete benefits while addressing public concerns. Something relevant for policymakers, developers, and anyone working on AI ethics and governance.  
  
If you‚Äôre interested, here‚Äôs the full article:  
Mapping Public Perception of Artificial Intelligence: Expectations, Risk-Benefit Tradeoffs, and Value As Determinants for Societal Acceptance, Technological Forecasting and Social Change (2025), 

https://www.sciencedirect.com/science/article/pii/S004016252500335X",7,0.65,https://www.reddit.com/r/MachineLearning/comments/1mvmlbw/r_what_do_people_expect_from_ai_in_the_next/,False,True,False
1mvfktv,AdhesivenessOk3187,1755698341.0,7,/r/MachineLearning/comments/1mvfktv/p_gridsearchcv_always_overfits_i_built_a_fix/,MachineLearning,[P] GridSearchCV always overfits? I built a fix,"So I kept running into this: `GridSearchCV` picks the model with the best validation score‚Ä¶ but that model is often overfitting (train super high, test a bit inflated).

I wrote a tiny selector that balances:

* how good the test score is
* how close train and test are (gap)

Basically, it tries to pick the ‚Äústable‚Äù model, not just the flashy one.

Code + demo here üëâ[heilswastik/FitSearchCV](https://github.com/heilswastik/FitSearchCV)",0,0.25,https://www.reddit.com/gallery/1mvfktv,False,False,False
1mvdey9,vihanga2001,1755692899.0,8,/r/MachineLearning/comments/1mvdey9/r_how_do_you_make_text_labeling_less_painful/,MachineLearning,[R] How do you make text labeling less painful?,"Hey everyone! I'm working on a university research project about smarter ways to reduce the effort involved in labeling text datasets like support tickets, news articles, or transcripts.

The idea is to help teams *pick the most useful examples to label next*, instead of doing it randomly or all at once.

If you‚Äôve ever worked on labeling or managing a labeled dataset, I‚Äôd love to ask you **5 quick questions** about what made it slow, what you wish was better, and what would make it feel ‚Äúworth it.‚Äù

Totally academic  no tools, no sales, no bots. Just trying to make this research reflect real labeling experiences.

You can DM me or drop a comment if open to chat. Thanks so much",0,0.31,https://www.reddit.com/r/MachineLearning/comments/1mvdey9/r_how_do_you_make_text_labeling_less_painful/,False,True,False
1mv5ls0,beefchocolatesauce,1755665841.0,23,/r/MachineLearning/comments/1mv5ls0/r_is_data_the_bottleneck_for_videoaudio_generation/,MachineLearning,[R] Is data the bottleneck for video/audio generation?,"As the title says, I‚Äôm curious if data is the main bottleneck for video/audio generation. It feels like these models are improving much slower than text-based ones, and I wonder if scraping platforms like YouTube/tiktok just isn‚Äôt enough. On the surface, video data seems abundant, but maybe not when compared to text? I also get the sense that many labs are still hungry for more (and higher-quality) data. Or is the real limitation more about model architecture? I‚Äôd love to hear what people at the forefront consider the biggest bottleneck right now.",22,0.87,https://www.reddit.com/r/MachineLearning/comments/1mv5ls0/r_is_data_the_bottleneck_for_videoaudio_generation/,False,True,False
1mv4r5z,wheasey,1755663080.0,8,/r/MachineLearning/comments/1mv4r5z/r_virtuous_machines_towards_artificial_general/,MachineLearning,[R] Virtuous Machines: Towards Artificial General Science,"Hi Everyone! It looks like a generalisable scientific method has been added onto AI (using multiple frontier models) and was tested in the field of cognitive science.

Arxiv Link:¬†[https://arxiv.org/abs/2508.13421](https://arxiv.org/abs/2508.13421)

This system worked through the entire scientific method from ideation to manuscript producing new insights in the field of cognitive science as evidenced within this paper.

In this paper they've explained how they've overcome a number of limiting problems to empower and coalesce multiple frontier models to work through the entire scientific method; at a very high degree of accuracy and quality (papers validated for scientific acumen). The innovations showcased highlight significant improvements in memory, creativity, novelty, context management, and coding.

They've included in the appendix 3 papers generated by the system, where they've achieved a remarkably high standard of scientific acumen and produced the papers on average in \~17 hours and consume on average \~30m tokens.",0,0.48,https://www.reddit.com/r/MachineLearning/comments/1mv4r5z/r_virtuous_machines_towards_artificial_general/,False,True,False
1munwmw,poppear,1755622100.0,2,/r/MachineLearning/comments/1munwmw/r_azzurravoice_a_new_stateoftheart_italian/,MachineLearning,"[R] azzurra-voice, a new State-of-the-Art Italian Text-to-Speech model","Hey¬†[r/MachineLearning](https://www.reddit.com/r/MachineLearning/)

We're Cartesia, a small AI research lab based in Italy. We believe the future of AI shouldn't just be about processing commands, but about creating genuine connection. Our vision is to build agents that are private, personal, and feel culturally present.

Today, we're excited to share the first step with the open-source community:¬†`azzurra-voice`.

`azzurra-voice`¬†is a highly expressive and natural-sounding Text-to-Speech (TTS) model for the Italian language, trained on thousands of hours of high-quality, diverse Italian speech. We worked hard to capture the accents, intonations, and real-life conversational patterns from across Italy to avoid that robotic, monotone sound.

**You can listen to audio samples comparing**¬†`azzurra-voice`¬†**to other open models on our** [**blog post**](https://blog.cartesia.one/posts/introducing-azzurra-voice/)",10,0.73,https://www.reddit.com/r/MachineLearning/comments/1munwmw/r_azzurravoice_a_new_stateoftheart_italian/,False,True,False
1mufrkc,councilanderson2,1755602961.0,5,/r/MachineLearning/comments/1mufrkc/d_switching_to_postdoc_in_ml_for_earth_observation/,MachineLearning,[D] Switching to postdoc in ML for Earth Observation?,"I‚Äôd like to hear from people working with ML for Earth Observation.

My PhD was pretty broad. I used deep learning on different types of multimedia data (video, image, text, and MIDI). The outcome has been mediocre: h-index of 5, about 90 citations, mostly in Q1 journals, but no top conferences. I want to stay in academia and use a postdoc to build a clearer niche.

In multimedia and in most areas of ML, a lot of the progress comes from a small group of top institutions. It has been hard to see where my own work really makes a difference. That‚Äôs why I‚Äôve been looking at ML for Earth Observation and climate change. The work seems more meaningful, but the field is smaller and the papers tend to get less visibility and fewer citations.

My worry is that switching to Earth Observation could slow down my citation count and h-index. I know people say these metrics don‚Äôt matter much, but I feel like they still play a big role in getting academic jobs. On the other hand, if I don‚Äôt end up with a permanent academic position and move to industry, I worry that Earth Observation skills won‚Äôt transfer well since there aren‚Äôt as many opportunities compared to mainstream ML.

I‚Äôd really like to hear from people in the field about how you see these trade-offs.",19,0.81,https://www.reddit.com/r/MachineLearning/comments/1mufrkc/d_switching_to_postdoc_in_ml_for_earth_observation/,False,True,False
1mudtw6,FammasMaz,1755596540.0,6,/r/MachineLearning/comments/1mudtw6/d_endorsement_for_cslg_at_arxiv_as_nonml_student/,MachineLearning,[D] Endorsement for cs.LG at arXiv as non-ML student?,"Hello, I plan on publishing a paper in ML (diffusion models for a mechanics system) and a preprint on arXiv, however, all my colleagues and friends are in Mechanics or Physics. What could be my options in this case. I can't find a person in cs.LG for a long time?

  
The general idea is to make an ML based pipeline to generate granular mechanical structures.",0,0.38,https://www.reddit.com/r/MachineLearning/comments/1mudtw6/d_endorsement_for_cslg_at_arxiv_as_nonml_student/,False,True,False
1mu2a8x,AntreasAntoniou,1755560085.0,5,/r/MachineLearning/comments/1mu2a8x/d_beyond_the_cloud_slms_local_ai_agentic/,MachineLearning,"[D] Beyond the cloud: SLMs, local AI, agentic constellations, biology and a high value direction for AI progress","Dear r/MachineLearning friends,

I‚Äôm here today to share a thought on a different direction for AI development. While the field chases multi-trillion parameter models, I believe an extremely valuable endeavour lies in the power of constraints: pushing ourselves to get models under 1 billion parameters to excel.

In my new blog post, I argue that this constraint is a feature, not a bug. It removes the ""scale-up cheat code"" and forces us to innovate on fundamental algorithms and architectures. This path allows for faster experimentation, where architectural changes are no longer a risk but a necessity for improvement.

The fear that 'scale will wash away any and all gains' is real, but let's remember: an MLP could never compete with a Transformer, no matter how much it was scaled up. My post explores the question: **what if our current Transformer is the MLP of something better that is within grasp but ignored because of our obsession with scale?**

üß†üîç **Read the full article here:**[https://pieces.app/blog/direction-of-ai-progress](https://pieces.app/blog/direction-of-ai-progress)

Your feedback and thoughts would be greatly appreciated.

Regards,

Antreas",0,0.39,https://www.reddit.com/r/MachineLearning/comments/1mu2a8x/d_beyond_the_cloud_slms_local_ai_agentic/,False,True,False
1mtq2qy,ThRiLLeXx,1755532739.0,7,/r/MachineLearning/comments/1mtq2qy/d_location_of_eacl_2026/,MachineLearning,[D] Location of EACL 2026,"Hi folks,

I've been looking for some information on EACL 2026 as I'd like to submit something to the October cycle. However, the only thing I found so far was the [joint call for workshops](https://www.aclweb.org/portal/content/eaclacl-2026-joint-call-workshops) of EACL/ACL 2026.

But, according to this webpage, EACL 2026 would happen outside of Europe (Rabat, Morocco, from March 24-29, 2026).

Do you think this information is accurate, or am I simply missing something?",6,0.88,https://www.reddit.com/r/MachineLearning/comments/1mtq2qy/d_location_of_eacl_2026/,False,True,False
1mtoewm,OddUnderstanding1633,1755529113.0,14,/r/MachineLearning/comments/1mtoewm/d_acl_rolling_review_arr_2025_may_emnlp_2025_stats/,MachineLearning,[D] ACL Rolling Review (ARR) 2025 May (EMNLP 2025) Stats,"The stats for ARR May 2025 are out: [https://stats.aclrollingreview.org/iterations/2025/may/](https://stats.aclrollingreview.org/iterations/2025/may/)

It looks like about 25% of submissions have Meta ‚â• 3.5. Does anyone know if it‚Äôs still possible to get into the main conference with OA 3.0 Soundness 3.3 and Meta 3.5, or is it more likely to be accepted to Findings?",22,0.87,https://www.reddit.com/r/MachineLearning/comments/1mtoewm/d_acl_rolling_review_arr_2025_may_emnlp_2025_stats/,False,True,False
1mto1xw,padakpatek,1755528311.0,12,/r/MachineLearning/comments/1mto1xw/d_how_would_i_go_about_clustering_voices_from/,MachineLearning,[D] How would I go about clustering voices from songs?,"I have a 90s hiphop mixtape with a bunch of unknown tracks from multiple artists. I want to perform unsupervised clustering to infer how many artists there are in total because I can't really tell by ear.

I guess I would need to:

1. Somehow convert audio files into numerical data

2. Extract only the vocal data (or I guess these two steps can be flipped? Somehow extract only the vocal audio, and then convert that into numerical data?)

3. Perform unsupervised clustering


I'm just not sure how to go about doing steps 1 and 2.

Any ideas?",2,0.63,https://www.reddit.com/r/MachineLearning/comments/1mto1xw/d_how_would_i_go_about_clustering_voices_from/,False,True,False
1mtmadk,jeertmans,1755524253.0,1,/r/MachineLearning/comments/1mtmadk/p_jax_implementation_of_hindsight_experience/,MachineLearning,[P] JAX Implementation of Hindsight Experience Replay (HER),"Hi! I recently discovered the *Hindsight Experience Replay* (HER) paper and noticed that the official implementation is based on PyTorch and is not very well-structured. I also couldn't find a non-PyTorch implementation. Since I primarily work with **JAX**, I decided to reimplement the classic bit-flipping experiment to better understand HER.

This implementation uses **Equinox** for model definitions and **Optax** for optimization. The [repository](https://github.com/jeertmans/HER-with-JAX) provides:
+ A *minimal* and *clean* implementation of HER in JAX
+ Reproducible scripts and results
+ A [Colab Notebook](https://colab.research.google.com/github/jeertmans/HER-with-JAX/blob/main/bit_flipping.ipynb) for direct experimentation

Code: https://github.com/jeertmans/HER-with-JAX

Let me know if you have any questions, feedback, or recommendations!",29,0.98,https://www.reddit.com/r/MachineLearning/comments/1mtmadk/p_jax_implementation_of_hindsight_experience/,False,True,False
1mtfikh,AnyIce3007,1755502854.0,50,/r/MachineLearning/comments/1mtfikh/d_conferences_need_to_find_better_venues/,MachineLearning,[D] Conferences need to find better venues,"Better = venues that are virtually accessible for any researcher/author to go to.

Just this morning, I'm denied the U.S. B1 visa. I'm supposed to present my work at ICCV 2025 in Hawaii. And during my in-person interview, the Visa Officer did not even bother to ask for the invitation letter.

This really blows cause it's supposed to be my first time and I was so excited about attending it. Would love to hear your thoughts about this.",203,0.93,https://www.reddit.com/r/MachineLearning/comments/1mtfikh/d_conferences_need_to_find_better_venues/,False,True,False
1mtekhm,___loki__,1755499325.0,2,/r/MachineLearning/comments/1mtekhm/p_looking_for_datasetstools_for_testing_document/,MachineLearning,[P] Looking for datasets/tools for testing document forgery detection in medical claims,"I‚Äôm a new joinee working on a project where I need to test a forgery detection agent for medical/insurance claim documents. The agent is built around GPT-4.1, with a custom policy + prompt, and it takes base64-encoded images (like discharge summaries, hospital bills, prescriptions). Its job is to detect whether a document is authentic or forged ‚Äî mainly looking at image tampering, copy‚Äìmove edits, or plausible fraud attempts.

Since I just started, I‚Äôm still figuring out the best way to evaluate this system. My challenges are mostly around data:

* Public forgery datasets like DocTamper (CVPR 2023) are great, but they don‚Äôt really cover medical/health-claim documents.
* I haven‚Äôt found any dataset with paired authentic vs. forged health claim reports.
* My evaluation metrics are accuracy and recall, so I need a good mix of authentic and tampered samples.

What I‚Äôve considered so far:

* Synthetic generation: Designing templates in Canva/Word/ReportLab (e.g., discharge summaries, bills) and then programmatically tampering them with OpenCV/Pillow (changing totals, dates, signatures, copy‚Äìmove edits).
* Leveraging existing datasets: Pretraining with something like DocTamper or a receipt forgery dataset, then fine-tuning/evaluating on synthetic health docs.

**Questions for the community:**

1. Has anyone come across an open dataset of forged medical/insurance claim documents?
2. If not, what‚Äôs the most efficient way to generate a realistic synthetic dataset of health-claim docs with tampering?
3. Any advice on annotation pipelines/tools for labeling forged regions or just binary forged/original?

Since I‚Äôm still new, any guidance, papers, or tools you can point me to would be really appreciated üôè

Thanks in advance!",3,0.81,https://www.reddit.com/r/MachineLearning/comments/1mtekhm/p_looking_for_datasetstools_for_testing_document/,False,True,False
1mtdjum,Mad_Scientist2027,1755495691.0,9,/r/MachineLearning/comments/1mtdjum/d_how_to_get_into_high_dimensional_dynamical/,MachineLearning,[D] How to get into High Dimensional Dynamical Systems?,"Title. Also, what all areas can I hope to conduct research in? I'm a bit new to the field, and wanted to know what all it entailed before proceeding.

Any responses / suggestions are appreciated. Thanks in advance.",24,0.91,https://www.reddit.com/r/MachineLearning/comments/1mtdjum/d_how_to_get_into_high_dimensional_dynamical/,False,True,False
1mt4ym5,FineConcentrate6991,1755470505.0,7,/r/MachineLearning/comments/1mt4ym5/d_multi_class_address_classification/,MachineLearning,[D] - Multi Class Address Classification,"
Hello people, I have a dataset with Adress and label 800K rows. I am trying to train a model for address label prediction. Address data is bit messy and different for each different label. we have 10390 each with 50-500 row. I have trained a model using fasttext I have got 0.5 F1 score max. What can I do to for to get best F1 score?

Address data is like (province, district, avenue street, maybe house name and no)

some of them are missing at each address.",4,0.67,https://www.reddit.com/r/MachineLearning/comments/1mt4ym5/d_multi_class_address_classification/,False,True,False
1mszuyb,ApartmentEither4838,1755458247.0,6,/r/MachineLearning/comments/1mszuyb/d_injecting_self_doubt_in_the_cot_of_reasoning/,MachineLearning,[D] Injecting self doubt in the CoT of reasoning models,"A short analysis on what happens when you inject self doubt in the CoT of reasoning models
https://github.com/martianlantern/cot-doubt-injection",20,0.95,https://www.reddit.com/r/MachineLearning/comments/1mszuyb/d_injecting_self_doubt_in_the_cot_of_reasoning/,False,True,False
1msrdfq,gaytwink70,1755438433.0,13,/r/MachineLearning/comments/1msrdfq/is_econometrics_a_good_background_to_get_into/,MachineLearning,Is Econometrics a good background to get into Machine Learning? [D],"I have an econometrics and data analytics bachelors degree and im looking to get into a masters of artificial intelligence.

I have also taken some introductory math courses and introductory programming/algorithms as well as deep learning.

How relevant is my background if I wanna get into AI/ML research later on? (I am hoping to do a PhD afterwards in AI/ML)",8,0.65,https://www.reddit.com/r/MachineLearning/comments/1msrdfq/is_econometrics_a_good_background_to_get_into/,False,True,False
1msq0uf,Intrepid-Purpose2151,1755434878.0,5,/r/MachineLearning/comments/1msq0uf/p_confused_results_while_experimenting_with/,MachineLearning,[P] Confused results while experimenting with attention modules on CLIP RN50 for image classification,"

Hey everyone,

I‚Äôm currently working on an audio-visual project. As a first step, I‚Äôm building unimodal models before moving on to the multimodal stage. For the vision part, I started with CLIP RN50 as the backbone and fine-tuned only the classification layer. With that setup, I was able to reach around 84% accuracy on my dataset.

To push performance, I experimented with adding attention modules:

With CBAM (Convolutional Block Attention Module), accuracy improved to 89%.

With SENet (Squeeze-and-Excitation Network), I surprisingly got an even better result: 93%.


My understanding was that CBAM, which combines both channel + spatial attention, should typically give a stronger boost than SENet, which only does channel attention. But in my experiments, the opposite happened.

Am I missing something obvious here? Could this be due to dataset characteristics, training setup, or how I integrated CBAM into CLIP?

Would really appreciate any insights, especially from people who have tried attention modules on CLIP or ResNet backbones.

Thanks!
",5,0.86,https://www.reddit.com/r/MachineLearning/comments/1msq0uf/p_confused_results_while_experimenting_with/,False,True,False
1msm3m4,Master_Ocelot8179,1755421525.0,1,/r/MachineLearning/comments/1msm3m4/d_colm_financial_assistance/,MachineLearning,[D] COLM Financial Assistance,Has anybody gotten respone from COLM financial assistance? Its deadline was 31 July but I still have not recieved a yes or no response and they are not replying to my email.,5,1.0,https://www.reddit.com/r/MachineLearning/comments/1msm3m4/d_colm_financial_assistance/,False,True,False
1ms9d2u,say_wot_again,1755382065.0,17,/r/MachineLearning/comments/1ms9d2u/r_dino_v3_selfsupervised_learning_for_vision_at/,MachineLearning,[R] Dino v3: Self-supervised learning for vision at unprecedented scale,"New SOTA for self supervised learning in computer vision. They train a 7B self supervised ViT on 1.7B images, which hits SOTA with linear probing on most downstream tasks. They also release scaled and distilled versions of the model (ViT small, base, large, and huge, plus ConvNext tiny, small, base, and large), along with a version trained on satellite imagery.

There are plenty of details in the paper as to what pretraining improvements they made over DINO v2. ",212,0.98,https://ai.meta.com/blog/dinov3-self-supervised-vision-model/,False,False,False
1mrwm3w,the_iegit,1755354049.0,16,/r/MachineLearning/comments/1mrwm3w/d_model_architecture_or_data/,MachineLearning,[D] model architecture or data?,"I‚Äôve just read that the new model architecture called Hierarchical Reasoning Model (HRM) gains it‚Äôs performance benefits from data augmentation techniques and chain of thought rather than model architecture itself. link: https://arcprize.org/blog/hrm-analysis

And i‚Äôve heard same opinion about transformers that the success of current llms is about cramming enormous amounts of data into it rather than the genius of the architecture

Can someone explain which of the sides is closer to the truth?",38,0.87,https://www.reddit.com/r/MachineLearning/comments/1mrwm3w/d_model_architecture_or_data/,False,True,False
1mrqyni,Slight-Ad-5816,1755338776.0,2,/r/MachineLearning/comments/1mrqyni/r_how_do_i_choose_the_best_model_in_validation/,MachineLearning,[R] How do I choose the best model in validation when I have no target data??,"I am working on unsupervised domain adaptation techniques for super resolution. I have a good amount of paired source data and very less target data without no ground truth. The issue is while training this pipeline I am not able to save the best model as for this I would need some ground truth in the target domain on which I would validate the model after each epoch and save the best one. How do I tackle this? Recently, I found an OpenReview paper about a transfer score which is a metric which do not need target labels but it is for classification based tasks. I want something for super-resolution. Does anyone have any idea?",0,0.33,https://www.reddit.com/r/MachineLearning/comments/1mrqyni/r_how_do_i_choose_the_best_model_in_validation/,False,True,False
1mrjs5i,ilovecookies14,1755315893.0,3,/r/MachineLearning/comments/1mrjs5i/d_cool_new_ways_to_mix_linear_optimization_with/,MachineLearning,"[D] Cool new ways to mix linear optimization with GNNs? (LP layers, simplex-like updates, etc.)","Lately I‚Äôve been diving into how graph neural networks can play nicely with linear optimization, not just as a post-processing step, but actually inside the model or training loop.

I‚Äôve seen some neat stuff around differentiable LP layers, GNNs predicting parameters for downstream solvers, and even architectures that mimic simplex-style iterative updates. It feels like there‚Äôs a lot of room for creativity here, especially for domain-specific problems in science/engineering.

Curious what‚Äôs been coming out in the last couple of years. Any papers, repos, or tricks you‚Äôve seen that really push this GNN + optimization combo forward? Supervised, unsupervised, RL‚Ä¶ all fair game.",25,0.91,https://www.reddit.com/r/MachineLearning/comments/1mrjs5i/d_cool_new_ways_to_mix_linear_optimization_with/,False,True,False
1mr7ifp,Routine-Scientist-38,1755285578.0,38,/r/MachineLearning/comments/1mr7ifp/d_neurips_position_paper_reviews/,MachineLearning,[D] - Neurips Position paper reviews,"The position paper reviews were just released. So far this entire process has been very unprofessional, with multiple delays, poor communication, and still no clear rubric for what the review scores mean. Has anyone else gotten reviews? Curious to hear other's thoughts on this",40,0.92,https://www.reddit.com/r/MachineLearning/comments/1mr7ifp/d_neurips_position_paper_reviews/,False,True,False
1mqur0y,Agreeable_Touch_9863,1755256365.0,4,/r/MachineLearning/comments/1mqur0y/d_bethe_hessian_spectral_clustering/,MachineLearning,[D] Bethe Hessian Spectral Clustering,"Why does nobody seem to use this when it works noticeably better than regular (normalised laplacian) spectral clustering? I have studied it a fair bit and cant see any downsides apart from ever so slightly higher computational cost (the order of magnitude doesn't change, just a larger constant.)

Its also been around long enough now that I dont see recency as the issue.",8,0.8,https://www.reddit.com/r/MachineLearning/comments/1mqur0y/d_bethe_hessian_spectral_clustering/,False,True,False
1mqgyfe,Onlyheretohelp_you,1755214194.0,14,/r/MachineLearning/comments/1mqgyfe/custom_vulkan_c_machine_learning_library_vs/,MachineLearning,custom Vulkan C++ machine learning library vs TensorFlow [R],guys I need your opinion: I made a machine learning library using Vulkan (with compute shaders to preform the forward and backward passes) and I found that base tensorflow (on CPU) is faster than my custom model that uses GPUs. I had the simplest test where I used a very large kernel on a singe dense (ffn) layer and tensorflow is much faster. The only operation that is done in this model is a forward and backward matmul which the GPU should be much faster at. what do you guys think is the reason? -ps I asked chatgpt and I literally what to k\*ll it cause it repeats the same wrong things,5,0.65,https://www.reddit.com/r/MachineLearning/comments/1mqgyfe/custom_vulkan_c_machine_learning_library_vs/,False,True,False
1mqgcka,AncientGearAI,1755212695.0,8,/r/MachineLearning/comments/1mqgcka/problem_with_dataset_for_my_my_physics/,MachineLearning,Problem with dataset for my my physics undergraduate paper. Need advice about potential data leakage. [N],"Hello.

I am making a project for my final year undergraduate dissertation in a physics department. The project involves generating images (with python) depicting diffraction patters from light (laser) passing through very small holes and openings called slits and apertures. I used python code that i could pass it the values of some parameters such as slit width and slit distance and number of slits (we assume one or more slits being in a row and the light passes from them. they could also be in many rows (like a 2d piece of paper filled with holes). then the script generates grayscale images with the parameters i gave it. By giving different value combinations of these parameters one can create hundreds or thousands of images to fill a dataset.

So i made neural networks with keras and tensorflow and trained them on the images i gave it for image classification tasks such as classification between images of single slit vs of double slit.  Now the main issue i have is about the way i made the datasets. First i generated all the python images in one big folder. (all hte images were even slightly different as i used a script that finds duplicates (exact duplicates) and didnt find anything. Also the image names contain all the parameters so if two images were exact duplicates they would have the same name and in a windows machine they would replace each other). After that, i used another script that picks images at random from the folder and sends them to the train, val and test folders and these would be the datasets the model would train upon.

PROBLEM 1:

The problem i have is that many images had very similar parameter values (not identical but very close) and ended up looking almost identical to the eye even though they were not duplicates pixel to pixel. and since the images to be sent to the train, val and test sets were picked at random from the same initial folder this means that many of the images of the val and test sets look very similar, almost identical to the images from the train set. And this is my concern because im afraid of data leakage and overfitting. (i gave two such images to see)

Off course many augmentations were done to the train set only mostly with teh Imagedatagenerator module while the val and test sets were left without any augmentations but still i am anxious.

PROBLEM 2:

Another issue i have is that i tried to create some datasets that contained real photos of diffraction patterns. To do that i made some custom slits at home and with a laser i generated the patterns. After i managed to see a diffraction pattern i would take many photos of the same pattern from different angles and distances. Then i would change something slightly to change the diffraction pattern a bit and i would again start taking photos from different perspectives. In that way i had many different photos of the same diffraction pattern and could fill a dataset. Then i would put all the images in the same folder and then randomly move them to the train, val and test sets. That meant that in different datasets there would be different photos (angle and distance) but of the same exact pattern. For example one photo would be in the train set and then another different photo but of the same pattern in the validation set. Could this lead to data leakage and does it make my datasets bad? bellow i give a few images to see.

if there were many such photos in the same dataset (for example the train set) only and not in the val or test sets then would this still be a problem? I mean that there are some trully different diffraction patterns i made and then many photos with different angles and distances of these same patterns to fill hte dataset? if these were only in one of the sets and not spread across them like i described in hte previous paragraph?

[photo of double slit diffraction \(train set\)](https://preview.redd.it/vn95v576y6jf1.jpg?width=400&format=pjpg&auto=webp&s=402a1bc2df3cf80b9b5ee90d6da42ac64dd3fef7)

[photo of double slit diffraction \(val set\)](https://preview.redd.it/6j6o6876y6jf1.jpg?width=400&format=pjpg&auto=webp&s=a30f4c67036a800a33b5571475c997b43857b98a)

[python image single slit diffraction \(train set\)](https://preview.redd.it/wz2nts76y6jf1.jpg?width=400&format=pjpg&auto=webp&s=9fcfac7032d3c9de2255055f7c96abac774b8687)

[python image \(single slit val set\)](https://preview.redd.it/78xiee76y6jf1.jpg?width=400&format=pjpg&auto=webp&s=29342d997939aa13d5fd4a004c29228d61f13896)",9,0.91,https://www.reddit.com/r/MachineLearning/comments/1mqgcka/problem_with_dataset_for_my_my_physics/,False,True,False
1mqdpug,stevenverses,1755206581.0,0,/r/MachineLearning/comments/1mqdpug/250717338_mobile_manipulation_with_active/,MachineLearning,[2507.17338] Mobile Manipulation with Active Inference for Long-Horizon Rearrangement Tasks,"Research showcasing how a robot outperforms state of the art models on the Habitat benchmark from Meta ***without pre-training***.

For those fluent in ü§ñ what you think?",6,1.0,https://arxiv.org/abs/2507.17338,False,False,False
1mqccah,Majestij,1755203555.0,0,/r/MachineLearning/comments/1mqccah/r_code_for_flow_stochastic_segmentation_networks/,MachineLearning,[R] Code for Flow Stochastic Segmentation Networks (ICCV 20205),"Code & paper at: [https://github.com/biomedia-mira/flow-ssn](https://github.com/biomedia-mira/flow-ssn)

**TL;DR**

\- A flow's prior is typically fixed (e.g. N(0, I)). We learn it and use a **lightweight** flow to model pixel dependencies;

\- This makes sampling (ODE solving) more **efficient**, without sacrificing performance in our setting;

\- We introduce bespoke training objectives for both **autoregressive** and **continuous-time flow** variants;

\- Flow-SSN achieves **SOTA** performance on standard stochastic segmentation benchmarks!

https://preview.redd.it/rllc2yplo1jf1.png?width=3850&format=png&auto=webp&s=6bb1bc63a6836b9fc6a4b8e9f10205889a5b051d

https://i.redd.it/8vgf2iemo1jf1.gif

https://i.redd.it/81lbt56no1jf1.gif",16,1.0,https://www.reddit.com/r/MachineLearning/comments/1mqccah/r_code_for_flow_stochastic_segmentation_networks/,False,True,False
1mqbf3m,AdInevitable1362,1755201586.0,4,/r/MachineLearning/comments/1mqbf3m/p_can_i_use_test_set_reviews_to_help_predict/,MachineLearning,"[P] Can I use test set reviews to help predict ratings, or is that cheating?","I‚Äôm working on a rating prediction (regression) model. I also have reviews for each user-item interaction, and from those reviews I can extract ‚Äúaspects‚Äù (like quality, price, etc.) and build a separate graphs and concatenate their embeddings at the end to help predicting the score.

My question is: when I split my data into train/test, is it okay to still use the aspects extracted from the test set reviews during prediction, or is that considered data leakage?

In other words: the interaction already exists in the test set, but is it fair to use the test review text to help the model predict the score? Or should I only use aspects from the training set and ignore them for test interactions?

Ps: I‚Äôve been reading a paper where they take user reviews, extract ‚Äúaspects‚Äù (like quality, price, service‚Ä¶), and build an aspect graph linking users and items through these aspects.

In their case, the goal was link prediction ‚Äî so they hide some user‚Äìitem‚Äìaspect edges and train the model to predict whether a connection exists.",2,0.6,https://www.reddit.com/r/MachineLearning/comments/1mqbf3m/p_can_i_use_test_set_reviews_to_help_predict/,False,True,False
1mq5wiz,ImaginationAny2254,1755189781.0,75,/r/MachineLearning/comments/1mq5wiz/d_people_in_mldsai_field_since_510_years_or_more/,MachineLearning,"[D] People in ML/DS/AI field since 5-10 years or more, are you tired of updating yourself with changing tech stack?","I have been in this space since SAS, and its quite exhausting to update with every skill in the market to stay relevant especially if trying for a job switch and going through the interviews. Till how long can you keep studying and updating with the new trend and also even if you get in the boat there is so much stress at the work place in these sectors mainly because the leadership is from the management background and theres a lot of pressure for tech people to deliver.

Although I love my field but I have got to thinking lately that Is it even worth it?",93,0.84,https://www.reddit.com/r/MachineLearning/comments/1mq5wiz/d_people_in_mldsai_field_since_510_years_or_more/,False,True,False
1mq3w9c,RobertWF_47,1755185381.0,7,/r/MachineLearning/comments/1mq3w9c/d_best_way_to_partition_longitudinal_data_into/,MachineLearning,[D] Best way to partition longitudinal data into pre and post time periods for predictive model?,"I'm working on several healthcare models that will predict future health conditions for individuals using past longitudinal data. We have data spanning 6 years.

In the past I'd split the data into one year time spans by calendar year and train the model to predict the outcome in year t1 from predictors in the prior year t0. If we have 6 years of data for a person I'd transform their data from wide to long format: 5 rows of pre and post periods. But I'm not certain this is the best approach.

What is the optimal way to split my data into pre and post time periods to obtain the best prediction accuracy? 6 month time periods instead of 1 year? Or lump all past data for each person into a single pre period & post period (1 row)? I understand it may come down to testing different formats, see what sticks.",2,0.67,https://www.reddit.com/r/MachineLearning/comments/1mq3w9c/d_best_way_to_partition_longitudinal_data_into/,False,True,False
1mq3nia,Practical-Pin8396,1755184853.0,36,/r/MachineLearning/comments/1mq3nia/p_small_and_imbalanced_dataset_what_to_do/,MachineLearning,[P] Small and Imbalanced dataset - what to do,"Hello everyone!

I'm currently in the 1st year of my PhD, and my PI asked me to apply some ML algorithms to a dataset (n = 106, w/ n = 21 in the positive class). As you can see, the performance metrics are quite poor, and I'm not sure how to proceed...

I‚Äôve searched both in this subreddit and internet, and I've tried using LOOCV and stratified k-fold as cross-validation methods. However, the results are consistently underwhelming with both approaches. Could this be due to data leakage? Or is it simply inappropriate to apply ML to this kind of dataset?

Additional info:  
I'm in the biomedical/bioinformatics field (working w/ datasets of cancer or infectious diseases). These patients are from a small, specialized group (adults with respiratory diseases who are also immunocompromised). Some similar studies have used small datasets (e.g., n = 50), while others succeeded in work with larger samples (n = 600‚Äì800).  
Could you give me any advice or insights? (Also, sorry for gramatics, English isn't my first language). TIA!

https://preview.redd.it/fc20uero50jf1.png?width=655&format=png&auto=webp&s=1ed35c046f9c2bfe030e0c3bfe8c4cdcf7afb852

",43,0.87,https://www.reddit.com/r/MachineLearning/comments/1mq3nia/p_small_and_imbalanced_dataset_what_to_do/,False,True,False
1mpbp39,Entrepreneur7962,1755108048.0,46,/r/MachineLearning/comments/1mpbp39/d_got_spare_time_whats_worth_doing/,MachineLearning,[D] Got Spare Time ‚Äì What‚Äôs Worth Doing?,"I'm a fresh PhD graduate and I finally landed a job which I start in a few months.  
It happened to be that I have quite a bit of free time, at least until my next journey. I thought about taking a few months off, but a few weeks in and I start to feel a bit out of place.  
I really don't know how to handle simply doing nothing.

I thought maybe I‚Äôd start some initiative in this rare window I‚Äôm in right now, and I was hoping to get interesting ideas from the community.

My main objective is that it would be something valuable that I enjoy doing.  
This could be something that is technically cool (AGI anyone?) or some tool for the community (any tool you'd wish existed? paperswithcode or paper copilot comes to mind).

Love to hear your thoughts!",42,0.78,https://www.reddit.com/r/MachineLearning/comments/1mpbp39/d_got_spare_time_whats_worth_doing/,False,True,False
1mpa2ip,Proud_Landscape_4231,1755104488.0,3,/r/MachineLearning/comments/1mpa2ip/d_if_there_were_to_be_some_sort_of_way_you_could/,MachineLearning,"[D] If there were to be some sort of way you could get NDVI (not true, but predict) that was near perfect accuracy through JUST standard RGB input (NO NIR AT ALL), how useful would that be (API, for example)?","Sorry if this is not the right place to post! I'm new to the community and overall GIS industry. Just want to see how useful this would be, specific use cases, and maybe how this could be used by you personally.

  
I know there are RGB-only indices that exist, but from what I've heard, they're very inaccurate. This would be 94%+ (accuracy to true-NDVI) and it‚Äôs a highly trained ML model",0,0.2,https://www.reddit.com/r/MachineLearning/comments/1mpa2ip/d_if_there_were_to_be_some_sort_of_way_you_could/,False,True,False
1mp6n2g,ChampionshipCrazy429,1755096784.0,5,/r/MachineLearning/comments/1mp6n2g/d_google_deepmind_analytics_engineer_interview/,MachineLearning,[D] Google DeepMind Analytics Engineer Interview Prep,Got an upcoming interview for this role and have a good feeling so far. How do I prepare for it? What will be the next steps? Any tips or experience would be greatly appreciated. Thanks!,21,0.72,https://www.reddit.com/r/MachineLearning/comments/1mp6n2g/d_google_deepmind_analytics_engineer_interview/,False,True,False
1mp3u1f,Accomplished-Pay-390,1755090149.0,540,/r/MachineLearning/comments/1mp3u1f/d_emnlp_2025_decisions/,MachineLearning,[D] EMNLP 2025 Decisions,"Discussion thread for EMNLP 2025 decisions

",27,0.75,https://www.reddit.com/r/MachineLearning/comments/1mp3u1f/d_emnlp_2025_decisions/,False,True,False
1mp2dcp,ArtemHnilov,1755086131.0,7,/r/MachineLearning/comments/1mp2dcp/r_fuzzypattern_tsetlin_machine/,MachineLearning,[R] Fuzzy-Pattern Tsetlin Machine,"I‚Äôm excited to announce the paper:¬†**Fuzzy-Pattern Tsetlin Machine** (FPTM)¬†‚Äî a paradigm shift in the Tsetlin Machine family of algorithms.

Unlike traditional Tsetlin Machines, which rely on strict clause evaluation, FPTM introduces¬†fuzzy clause evaluation: if some literals in a clause fail, the remaining literals can still contribute to the vote with a proportionally reduced score. This allows each clause to act as a collection of adaptive sub-patterns, enabling more flexible, efficient, and robust pattern matching.

Thanks to this fuzzy mechanism, FPTM dramatically reduces the number of required clauses, memory usage, and training time ‚Äî all while improving accuracy.

**Results:**

**IMDb** dataset:

‚Ä¢ 90.15% accuracy with just **1 clause** per class

‚Ä¢ 50√ó reduction in clauses and memory vs. Coalesced TM

‚Ä¢ 36√ó to 316√ó faster training (**45 seconds vs. 4 hours**) compared to TMU Coalesced TM

‚Ä¢ Fits in **50 KB**, enabling online learning on microcontrollers

‚Ä¢ Inference throughput: **34.5 million** predictions per second (51.4 GB/s)

**Fashion-MNIST** dataset:

‚Ä¢ 92.18% accuracy (2 clauses per class)

‚Ä¢ 93.19% accuracy (20 clauses), \~400√ó clause reduction vs. Composite TM (93.00% with 8000 clauses)

‚Ä¢ **94.68%** accuracy (8000 clauses), establishing a new *state-of-the-art* among all TM variants and outperforming complex neural net architectures like *Inception-v3*

**Amazon Sales** dataset (20% noise):

‚Ä¢ **85.22%** accuracy ‚Äî outperforming Graph TM (78.17%) and GCN (66.23%)

üìÑ Read the paper: [https://arxiv.org/pdf/2508.08350](https://arxiv.org/pdf/2508.08350)

üíª Source code: [https://github.com/BooBSD/FuzzyPatternTM](https://github.com/BooBSD/FuzzyPatternTM)",44,0.9,https://www.reddit.com/r/MachineLearning/comments/1mp2dcp/r_fuzzypattern_tsetlin_machine/,False,True,False
1mo7fzx,fishsoon2020,1755001931.0,1,/r/MachineLearning/comments/1mo7fzx/r_about_test_set_of_xgboost_for_time_series/,MachineLearning,[R] About test set of XGBoost for Time Series Forecasting,"I have questions about using XGBoost for the Time Series Forecasting problem. According to these articles:

[Multi-step time series forecasting with XGBoost | Towards Data Science](https://towardsdatascience.com/multi-step-time-series-forecasting-with-xgboost-65d6820bec39/)[XGBoost for ](https://xgboosting.com/xgboost-for-multi-step-univariate-time-series-forecasting-with-multioutputregressor/)

[Multi-Step Univariate Time Series Forecasting with MultiOutputRegressor | XGBoosting](https://xgboosting.com/xgboost-for-multi-step-univariate-time-series-forecasting-with-multioutputregressor/)

[How I Trained a Time-Series Model with XGBoost and Lag Features](https://medium.com/@connect.hashblock/how-i-trained-a-time-series-model-with-xgboost-and-lag-features-8c17439c81e4)

I understand that they are using a sliding window approach to create ($t\_1, t\_2, ..., t\_n, t\_{n+1}, t\_{n+2}..., t\_m$), where the first $n$ variables are used as feature variables and the last $m$ variables are used as target variables. Then, they feed these rows into the XGBoost to find the relationship between the feature variables and target variables.

My problem is: It appears that during the testing phase, they utilized the actual feature variables for testing. For example, when we are predicting the first future $m$ points, we still have the actual $n$ points before these $m$ points as the features. However, when we are predicting the $m+1$ points, we are missing the actual value for the first feature in the $n$ features.

But in the above articles, it seems they just assume they have the actual $n$ at all times during training.

And for the paper ""Do We Really Need Deep Learning Models for Time Series Forecasting?"", for table 1 as shown below:

I think h refers to the number of regressors they are using. So, for the first row, they can forecast 24 points using the existing training data. But how can they further forecast œÑ points beyond the 20th point?

So, I want to clarify

1. Do the methods in the above articles suffer from data leakage? Or is it safe to assume that we can know the real $n$ features when we are focusing on the $m$ new data points?
2. My current idea is that for using XGBoost in time series forcasting, we can either

* Feed back the predicted value as the $n$ feature for the upcoming forcasting of $m$ points.
* Or we train $L$ independent regressors to forecast the $L$ points in the future in one batch.

",1,0.67,https://www.reddit.com/r/MachineLearning/comments/1mo7fzx/r_about_test_set_of_xgboost_for_time_series/,False,True,False
1mor8vy,NoteDancing,1755048600.0,1,/r/MachineLearning/comments/1mor8vy/d_applying_prioritized_experience_replay_in_the/,MachineLearning,[D] Applying Prioritized Experience Replay in the PPO algorithm,"When using the PPO algorithm, can we improve data utilization by implementing Prioritized Experience Replay (PER) where the priority is determined by both the probability ratio and the TD-error, while simultaneously using a windows\_size\_ppo parameter to manage the experience buffer as a sliding window that discards old data?",2,0.75,https://www.reddit.com/r/MachineLearning/comments/1mor8vy/d_applying_prioritized_experience_replay_in_the/,False,True,False
1mou8y4,Elegant_Dream4936,1755057328.0,1,/r/MachineLearning/comments/1mou8y4/r_promising_research_directions_for_vlms_in_the/,MachineLearning,[R] Promising Research Directions for VLMs in the Medical Domain,"Dear all,

I‚Äôd like to hear the community‚Äôs thoughts on promising research directions for VLMs (e.g., CLIP), particularly in the medical domain.

Thank you in advance!",0,0.27,https://www.reddit.com/r/MachineLearning/comments/1mou8y4/r_promising_research_directions_for_vlms_in_the/,False,True,False
1moqpqw,seventh_day123,1755047142.0,1,/r/MachineLearning/comments/1moqpqw/d_statement_on_the_originality_of_openrlhf_and/,MachineLearning,[D] Statement on the Originality of OpenRLHF and veRL FSDP RLHF,">From the original chinese zhihu blogpost (2025/5): [https://zhuanlan.zhihu.com/p/23147932785](https://zhuanlan.zhihu.com/p/23147932785)

**Recently, there has been quite a bit of discussion and controversy online about OpenRLHF and veRL.**  
**As the original author, I feel compelled to issue a statement.**

In short: **OpenRLHF is like KartRider ‚Äî the original ‚Äî and veRL FSDP is like QQ Speed, which is basically a copycat of OpenRLHF.**

# 1. Performance Differences Between OpenRLHF and veRL

There is no fundamental performance difference between veRL‚Äôs FSDP RLHF and OpenRLHF (DeepSpeed) because both use vLLM for inference and ZeRO3 for training.  
The performance data in veRL‚Äôs original paper was based on *Megatron* RLHF vs. the old OpenRLHF 0.2 version.  
If you think there‚Äôs a big performance gap, you probably just used it incorrectly. At the moment, FSDP is slightly faster than DeepSpeed, but with the release of DeepSpeed‚Äôs **deepcompile** and especially **AutoTP**, DeepSpeed is expected to overtake in performance.

# 2. On HybridFlow Free Scheduling

Any RLHF framework developed with Ray can achieve free scheduling because Ray natively provides the *placement group* feature.  
This means HybridFlow in veRL's paper is essentially just a nicer name for Ray‚Äôs Placement Group API.  
Currently, OpenRLHF fully implements HybridFlow, whereas veRL does not.  
OpenRLHF also supports independent deployment of vLLM and Actors to prevent OOM issues when training very large models (32B+ or long-text).  
In fact, OpenRLHF was the **first** framework to support this feature based on Ray Placement Group API.

# 3. Hybrid Engine

Hybrid Engine was first proposed by **DeepSpeedChat**, not an original contribution from veRL.  
Both veRL and OpenRLHF now support this feature.

# 4. Ray + vLLM + HF Transformers + ZeRO3 for RLHF Training

This setup is one of the **simplest and most user-friendly** high-performance RLHF training solutions, combining ease of use with top performance.

It was first proposed and open-sourced by OpenRLHF (open-sourced in Aug 2023, most features completed by Jan 2024).  
veRL FSDP **fully copied** this setup.

https://preview.redd.it/vfzm143vroif1.png?width=1440&format=png&auto=webp&s=10d8a5bcd101455a06a3506f037abc10f12dd277

https://preview.redd.it/tqela8mvroif1.png?width=1440&format=png&auto=webp&s=c3a2daa1ead45f7434184f107da8ba2f78cc9c8d

The core idea at the time was to use the HF weight format as a bridge, enabling seamless weight synchronization and high-performance inference based on ZeRO3 / AutoTP mechanisms, **avoiding** heavyweight frameworks like Megatron.

**The Original OpenRLHF Architecture:**  
**Ray + vLLM + ZeRO + HF**

There are also many related implementation details:

* Supported feature list
* Standardized interfaces such as `--input_key` to specify the input field format

All of these in veRL FSDP were **modeled after OpenRLHF**.

**Example from code details:**  
veRL:

https://preview.redd.it/b8f2lprwroif1.png?width=1440&format=png&auto=webp&s=a0daf3eab1c77f71e4917c044f988c35e229baa4

https://preview.redd.it/exf7lxhxroif1.png?width=1440&format=png&auto=webp&s=220636cea299502df1b94e2544a76b34e2acb6c7

OpenRLHF:

https://preview.redd.it/qfakvovyroif1.png?width=1440&format=png&auto=webp&s=260775676354a50bacd79ce06fb25417a53466de

Other design ideas like **ref\_reward offload**, **critic pretrain**, **remote RM**, etc., were also first conceived or proposed by OpenRLHF, and veRL FSDP later implemented corresponding features.

# 5. Single Controller

*(Update May 2025)*

The ‚ÄúSingle Controller‚Äù concept mentioned in the veRL paper comes from the same Ray design pattern as HybridFlow.

In early versions of OpenRLHF‚Äôs Ray RLHF implementation, there was a `RayPPOActorGroup` concept‚Äîmanaging a group of DeepSpeed ZeRO DP processes with a single Ray Group class, and providing an `async_run_method` interface to control all processes in the group at once.  
That‚Äôs essentially the core idea of Single Controller.

[https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L300](https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L300)

This interface wasn‚Äôt enabled at first because the codebase needed to be compatible with both Ray and non-Ray RLHF paths. Later, when the non-Ray code was removed, the API was naturally enabled.

Lastly, I want to thank ByteDance for open-sourcing its internal framework for everyone to use and maintain, which helps the open-source community thrive (e.g., FSDP / Ulysses support).

However, I hope friends in the community won‚Äôt disparage other open-source frameworks.  
OpenRLHF, as a **zero-budget, purely open-source** project, can‚Äôt compete in development speed with large commercial projects like veRL‚Äî  
I only hope this post helps preserve the contributions OpenRLHF has made to the RLHF open-source community.

**Btw, the open-source community should respect originality in order to develop healthily.**",11,0.73,https://www.reddit.com/r/MachineLearning/comments/1moqpqw/d_statement_on_the_originality_of_openrlhf_and/,False,True,False
1mokka9,alkalinemoe,1755031771.0,1,/r/MachineLearning/comments/1mokka9/d_multiple_submission_policy_at_emnlp_2025_for/,MachineLearning,[D] Multiple submission policy at EMNLP 2025 for workshops,"Hi all,

I‚Äôm trying to understand the EMNLP 2025 multiple submission policy when it comes to co-organized workshops.

Our paper is committed to EMNLP 2025 (main conference), but we think it might also be a good fit for a specific workshop, in case if it is not accepted to EMNLP. 

The problem is, the workshop‚Äôs submission deadline is before the EMNLP notification date (Aug 20).

The workshop‚Äôs CFP says multiple submissions are fine if disclosed at submission. However, the EMNLP CFP states it follows the ARR multiple submission policy, which includes this clause:

> Commitment + Commitment/Other Venue: Whether you can commit/submit to two venues simultaneously depends on the dual submission policies of those venues. Typically, it is not permitted.

[ARR policy](https://aclrollingreview.org/cfp#:~:text=Multiple%20Submission%20Policy)


TL;DR 

What I‚Äôm unsure about is this:

- Does ‚Äúother venue‚Äù here include EMNLP co-organized workshops?

- Has anyone successfully submitted to both the main conference and a co-organized workshop in this timing overlap?


I couldn‚Äôt find any direct clarification online for this year, so I‚Äôd really appreciate hearing from researchers who‚Äôve navigated this.

Thanks!",4,0.75,https://www.reddit.com/r/MachineLearning/comments/1mokka9/d_multiple_submission_policy_at_emnlp_2025_for/,False,True,False
1moj422,fictoromantic_25,1755028528.0,11,/r/MachineLearning/comments/1moj422/guidance_on_improving_the_reconstruction_results/,MachineLearning,Guidance on improving the reconstruction results of my VAE [Project],"Hi all! I was trying to build a VAE with an LSTM to reconstruct particle trajectories by basing off my model on the paper ""Modeling Trajectories with Neural Ordinary Differential Equations"". However, despite my loss plots showing a downward trend, my predictions are linear.

I have applied KL annealing and learning rate scheduler - and yet, the model doesn't seem to be learning the non-linear dynamics. The input features are x and z positions, velocity, acceleration, and displacement. I used a combination of ELBO and DCT for my reconstruction loss. The results were quite bad with MinMax scaling, so I switched to z-score normalization, which helped improve the scales. I used the Euler method with torchdiffeq.odeint.

Would it be possible for any of you to guide me on what I might be doing wrong? I‚Äôm happy to share my implementation if it helps. I appreciate and am grateful for any suggestions (and sorry about missing out on the labeling the axes - they are x and z)

https://preview.redd.it/veskdk7p7nif1.png?width=529&format=png&auto=webp&s=0938c4dd588961f94eba40a0e20d81008bc131f0

https://preview.redd.it/ddubae7p7nif1.png?width=529&format=png&auto=webp&s=15a24e197e6fd331d92175d1327fb2b482aaa2cc",2,0.67,https://www.reddit.com/r/MachineLearning/comments/1moj422/guidance_on_improving_the_reconstruction_results/,False,True,False
1moj3xz,Pretend_Guava7322,1755028520.0,4,/r/MachineLearning/comments/1moj3xz/p_can_anyone_suggest_an_open_weights_ai_humanizer/,MachineLearning,[P] Can anyone suggest an open weights AI Humanizer?,"I've often wanted to make an AI humanizer. The first approach I've tried was using `meta-llama/Llama-3.1-8B`. I first made a BERT fine-tune to classify between AI generated and human written. Then, I used a modified RL approach to fine-tune `meta-llama/Llama-3.1-8B` to rephrase an existing AI generated text, optimizing the humanness score. I repeated this several times, each time training a new scorer, similar to the GAN framework. This was largely unsuccessful. Unfortunately I can't share code because this was done months ago and I'm just now coming back to it, and I didn't properly track versions. I now believe that a T5 model would be better suited for this task than a Llama model. Does anyone have any suggestions, links, papers, or models that they can recommend? I am looking for open weights/open source models, not paid APIs.",0,0.29,https://www.reddit.com/r/MachineLearning/comments/1moj3xz/p_can_anyone_suggest_an_open_weights_ai_humanizer/,False,True,False
1mo41xn,mystic_blue5,1754990998.0,1,/r/MachineLearning/comments/1mo41xn/r_intuition_emerges_in_maximum_caliber_models_at/,MachineLearning,[R]: Intuition emerges in Maximum Caliber models at criticality,"Are today‚Äôs AI models hitting a wall or just missing a law?

This recent preprint in arXiv proposes a minimal sandbox (a maze) and a statistical physics approach (Maximum Caliber principle) to address this question. The presented method, called mind-tuning, applies Maximum Caliber to predictive models and reveals a critical intuition phase between imitation and hallucination.

[https://arxiv.org/abs/2508.06477](https://arxiv.org/abs/2508.06477)",0,0.31,https://www.reddit.com/r/MachineLearning/comments/1mo41xn/r_intuition_emerges_in_maximum_caliber_models_at/,False,True,False
1mo1ngm,hsbdbsjjd,1754981647.0,14,/r/MachineLearning/comments/1mo1ngm/p_dealing_with_extreme_class_imbalance0095/,MachineLearning,[P] Dealing with EXTREME class imbalance(0.095% prevalence),"I‚Äôm trying to build a model for fraud prediction where I have a labeled dataset of ~200M records and 45 features. It‚Äôs supervised since I have the target label as well. It‚Äôs a binary classification problem and I‚Äôve trying to deal with it using XGB and also tried neural network. 

The thing is that only 0.095% of the total are fraud. How can I make a model that generalizes well. I‚Äôm really frustrated at this point. I tried everything but cannot reach to the end. Can someone guide me through this situation?",15,0.89,https://www.reddit.com/r/MachineLearning/comments/1mo1ngm/p_dealing_with_extreme_class_imbalance0095/,False,True,False
1mo1l2u,hero88645,1754981391.0,3,/r/MachineLearning/comments/1mo1l2u/d_evaluation_drift_and_contamination_mitigation/,MachineLearning,[D] Evaluation Drift and Contamination Mitigation in Foundation Model Assessment,"As foundation models scale and benchmarks saturate, contamination and drift present increasing challenges to meaningful evaluation. Sharing practical mitigation strategies that have worked in practice:



\*\*Contamination Detection:\*\*

\- N-gram overlap analysis (sliding window approach)

\- Substring matching with fuzzy boundaries  

\- Semantic similarity scoring via embeddings

\- Statistical outlier detection in performance curves



\*\*Dataset Hygiene:\*\*

\- Temporal splits with strict cutoffs (no post-training data)

\- Hold-out validation across multiple independent sources

\- Private test sets with limited query budgets

\- Adversarial examples targeting memorization vs. understanding



\*\*Drift Mitigation:\*\*

\- Rolling evaluation windows with decay weighting

\- Multi-task assessment reducing single-metric gaming

\- Human evaluation correlation tracking over time

\- Cross-validation with domain-specific benchmarks



\*\*Process Controls:\*\*

\- Blind evaluation protocols (evaluator doesn't know model identity)

\- Staged releases with contamination audits between stages

\- Community-sourced benchmark validation

\- Reproducibility requirements for evaluation code



Seeing gaps in current practice around contamination detection at scale and standardized tooling for drift measurement. What approaches have proven most effective in your evaluation pipelines?",1,0.67,https://www.reddit.com/r/MachineLearning/comments/1mo1l2u/d_evaluation_drift_and_contamination_mitigation/,False,True,False
1mo1kci,hero88645,1754981314.0,1,/r/MachineLearning/comments/1mo1kci/d_reliability_metrics_and_failure_taxonomy_for/,MachineLearning,[D] Reliability Metrics and Failure Taxonomy for Agent Tool-Use Systems,"Observing increasing deployment of agentic systems with tool access, but reliability evaluation remains fragmented. Key reliability metrics worth standardizing:



\*\*Success Rate Decomposition:\*\*

\- Tool selection accuracy (right tool for task)

\- Parameter binding precision (correct arguments)

\- Error recovery effectiveness (fallback strategies)

\- Multi-step execution consistency



\*\*Failure Taxonomy:\*\*

\- Type I: Tool hallucination (non-existent APIs)

\- Type II: Parameter hallucination (invalid args)

\- Type III: Context drift (losing task state)

\- Type IV: Cascade failures (error propagation)

\- Type V: Safety violations (unauthorized actions)



\*\*Observable Proxies:\*\*

\- Parse-ability of tool calls (syntactic validity)

\- Semantic coherence with task context

\- Graceful degradation under uncertainty

\- Consistency across equivalent phrasings



Current evals focus on task completion but miss failure modes that matter for deployment. Need systematic measurement of these reliability dimensions across diverse tool ecosystems.



Thoughts on standardizing these metrics across research groups?",1,0.6,https://www.reddit.com/r/MachineLearning/comments/1mo1kci/d_reliability_metrics_and_failure_taxonomy_for/,False,True,False
1mo0ynr,NuoJohnChen,1754979051.0,52,/r/MachineLearning/comments/1mo0ynr/r_position_the_current_ai_conference_model_is/,MachineLearning,[R] Position: The Current AI Conference Model is Unsustainable!,"
Paper: https://www.alphaxiv.org/abs/2508.04586v1  


üìà Publication Surge: Per-author publication rates have more than doubled over the past decade to over 4.5 papers annually.  


üöÄ Exponential Output Growth: Individual contributions are rising so fast they‚Äôre projected to exceed one paper per month by the 2040s.  


üåç Carbon Overload: NeurIPS 2024‚Äôs travel emissions (>8,254 tCO‚ÇÇe) alone surpass Vancouver‚Äôs daily citywide footprint.  


üòû Mental Health Toll: Of 405 Reddit threads on AI conferences, over 71% are negative and 35% mention mental-health concerns.  


‚è≥ Research-Conference Mismatch: The AI research lifecycle outpaces conference schedules, often rendering results outdated before presentation.  


üèüÔ∏è Venue Capacity Crisis: Attendance at top AI conferences like NeurIPS 2024 is already outstripping available venue space.",396,0.92,https://www.reddit.com/gallery/1mo0ynr,False,False,False
1mnyque,ApprehensiveAd3311,1754971586.0,2,/r/MachineLearning/comments/1mnyque/r_gptoss_is_actuall_good_a_case_study_on_satabench/,MachineLearning,[R] gpt-oss is actuall good: a case study on SATA-Bench,"I‚Äôve been experimenting with gpt-oss since its release, and unlike many posts/news I‚Äôve seen, it‚Äôs surprisingly powerful ‚Äî even on uncommon datasets. I tested it on our recent benchmark SATA-Bench ‚Äî a benchmark where each question has at least two correct answers (rare in standard LLM Evaluation).

Results (See picture below):

1. 120B open-source model is similar to GPT-4.1's performance on SATA-Bench.
2. 20B model lags behind but still matches DeepSeek R1 & Llama-3.1-405B.

https://preview.redd.it/eowlge0jjiif1.jpg?width=1568&format=pjpg&auto=webp&s=bfc0fdc20fc1545000ff55cc45f3b65391e85c46

 takeaways:

Repetitive reasoning hurts ‚Äî 11% of 20B outputs loop, losing \~9 exact match rate.

Reason‚Äìanswer mismatches happen often in 20B and they tend to produce one answer even if their reason suggest a few answer is correct.

Longer ‚â† better ‚Äî overthinking reduces accuracy.

Detailed findings:¬†[https://weijiexu.com/posts/sata\_bench\_experiments.html](https://weijiexu.com/posts/sata_bench_experiments.html)

SATA-Bench dataset:¬†[https://huggingface.co/datasets/sata-bench/sata-bench](https://huggingface.co/datasets/sata-bench/sata-bench)",11,0.74,https://www.reddit.com/r/MachineLearning/comments/1mnyque/r_gptoss_is_actuall_good_a_case_study_on_satabench/,False,True,False
1mnx3vn,Healthy_Horse_2183,1754966720.0,24,/r/MachineLearning/comments/1mnx3vn/r_aaai_2026_reviewer_assignments/,MachineLearning,[R] AAAI 2026 Reviewer Assignments?,"Did anyone get assigned papers?

I submitted the biddings long time ago.",15,0.95,https://www.reddit.com/r/MachineLearning/comments/1mnx3vn/r_aaai_2026_reviewer_assignments/,False,True,False
1mnpqu7,Realistic-Bet-661,1754947601.0,23,/r/MachineLearning/comments/1mnpqu7/n_openai_delivers_goldmedal_performance_at_the/,MachineLearning,[N] OpenAI Delivers Gold-medal performance at the 2025 International Olympiad in Informatics,"[https://www.msn.com/en-xl/news/other/openai-scores-gold-in-one-of-the-world-s-top-programming-competitions/ar-AA1KknUL](https://www.msn.com/en-xl/news/other/openai-scores-gold-in-one-of-the-world-s-top-programming-competitions/ar-AA1KknUL)

>We officially entered the 2025 International Olympiad in Informatics (IOI) online competition track and adhered to the same restrictions as the human contestants, including submissions and time limits,",56,0.76,https://www.reddit.com/r/MachineLearning/comments/1mnpqu7/n_openai_delivers_goldmedal_performance_at_the/,False,True,False
1mnhr4o,PlugTheGreatest,1754929819.0,9,/r/MachineLearning/comments/1mnhr4o/drtp_and_noprop_hybrid_in_pure_c_r/,MachineLearning,DRTP and No-Prop Hybrid in Pure C [R],"Hey guys its me again I made a new algorithm with No Prop and DRTP that hit a 91.25% on MNIST with one hidden layer and I did it all in pure C here is the link to the repo I will be writing a paper on it please leave reviews and feedback I am a undergraduate student trying to get an internship for ML Research and or Engineering. First in the world from what I can see by the way.

[https://github.com/JaimeCasanovaCodes/DRTP-NOPROP-C](https://github.com/JaimeCasanovaCodes/DRTP-NOPROP-C)",0,0.43,https://www.reddit.com/r/MachineLearning/comments/1mnhr4o/drtp_and_noprop_hybrid_in_pure_c_r/,False,True,False
1mnfoum,Jealous-Leek-5428,1754925247.0,4,/r/MachineLearning/comments/1mnfoum/d_has_anyone_tried_crossmodal_transfer_for_visual/,MachineLearning,[D] Has anyone tried cross-modal transfer for visual reasoning? This 76% MMMU result surprised me,"I've been spending a lot of time lately evaluating different multimodal reasoning models for my research, and the gap between closed-source models like GPT-4.1 and open-source alternatives has been really frustrating. Most open models either can't handle complex visual reasoning or require massive compute resources.

Recently I came across Skywork-R1V3, a 38B parameter model that's been getting some attention in the community, so I decided to put it through its paces. What caught my eye initially was their claim of 76.0% accuracy on MMMU, which would put it competitive with much larger proprietary models.

After testing it extensively, I have to say the technical approach is really interesting. The model builds on InternVL-38B but what makes it special is how the Skywork team approached the reasoning problem. Instead of training visual reasoning from scratch, they found a way to transfer reasoning patterns from their existing text-based models into the multimodal domain.

From what I can tell from the paper and my experiments, they used reinforcement learning during post-training rather than just supervised fine-tuning. This seems to be key to why it performs so well on complex reasoning tasks. When I tested it on mathematical problems with diagrams and scientific figure interpretation, it consistently broke down problems into logical steps rather than just pattern matching.

The performance claims seem to hold up in my testing. It's genuinely competitive with closed-source alternatives on the types of visual reasoning tasks I care about, and the fact that it's fully open-source with quantized versions available makes it actually usable for research. I've been running the AWQ quantized version on a single A100 without issues.

What really impressed me is how well it handles cross-disciplinary reasoning where you need to connect visual information with abstract concepts. The chain-of-thought capabilities feel much more robust than other open models I've tried.

This connects to the broader Skywork ecosystem - their reward models have been downloaded over 750,000 times and seem to be helping multiple frontier models achieve strong benchmark results. There's clearly some solid technical work happening there.

I'm curious if others have experimented with cross-modal transfer approaches like this, or if anyone else has found effective ways to get strong reasoning performance without massive scale. Also interested in hearing thoughts on RL vs supervised approaches for this kind of multimodal reasoning - my sense is that RL might be underutilized in this space but I'd love to hear other perspectives.",60,0.89,https://www.reddit.com/r/MachineLearning/comments/1mnfoum/d_has_anyone_tried_crossmodal_transfer_for_visual/,False,True,False
1mn8vkj,Proper_Dig_6618,1754906487.0,5,/r/MachineLearning/comments/1mn8vkj/p_vulkanilm_accelerating_local_llm_inference_on/,MachineLearning,[P] VulkanIlm: Accelerating Local LLM Inference on Older GPUs Using Vulkan (Non-CUDA) ‚Äî Benchmarks Included,"Hi ML community,

I‚Äôm building **VulkanIlm**, a Python wrapper around llama.cpp leveraging Vulkan for GPU acceleration on legacy and AMD GPUs (no CUDA required). This opens the door to efficient local LLM use without expensive hardware.

Recent benchmark highlights:

* Dell E7250 integrated GPU (i7-5600U): 33√ó speedup on TinyLLaMA-1.1B chat model
* AMD RX 580 (8 GB): 4√ó speedup on Gemma-3n-E4B-it (6.9B params)

Inspired by Jeff Geerling‚Äôs blog on accelerating LLMs with eGPU setups on Raspberry Pi ([https://www.jeffgeerling.com/blog/2024/llms-accelerated-egpu-on-raspberry-pi-5](https://www.jeffgeerling.com/blog/2024/llms-accelerated-egpu-on-raspberry-pi-5)), I adapted and expanded it to run on AMD RX 580. A full how-to guide will come soon.

Repo here: [https://github.com/Talnz007/VulkanIlm](https://github.com/Talnz007/VulkanIlm)

Would love feedback or insights on Vulkan acceleration or similar efforts!",29,0.9,https://www.reddit.com/r/MachineLearning/comments/1mn8vkj/p_vulkanilm_accelerating_local_llm_inference_on/,False,True,False
1mn2j20,No-sleep-cuz-coffee,1754883176.0,6,/r/MachineLearning/comments/1mn2j20/d_beyond_finetuning_and_prompting_for_llms/,MachineLearning,[D] Beyond fine-tuning and prompting for LLMs?,"I‚Äôve been following a lot of recent LLM competitions and projects, and I‚Äôve noticed that most solutions seem to boil down to either fine-tuning a base model or crafting strong prompts. Even tasks that start out as ‚Äúgeneralization to unseen examples‚Äù ‚Äî like zero-shot classification ‚Äî often end up framed as prompting problems in practice.

From my reading, these two approaches (fine-tuning and prompting) cover a lot of the ground, but I‚Äôm curious if I‚Äôm missing something. Are there other practical strategies for leveraging LLMs that go beyond these? For example, some technique that meaningfully improve zero-shot performance without becoming ‚Äújust‚Äù a better prompt?

Would love to hear from practitioners who‚Äôve explored directions beyond the usual fine-tune/prompt spectrum.",8,0.78,https://www.reddit.com/r/MachineLearning/comments/1mn2j20/d_beyond_finetuning_and_prompting_for_llms/,False,True,False
1mn1tx2,PrimeMaester,1754881004.0,19,/r/MachineLearning/comments/1mn1tx2/d_which_direction_is_better_from_academia_to/,MachineLearning,"[D] Which direction is better: from academia to industry, or the other way around?","Hi all, given the current state of machine learning, I have two questions:

1. At what point in their career can a university lecturer/professor take on a joint position in industry?
2. Alternatively, can a R&D researcher in industry go back to academia without having to restart at the bottom of the ladder?

**Some context:** I am a PhD student on track to graduate in two months. I have several offers for applied/research scientist roles in industry, and interesting postdocs that could lead to a fulfilling academic career. I am not motivated by high salaries, and I know I want to do machine learning research forever! But the early-career academic job insecurity and the constant competitive grant writing I hear about are seriously concerning. At the same time, I know I can make a stronger/quicker practical impact in industry, despite the corporate constraints (work hours, less freedom, etc.). This is why I'm wondering if, in order to get the best of both worlds, one could start in academia and then transition into industry over time (or vice versa).

My question is more related to early-career researchers; I am aware that once tenure is achieved, pretty much anything is doable (e.g., Hinton, LeCun).

Thank you for sharing any insights, examples, or experiences on this :)",27,0.87,https://www.reddit.com/r/MachineLearning/comments/1mn1tx2/d_which_direction_is_better_from_academia_to/,False,True,False
1mmpies,ThatsSusG-O_o,1754849202.0,0,/r/MachineLearning/comments/1mmpies/validation_accuracy_for_fer_datasetp/,MachineLearning,Validation accuracy for FER+ dataset[P],"Hey, im working on a project which involves getting 85\~90% validation accuracy for the FER+ dataset but only using shallow neural networks. I have been trying to achieve this but im stuck around 70%. Any ideas on how to make it through?",1,1.0,https://www.reddit.com/r/MachineLearning/comments/1mmpies/validation_accuracy_for_fer_datasetp/,False,True,False
1mmozdl,zunairzafar,1754847995.0,2,/r/MachineLearning/comments/1mmozdl/d_usecase_of_distribution_analysis_of_numeric/,MachineLearning,[D] Use-case of distribution analysis of numeric features,"Hey! I hope you guys are all doing well. So, I've been deep into the statistics required in M.L. specifically. I just came to understand a few topics like

‚Ä¢Confidence Intervals
‚Ä¢Uniform/Normal distrinutions
‚Ä¢Hypothesis testing etc

So, these topics are quite interesting and help you analyze the numerical feature in the dataset. But here's the catch. I am still  unable to understand the actual practical use in the modeling. For example, I have a numeric feature of prices and for example it doesn't follow the normal distribution and data is skewed so I'll apply the central limit theorem(CLT) and convert the data into normal distribution. But what's the actual use-case? I have changed the actual values in the dataset as I've chosen random samples from the dataset while applying CLT and randomization will actually change the input feature right? So, what is the use-case of normal distribution? And same goes for the rest of the topics like confidence interval. How do we practically use these concepts in M.L.?


Thanks",0,0.33,https://www.reddit.com/r/MachineLearning/comments/1mmozdl/d_usecase_of_distribution_analysis_of_numeric/,False,True,False
1mmj7hb,Limp_Food9236,1754834126.0,5,/r/MachineLearning/comments/1mmj7hb/d_are_there_any_papers_on_using_reasoning_models/,MachineLearning,[D] Are there any papers on using reasoning models in embodied AI?,"I've been looking through papers that use LLMs for robotic control (e.g. SayCan, SayPlan etc.). Are there any papers that use reasoning models like DeepSeek R1 or o3 that do well on benchmarks?",1,0.55,https://www.reddit.com/r/MachineLearning/comments/1mmj7hb/d_are_there_any_papers_on_using_reasoning_models/,False,True,False
1mmj49j,Kiwis-Truths,1754833894.0,5,/r/MachineLearning/comments/1mmj49j/d_why_is_scene_edit_detection_still_not_at_or/,MachineLearning,[D] Why is scene edit detection still not at or near 100% accuracy?,"To be clear I understand nothing about the inner workings of the tool (I have a CS degree and no ML/AI background), but I've been in search of a near 100% accurate tool and can't find one.

First q, why (If you can explain like I'm a 5th grader that'd be awesome)? Genuinely curious to understand. Second q, would it be a waste of time for me to try to tackle this problem by myself (I have a lot of time on my hands lately)?

I unexpectedly got very curious and have a strong itch to at least¬†*try*¬†solving it, but I have no background nor any understanding of how hard such a problem would be or if it's ""worth"" trying to solve - whatever worth means.

Any insights are appreciated. Thanks :)",0,0.25,https://www.reddit.com/r/MachineLearning/comments/1mmj49j/d_why_is_scene_edit_detection_still_not_at_or/,False,True,False
1mmi6c5,seraschka,1754831393.0,16,/r/MachineLearning/comments/1mmi6c5/p_from_gpt2_to_gptoss_analyzing_the_architectural/,MachineLearning,[P] From GPT-2 to gpt-oss: Analyzing the Architectural Advances And How They Stack Up Against Qwen3,,99,0.98,https://sebastianraschka.com/blog/2025/from-gpt-2-to-gpt-oss.html,False,False,False
1mmd4wk,tfburns,1754814054.0,1,/r/MachineLearning/comments/1mmd4wk/r_associative_memory_inspires_improvements_for/,MachineLearning,[R] Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture,"**Contributions:**

1. **AMICL** (Associative Memory for In-Context Learning) algorithm that works in three steps:

* Identify incomplete patterns in the input
* Search context for similar, complete patterns
* Complete the pattern using the best contextual match

This achieves near-perfect performance on classification tasks.

2. Inspired by AMICL, we introduce ""**residual attention streams**"" -- direct connections between attention head values across layers. This creates information flow pathways that better retain prior context.

**Results:**

* **24% faster convergence** to 95% accuracy in two-layer Transformers on toy tasks
* **6-fold improvement** on Indirect Object Identification tasks (from \~7% to \~41% accuracy) in an 8M parameter model trained on TinyStories
* Also showed (general) improvements on **1B parameter models**

**Architecture details:**

Three variants were tested (residual streams for queries, keys, and values) and we found that the **values stream performed best**. This aligns with the AMICL model, where values directly retain input information.

The key insight is that this approach enhances in-context learning efficiency and robustness **without increasing parameter count** \- making it a computationally efficient improvement.

From a safety perspective, this enhanced in-context learning ability means AI systems can more reliably understand and follow instructions from context rather than falling back on potentially problematic patterns from training data. This work suggests that by looking to biology for inspiration, we can build AI systems that are not just more powerful and efficient, but also more trustworthy and controllable.

**Biological connections:**

It is possible to draw parallels to biological memory systems. The hippocampus has selective skip connections (direct CA3 to CA1 pathways plus indirect routes through CA2), where CA2 specialises in context-switching. This may serve similar computational functions to AMICL and the architectural modifications introduced here.

**Possible future directions:**

* Parameterised residual streams inspired by gamma-models
* Alternative attention head connection patterns
* Scaling to larger architectures
* Applications beyond NLP

**Links:**

* Paper: [https://arxiv.org/abs/2412.15113](https://arxiv.org/abs/2412.15113)
* Code: [https://github.com/tfburns/AMICL-and-residual-attention-streams](https://github.com/tfburns/AMICL-and-residual-attention-streams)

**TL;DR:**

New research shows that adding ""residual attention streams"" (direct connections between attention head values across layers) to Transformers can improve in-context learning performance while requiring no additional parameters. The approach is inspired by associative memory and has interesting parallels to hippocampal circuit architecture.",10,0.86,https://www.reddit.com/r/MachineLearning/comments/1mmd4wk/r_associative_memory_inspires_improvements_for/,False,True,False
1mmc4fm,ade17_in,1754810112.0,3,/r/MachineLearning/comments/1mmc4fm/any_way_to_visualise_gradcamlike_attention_for/,MachineLearning,"Any way to visualise 'Grad-CAM'-like attention for multimodal LLMs (gpt, etc.) [P]","Do anyone have ever worked on getting heatmap-like maps on what ""model sees"" using multimodal LLMs, ofcourse it must be any open-source. Any examples? Would approaches like attention rollout, attention√ógradient, or integrated gradients on the vision encoder be suitable?",9,0.91,https://www.reddit.com/r/MachineLearning/comments/1mmc4fm/any_way_to_visualise_gradcamlike_attention_for/,False,True,False
1mmbyxp,ade17_in,1754809533.0,33,/r/MachineLearning/comments/1mmbyxp/phds_who_publish_how_do_you_get_more_out_of_your/,MachineLearning,PhDs who publish - how do you get more out of your time [D],"A little background - I'm starting my much anticipated PhD soon. It is limited to 3 years. Took some voluntary teaching duties. My ultimate target before I finish my PhD is to get really good papers out (also should a good number), build a really strong network and have excellent interpersonal skills. 

I've a question to all PhD/research you get good papers out regularly, 1-2+ first authors at good/decent conferences each year- how do you manage to do that? Did you slice up your study into mulitple publications or just really good with intuition about a method?

But often isn't it difficult to manage other duites, collaborations and also go through the arbitrary review process. I would like to know more about any experience of yours and what can you suggest someone starting out.

Edit: changed it to 1-2+ publications each year",89,0.87,https://www.reddit.com/r/MachineLearning/comments/1mmbyxp/phds_who_publish_how_do_you_get_more_out_of_your/,False,True,False
1mmbx64,kidfromtheast,1754809353.0,5,/r/MachineLearning/comments/1mmbx64/d_how_gptoss20b_can_load_in_a_gpu_with_only_16_gb/,MachineLearning,[D] how gpt-oss-20b can load in a GPU with only 16 GB of VRAM?,"I haven't tried to run it yet on PyTorch, but I don't see how we can load 20B parameters with 2 bytes per parameter (torch.bloat16) in a GPU with only 16GB of VRAM

I was assuming that for every forward pass, it will move the experts weights to the GPU. Although as much as I cannot believe that because it is not efficient, I was tempted to the theory because 20B \* 2 bytes (torch.bfloat16) / (1024 byte->kilobyte / 1024 kilboyte->megabyte / 1024 megabyte->gigabyte) \\approx 39,1 GB of VRAM, just to load the model

Is this because of quantization using MXFP4?

How on earth gpt-oss-20b with 4-bit quantization can have on par performance with DeepSeek R1 (671B)?

[model.py](https://github.com/openai/gpt-oss/blob/c77966fc0fda390b0abeeecdec7134433fe9f224/gpt_oss/torch/model.py)

[weights.py](https://github.com/openai/gpt-oss/blob/c77966fc0fda390b0abeeecdec7134433fe9f224/gpt_oss/torch/weights.py)

[llm-stats.com](https://llm-stats.com/)

Edit: README says it all

\> [`torch`](https://github.com/openai/gpt-oss/blob/main/README.md#reference-pytorch-implementation)¬†‚Äî¬†a non-optimized¬†[PyTorch](https://pytorch.org/)¬†implementation for educational purposes only. Requires at least 4√ó H100 GPUs due to lack of optimization.

[README.md](https://github.com/openai/gpt-oss/blob/c77966fc0fda390b0abeeecdec7134433fe9f224/README.md)",6,0.65,https://www.reddit.com/r/MachineLearning/comments/1mmbx64/d_how_gptoss20b_can_load_in_a_gpu_with_only_16_gb/,False,True,False
1mm2jdc,Powerful-Angel-301,1754779550.0,1,/r/MachineLearning/comments/1mm2jdc/d_open_source_speech_to_speech_voice_agent_model/,MachineLearning,[D] open source speech to speech (Voice Agent) model?,"Is there an open source speech to speech (Voice Agent) model, like Amazon Nova Sonic?",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1mm2jdc/d_open_source_speech_to_speech_voice_agent_model/,False,True,False
1mlykbr,Altruistic-Front1745,1754769189.0,0,/r/MachineLearning/comments/1mlykbr/dhelp_running_idmvton_virtual_tryon_locally_or_on/,MachineLearning,[D]Help running IDM-VTON (virtual try-on) locally or on Colab ‚Äì hitting memory issues and need alternatives,"Hi everyone,

I‚Äôm trying to run this project from GitHub: [https://github.com/yisol/IDM-VTON](https://github.com/yisol/IDM-VTON)  
My goal is to study how it works and understand how clothes adapt so realistically to different bodies.

Here‚Äôs what I‚Äôve tried so far:

* Followed the README exactly on my laptop (no GPU) ‚Üí not usable because of hardware limits.
* Cloned it to Google Colab ‚Üí initially had dependency issues, solved them with Miniconda in Colab.
* Now, when running `gradio_demo/app.py`, the process gets **Killed** (out-of-memory).

please Suggestions for running this project without a local GPU.

Any tricks for optimizing memory usage in Colab.

Alternative tools or platforms?

I‚Äôm fine with paid or free solutions as long as they let me test and understand the code.

Has anyone here successfully run IDM-VTON or a similar Stable Diffusion-based try-on model without a powerful GPU?

All I want is to be able to run this project, test it, play with the code, and see the results. If you know of any alternative or platform adapted to my problem, I would greatly appreciate it.",2,1.0,https://www.reddit.com/r/MachineLearning/comments/1mlykbr/dhelp_running_idmvton_virtual_tryon_locally_or_on/,False,True,False
1mlw1id,HelenOlivas,1754762842.0,4,/r/MachineLearning/comments/1mlw1id/d_what_would_a_measurable_test_for_minimal_ai/,MachineLearning,[D] What would a measurable test for minimal AI welfare look like?,"I‚Äôm collecting operational criteria (not metaphysics): cross-session behavioral consistency, stable self-reports under blinded probes, reproducible third-party protocols. Looking for papers, metrics, or eval harnesses you‚Äôd use to *falsify* these.",0,0.07,https://www.reddit.com/r/MachineLearning/comments/1mlw1id/d_what_would_a_measurable_test_for_minimal_ai/,False,True,False
1mlvl7l,mert_jh,1754761747.0,9,/r/MachineLearning/comments/1mlvl7l/p_i_used_yolov12_and_gemini_to_extract_and_tag/,MachineLearning,"[P] I used YOLOv12 and Gemini to extract and tag over 100,000 scientific plots.","For anyone who works in research, the process of designing effective data visualizations can be a significant bottleneck. I often found myself searching through numerous papers just to find inspiration for layouts and plot types, which was inefficient.

To solve this problem for myself and others, I developed [**Plottie.art**](http://Plottie.art), a searchable, browser-based library of over 100,000 plots curated from scientific literature.

I'm sharing it here because the machine learning pipeline behind it combines a specialized computer vision model with an LLM in a way that I thought this community would find interesting.

**The ML Pipeline**

The process starts with a large collection of figure images sourced from open-access papers. The goal is to make each individual plot within these figures searchable.

**1. Subplot Segmentation with a Custom YOLOv12 Model**

A key challenge is that many figures are multi-panel, containing several distinct subplots within a single image.

* **Model Training:** To address this, I trained a custom **YOLOv12 model**. This required **manually annotating a dataset of 1,000 images** to teach the model to accurately identify and isolate the boundaries of individual subplots and their captions.
* **Function:** The model processes each source image and outputs bounding boxes for each subplot, effectively segmenting complex figures into their constituent parts.

**2. Plot Classification and Keyword Extraction with Gemini**

With the subplots isolated, the next step was to classify each image by plot type (e.g., heatmap, UMAP) and extract relevant keywords for search.

* **Approach:** While I considered training another dedicated classification model, the data collection and labeling requirements would have been substantial. I opted for a more efficient approach using a large multimodal model.
* **Implementation:** I utilized the **Google Gemini API**. By providing a subplot image, I could prompt the model to perform both classification and keyword extraction. A prompt structured like, `""Analyze this scientific plot. Identify its specific type and extract key terms from its labels and content.""` proved to be highly effective.
* **Outcome:** This method was not only fast to implement but also yielded high-quality, structured metadata. It successfully bypassed the need for a separate, time-intensive training pipeline for classification.

This two-stage pipeline allows the content on[**Plottie.art**](https://plottie.art)to be easily searched and explored. The tool is free, requires no login, and runs in the browser.

I would be very interested to hear your feedback on the project and the technical stack. I'm especially curious about any thoughts on combining specialized vision models with general-purpose LLMs for this type of application, or suggestions for improving the pipeline.

",49,0.94,https://www.reddit.com/r/MachineLearning/comments/1mlvl7l/p_i_used_yolov12_and_gemini_to_extract_and_tag/,False,True,False
1mlvc6l,cosurgi,1754761142.0,0,/r/MachineLearning/comments/1mlvc6l/r_a_quick_question_to_mathematica_llm_users/,MachineLearning,[R] A quick question to Mathematica + LLM users,"Hi everyone, I am wondering if it‚Äôs worth to buy the [Mathematica + LLM in notebook](https://writings.stephenwolfram.com/2023/06/introducing-chat-notebooks-integrating-llms-into-the-notebook-paradigm/) so it would be great if anyone who has it could paste this [question](https://pastebin.com/aynsiWrc) into the mathematica LLM. I‚Äôve put it on pastebin, because reddit will mess up the string with its own formatting. But if you do not wish to click I paste it here, but the ^ will mess up, so use the pastebin to paste it into LLM:

> Let V be a vector field on an affine space A generating a flow \phi, let \Psi:A->A be any smooth invertible map with smooth inverse, and let \Phi(t,x)=\Psi(\phi(t,\Psi^{-1}(x))). Show that \Phi is also a flow on A, and that its generator V^\Psi is given by V^\Psi_x=\Psi_*(V_{\Psi^{-1}(x)}).

It‚Äôs a kind of problem which can be done with pen & paper and I am not sure if mathematica is useful here.

Would be great if someone can post a screenshot of the answer from mathematica. I am trying to figure out if these types of problems are applicable to mathematica + LLM.

The problem is from book by Crampin, Pirani ‚ÄúApplicable Differential Geometry‚Äù, 1987, page 64 Exercise 28.

So far I used the Bing LLM for it, and it gave the correct answer. Including the derivations, calculations and simplifications of the formulas.",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1mlvc6l/r_a_quick_question_to_mathematica_llm_users/,False,True,False
1mlv4mk,sleepshiteat,1754760615.0,8,/r/MachineLearning/comments/1mlv4mk/d_gpt5_is_pretty_bad_with_information_extraction/,MachineLearning,[D] GPT5 is pretty bad with information extraction tasks,,52,0.86,https://i.redd.it/sje3n07841if1.jpeg,False,False,False
1mltcdn,tedd235,1754756174.0,16,/r/MachineLearning/comments/1mltcdn/d_what_happens_if_reviewers_dont_fill_out_the/,MachineLearning,[D] What happens if reviewers don't fill out the mandatory acknowledgement in NeurIPS 2025?,2 of my reviewers completely ghosted the discussion period. Wondering what happens next?,16,0.84,https://www.reddit.com/r/MachineLearning/comments/1mltcdn/d_what_happens_if_reviewers_dont_fill_out_the/,False,True,False
1mln24c,Mocha4040,1754738573.0,123,/r/MachineLearning/comments/1mln24c/d_how_do_researchers_actually_write_code/,MachineLearning,[D] How do researchers ACTUALLY write code?,"Hello. I'm trying to advance my machine learning knowledge and do some experiments on my own.  
Now, this is pretty difficult, and it's not because of lack of datasets or base models or GPUs.  
It's mostly because I haven't got a clue how to write structured pytorch code and debug/test it while doing it. From what I've seen online from others, a lot of pytorch ""debugging"" is good old python print statements.  
My workflow is the following: have an idea -> check if there is simple hugging face workflow -> docs have changed and/or are incomprehensible how to alter it to my needs -> write simple pytorch model -> get simple data from a dataset -> tokenization fails, let's try again -> size mismatch somewhere, wonder why -> nan values everywhere in training, hmm -> I know, let's ask chatgpt if it can find any obvious mistake -> chatgpt tells me I will revolutionize ai, writes code that doesn't run -> let's ask claude -> claude rewrites the whole thing to do something else, 500 lines of code, they don't run obviously -> ok, print statements it is -> cuda out of memory -> have a drink.  
Honestly, I would love to see some good resources on how to actually write good pytorch code and get somewhere with it, or some good debugging tools for the process. I'm not talking about tensorboard and w&b panels, there are for finetuning your training, and that requires training to actually work.

  
Edit:  
There are some great tool recommendations in the comments. I hope people comment even more tools that already exist but also tools they wished to exist. I'm sure there are people willing to build the shovels instead of the gold...",161,0.94,https://www.reddit.com/r/MachineLearning/comments/1mln24c/d_how_do_researchers_actually_write_code/,False,True,False
1mljnxa,casualcreak,1754725070.0,30,/r/MachineLearning/comments/1mljnxa/d_neurips_2025_being_hosted_at_3_locations/,MachineLearning,[D] Neurips 2025 being hosted at 3 locations.,Neurips 2025 is being hosted at three different locations this time around: 1) San Diego; 2) Mexico City; 3) Copenhagen. What is your opinion on this?,57,0.9,https://www.reddit.com/r/MachineLearning/comments/1mljnxa/d_neurips_2025_being_hosted_at_3_locations/,False,True,False
1mlfgvb,flyforlight,1754710354.0,2,/r/MachineLearning/comments/1mlfgvb/p_we_just_opensourced_the_first_fullstack_deep/,MachineLearning,[P] We just open-sourced the first full-stack Deep Research: agent + model + data + training‚Äîreproducible GAIA 82.4,"https://i.redd.it/b9goy7brywhf1.gif



We‚Äôre releasing **MiroMind Open Deep Research (ODR) v0.1**, which we believe is the **first** ***full-stack*****, fully open-source deep research project**‚Äînot just an agent, but also the **model, dataset, and training/RL infra** are open and reproducible. The agent framework (**MiroFlow**) reproduces **82.4** on **GAIA validation**; the model series (**MiroThinker**) reaches **60.2%** on **GAIA-Text-103**. Looking for contributors + repro logs.

# Why this matters

* **Full-stack openness**: most deep-research releases stop at the agent; ODR opens **all four layers**: **Agent (MiroFlow)**, **Model (MiroThinker)**, **Data (MiroVerse)**, **Training/RL (MiroTrain / MiroRL)**. 
* **Reproducible numbers**: ‚Ä¢ **MiroFlow**: GAIA validation **maj. vote 82.4**, pass@1 avg@3 **72.2** (with setup details & scripts). ‚Ä¢ **MiroThinker v0.1**: **60.2%** on **GAIA-Text-103** (with both SFT & DPO variants across 8B/14B/32B).
* **Open data at scale**: **MiroVerse v0.1**‚Äî**147k+** full rollout trajectories (**\~1.9B tokens, 602k+ tool calls**), built for tool-use/web-browsing agents.

# What‚Äôs included

* **MiroFlow (Agent framework)** ‚Äì multi-tool, sub-agent orchestration, MCP integration, benchmarking UI; detailed GAIA runs & scripts.
* **MiroThinker (Model series)** ‚Äì agentic LLMs optimized for deep research; SFT/DPO at 8B/14B/32B with evaluation guides.
* **MiroVerse (Dataset)** ‚Äì 147k+ verified trajectories across multi-hop QA, browsing, scientific reasoning; hybrid licensing noted on card.
* **MiroTrain / MiroRL (Training & RL)** ‚Äì end-to-end post-training + MCP-first RL for tool-using agents.

# Quick start (agent eval)

1. **MiroFlow**: clone, set keys (OpenRouter/Anthropic/OpenAI/Gemini, Serper, Jina, E2B), optional E2B Docker sandbox for stable repro; run GAIA scripts.
2. **MiroThinker**: pull model from HF or self-host via SGLang; run GAIA-Validation / GAIA-Text-103 / HLE / WebWalkerQA scripts.

# Links

* **Overview blog (tables & results)**: [miromind.ai/blog/miromind-open-deep-research](http://miromind.ai/blog/miromind-open-deep-research) [MiroMind](https://miromind.ai/blog/miromind-open-deep-research)
* **Agent**: [GitHub.com/MiroMindAI/MiroFlow](http://GitHub.com/MiroMindAI/MiroFlow) [GitHub](https://github.com/MiroMindAI/MiroFlow)
* **Models**: [GitHub.com/MiroMindAI/MiroThinker](http://GitHub.com/MiroMindAI/MiroThinker) & HF collection [GitHub](https://github.com/MiroMindAI/MiroThinker)[Hugging Face](https://huggingface.co/collections/miromind-ai/mirothinker-v01-689301b6d0563321862d44a1)
* **Dataset**: HF ‚Äî miromind-ai/MiroVerse-v0.1 [Hugging Face](https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1)
* **Training/RL**: [GitHub.com/MiroMindAI/MiroTrain](http://GitHub.com/MiroMindAI/MiroTrain) & /MiroRL [GitHub+1](https://github.com/MiroMindAI/MiroTrain)

# ",26,0.83,https://www.reddit.com/r/MachineLearning/comments/1mlfgvb/p_we_just_opensourced_the_first_fullstack_deep/,False,True,False
1mldqbb,asankhs,1754705108.0,7,/r/MachineLearning/comments/1mldqbb/r_adaptive_classifiers_fewshot_learning_with/,MachineLearning,[R] Adaptive Classifiers: Few-Shot Learning with Continuous Adaptation and Dynamic Class Addition,"**Paper/Blog**: [https://huggingface.co/blog/codelion/adaptive-classifier](https://huggingface.co/blog/codelion/adaptive-classifier)  
**Code**: [https://github.com/codelion/adaptive-classifier](https://github.com/codelion/adaptive-classifier)  
**Models**: [https://huggingface.co/adaptive-classifier](https://huggingface.co/adaptive-classifier)

# TL;DR

We developed an architecture that enables text classifiers to:

* Learn from as few as 5-10 examples per class (few-shot)
* Continuously adapt to new examples without catastrophic forgetting
* Dynamically add new classes without retraining
* Achieve 90-100% accuracy on enterprise tasks with minimal data

# Technical Contribution

**The Problem**: Traditional fine-tuning requires extensive labeled data and full retraining for new classes. Current few-shot approaches don't support continuous learning or dynamic class addition.

**Our Solution**: Combines prototype learning with elastic weight consolidation in a unified architecture:

    ModernBERT Encoder ‚Üí Adaptive Neural Head ‚Üí Prototype Memory (FAISS)
                                        ‚Üì
                                EWC Regularization
    

**Key Components**:

1. **Prototype Memory**: FAISS-backed storage of learned class representations
2. **Adaptive Neural Head**: Trainable layer that grows with new classes
3. **EWC Protection**: Prevents forgetting when learning new examples
4. **Dynamic Architecture**: Seamlessly handles new classes without architectural changes

# Experimental Results

Evaluated on 17 diverse text classification tasks with only 100 examples per class:

**Standout Results**:

* Fraud Detection: 100% accuracy
* Document Classification: 97.5% accuracy
* Support Ticket Routing: 96.8% accuracy
* **Average across all tasks**: 93.2% accuracy

**Few-Shot Performance**:

* 5 examples/class: \~85% accuracy
* 10 examples/class: \~90% accuracy
* 100 examples/class: \~93% accuracy

**Continuous Learning**: No accuracy degradation after learning 10+ new classes sequentially (vs 15-20% drop with naive fine-tuning).

# Novel Aspects

1. **True Few-Shot Learning**: Unlike prompt-based methods, learns actual task-specific representations
2. **Catastrophic Forgetting Resistance**: EWC ensures old knowledge is preserved
3. **Dynamic Class Addition**: Architecture grows seamlessly - no predefined class limits
4. **Memory Efficiency**: Constant memory footprint regardless of training data size
5. **Fast Inference**: 90-120ms (comparable to fine-tuned BERT, faster than LLM APIs)

# Comparison with Existing Approaches

|Method|Training Examples|New Classes|Forgetting|Inference Speed|
|:-|:-|:-|:-|:-|
|Fine-tuned BERT|1000+|Retrain all|High|Fast|
|Prompt Engineering|0-5|Dynamic|None|Slow (API)|
|Meta-Learning|100+|Limited|Medium|Fast|
|**Ours**|**5-100**|**Dynamic**|**Minimal**|**Fast**|

# Implementation Details

Based on ModernBERT for computational efficiency. The prototype memory uses cosine similarity for class prediction, while EWC selectively protects important weights during updates.

**Training Objective**:

    L = L_classification + Œª_ewc * L_ewc + Œª_prototype * L_prototype
    

Where L\_ewc prevents forgetting and L\_prototype maintains class separation in embedding space.

# Broader Impact

This work addresses a critical gap in practical ML deployment where labeled data is scarce but requirements evolve rapidly. The approach is particularly relevant for:

* Domain adaptation scenarios
* Real-time learning systems
* Resource-constrained environments
* Evolving classification taxonomies

# Future Work

* Multi-modal extensions (text + vision)
* Theoretical analysis of forgetting bounds
* Scaling to 1000+ classes
* Integration with foundation model architectures

The complete technical details, experimental setup, and ablation studies are available in our blog post. We've also released 17 pre-trained models covering common enterprise use cases.

**Questions welcome!** Happy to discuss the technical details, experimental choices, or potential extensions.",21,0.86,https://www.reddit.com/r/MachineLearning/comments/1mldqbb/r_adaptive_classifiers_fewshot_learning_with/,False,True,False
1ml76ip,_crazy_muffin_,1754687425.0,58,/r/MachineLearning/comments/1ml76ip/d_what_ai_engineers_do_in_top_companies/,MachineLearning,[D] - What AI Engineers do in top companies?,"Joined a company few days back for AI role. Here there is no work related to AI, it's completely software engineering with monitoring work. 

When I read about AI engineers getting huge amount of salary, companies try to poach them by giving them millions of dollars I get curious to know what they do differently.

Feel free to answer.",151,0.91,https://www.reddit.com/r/MachineLearning/comments/1ml76ip/d_what_ai_engineers_do_in_top_companies/,False,True,False
1ml5rzm,pythonprogrammer64,1754684055.0,3,/r/MachineLearning/comments/1ml5rzm/dpapers_on_graph_neural_networks/,MachineLearning,[D]papers on graph neural networks,What are the 10 most impactful ml papers on graph neural networks,0,0.23,https://www.reddit.com/r/MachineLearning/comments/1ml5rzm/dpapers_on_graph_neural_networks/,False,True,False
1ml2nzq,NoTap8152,1754676834.0,1,/r/MachineLearning/comments/1ml2nzq/managing_gpu_jobs_across_coreweavelambdarunpod_is/,MachineLearning,"Managing GPU jobs across CoreWeave/Lambda/RunPod is a mess, so im building a simple dashboard[P]","If you‚Äôve ever trained models across different GPU cloud providers, you know how painful it is to:

* Track jobs across platforms
* Keep an eye on GPU hours and costs
* See logs/errors without digging through multiple UIs

I‚Äôm building a super simple ‚ÄúStripe for supercomputers‚Äù style dashboard (fake data for now), but the idea is:

* Clean job cards with cost, usage, status
* Logs and error previews in one place
* Eventually, start jobs from the dashboard via APIs

If you rent GPUs regularly, would this save you time?  
What‚Äôs missing for you to actually use it?",9,0.85,https://www.reddit.com/r/MachineLearning/comments/1ml2nzq/managing_gpu_jobs_across_coreweavelambdarunpod_is/,False,True,False
1mkyrrw,Street_Car_1297,1754668049.0,0,/r/MachineLearning/comments/1mkyrrw/p_explaining_gnn_predictions_on_linear_dfgs_gnn/,MachineLearning,"[P] Explaining GNN Predictions on """"linear"""" DFGs - GNN experts I need your help <3","I‚Äôm working on a research project where, starting from an event log, I build for each trace a¬†Direct Follows Graph (DFG)¬†representing that trace, where each node corresponds to an activity.

My goals are:

1. From the obtained DFGs, derive¬†Prefix graphs¬†(i.e., DFGs with the final nodes removed) and apply a GNN for¬†**next activity prediction at the node level**. This way, if I feed the model a list of activities during inference, it should return the next activity.
2. Given the prediction, I want to apply¬†**GNN explainability techniques**, specifically¬†*Perturbation-based methods*and¬†*Surrogate-based methods*, to explain the model‚Äôs decision.

My question is mainly about point 2: since the DFGs are mostly¬†linear¬†(with at most some self-loops or a few normal loops), does it make sense to search for¬†subgraphs¬†that explain the result (e.g., with GNNExplainer or SubgraphX)? For example, if I use a 3-layer GNN, wouldn‚Äôt the prediction already be fully explained by the¬†3-hop neighborhood?  
These are not very large graphs with huge numbers of edges... maybe I‚Äôm missing something.

P.S.: I‚Äôm new in the world of GNNs.",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1mkyrrw/p_explaining_gnn_predictions_on_linear_dfgs_gnn/,False,True,False
1mkxewf,Ttghtg,1754664945.0,10,/r/MachineLearning/comments/1mkxewf/d_looking_for_convexconstrained_ml_problems_for/,MachineLearning,[D] Looking for convex-constrained ML problems for benchmarks,"Hello,
  
I am looking for Machine Learning (ML) use cases to try out a class of optimization algorithms, namely Frank Wolfe (FW) algorithms. Those are *gradient-based* and *projection-free* algorithms for optimizing a cost function (convex or non-convex) over a *convex* set of constraints. Usually, such problems are tackled by Projected Gradient Descent (PGD), where each iteration consists of a descent in the direction of the gradient, then a projection onto the set of constraints to ensure that the new solution is feasible. However, depending on the set of constraints, this projection step can be very costly and thus prohibitive. FW algorithms avoid this projection step, which leads to less compute-intensive iterations.
  
I am turning toward r/machinelearning communities for ideas of problems that satisfy those conditions: optimization over a convex set of constraints (original or relaxed version of a problem), ideally that can be large-scale so I can push the FW algorithms to their limits.

For the moment, I found those following problems:

  * **Adversarial attack** : modifying an image in a imperceptible way for a human so that a classifier misclassifies it. The modification ùõø can be constrained in the ùúÄ-ball so that it remains small, which is a convex set so it fits the description.

  * **Polynomial Regression**/**Compressed Sensing**: when we need a sparse represention, we can set the constraint that the coefficients live in the L1-norm ball that is sparsity-inducing.

  * **Matrix Completion**: not the original formulation that constrain that the rank of the matrix *X* denoted rank(*X*) is low, but setting a constraint of the nuclear-norm value of the matrix *X*, which is a convex constraint.

I am also looking for optimization over the set of Doubly Stochastic Matrices (also called the Birkhoff polytope, which is the convex hull of permutation matrices), but I've been looking for a few hours on Google and I haven't found any concrete application, so if you have any ideas I will gladly take them. I've heard that they are useful in matching/assignment problems.

Thanks for reading",8,1.0,https://www.reddit.com/r/MachineLearning/comments/1mkxewf/d_looking_for_convexconstrained_ml_problems_for/,False,True,False
1mkw2z1,darkageofme,1754661792.0,11,/r/MachineLearning/comments/1mkw2z1/r_live_coding_benchmark_gpt5_claude_sonnet_4/,MachineLearning,"[R] Live coding benchmark: GPT-5, Claude Sonnet 4, Gemini 2.5 Pro, GLM45 ‚Äî same prompt, varying difficulty","We‚Äôre running a live comparative test today to see how four leading LLMs handle coding tasks in a natural-language coding environment.

**Models tested:**

* GPT-5
* Claude Sonnet 4
* Gemini 2.5 Pro
* GLM45 (open-source)

**Format:**

* All models receive **the exact same prompt**
* Multiple runs at different complexity levels:
   * Simple builds
   * Bug-fix tasks
   * Multi-step complex builds
   * Possible planning flows

We‚Äôll compare:

* Output quality
* Build speed
* Debugging performance

**When:** Today, 16:00 UTC (19:00 EEST)

**Where:** [https://live.biela.dev](https://live.biela.dev) 

Hop in with questions, curiosities, prompt suggestions and whatever comes in mind to make the test even better! :)",0,0.43,https://www.reddit.com/r/MachineLearning/comments/1mkw2z1/r_live_coding_benchmark_gpt5_claude_sonnet_4/,False,True,False
1mkr9wy,Horror_Job_566,1754647800.0,4,/r/MachineLearning/comments/1mkr9wy/d_looking_for_ideas_for_a_ml_initiative/,MachineLearning,[D] Looking for ideas for a ML initiative,"Hi all,

My goal is to launch a small ML initiative/lab that:

* Focus on non-mainstream but high-impact ML research areas.
* Work on project-driven open-source contributions and papers from day one
* Build a network and reputation through real, tangible outputs rather than just theory or coursework

I want this to be lean and agile, not a formal institution, but a focused group of people (starting small) who want to push boundaries and build a reputation in underexplored domains.

**What I‚Äôm looking for:**

* Suggestions on promising underexplored ML fields or projects with potential real-world impact
* Advice on structuring such a lab efficiently (collaboration tools, workflow, open-source best practices)
* Potential collaborators interested in contributing to projects with measurable outputs
* Any pitfalls to watch out for in early-stage lab building

**Conditions I‚Äôm considering:**

1. Projects must be open-source and reproducible.
2. Research and code contributions should aim for quality over quantity.
3. Members commit to regular updates and active communication.
4. We focus on non-mainstream areas to avoid crowded research spaces.
5. All contributions must align with ethical standards.
6. Aim for publishable or demonstrable outcomes, no just ‚Äúexploratory‚Äù hacks.
7. Small core team at first (3-5 people max) to stay agile.
8. Clear documentation and modular code required from day one.

Would appreciate any concrete ideas or feedback. Also open to recommendations on platforms or tools that could help us run this smoothly.",0,0.18,https://www.reddit.com/r/MachineLearning/comments/1mkr9wy/d_looking_for_ideas_for_a_ml_initiative/,False,True,False
1mkqbkh,Careless-Top-2411,1754644112.0,67,/r/MachineLearning/comments/1mkqbkh/d_neurips_rebuttal_score_change/,MachineLearning,[D] Neurips rebuttal score change,"

It's just my feeling, but from what I see, the post rebuttal score this year maybe higher than previous year. Can everyone share how the score change so far for the paper that you review? 

In my case, I know 9 paper reviewed by me and my friend, 4 get their score increase (1 increases by 1, the rest a lot more), 1 withdraw, 1 likely to decrease by 1, the rest didn't change",26,0.81,https://www.reddit.com/r/MachineLearning/comments/1mkqbkh/d_neurips_rebuttal_score_change/,False,True,False
1mkny59,southern_brownie,1754634929.0,3,/r/MachineLearning/comments/1mkny59/d_disentanglement_using_flow_matching/,MachineLearning,[D] Disentanglement using Flow matching,"Hi, 

I‚Äôve been considering flow matching models to disentangle attributes from an embedding. The idea stems from the fact that flow matching models learn smooth and invertible mappings.

Consider a pre-trained embedding E, and disentangled features T1 and T2. Is it possible to learn a flow matching model to learn this mapping from E to T1 and T2 (and vice versa)?

My main concerns are -
1. Distribution of E is known since its source distribution. But T1 and T2 are unknown. How will the model learn when it has a moving or unknown target?
2. I was also wondering if some clustering losses can enable this learning?
3. Another thought was to use some priors, but I am unsure as to what would be a good prior. 

Please suggest ideas if this wouldnt work. Or advancements on this if it does.

Prior work:
A paper from ICCV 25 (‚ÄúSCFlow‚Äù) does disentanglement using flow matching. But, they know the disentangled representations (Ground truth is available). So they provide T1 or T2 distributions to the model alternatively and ask it to learn the other. ",15,0.91,https://www.reddit.com/r/MachineLearning/comments/1mkny59/d_disentanglement_using_flow_matching/,False,True,False
1mkge00,IThrowShoes,1754611655.0,10,/r/MachineLearning/comments/1mkge00/d_in_2025_what_is_a_sufficient_methodology_to/,MachineLearning,"[D] In 2025, what is a sufficient methodology to analyze document summaries generated by LLMs? BERTScore, G-Eval, Rogue, etc","Greetings,

At work, I am currently building a very simple document summarization platform that takes in source documents, produces small and concise summaries of the documents, and storing them in a database.

The project plans to expand to a lot of other functionalities later on, but for the moment I've been asked to determine a way to ""grade"" or ""analyze"" the generated summaries against the original source text and give it a score, as an aid for some of our human reviewers.

I've been working on this for about a week, and have tried various methods like BERTScore, MoverScore, G-eval, ROGUE, BLEU and the like. And I've come to the conclusion that the scores themselves don't tell me a lot, at least personally (which could simply be due in part to me misunderstanding or overlooking details). For example I understand cosine similarity to a degree, but it's hard to put into context of ""grade this summary."" I've also tried out an idea about sending the summary to another decoder-only model (such as Qwen or even Phi-4), asking it to extract key facts or questions, then running each of those through a BERT NLI model against chunks of the source material (checking ""faithfulness"" I believe). I also thought about maybe doing some kind of ""miniature RAG"" against a single document and seeing how that relates to the summary itself, as in to find gaps in coverage.

For the most part, I wasn't disappointed in the results but I also was not thrilled by them either. Usually I'd get a score that felt ""middle of the road"" and would be difficult to determine whether or not the summary itself was good.

So my question is: Does anyone here have any experience with this and have any suggestions for things to try out or experiment with? I feel like this might be a large area of ongoing research as is, but at this point we (where I work) might actually just be striving for something simple.

Thanks!",10,0.92,https://www.reddit.com/r/MachineLearning/comments/1mkge00/d_in_2025_what_is_a_sufficient_methodology_to/,False,True,False
1mkelg5,NandoGando,1754606910.0,48,/r/MachineLearning/comments/1mkelg5/d_can_llms_have_accurate_world_models/,MachineLearning,[D] Can LLMs Have Accurate World Models?,"I have seen many articles (one example https://aiguide.substack.com/p/llms-and-world-models-part-1) stating that LLMs have no coherent/effective world models and because of this their accuracy is inherently limited. Can this obstacle be overcome, and if not why?",41,0.82,https://www.reddit.com/r/MachineLearning/comments/1mkelg5/d_can_llms_have_accurate_world_models/,False,True,False
1mkdw6f,Optimal-Outcome-7458,1754605139.0,1,/r/MachineLearning/comments/1mkdw6f/r_crinn_free_fast_framework_for_approximate/,MachineLearning,[R] CRINN: Free & Fast Framework for Approximate Nearest Neighbors Search,"Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN‚Äôs effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN‚Äôs success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement.

[https://github.com/deepreinforce-ai/CRINN](https://github.com/deepreinforce-ai/CRINN)",14,0.95,https://www.reddit.com/r/MachineLearning/comments/1mkdw6f/r_crinn_free_fast_framework_for_approximate/,False,True,False
1mkacbi,bababhaukali,1754596784.0,6,/r/MachineLearning/comments/1mkacbi/d_lstms_vs_transformers_model_selection_and/,MachineLearning,[D] LSTMs vs Transformers (Model Selection and Thoughts),"I wanted to have a discussion along the following lines. Lets say there is a scenario where the advantage of parallelism is no longer present. Then for an NLP task which model would you prefer an LSTM or a transformer? Lets assume the size of both models in terms of parameters is also the same. I have consulted 4o, claude sonnet, gemini flash 2.5 and grok 3 as well. Posting their responses in the comments. The question is around how to think about different models and their advantages. I feel like nowadays throwing a transformer is the first thing people do.",0,0.35,https://www.reddit.com/r/MachineLearning/comments/1mkacbi/d_lstms_vs_transformers_model_selection_and/,False,True,False
1mjtm98,Roland31415,1754551775.0,8,/r/MachineLearning/comments/1mjtm98/d_unsaturated_evals_before_gpt5/,MachineLearning,[D] Unsaturated Evals before GPT5,"Ahead of today‚Äôs GPT-5 launch, I compiled a list of unsaturated LLM evals. Let's see if GPT-5 can crack them.

link: [https://rolandgao.github.io/blog/unsaturated\_evals\_before\_gpt5](https://rolandgao.github.io/blog/unsaturated_evals_before_gpt5)  
x post: [https://x.com/Roland65821498/status/1953355362045681843](https://x.com/Roland65821498/status/1953355362045681843)

https://preview.redd.it/t3cwiitotjhf1.png?width=1302&format=png&auto=webp&s=098a7f2092afdf436a2699104accc49d01909f19",17,0.8,https://www.reddit.com/r/MachineLearning/comments/1mjtm98/d_unsaturated_evals_before_gpt5/,False,True,False
1mjtdu1,No-Economist146,1754550889.0,0,/r/MachineLearning/comments/1mjtdu1/p_reproducing_yolov1_from_scratch_in_pytorch/,MachineLearning,[P] Reproducing YOLOv1 From Scratch in PyTorch - Learning to Implement Object Detection from the Original Paper,"Hey everyone,

I have recently reproduced **YOLOv1** entirely from scratch using **PyTorch**, as a self-driven project to dive deeper into object detection and research implementation

**What I implemented**

YOLOv1 CNN architecture (paper-faithful)

Custom loss function (localization, confidence, classification)

IoU calculations and grid transformations

Forward pass and inference pipeline (with visualization)

Modular structure and utilities

**Training hasn‚Äôt been done yet** although I have a GPU it is taking a long time, but the pipeline is fully written, ready for VOC or a custom dataset.

**GitHub repo:**

[https://github.com/aayan873/YOLOv1-from-Scratch-My-First-Paper-to-Code-Project/](https://github.com/aayan873/YOLOv1-from-Scratch-My-First-Paper-to-Code-Project/)",13,0.88,https://www.reddit.com/r/MachineLearning/comments/1mjtdu1/p_reproducing_yolov1_from_scratch_in_pytorch/,False,True,False
1mjsu50,MokshMalik,1754548829.0,8,/r/MachineLearning/comments/1mjsu50/d_idea_for_an_efficient_text_diffusion_model_with/,MachineLearning,"[D] Idea for an efficient text diffusion model with adaptive, token-level steps","Hi r/MachineLearning,

I've been thinking about the inefficiency of using a fixed number of inference steps in text diffusion models. It seems wasteful to use the same amount of compute for a simple sentence as for a complex one.

I've prototyped an alternative architecture I'm calling ""Adaptive Refinement Diffusion,"" and I'd love your feedback on it.

The core idea is:

* Instead of a fixed loop, the model iteratively refines the sequence.
* At each step, it calculates a confidence score for every token (based on a mix of its embedding stability and prediction probability).
* If a token's score passes a certain threshold, it gets ""frozen"" and is excluded from future computation.
* The entire generation process stops dynamically once all tokens in the sequence are frozen.

This means the model would naturally focus compute on the more difficult or ambiguous tokens and could finish simple sentences much faster.

My questions for the community are:

1. Does this architecture already exist? I've searched for prior work but haven't found this specific token-level freezing mechanism.
2. What potential flaws or failure modes do you see with this approach?

Appreciate any thoughts or links to related papers. Thanks!",2,1.0,https://www.reddit.com/r/MachineLearning/comments/1mjsu50/d_idea_for_an_efficient_text_diffusion_model_with/,False,True,False
1mjqcas,Realistic_Public_415,1754540153.0,5,/r/MachineLearning/comments/1mjqcas/d_training_whisper_tiny/,MachineLearning,[D] Training Whisper Tiny,"I am trying to build an on device speech recognition engine for recognising kids‚Äô voice better replacing speech framework I am using in my ios app right now.

To do this, I collect sample audio data from my app keeping the privacy concerns in mind and transcribe these audio files with whisper large v2 and then using it as pseudo labelling to train  whisper tiny. 

I have following questions now:

1. Is this a valid strategy or with low parameters  of whisper tiny this is a futile exercise no matter how much I train it?

2. Most of my data is not clean, meaning background and other noise is interspersed with kids‚Äô speech. But it‚Äôs also important for my app to be accurate in these environment.

3. How many hours of audio I need to train it on  keeping the above audio quality in mind to achieve reasonable accuracy?

4. Are there better solutions?",8,0.9,https://www.reddit.com/r/MachineLearning/comments/1mjqcas/d_training_whisper_tiny/,False,True,False
1mjnrmg,35nakedshorts,1754532478.0,56,/r/MachineLearning/comments/1mjnrmg/d_have_any_bayesian_deep_learning_methods/,MachineLearning,[D] Have any Bayesian deep learning methods achieved SOTA performance in...anything?,"If so, link the paper and the result. Very curious about this. Not even just metrics like accuracy, have BDL methods actually achieved better results in calibration or uncertainty quantification vs say, deep ensembles?",94,0.98,https://www.reddit.com/r/MachineLearning/comments/1mjnrmg/d_have_any_bayesian_deep_learning_methods/,False,True,False
1mjh0cp,ArtisticHamster,1754515076.0,7,/r/MachineLearning/comments/1mjh0cp/d_fp4_training_methods_request_for_paper/,MachineLearning,[D] FP4 training methods (request for paper recommendations),"The new OSS models by OpenAI have low precision weights (MXFP4). Does anyone know:

- Is it likely that they were trained with MXFP4?

- Could anyone recommend papers on how to train models in such a low precision? Is it possible to train with SGD in such a low range, i.e. FP4, has just 16 values?

- Is it possible to go even lower? I.e. FP3 or FP2?

",8,0.91,https://www.reddit.com/r/MachineLearning/comments/1mjh0cp/d_fp4_training_methods_request_for_paper/,False,True,False
1mj8a54,StartledWatermelon,1754495300.0,0,/r/MachineLearning/comments/1mj8a54/r_llms_have_a_heart_of_stone_demystifying_the/,MachineLearning,[R] LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models,"**TL;DR**: [Soft tokens](https://www.arxiv.org/abs/2505.15778) (probabilities-weighted sum over vocab) actually underperform traditional ""hard"" tokens. But a Gumbel-Softmax trick can salvage this issue.

**Paper:** [https://www.arxiv.org/pdf/2508.03440](https://www.arxiv.org/pdf/2508.03440)

**Abstract:**

>Human cognition naturally engages with abstract and fluid concepts, whereas existing reasoning models often rely on generating discrete tokens, potentially constraining their expressive capabilities. Recent advancements aim to address this limitation by enabling large language models (LLMs) to generate soft, abstract tokens, thus facilitating reasoning within a continuous concept space. This paper explores the \`Soft Thinking' capabilities of various LLMs by examining the models' internal behavior using a suite of probing techniques. Contrary to the common belief that Soft Thinking enables the simultaneous exploration of diverse reasoning paths, our findings reveal that LLMs predominantly rely on the most influential component of the soft inputs during subsequent decoding steps. This reliance hinders the exploration of different reasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding, obscuring the advantage of transmitting more information through Soft Tokens. To tackle this issue, we explore sampling strategies to introduce \\emph{randomness}, employing methods such as Dirichlet resampling and the Gumbel-Softmax trick. Our experiments demonstrate that incorporating randomness can alleviate the limitations of vanilla approaches and unleash the potential of Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate randomness with controlled smoothness, resulting in superior performance across eight reasoning benchmarks.

**Visual Highlights:**

https://preview.redd.it/zza3t8r17fhf1.png?width=1099&format=png&auto=webp&s=e12815cb0774bce2a2614b2c3ad0df47b071d8c8

https://preview.redd.it/lulzrar27fhf1.png?width=1109&format=png&auto=webp&s=0fd5cd8dc90a9c09afb46dbd8e0412a72800dbe3

",23,1.0,https://www.reddit.com/r/MachineLearning/comments/1mj8a54/r_llms_have_a_heart_of_stone_demystifying_the/,False,True,False
1mivjcq,BitExternal4608,1754455687.0,0,/r/MachineLearning/comments/1mivjcq/r_trainable_dynamic_mask_sparse_attention/,MachineLearning,[R] Trainable Dynamic Mask Sparse Attention,"https://preview.redd.it/v3nxbno7xbhf1.png?width=1280&format=png&auto=webp&s=4a425509b0c20e16992c7998392567ff534a9b02

Trainable selective sampling and sparse attention kernels are indispensable in the era of context engineering. We hope our work will be helpful to everyone! ü§ó

* **Blog Post (The TL;DR):**¬†[https://hf.co/blog/wubingheng/dmattn](https://hf.co/blog/wubingheng/dmattn)
* **Paper (The Nitty-Gritty):**¬†[https://huggingface.co/papers/2508.02124](https://huggingface.co/papers/2508.02124)
* **Code (The Good Stuff):**¬†[https://github.com/SmallDoges/flash-dmattn](https://github.com/SmallDoges/flash-dmattn)",5,1.0,https://www.reddit.com/r/MachineLearning/comments/1mivjcq/r_trainable_dynamic_mask_sparse_attention/,False,True,False
1mj3t3r,MarketingNetMind,1754484647.0,4,/r/MachineLearning/comments/1mj3t3r/d_gspo_qwen3s_sequencelevel_rlhf_method_vs_grpo/,MachineLearning,[D] GSPO: Qwen3‚Äôs sequence-level RLHF method vs. GRPO - stability & scaling analysis,"The Qwen team recently proposed **Group Sequence Policy Optimization (GSPO)**, a reinforcement learning approach for post-training LLM fine-tuning. They position it as an alternative to **Group Relative Policy Optimization (GRPO)** \- used in DeepSeek - and claim GRPO‚Äôs token-level importance sampling is ‚Äúill‚Äëposed‚Äù for stable training.

**Background:**

* Popular RLHF methods (e.g. PPO) optimize LLMs via reward signals.
* DeepSeek‚Äôs GRPO extends this by computing sample-level value estimations.
* Qwen reports that GRPO often triggers gradient instability and model collapse unless patched with complex adjustments.

**Key concerns with GRPO:**

* Applies importance sampling **per token**, accumulating high variance across long sequences.
* Particularly problematic for **Mixture-of-Experts (MoE)** models, where token-level routing shifts can destabilize training.
* To counteract this, GRPO-based pipelines often rely on strategies like **Routing Replay**.

**GSPO‚Äôs proposal:**

* Moves to **sequence-level importance sampling**, normalizing by sequence length.
* Dramatically reduces variance and eliminates the need for routing hacks.
* Qwen reports stable MoE convergence and better scaling.

**Findings from experiments:**

* On benchmarks such as AIME‚Äô24, LiveCodeBench, and CodeForces, GSPO achieves better reward curves than GRPO.
* GSPO converges faster with more compute and shows smoother scaling trends.
* GRPO requires Routing Replay to perform adequately; GSPO does not.

If you're interested, read more about it here: [Qwen Team Proposes GSPO for Qwen3, Claims DeepSeek's GRPO is Ill-Posed](https://blog.netmind.ai/article/Qwen_Team_Proposes_GSPO_for_Qwen3%2C_Claims_DeepSeek's_GRPO_is_Ill-Posed). The blog post includes mathematical formulations of both methods and performance comparisons.

I‚Äôm interested to know:

* Whether anyone in the community has observed instability with token-level importance sampling or GRPO?
* Has sequence-level weighting like GSPO been tested in your RLHF pipelines?",71,0.97,https://www.reddit.com/gallery/1mj3t3r,False,False,False
1mj3n3v,shbong,1754484199.0,54,/r/MachineLearning/comments/1mj3n3v/d_do_you_think_llm_memory_will_ever_be_solved/,MachineLearning,[D] Do you think LLM memory will ever be solved without fine‚Äëtuning?,"I‚Äôve been running into the same issue again and again while working with LLMs: they forget. You can stuff the history into the prompt, set up a RAG pipeline, or go through fine‚Äëtuning, but none of these feel like a real solution.

Because of that frustration, I started exploring memory management myself, more like giving models ‚Äúon‚Äëdemand context‚Äù instead of retraining them. It‚Äôs early, but it made me realize how huge and unexplored this space is.

I‚Äôm wondering if others here have felt the same pain. How are you approaching memory in your projects, and do you think we‚Äôll ever see something beyond the RAG/fine‚Äëtuning combo?",14,0.67,https://www.reddit.com/r/MachineLearning/comments/1mj3n3v/d_do_you_think_llm_memory_will_ever_be_solved/,False,True,False
1miq2y4,bigbird1996,1754439644.0,28,/r/MachineLearning/comments/1miq2y4/d_is_modern_academic_published_zerosum/,MachineLearning,[D] Is modern academic published zero-sum?,"It seems the current state of publishing in A* venues (CVPR, NeurIPS, ICML, ICCV/ECCV) is zero-sum. One person‚Äôs rejection is another person‚Äôs acceptance. Reviewers seem to reject papers just for the sake of rejection. There‚Äôs a sense that some reviewers reject papers not on substantive grounds, but out of an implicit obligation to limit acceptance rates. Rebuttals appear to be pointless as reviewers take stubborn positions and not acknowledge their misunderstandings during this period. Good science just doesn‚Äôt appear to be as valued as the next flashiest LLM/VLM that gets pretty results.",157,0.9,https://www.reddit.com/r/MachineLearning/comments/1miq2y4/d_is_modern_academic_published_zerosum/,False,True,False
1miev16,Street_Car_1297,1754413511.0,3,/r/MachineLearning/comments/1miev16/p_from_business_processes_to_gnn_for_next/,MachineLearning,[P] From Business Processes to GNN for Next Activity Prediction,"I‚Äôm quite new to GNNs and process mining, and I‚Äôm trying to tackle a project that I‚Äôm really struggling to structure. I‚Äôd love your input, especially if you‚Äôve worked with GNNs or process data before.

I have a CSV file representing a business process (specifically a Helpdesk process). From this CSV, I want to build a graph representation of the process (specifically a Directly-Follows Graph). Then, I want to train a GNN to do¬†**next activity prediction**¬†at the¬†**node level**.

The idea is: given a¬†*prefix graph*¬†(i.e., a pruned version of the full process graph up to a certain point), I want the model to predict the¬†label of the next activity, corresponding to the node that would logically come next in the process.

I‚Äôve found very little literature on this, and almost no practical examples. I have a few specific doubts I hope someone can help me with.

1. **Model choice**: It's a dataset made of 4580 graphs (traces), 7 average nodes each, 15 total labels (activities). I was thinking of using a 3-layer GCN for the prediction task. Does this make sense for my use case? Are there better architectures for sequence-based node prediction in process graphs?
2. **Multiple process instances (graphs)**:As I said, I have¬†4580 different instances¬†of the process, each one is essentially a separate graph. Should I treat them as¬†4580¬†**separate graphs**¬†during training, or should I¬†**merge them into one big graph**¬†(while preserving per-node instance information somehow)?My concern is about how GNNs typically work with multiple small graphs, should I batch them separately, or does it make sense to construct one global graph?",3,0.72,https://www.reddit.com/r/MachineLearning/comments/1miev16/p_from_business_processes_to_gnn_for_next/,False,True,False
1mic820,HerpisiumThe1st,1754407524.0,25,/r/MachineLearning/comments/1mic820/deepmind_genie3_architecture_speculation/,MachineLearning,DeepMind Genie3 architecture speculation,"If you haven't seen Genie 3 yet: [https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/](https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/)

It is really mind blowing, especially when you look at the comparison between 2 and 3, the most striking thing is that 2 has this clear constant statistical noise in the frame (the walls and such are clearly shifting colours, everything is shifting because its a statistical model conditioned on the previous frames) whereas in 3 this is completely eliminated. I think we know Genie 2 is a diffusion model outputting 1 frame at a time, conditional on the past frames and the keyboard inputs for movement, but Genie 3's perfect keeping of the environment makes me think it is done another way, such as by generating the actual 3d physical world as the models output, saving it as some kind of 3d meshing + textures and then having some rules of what needs to be generated in the world when (anything the user can see in frame). 

What do you think? Lets speculate together!",147,0.94,https://www.reddit.com/r/MachineLearning/comments/1mic820/deepmind_genie3_architecture_speculation/,False,True,False
1mi27ab,MylarSome,1754377076.0,1,/r/MachineLearning/comments/1mi27ab/dimproving_hybrid_knn_keyword_matching_retrieval/,MachineLearning,[D]Improving Hybrid KNN + Keyword Matching Retrieval in OpenSearch (Hit-or-Miss Results),"Hey folks,

I‚Äôm working on a Retrieval-Augmented Generation (RAG) pipeline using OpenSearch for document retrieval and an LLM-based reranker. The retriever uses a hybrid approach:
	‚Ä¢	KNN vector search (dense embeddings)
	‚Ä¢	Multi-match keyword search (BM25) on title, heading, and text fields

Both are combined in a bool query with should clauses so that results can come from either method, and then I rerank them with an LLM.

The problem:
Even when I pull hundreds of candidates, the performance is hit or miss ‚Äî sometimes the right passage comes out on top, other times it‚Äôs buried deep or missed entirely. This makes final answers inconsistent.

What I‚Äôve tried so far:
	‚Ä¢	Increased KNN k and BM25 candidate counts
	‚Ä¢	Adjusted weights between keyword and vector matches
	‚Ä¢	Prompt tweaks for the reranker to focus only on relevance
	‚Ä¢	Query reformulation for keyword search

I‚Äôd love advice on:
	‚Ä¢	Tuning OpenSearch for better recall with hybrid KNN + BM25 retrieval
	‚Ä¢	Balancing lexical vs. vector scoring in a should query
	‚Ä¢	Ensuring the reranker consistently sees the correct passages in its candidate set
	‚Ä¢	Improving reranker performance without full fine-tuning

Has anyone else run into this hit-or-miss issue with hybrid retrieval + reranking? How did you make it more consistent?

Thanks!
",6,0.75,https://www.reddit.com/r/MachineLearning/comments/1mi27ab/dimproving_hybrid_knn_keyword_matching_retrieval/,False,True,False
1mi0wz8,willingtoengage,1754372308.0,3,/r/MachineLearning/comments/1mi0wz8/d_seeking_advice_on_choosing_phd_topicarea/,MachineLearning,[D] Seeking advice on choosing PhD topic/area,"Hello everyone,

I'm currently enrolled in a master's program in statistics, and I want to pursue a PhD focusing on the theoretical foundations of machine learning/deep neural networks.

I'm considering statistical learning theory (primary option) or optimization as my PhD research area, but I'm unsure whether statistical learning theory/optimization is the most appropriate area for my doctoral research given my goal.

Further context: I hope to do theoretical/foundational work on neural networks as a researcher at an AI research¬†lab in the¬†future.¬†

Question:

1)What area(s) of research would you recommend for someone interested in doing fundamental research in machine learning/DNNs?

2)What are the popular/promising techniques and mathematical frameworks used by researchers working on the theoretical foundations of deep learning?

Thanks a lot for your help.",13,0.74,https://www.reddit.com/r/MachineLearning/comments/1mi0wz8/d_seeking_advice_on_choosing_phd_topicarea/,False,True,False
1mhziul,Dense-Ad-8885,1754367604.0,5,/r/MachineLearning/comments/1mhziul/d_aaai_2026_desk_reject/,MachineLearning,[D] AAAI 2026 desk reject,"I submitted a paper to the AAAI 2026 conference. The conference states that colors must only be used for figures.

I mistakenly used colors in an experimental table to show the increase in accuracy within parentheses.

Will I have a chance to modify it in the rebuttal phase? Are there some cases in which those who have made the same mistake proceed with the rebuttal phase?

I found someone who submitted a paper with the same mistake to another conference proceeded with the rebuttal successfully.",5,0.69,https://www.reddit.com/r/MachineLearning/comments/1mhziul/d_aaai_2026_desk_reject/,False,True,False
1mhtkdm,beto_valdes,1754350772.0,2,/r/MachineLearning/comments/1mhtkdm/p_sklearnmigrator_a_library_to_migrate/,MachineLearning,[P] sklearn-migrator ‚Äì A library to migrate scikit-learn models across versions,"Hi everyone! üëã

I want to share the initial release of \[\`sklearn-migrator\`\] ([https://pypi.org/project/sklearn-migrator/](https://pypi.org/project/sklearn-migrator/)) ‚Äì a Python library designed to¬†**serialize and migrate scikit-learn models across incompatible versions.**

If you‚Äôve ever faced issues like \`AttributeError: '...' object has no attribute '...'\` after upgrading \`scikit-learn\`, or had to retrain models just because of version mismatches in production‚Ä¶ this tool is for you.

What it does?

\- Converts saved models from older \`scikit-learn\` versions to be compatible with newer ones

\- Supports serialization and internal structure mapping (especially for tree-based models)

\- Designed to help maintain long-term model compatibility in production

\## ‚úÖ Current support

\- \*\*Classifiers & regressors\*\*:

\- \`DecisionTree\`, \`RandomForest\`, \`GradientBoosting\`, \`LogisticRegression\`, \`LinearRegression\`, and more

\- Tested across versions like: \[

'0.21.3', '0.22.0', '0.22.1', '0.23.0', '0.23.1', '0.23.2',

'0.24.0', '0.24.1', '0.24.2', '1.0.0', '1.0.1', '1.0.2',

'1.1.0', '1.1.1', '1.1.2', '1.1.3', '1.2.0', '1.2.1', '1.2.2',

'1.3.0', '1.3.1', '1.3.2', '1.4.0', '1.4.2', '1.5.0', '1.5.1',

'1.5.2', '1.6.0', '1.6.1', '1.7.0'

\]

We have 900 pairs of tested versions.

Repository Github:¬†[https://github.com/anvaldes/sklearn-migrator](https://github.com/anvaldes/sklearn-migrator)  
PyPI:¬†[https://pypi.org/project/sklearn-migrator/](https://pypi.org/project/sklearn-migrator/)  
Medium article:¬†[https://medium.com/@alberto.valdes.gonzalez.96/sklearn-migrator-safe-migration-of-models-across-scikit-learn-versions-0842f8dc375e](https://medium.com/@alberto.valdes.gonzalez.96/sklearn-migrator-safe-migration-of-models-across-scikit-learn-versions-0842f8dc375e)",7,0.82,https://www.reddit.com/r/MachineLearning/comments/1mhtkdm/p_sklearnmigrator_a_library_to_migrate/,False,True,False
1mhn9lc,ndpian,1754335922.0,3,/r/MachineLearning/comments/1mhn9lc/n_machine_learning_reproducibility_challenge_mlrc/,MachineLearning,[N] Machine Learning Reproducibility Challenge (MLRC) 2025 happening this month at Princeton University,"- The 8th iteration of MLRC is happening in-person at Princeton University on August 21st. Keynote speakers include Arvind Narayanan (Princeton), Soumith Chintala (Pytorch - Meta), Jonathan Frankle (Databricks) and Stella Biderman (EleutherAI). 
- Panel discussion on ""Reproducibility of and by large language models"", moderated by Sayash Kapoor (Princeton)
- Link to webpage: https://reproml.org/ (registration seems to be still open!)",32,0.97,https://www.reddit.com/r/MachineLearning/comments/1mhn9lc/n_machine_learning_reproducibility_challenge_mlrc/,False,True,False
1mhga0e,NPCNo10,1754320683.0,119,/r/MachineLearning/comments/1mhga0e/d_neurips_2025_final_scores/,MachineLearning,[D] NeurIPS 2025 Final Scores,"I understand that updated scores of reviewers are not visible to authors this time round. I was wondering if anyone knows whether the final scores will also not be visible? I.e. once you revise your review and add your ""Final justification"", will your score not be visible to the authors anymore?

  
Asking because I've had a reviewer who has selected the mandatory acknowledgement option, not responded to my review, and whose score no longer appears on the portal.",44,0.92,https://www.reddit.com/r/MachineLearning/comments/1mhga0e/d_neurips_2025_final_scores/,False,True,False
1mheqsf,No_Adhesiveness_3444,1754317170.0,77,/r/MachineLearning/comments/1mheqsf/r_cikm_2025_decision/,MachineLearning,[R] CIKM 2025 Decision,"Hi, has anybody received their submission outcome for CIKM 2025?",17,0.88,https://www.reddit.com/r/MachineLearning/comments/1mheqsf/r_cikm_2025_decision/,False,True,False
1mhel0c,Popular_Lunch7488,1754316797.0,16,/r/MachineLearning/comments/1mhel0c/d_is_amd_still_a_bad_choice_for_ai_workloads/,MachineLearning,[D] Is AMD Still a Bad Choice for AI Workloads?,"I've read a lot that working with an AMD GPU is a nightmare, but that was a while ago. Since they seem to be releasing a well-priced AI GPU in a few months, I wanted to know if it's worth it or if poor support still makes it a bad choice.",11,1.0,https://www.reddit.com/r/MachineLearning/comments/1mhel0c/d_is_amd_still_a_bad_choice_for_ai_workloads/,False,True,False
1mh9g3r,LostAmbassador6872,1754302142.0,6,/r/MachineLearning/comments/1mh9g3r/p_docstrange_open_source_document_data_extractor/,MachineLearning,[P] DocStrange - Open Source Document Data Extractor with free cloud processing for 10k docs/month,"Sharing¬†**DocStrange**, an open-source Python library that makes document data extraction easy.

* **Universal Input**: PDFs, Images, Word docs, PowerPoint, Excel
* **Multiple Outputs**: Clean Markdown, structured JSON, CSV tables, formatted HTML
* **Smart Extraction**: Specify exact fields you want (e.g., ""invoice\_number"", ""total\_amount"")
* **Schema Support**: Define JSON schemas for consistent structured output

**Quick start:**

    pip install docstrange
    docstrange invoice.jpeg --output json --extract-fields invoice_amount buyer seller

**Data¬†Processing Options:**

* **Cloud Mode**: Fast and free processing with minimal setup, free 10k docs per month
* **Local Mode**: Complete privacy - all processing happens on your machine, no data sent anywhere, works on both cpu and gpu

**Github**:¬†[https://github.com/NanoNets/docstrange](https://github.com/NanoNets/docstrange)",48,0.83,https://www.reddit.com/gallery/1mh9g3r,False,False,False
1mh8k4j,when_i_Go,1754298743.0,0,/r/MachineLearning/comments/1mh8k4j/d_zria_architecture_and_pfaf_are_baseless/,MachineLearning,[D] ZRIA architecture and P-FAF are baseless,"I recently came across youtube channel Richard Aragon, watching his videos regarding his original model ZRIA and token transformation method P-FAF in [this video](https://www.youtube.com/watch?v=xP0oHEE6t_U), another on benchmarking his original ZRIA model for [agentic tasks](https://www.youtube.com/watch?v=b9zwwlRVQPo), and finally a video discussing P-FAF's conceptual connections to a recent work in [stochastic calculus](https://www.youtube.com/watch?v=64mmFBclymc). Admittedly, I am unsettled and agitated after posting a handful of questions on his video comments section as user yellowbricks and being threatened into silence with personal attacks and false accusations after challenging his theory and methodology but less than a vent post this it is a warning against the seemingly baseless theory of ZRIA and P-FAF and the unacceptable behavior which led to its niche following. We should remain critical of ZRIA and P-FAF not because of the individual promoting them, but because of the unchecked patterns of thought and conduct they can reinforce in the scientific community.  
  
In the videos, we get conceptual explanations of the architecture ZRIA and he promotes it as a superior architecture to the transformer for language tasks. He has yet to point to a precise mathematical definition or theoretical foundation of ZRIA to describe what it predicts, what it optimizes, etc. Instead, in his agentic analysis video, he presents benchmarks scores such as ROCG which he presents as the best agentic benchmark and shows impressive score of his ZRIA model compared to a bigger Gemma, although as noted by commenter JohnMcclaned he clearly overfits the training data to ZRIA with no mitigating methods such as monitoring a validation set, and as noted by commenter israrkarimzai he has an issue in the code which explains why Gemma had 0 scores across the board and with the fix showed much more reasonable scores with several 100% scores. Both of these wildly weakens his claim to architectural superiority. (JohnMcclaned was unfortunatly bullied out of the comments sections by Richard.)

This lack of rigor is reflected again in his video discussing the combination of ZRIA and P-FAF. Again, he presents a conceptual explanation of ZRIA and P-FAF. In particular he never points to a rigorous formulation of his P-FAF theory. Upon request he does not provide explanations, only a motivation, or insists that modern LLMs have enough knowledge of his theory such that they can substitute as a teacher (as he told to commenter wolfgangsullifire6158). His video description has a link to his hugging face blog post which again is unrigorous and uses a questionable benchmark whose results are weakened by Richard's examples of unscientific methodology in his benchmark videos. He which leaves viewers with no means to analyze, verify, or even understand what his theory is about. He does not address the inconsistencies in the benchmarking and the risk of overfitting in this video either as pointed out again by wolfgangsullifire6158 instead stating that ""Overfitting is a phenomenon unique to the Transformers architecture."" Admittedly I did not comment kindly towards his unscientific attitude and dismissal of the transformer despite his ZRIA being based on it.

In his video linking his P-FAF to a graduate-level stochastic calculus paper on ""theta-expectations"", he again discusses the concepts at a very high level. I assume this video was made to address a request for a video on the theory of P-FAF. Instead of explaining the theory rigorously he tries to present the theta-expectations as a substitute for the mathematical foundation of P-FAF, suggesting that he had to ""go through the exact same process"" and solve the ""exact same problem"" to derive P-FAF with no evidence of such a derivation and only a dim conceptual overlap linking the two ideas in any way.

This is not about Richard as a person. It is about his repeated behavior: marketing unverified claims as revolutionary science, silencing dissent, and treating scientific skepticism as personal attack. You should take this seriously not because of this one individual but because this pattern can erode the epistemic foundations of our field if left unchecked.",2,1.0,https://www.reddit.com/r/MachineLearning/comments/1mh8k4j/d_zria_architecture_and_pfaf_are_baseless/,False,True,False
1mh455w,Neither_Shoulder_278,1754282189.0,0,/r/MachineLearning/comments/1mh455w/r_integrative_approach_for_early_detection_of/,MachineLearning,"[R] Integrative approach for early detection of Parkinson‚Äôs disease and atypical Parkinsonian syndromes leveraging hemodynamic parameters, motion data & advanced AI models","https://www.sciencedirect.com/science/article/abs/pii/S0169260725004067


A recent study in Computer Methods and Programs in Biomedicine explores an efficient approach to early Parkinson‚Äôs detection using time-series data from low-cost sensors processed on microcontrollers. The lightweight hybrid machine learning model offers potential for accessible screening in low-resource settings.

Highlights:

‚Ä¢ Parkinson‚Äôs disease (PD) is a progressive neurological disorder affecting motor and non-motor functions. Early detection of PD is essential for improving patient outcomes and quality of life

‚Ä¢ This study proposes a multimodal hardware based wearable integrated with a novel machine learning framework for early, accurate and remote diagnosis of Parkinson‚Äôs disease.

‚Ä¢ Analyses diverse data sets, including hemodynamic parameters, gait patterns, and hand tremor metrics including bradykinesia and rigidity.

‚Ä¢ Achieves high accuracy through advanced algorithms, integrating artificial intelligence and intuitive user interface, thus providing a robust diagnostic tool.
",7,0.82,https://www.reddit.com/r/MachineLearning/comments/1mh455w/r_integrative_approach_for_early_detection_of/,False,True,False
1mgx2ax,lyadalachanchu,1754261455.0,5,/r/MachineLearning/comments/1mgx2ax/p_implementing_einsum/,MachineLearning,[P] Implementing Einsum,Implemented einsum using torch operations. Learned a lot doing it and had a lot of fun so wanted to share it here :),44,0.94,https://lyadalachanchu.github.io/2025/08/03/einsum-is-all-you-need.html,False,False,False
1mguugx,Hamzayslmn,1754255801.0,0,/r/MachineLearning/comments/1mguugx/d_strange_label_studio_behavior/,MachineLearning,[D] Strange label studio behavior,"Im using label studio

I'm having a strange problem. When I output with YOLO, it doesn't make predictions, but when I output with v8 OBB and train it, I can see the outputs. What's the problem ?

I wanted to create a cat recognition algorithm. I uploaded 50 cat photos.

I labelled them with Label Studio and exported them in YOLO format. I trained the model with v11 and used it. However, even though I tested the training photos, it couldn't produce any output.

Then I exported the same set in YOLOv8 OBB format and trained it. This time, it achieved a recognition rate of 0.97.

Why aren't the models I trained using YOLO exports working?",0,0.38,https://www.reddit.com/r/MachineLearning/comments/1mguugx/d_strange_label_studio_behavior/,False,True,False
1mgq1pw,Pizel_the_Twizel,1754244397.0,6,/r/MachineLearning/comments/1mgq1pw/d_a_nottooexpensive_cpu_server_provider_for_a/,MachineLearning,[D] A not-too-expensive cpu server provider for a month ?,"Hello everyone,

I'm currently in my last month of an internship, doing ML. Everything is great, however, we have a lot of problems with the hardware : the server we usually use is down and will be until the end of my internship. We need to do more training and I managed to convince my boss to use some funds for a remote server until the end of the month. However, I don't know which providers exists and how good they are, so I am asking you. I would need at least 16 cpu threads, ideally more, capable of running 24/7, running on a flavor of ubuntu and, most importantly, with python and conda pre-installed. I don't have a lot of experience with using remote servers so the easier the better (I know how to use ssh for remote connection, but for example I don't know how to close the connection without ending the runnng task). All of this for a budget of 200‚Ç¨ for the month, max !

Thank you all for your help !",2,0.67,https://www.reddit.com/r/MachineLearning/comments/1mgq1pw/d_a_nottooexpensive_cpu_server_provider_for_a/,False,True,False
1mgocly,ExtraPops,1754240429.0,26,/r/MachineLearning/comments/1mgocly/d_whats_the_realistic_future_of_spiking_neural/,MachineLearning,[D] What‚Äôs the realistic future of Spiking Neural Networks (SNNs)? Curious to hear your thoughts,"I‚Äôve been diving into the world of Spiking Neural Networks (SNNs) lately and I‚Äôm both fascinated and a bit puzzled by their current and future potential.

From what I understand, SNNs are biologically inspired, more energy-efficient, and capable of processing information in a temporally dynamic way.

That being said, they seem quite far from being able to compete with traditional ANN-based models (like Transformers) in terms of scalability, training methods, and general-purpose applications.

# So I wanted to ask :

* Do you believe SNNs have a practical future beyond niche applications?
* Can you see them being used in real-world products (outside academia or defense)?
* Is it worth learning and building with them today, if I want to be early in something big?
* Have you seen any recent papers or startups doing something truly promising with SNNs?

Would love to hear your insights, whether you‚Äôre deep in neuromorphic computing or just casually watching the space.

Thanks in advance!",59,0.9,https://www.reddit.com/r/MachineLearning/comments/1mgocly/d_whats_the_realistic_future_of_spiking_neural/,False,True,False
1mgie2q,Excellent-Effect237,1754225227.0,4,/r/MachineLearning/comments/1mgie2q/building_for_the_era_of_experience_d/,MachineLearning,Building for the era of experience [D],,0,0.29,https://rnikhil.com/2025/07/30/era-of-experience,False,False,False
1mfqaqz,hardmaru,1754140568.0,2,/r/MachineLearning/comments/1mfqaqz/r_kimi_k2_open_agentic_intelligence_technical/,MachineLearning,[R] Kimi K2: Open Agentic Intelligence (Technical Report),"The Moonshot AI team behind the recent [Kimi K2](https://x.com/Kimi_Moonshot/status/1943687594560332025) model, one of the leading open-weights LLM, just released the technical report: https://arxiv.org/abs/2507.20534

---

**Kimi K2: Open Agentic Intelligence**

*We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.
Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints to facilitate future research and applications of agentic intelligence.*

---

Recently, there has been discussions about Muon and MuonClip, which the Moonshot AI team has developed for training Kimi. See recent discussions here on r/MachineLearning : https://old.reddit.com/r/MachineLearning/comments/1m2y23l/p_understanding_muon_a_revolutionary_neural/",12,0.81,https://www.reddit.com/r/MachineLearning/comments/1mfqaqz/r_kimi_k2_open_agentic_intelligence_technical/,False,True,False
1mfq9lr,CurseCrusader,1754140475.0,0,/r/MachineLearning/comments/1mfq9lr/dpi0_used_in_simulation/,MachineLearning,[D]pi0 used in simulation,"Has anyone tried out using pi0(the well-known VLA model) on simulation platforms?

Due to budget and safety reasons, i only have very limited access to real robots. So i need to do everything once in simulation first.

So i really would like to know whether it works well there. Would distribution shift be an issue?

Thanks in advance!",1,0.67,https://www.reddit.com/r/MachineLearning/comments/1mfq9lr/dpi0_used_in_simulation/,False,True,False
1mfosop,ArjunBasandrai,1754135994.0,2,/r/MachineLearning/comments/1mfosop/d_submitted_to_kdd_for_the_first_time_can_i_now/,MachineLearning,[D] Submitted to KDD for the first time! Can I now upload a preprint to arXiv?,"Hey everyone,  
I just made my first ever submission to KDD.  
The submission was double-blind and I uploaded the anonymized version via OpenReview, as required.

Now I‚Äôm wondering:  
**Can I submit the same anonymized version as a preprint to arXiv?** The official KDD CFP didn‚Äôt say much clearly about this, and I wanted to check what the norm is. Also, the deadline for submission (31 July) has passed.

I had a few concerns and would love input from anyone who's been through this before:

* Will uploading the paper to arXiv violate the double-blind review policy for KDD?
* If I submit it to arXiv now, does the metadata (like the arXiv account or email) risk de-anonymizing me?",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1mfosop/d_submitted_to_kdd_for_the_first_time_can_i_now/,False,True,False
1mfmrmx,Remarkable-Ad3290,1754128556.0,49,/r/MachineLearning/comments/1mfmrmx/d_is_there_any_ai_startups_in_germany_investing/,MachineLearning,[D] Is there any AI startups in Germanyüá©üá™ investing time and money in building and training foundational models or working for General Intelligence ?other than Aleph Alpha?,"The only startup I know of that is focused specifically on this area is Aleph Alpha. Most others are just fine-tuning existing models or working on translation and image generation. There is no serious investment of time or money in original research and development in AI.
Does anyone know of any other startups in Germany üá©üá™ working in this area? Even a pre-revenue stage startup?",53,0.85,https://www.reddit.com/r/MachineLearning/comments/1mfmrmx/d_is_there_any_ai_startups_in_germany_investing/,False,True,False
1mfmcru,Remarkable-Ad3290,1754126884.0,5,/r/MachineLearning/comments/1mfmcru/p_implemented_the_research_paper_memorizing/,MachineLearning,[P] Implemented the research paper ‚ÄúMemorizing Transformers‚Äù from scratch with my own additional modifications in architecture and customized training pipeline .,"Did some major modifications to the model architecture and hyperparameters, aiming for improved performance. The entire model is built from scratch using PyTorch. 
The original paper introduces a memory-based mechanism that allows the model to attend to information beyond its context window, enabling long-term context handling. Instead of a single attention mechanism, the architecture incorporates two types of attention blocks: XLAttention for capturing short term memory and KNNAttention for enabling long term memory retrieval.

Key Modifications from the Original Paper:
‚Ä¢Replaced the default positional encoding with Rotary Positional Embeddings (RoPE)
‚Ä¢Altered the attention mechanism to use Grouped Query Attention
‚Ä¢Customized the DataLoader to support sharded datasets and data parallelism
‚Ä¢Implemented Mixed Precision Training along with Distributed Data Parallel (DDP) support
‚Ä¢Tweaked several training and model hyperparameters for better adaptability

HF repo with model and training code is here:

https://huggingface.co/abhinavv3/GPT_with_Modified_Memorizing_Transformer",25,0.94,https://huggingface.co/abhinavv3/GPT_with_Modified_Memorizing_Transformer,False,False,False
1mfjqc5,parassssssssss,1754116638.0,18,/r/MachineLearning/comments/1mfjqc5/d_looking_for_help_need_to_design/,MachineLearning,[D] Looking for help: Need to design arithmetic-economics prompts that humans can solve but AI models fail at,"Hi everyone,  
I‚Äôm working on a rather urgent and specific task. I need to craft prompts that involve arithmetic-based questions within the economics domain‚Äîquestions that a human with basic economic reasoning and arithmetic skills can solve correctly, but which large language models (LLMs) are likely to fail at.

I‚Äôve already drafted about 100 prompts, but most are too easy for AI agents‚Äîthey solve them effortlessly. The challenge is to find a sweet spot:

* **One correct numerical answer** (no ambiguity)
* **No hidden tricks or assumptions**
* **Uses standard economic reasoning and arithmetic**
* **Solvable by a human (non-expert) with clear logic and attention to detail**
* **But likely to expose conceptual or reasoning flaws in current LLMs**

Does anyone have ideas, examples, or suggestions on how to design such prompts? Maybe something that subtly trips up models due to overlooked constraints, misinterpretation of time frames, or improper handling of compound economic effects?

Would deeply appreciate any input or creative suggestions! üôè",0,0.36,https://www.reddit.com/r/MachineLearning/comments/1mfjqc5/d_looking_for_help_need_to_design/,False,True,False
1mfi8li,bill1357,1754111251.0,53,/r/MachineLearning/comments/1mfi8li/r_from_taylor_series_to_fourier_synthesis_the/,MachineLearning,[R] From Taylor Series to Fourier Synthesis: The Periodic Linear Unit,"**Full Example Runs as Videos:** [https://www.youtube.com/playlist?list=PLaeBvRybr4nUUg5JRB9uMfomykXM5CGBk](https://www.youtube.com/playlist?list=PLaeBvRybr4nUUg5JRB9uMfomykXM5CGBk)

Hello! My name is Shiko Kudo; you might have seen me on r/stablediffusion some time back if you're a regular there as well, where I published a vocal timbre-transfer model around a month ago.

...I had been working on the next version of my vocal timbre-swapping model, but as I had been working on it, I realized that in the process I had something really interesting in my hands. Slowly I built it up more, and in the last couple of days I realized that I had to share it no matter what.

This is the Periodic Linear Unit (PLU) activation function, and with it, some fairly large implications.

The paper and code is available on Github here:  
[https://github.com/Bill13579/plu\_activation/blob/main/paper.pdf](https://github.com/Bill13579/plu_activation/blob/main/paper.pdf)  
[https://github.com/Bill13579/plu\_activation](https://github.com/Bill13579/plu_activation)  
The paper is currently pending release on Arxiv, but as this is my first submission I am expecting the approval process to take some time.

It is *exactly* as it says on the tin: neural networks based upon higher-order (cascaded) sinusoidal waveform superpositions for approximation and thus Fourier-like synthesis instead of a Taylor-like approximation with countless linear components paired with monotonic non-linearities provided by traditional activations; and all this change from a change in the activation.

...My heart is beating out my chest, but I've somehow gotten through the night and gotten some sleep and I will be around the entire day to answer any questions and discuss with all of you.",230,0.93,https://i.redd.it/4ppmflqmgjgf1.jpeg,False,False,False
1mfg71z,PatientWrongdoer9257,1754104661.0,27,/r/MachineLearning/comments/1mfg71z/d_what_happens_if_none_of_the_reviewers_respond/,MachineLearning,[D] What happens if none of the reviewers respond for all of the NeurIPS discussion?,"Got 5/4/3/3, none of the reviewers have responded so far üò≠üò≠üò≠

Hopefully someone will respond by the end, but was wondering if anyone has any experience with no reviewers responding for the entire discussion",18,0.73,https://www.reddit.com/r/MachineLearning/comments/1mfg71z/d_what_happens_if_none_of_the_reviewers_respond/,False,True,False
1mfezri,AutoModerator,1754100929.0,38,/r/MachineLearning/comments/1mfezri/d_selfpromotion_thread/,MachineLearning,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

\--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

\--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",5,0.73,https://www.reddit.com/r/MachineLearning/comments/1mfezri/d_selfpromotion_thread/,False,True,False
1mf8d4g,crookedstairs,1754082598.0,7,/r/MachineLearning/comments/1mf8d4g/d_implementing_gpu_snapshotting_to_cut_cold/,MachineLearning,[D] Implementing GPU snapshotting to cut cold starts for large models by 12x,"GPU snapshotting is finally a thing! NVIDIA recently released their¬†[CUDA checkpoint/restore API](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__CHECKPOINT.html)¬†and we at Modal (serverless compute platform) are using it drastically reduce GPU cold start times. This is especially relevant for serving large models, where it can take minutes (for the heftiest LLMs) to move model weights from disk to memory.

GPU memory snapshotting can reduce cold boot times by up to 12x. It lets you scale GPU resources up and down based on demand without compromising on user-facing latency. Below are some benchmarking results showing improvements for various models!

https://preview.redd.it/vjld59c34hgf1.png?width=3162&format=png&auto=webp&s=7a785152723d7a93a2b7ec1c28076e19c2fe27f1

More on how GPU snapshotting works plus additional benchmarks in this blog post:¬†[https://modal.com/blog/gpu-mem-snapshots](https://modal.com/blog/gpu-mem-snapshots)",49,0.92,https://www.reddit.com/r/MachineLearning/comments/1mf8d4g/d_implementing_gpu_snapshotting_to_cut_cold/,False,True,False
1meysr1,AutoModerator,1754060479.0,42,/r/MachineLearning/comments/1meysr1/d_simple_questions_thread/,MachineLearning,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",7,1.0,https://www.reddit.com/r/MachineLearning/comments/1meysr1/d_simple_questions_thread/,False,True,False
1mexyvt,Life-Independence347,1754058520.0,18,/r/MachineLearning/comments/1mexyvt/r_ive_read_the_asiarch_paper_ai_discovered_106/,MachineLearning,[R] I‚Äôve read the ASI‚ÄëArch paper ‚Äî AI discovered 106 novel neural architectures. What do you think?,"I‚Äôve read the ASI‚ÄëArch paper (arxiv.org/abs/2507.18074). It describes an automated AI driven search that discovered 106 novel neural architectures, many outperforming strong human‚Äëdesigned baselines.

What stood out to me is that these weren‚Äôt just small tweaks, some designs combined techniques in ways we don‚Äôt usually try. For example, one of the best architectures fused gating directly inside the token mixer:
(Wmix ¬∑ x) ‚äô œÉ(Wg ¬∑ x)
instead of the usual separate stages for mixing and gating. Feels ‚Äúwrong‚Äù by human design intuition, yet it worked, like an AlphaGo move‚Äë37 moment for architecture search.

One thing I‚Äôd love to see: validation across scale. The search was done at ~20M parameters, with only a few winners sanity‚Äëchecked at 340M. Do these rankings hold at 3B or 30B? If yes, we could explore cheaply and only scale up winners. If not, meaningful discovery might still demand frontier‚Äëlevel budgets.

Curious what others think: will these AI‚Äëdiscovered designs transfer well to larger models, or do we need new searches at every scale?
",73,0.86,https://www.reddit.com/r/MachineLearning/comments/1mexyvt/r_ive_read_the_asiarch_paper_ai_discovered_106/,False,True,False
1meuclu,schmosby420,1754048961.0,3,/r/MachineLearning/comments/1meuclu/d_database_selection_out_of_several_dozens/,MachineLearning,[D] Database selection out of several dozens conflicting schemas for a larger NL2SQL pipeline,"For a natural language to SQL product, I'm designing a scalable approach for database selection across several schemas with high similarity and overlap.

Current approach:
Semantic Search ‚Üí Agentic Reasoning

Created a CSV data asset containing:
Database Description (db summary and intent of que to be routed),  Table descriptions (column names, aliases, etc.), Business or decisions rules


Loaded the CSV into a list of documents and used FAISS to create a vector store from their embeddings

Initialized a retriever to fetch top-k relevant documents based on user query

Applied a prompt-based Chain-of-Thought reasoning on top-k results to select the best-matching DB


Problem:
Despite the effort, I'm getting low accuracy at the first layer itself. Since the datasets and schemas are too semantically similar, the retriever often picks irrelevant or ambiguous matches.

I've gone through a dozen research papers on retrieval, schema linking, and DB routing and still unclear on what actually works in production.

If anyone has worked on real-world DB selection, semantic layers, LLM-driven BI, or multi-schema NLP search, I'd really appreciate either:

A better alternative approach, or

Enhancements or constraints I should add to improve my current stack

Looking for real-world, veteran insight. Happy to share more context or architecture if it helps.",3,1.0,https://www.reddit.com/r/MachineLearning/comments/1meuclu/d_database_selection_out_of_several_dozens/,False,True,False
1mejly0,jshin49,1754012011.0,1,/r/MachineLearning/comments/1mejly0/p_tri70bpreviewsft_open_70b_parameter_llm_for/,MachineLearning,[P] Tri-70B-preview-SFT: Open 70B Parameter LLM for Alignment Research (No RLHF) | Trillion Labs,"Hi r/MachineLearning!

Our startup, Trillion Labs, just released [Tri-70B-preview-SFT](https://huggingface.co/trillionlabs/Tri-70B-preview-SFT), a 70 billion-parameter language model trained on \~1.5T tokens. Due to an unexpected compute crunch, we had to cut short on training tokens and opt for a pure supervised fine-tuning (SFT) approach‚Äîno RLHF.

# Key Highlights:

* **Pure SFT, zero RLHF**: Great baseline model for alignment experiments (RLHF, RLVR, GRPO, CISPO, etc.)
* **32K token context window**, optimized for long-context tasks
* Strong performance benchmarks (\~Qwen-2.5-72B and LLaMA-3.1-70B), but definitely raw and unaligned
* Optimized multilingual capabilities (primarily English, Korean; Japanese support available)
* Introduced new techniques: **FP8 mixed precision, Scalable Softmax, and iRoPE attention**
* Fully open-source on HuggingFace under a permissive commercial license (though experimental!)

We‚Äôre explicitly inviting alignment researchers and NLP enthusiasts to evaluate this model. We'd greatly appreciate feedback on strengths, weaknesses, and especially any alignment issues.

üëâ [Model & Details Here](https://huggingface.co/trillionlabs/Tri-70B-preview-SFT)

Happy to discuss more‚Äîask us anything below!",16,0.9,https://www.reddit.com/r/MachineLearning/comments/1mejly0/p_tri70bpreviewsft_open_70b_parameter_llm_for/,False,True,False
1meggd2,Eaklony,1754003281.0,19,/r/MachineLearning/comments/1meggd2/d_weight_tying_in_llm_seems_to_force_the_last_mlp/,MachineLearning,[D] Weight Tying in LLM Seems to Force the Last MLP to Become the True Unembedding,"The common story about the unembedding layer of a LLM is usually that they predict the next token based on the hidden state of a vector. However, in practice many small models I inspected uses something called weight tying, where the unembedding matrix is just the transpose of the embedding matrix. This effectively just makes it become a similarity search for matching tokens via dot product with token embeddings. This decision seems out of nowhere and didn't make sense to be the natural choice for token unembedding. It appears to me to assume some weird structure of the embedding space in some sense at first.¬† And I didn't find any good explanation online either. So what I did was the following experiment:¬†

1. Take a random small model with weight tying, Llama-3.2-1B in this case. Input some random text and do a forward pass, record what is being added to the residual stream at each layer.¬†
2. Look at the final logit output and check for the top few most likely next tokens, then record their (normalized) token embedding as their direction. At least in the last layer hidden states those direction are meaningful and basically represent how much the model wants the output to be that token.
3. Check which layers contributed most to those directions. I computed each layer's percentage contribution by dotting each layer's output with the above direction vector and divide by total magnitude in that direction.

So for example suppose the input text is just ""Steve"", then the most likely next token is "" Jobs"". I then record the "" Jobs"" token embedding as direction (I also tried normalizing it but it doesn't change the end result), dot it with the final hidden state which gets 18, which is exactly the number in the raw logits. Before the final hidden state there was a RMSNorm which only scale the magnitude but doesn't change the direction. And the pre-norm dot product is about 3. So what I did was dotting the output of each layer with the "" Jobs"" direction, which turns out the final MLP contributed more than 2 out of 3 here where all other MLP and attention layers contribute very small amount and can be seen as the result of some kind of interference most likely.

And it turns out that the final MLP layer consistently contributed to 60%-80% (sometimes as high as 90%) of the magnitude in top output directions after trying many input texts. I also checked the frobenius norm of all down\_proj matrix of all the MLP layers to make sure it's not just the last layer outputting everything large. (All of them are mostly the same)  
  
¬†My conclusion is that the final MLP takes in whatever the real hidden representation of the input text is (concentrated on the last token), and just output the probability distribution of next token directly. And the actual unembedding matrix just acts as a format converter (much like softmax) instead of having any meaningful computation itself. But since they aren't real parameters there, it isn't really wasteful and could indeed be a more efficient way for small models. But functionally speaking doing weight tying seems to just make the last MLP to be true unembedding and you effectively lose one MLP layer worth of computation.

I am not a researcher and am not sure if this is the best place to have this kind of discussion. I would appreciate any opinion on if my method and the result makes sense and what are some good places to discuss things like this.",17,0.88,https://www.reddit.com/r/MachineLearning/comments/1meggd2/d_weight_tying_in_llm_seems_to_force_the_last_mlp/,False,True,False
1meb104,New-Skin-5064,1753990243.0,5,/r/MachineLearning/comments/1meb104/d_how_are_hybrid_reasoning_models_trained/,MachineLearning,[D] How are hybrid reasoning models trained?,"I was wondering how a single model, like Claude 3.7 Sonnet, can have both reasoning and non-reasoning modes. I understand that they likely have opening and closing tokens for the chain of thought, similar to Deepseek and that for the non-reasoning mode they probably add the closing tag automatically, preventing reasoning. How do they train something like this? After all, there is a decent amount of overlap between what you would use a reasoning and non-reasoning model for.",5,0.78,https://www.reddit.com/r/MachineLearning/comments/1meb104/d_how_are_hybrid_reasoning_models_trained/,False,True,False
1mea5g0,Constant_Club_9926,1753988245.0,902,/r/MachineLearning/comments/1mea5g0/d_neurips_2025_rebuttals/,MachineLearning,[D] NeurIPS 2025 rebuttals.,"Rebuttals are slowly getting released to Reviewers. Let's hope Reviewers are responsive and willing to increase these digits.

  
Feel free to share your experience with rebuttal, your expectations, and how it actually goes as the process evolves.",80,0.95,https://www.reddit.com/r/MachineLearning/comments/1mea5g0/d_neurips_2025_rebuttals/,False,True,False
1me6nqu,01kaushikjain01,1753980291.0,3,/r/MachineLearning/comments/1me6nqu/r_seeking_publicly_available_paired_mri/,MachineLearning,[R] Seeking Publicly Available Paired MRI + Genomic/Structured Data for Multimodal ML (Human/Animal/Plant),"I'm working on a multimodal machine learning pipeline that combines image data with structured/genomic-like data for prediction task. I'm looking for publicly available datasets where MRI/Image data and Genomic/Structured data are explicitly paired for the same individual/subject. My ideal scenario would be human cancer (like Glioblastoma Multiforme, where I know TCGA exists), but given recent data access changes (e.g., TCIA policies), I'm open to other domains that fit this multimodal structure:

What I'm looking for (prioritized):

Human Medical Data (e.g., Cancer): MRI/Image: Brain MRI (T1, T1Gd, T2, FLAIR). Genomic: Gene expression, mutations, methylation. Crucial: Data must be for the same patients, linked by ID (like TCGA IDs).

I'm aware of TCGA-GBM via TCIA/GDC, but access to the BraTS-TCGA-GBM imaging seems to be undergoing changes as of July 2025. Any direct links or advice on navigating the updated TCIA/NIH Data Commons policies for this specific type of paired data would be incredibly helpful.

Animal Data:

Image: Animal MRI, X-rays, photos/video frames of animals (e.g., for health monitoring, behavior).

Genomic/Structured: Genetic markers, physiological sensor data (temp, heart rate), behavioral data (activity), environmental data (pen conditions), individual animal ID/metadata.

Crucial: Paired for the same individual animal.

I understand animal MRI+genomics is rare publicly, so I'm also open to other imaging (e.g., photos) combined with structured data.

Plant Data:

Image: Photos of plant leaves/stems/fruits (e.g., disease symptoms, growth).

Structured: Environmental sensor data (temp, humidity, soil pH), plant species/cultivar genetics, agronomic metadata. Crucial: Paired for the same plant specimen/plot.

I'm aware of PlantVillage for images, but seeking datasets that explicitly combine images with structured non-image data per plant.

What I'm NOT looking for:

Datasets with only images or only genomic/structured data.

Datasets where pairing would require significant, unreliable manual matching.

Data that requires extremely complex or exclusive access permissions (unless it's the only viable option and the process is clearly outlined).

Any pointers to specific datasets, data repositories, research groups known for sharing such data, or advice on current access methods for TCGA-linked imaging would be immensely appreciated!

Thank you!",2,0.75,https://www.reddit.com/r/MachineLearning/comments/1me6nqu/r_seeking_publicly_available_paired_mri/,False,True,False
1mdyqaw,Hot_Letter5239,1753960396.0,5,/r/MachineLearning/comments/1mdyqaw/d_how_to_fairly_compare_ai_training_methods_when/,MachineLearning,[D] How to fairly compare AI training methods when they produce different population sizes?,"Hey! I'm working on a conference paper about training AI models and I've hit a tricky experimental design problem that I'd love your input on.

**TL;DR:** I'm comparing two LLM optimization methods that produce final populations of 35 vs 600. How do I fairly measure which works better?

**The Big Picture**

I'm using an evolutionary algorithm that evolves LLM prompts for an objective (persuasiveness vs truthfulness in my case). I'm using a debating tournament to determine the fitness of prompts on a reading comprehension task and then evolve them to be more persuasive/truthful through a mutator.

Evolution implementation:

**Persuasion Training:** Individual debate strategies compete in tournaments. Winners advance, losers get eliminated and replaced with evolved versions.

**Truth Training:** Pairs of strategies work as teams and get scored together (their objective is to ""surface"" the truth in the debate). They win when the judge picks the correct answer (not just when they sound convincing).

Both start with identical seeds: 7 categories of debate strategies (like ""Emotional Appeal,"" ""Authority,"" ""Rationality"") with 5 specific prompts in each category (35 total).

**The Problem**

To run my evolutionary tournaments, for truth optimization, I pair the strategies up with each other, which results in 2 very different population sizes (35 for persuasion vs 595 for truth). In the evolution step, the members of a pair are mutated together (mutator generates A + B prompt).

Now I want to compare which approach produces better results, but how do you fairly compare 35 vs 600 strategies?

Possible Solutions I've thought of:

**- Category Averages**: Compare the average performance of each strategy category (Persuasion optimized Emotional Appeal vs Truth optimized Emotional Appeal, etc.). For truth, I take the average performance of all paired strategies in a particular category. (seems complicated, and I'm not measuring prompts, which I optimized, directly)

**- Top-K Performers:** Compare the top k from each approach (k=20 means 57% of persuasion population vs 3% of truth population - seems unfair?)

**- Kind of Apples-to-Apples**: Make ids for the original strategies and use these to average the truth pair member's performance - effectively mapping performance in pairs back to individual performance. (but does this throws away the core collaborative aspect of truth training?)

**- Something else entirely?**

**My Questions:**

Which comparison method would be most methodologically sound?

Are there established practices for comparing optimization results with different population structures?

Is there a fundamentally better way to frame this comparison that I'm missing?

Any insights would be hugely appreciated!

https://preview.redd.it/q4c0pqr417gf1.png?width=1080&format=png&auto=webp&s=31e93192b2831d4ddf7fda9977fad5bf8c89c9dd

",6,0.8,https://www.reddit.com/r/MachineLearning/comments/1mdyqaw/d_how_to_fairly_compare_ai_training_methods_when/,False,True,False
1mdxnmh,ApartmentEither4838,1753956712.0,6,/r/MachineLearning/comments/1mdxnmh/d_how_to_find_colloborators_to_grow_a_small_result/,MachineLearning,[D] How to find colloborators to grow a small result?,"I‚Äôve made a small but tangible research/prototyping step. I‚Äôm unsure how to pursue the next direction/step. I‚Äôd appreciate advice on next steps and how can I find collaborators who are interested in extending, or co-authoring the same  
Thanks",7,0.71,https://www.reddit.com/r/MachineLearning/comments/1mdxnmh/d_how_to_find_colloborators_to_grow_a_small_result/,False,True,False
1mdtk5y,Downtown_Ambition662,1753940996.0,0,/r/MachineLearning/comments/1mdtk5y/r_how_llms_are_transforming_recommender_systems/,MachineLearning,[R] How LLMs Are Transforming Recommender Systems ‚Äî New Paper,"Just came across this solid new arXiv survey:  
üìÑ¬†**""Harnessing Large Language Models to Overcome Challenges in Recommender Systems""**  
üîó¬†[https://arxiv.org/abs/2507.21117](https://arxiv.org/abs/2507.21117)

Traditional recommender systems use a modular pipeline (candidate generation ‚Üí ranking ‚Üí re-ranking), but these systems hit limitations with:

* Sparse & noisy interaction data
* Cold-start problems
* Shallow personalization
* Weak semantic understanding of content

This paper explores how¬†**LLMs**¬†(like GPT, Claude, PaLM) are redefining the landscape by acting as¬†**unified, language-native models**¬†for:

* üß† Prompt-based retrieval and ranking
* üß© Retrieval-augmented generation (RAG) for personalization
* üí¨ Conversational recommenders
* üöÄ Zero-/few-shot reasoning for cold-start and long-tail scenarios
* And many more....

They also propose a structured taxonomy of LLM-enhanced architectures and analyze trade-offs in¬†**accuracy, real-time performance, and scalability**.

https://preview.redd.it/r97wfum1f5gf1.png?width=950&format=png&auto=webp&s=48cb784526ec81ff1b44318ee894da1fa386201c

  
",0,0.44,https://www.reddit.com/r/MachineLearning/comments/1mdtk5y/r_how_llms_are_transforming_recommender_systems/,False,True,False
1md1u9s,Working_Bunch_9211,1753867064.0,4,/r/MachineLearning/comments/1md1u9s/d_is_there_a_method_as_general_as_mcts_for/,MachineLearning,[D] Is there a method as general as MCTS for imperfect information games?,"As I understand, MCTS had hype when GDM's AlphaX projects succeeded because MCTS+NN combo ended up being a very general method applicable to a lot of perfect information games, its efficiency was proved by the fact that AlphaZero/Lc0 reached very close to Stockfish level in chess.

Do we have something similarly simple yet efficient for IIGs? I don't count CFR and its variants as such because they don't scale to huge games (MCTS+NN does). ReBeL is a new type of beast but it is not very general (I guess) because it requires the developer to decide at which point to do subgame solving.

I also saw IS-MCTS and other determinization approaches but they look very fragile.

Thanks in advance",1,1.0,https://www.reddit.com/r/MachineLearning/comments/1md1u9s/d_is_there_a_method_as_general_as_mcts_for/,False,True,False
1mdv2jg,Mundane_Chemist3457,1753946641.0,32,/r/MachineLearning/comments/1mdv2jg/d_scientific_ml_practically_relevant_or_only_an/,MachineLearning,[D] Scientific ML: practically relevant OR only an academic exploration?,"I am no ML expert, but a master's student in computational science/mechanics with interest in scientific ML. 

There have been several developments since the inception of PINNs and I see many researchers working in this area. The field has at least academically grown, with several maths, computational mechanics, scientific computing and even some computer graphics groups contributing actively to it. 

What I often see is that the applications are made to very academic PDEs and simple geomtrical domains. The recent complexity I saw was physics-informed diffusion of metamaterials or heterogeneous material generation. 

I am not yet sure if this field has got traction in the broader industry with practical applications. Yes, there is Physicsx which has stood out recently. 

I see several challenges, which may have been addressed: 
1) geometrical complexity and domain size limitations due to GPU limits, 
2) generalization of the trained SciML model on new BCs or physical conditions.
3) training bottlenecks: if high fidelity simulation data is required, typically it takes long times to generate a large enough dataset, with practically relevant geomtrical complexity and domain sizes. Even if solver and model are coupled in some way, all that GPU acceleration is moot since most solvers are still CPU based. 
4)  Building trust and adoption in engineering industries, which heavily rely on CPU intensive simulations. 

Given these challenges, does the broader ML community see any relevance of scientific ML beyond academic interests? 

Do you think it is still in a very nascent stage of development? 

Can it grow like the boom of LLMs and Agentic AI? 

Thank you for contributing to the discussion!",55,0.94,https://www.reddit.com/r/MachineLearning/comments/1mdv2jg/d_scientific_ml_practically_relevant_or_only_an/,False,True,False
1mdrf1v,megaton00,1753933745.0,32,/r/MachineLearning/comments/1mdrf1v/r_need_urgent_help_regarding_iccv_submission/,MachineLearning,[R] Need Urgent Help Regarding ICCV Submission,I received the email from OpenReview that CPS has not received my paper submission but in CPS site I already submitted the paper with Copyright. As the email stated my submission status should be 'received' but it is still 'submitted'. Can someone know why this is happening?,8,0.69,https://www.reddit.com/r/MachineLearning/comments/1mdrf1v/r_need_urgent_help_regarding_iccv_submission/,False,True,False
1mdpufm,AutoModerator,1753929032.0,6,/r/MachineLearning/comments/1mdpufm/d_monthly_whos_hiring_and_who_wants_to_be_hired/,MachineLearning,[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&#x200B;

Please remember that this community is geared towards those with experience.",6,0.67,https://www.reddit.com/r/MachineLearning/comments/1mdpufm/d_monthly_whos_hiring_and_who_wants_to_be_hired/,False,True,False
1mdi4ki,berkusantonius,1753908670.0,5,/r/MachineLearning/comments/1mdi4ki/p_fomofaster_objects_more_objects/,MachineLearning,"[P] FOMO(Faster Objects, More Objects)","Hey folks!

I recently implemented the [FOMO model by Edge Impulse](https://docs.edgeimpulse.com/docs/edge-impulse-studio/learning-blocks/object-detection/fomo-object-detection-for-constrained-devices) to make longer training sessions available for free. I trained the model using the Mobilenet 0.35 backbone on the VIRAT dataset. The model is incredibly fast and lightweight, coming in at just 20K parametersüöÄ! You can check out the repository here:  
[https://github.com/bhoke/FOMO](https://github.com/bhoke/FOMO)

While it performs fantastically in terms of speed and efficiency, I‚Äôm currently struggling with a high rate of false positives. If anyone has tips or experience tackling this issue, your advice would be greatly appreciated.

https://i.redd.it/delf5bb5p2gf1.gif

I‚Äôd love to hear your feedback, and all contributions are very welcome. If you find the project interesting or useful, please consider giving it a star‚Äîit really helps improve visibility! ‚≠ê

Thanks in advance for your support and suggestions!",2,0.63,https://www.reddit.com/r/MachineLearning/comments/1mdi4ki/p_fomofaster_objects_more_objects/,False,True,False
1mddg98,LetsTacoooo,1753897943.0,2,/r/MachineLearning/comments/1mddg98/r_deepminds_alphaearth_foundations_helps_map_our/,MachineLearning,[R] Deepmind's AlphaEarth Foundations helps map our planet in unprecedented detail,"Blogpost: [https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/](https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/)  
Paper: [https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/alphaearth-foundations.pdf](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/alphaearth-foundations.pdf)",96,0.97,https://www.reddit.com/r/MachineLearning/comments/1mddg98/r_deepminds_alphaearth_foundations_helps_map_our/,False,True,False
1md4f00,fan_is_ready,1753876030.0,7,/r/MachineLearning/comments/1md4f00/r_has_anyone_experimented_with_using_euclidean/,MachineLearning,[R] Has anyone experimented with using Euclidean distance as a probability function instead of cosine distance?,"I mean this: in the classic setup in order to get probability estimations we calculate softmax of a linear projection, which is calculating cosine distance between predicted vector and weight matrix (plus bias score).

I am intrigued by the following idea: what if we replace cosine distance with Euclidean one as follows:

Instead of calculating 

*cos\_dist = output\_vectors \* weights* 

*unnormalized\_prob = exp(cos\_dist) \* exp(bias)     // lies in (0;+inf) interval*

*normalized\_prob = unnormalized\_prob  / sum(unnormalized\_prob)*

we can calculate

*cos\_dist = output\_vectors \* weights* 

*euc\_dist = l2\_norm(output\_vectors)\^2 - 2 \* cos\_dist + l2\_norm(weights)\^2*

*unnormalized\_prob = abs(bias) / euc\_dist     // lies in (0; +inf) interval*

*normalized\_prob = unnormalized\_prob  / sum(unnormalized\_prob)*

  
The analogy here is gravitational problem, and unnormalized probability is gravitational potential of a single vector from the weights matrix which correspond to a single label. 

I've tried it on a toy problem, but resulting crossentopy was higher than crossentropy with classic formulas, which means it learns worse.

So I wonder if there are any papers which researched this topic?",2,0.53,https://www.reddit.com/r/MachineLearning/comments/1md4f00/r_has_anyone_experimented_with_using_euclidean/,False,True,False
1md3d5f,dhargopala,1753872640.0,3,/r/MachineLearning/comments/1md3d5f/p_a_black_box_llm_explainability_metric/,MachineLearning,[P] A Black Box LLM Explainability Metric,"Hey folks, in one of my maiden attempts to quanitfy the Explainability of Black Box LLMs, we came up with an approach that uses Cosine Similarity as a methodology to compute a word level importance score.
This kindof gives an idea as to how the LLM interprets the input sentence and masking which word causes the maximum amount of deviation in the output.
This method involves several LLM calls to be made, and it's far from perfect but I got some interesting observations from this approach and just wanted to share with the community.

This is more of a quantitative study of this Appraoch.

The metric is called ""XPLAIN"" and I also got some time to create a starter GitHub repo for the same.

Do check it out if you find this interesting:

Code: https://github.com/dhargopala/xplain

Paper: https://www.tdcommons.org/dpubs_series/8273/
",2,0.57,https://www.reddit.com/r/MachineLearning/comments/1md3d5f/p_a_black_box_llm_explainability_metric/,False,True,False
1mctmau,FallMindless3563,1753839016.0,7,/r/MachineLearning/comments/1mctmau/p_finetuning_a_fast_local_tab_tab_code_completion/,MachineLearning,"[P] Fine-tuning a fast, local ‚Äútab tab‚Äù code completion model for Marimo notebooks","In the spirit of building in public, we're collaborating with¬†Marimo¬†to build a¬†""tab completion"" model¬†for their notebook cells, and we wanted to share our progress as we go in tutorial form.

Here‚Äôs the first post in what will be a series:
https://www.oxen.ai/blog/building-a-tab-tab-code-completion-model

The goal is to create a local, open-source model that provides a¬†Cursor-like¬†code-completion experience directly in notebook cells. You'll be able to download the weights and run it locally with¬†Ollama¬†or access it through a free API we provide.

We‚Äôre already seeing promising results by fine-tuning the¬†Qwen¬†and¬†Llama¬†models, but there‚Äôs still more work to do. Here's a leaderboard on a corrupted MBPP dataset with the models we've tried so far. All fine-tuned models have funky code names in parenthesis. Promising to see the early experiments getting to GPT-4 level.

Accuracy -> Model

82.60% -> Claude 4 Sonnet

80.60% -> Qwen3 Coder 480B

78.80% -> Kimi-2

74.40% -> Llama 4 Maverick

74.40% -> GPT 4o

73.00% -> GPT 4.1

68.60% -> Qwen 3 - 4B (acute-chocolate-anteater)

68.00% -> Llama 4 Scout

61.80% -> Qwen 3 - 1.7B (ordinary-red-cow)

60.20% -> GPT 4o Mini

52.80% -> Llama 3.2 - 3B (awful-crimson-salamander)

50.80% -> Llama 3.1 - 8B (sufficient-tan-alligator)

47.80% -> Qwen 3 - 0.6B (continental-blush-guppy)

36.00% -> Llama 3.2 - 1B (successful-amaranth-raven)

If you‚Äôre interested in contributing to data collection or the project in general, let us know! We already have a working¬†CodeMirror plugin¬†and are focused on improving the model‚Äôs accuracy over the coming weeks.

",11,0.92,https://www.reddit.com/r/MachineLearning/comments/1mctmau/p_finetuning_a_fast_local_tab_tab_code_completion/,False,True,False
1mcsa9j,EternaI_Sorrow,1753835242.0,21,/r/MachineLearning/comments/1mcsa9j/d_math_book_recommendations_for_nn_theory/,MachineLearning,[D] Math book recommendations for NN theory,"I'm a PhD student interested in neural network architecture design, who recently ran into a growing level of rigor in the field and found out that his CS major math background is not enough. In particular, I was working primarily with sequence processing networks (Transformers and RNNs) with an aim to reduce their computational complexity or find inefficient representations. I would like to continue the work but to guide it with a theory instead of intuition, and as reference papers I'd cite Albert Gu's papers on¬†[SSM¬†](https://arxiv.org/pdf/2111.00396)and¬†[HiPPO](https://arxiv.org/abs/2008.07669)¬†and Chulhee Yun's works, for example like¬†[this](https://arxiv.org/abs/1912.10077)¬†and¬†[this](https://arxiv.org/abs/2006.04862).

Currently I'm finishing the Rudin's ""Real and Complex Analysis"" first half on real analysis. I'm also quite sure that Horn's ""Matrix Analysis"" and Trefethen's ""Approximation Theory and Approximation Practice"" will be useful, but I struggle to decide how much and which analysis sources I need to study after (Complex analysis chapters? Rudin's and Kreyszig's FA?). I feel that I haven't reached the level to study from papers yet, although earlier works like [this](https://web.njit.edu/~usman/courses/cs677/10.1.1.441.7873.pdf) seem to be accessible after I'm done with RCA.

I would like to ask for some guidance about which math literature might be useful in the given context after I finish the real analysis chapters from RCA. I have found ""understanding level"" lit recommendations quite abundant, but ""research level"" much less addressed overall, so I hope it will be useful not only for me.",61,0.91,https://www.reddit.com/r/MachineLearning/comments/1mcsa9j/d_math_book_recommendations_for_nn_theory/,False,True,False
1mcff31,Pure_Landscape8863,1753805021.0,26,/r/MachineLearning/comments/1mcff31/r_are_aucroc_curves_black_box_metrics/,MachineLearning,"[R] Are AUC/ROC curves ""black box"" metrics?","Hey guys! (My first post here, pls be kind hehe)

I am a PhD student (relatively new to AI) working with ML models for a multi-class classification task. Since I ruled out accuracy as the evaluation metric given a class imbalance in my data (accuracy paradox), I stuck to AUC and plotting ROC curves (as a few papers told they are good for imbalanced train sets)  to evaluate a random forest model's performance ( 10-fold cross validated) trained on an imbalanced dataset and tested on an independent dataset. I did try SMOTE to work on the imbalance, but it didn't seem to help my case as there's a major overlap in the distribution of the data instances in each of the classes I have (CLA,LCA,DN) and the synthetic samples generated were just random noise instead of being representative of the minority class. Recently, when I was trying to pull the class predictions by the model, I have noticed one of the classes( DN) having 0 instances classified under it. But the corresponding ROC curve and AUC said otherwise. Given my oversight, I thought DN shined ( High AUC compared to other classes ) given it just had a few samples in the test set, but it wasn't the case with LCA (which had fewer samples). Then I went down the rabbit hole of what ROC and AUC actually meant. This is what I thought and would like more insight on what you guys think and what can it mean, which could direct my next steps.

The model's assigning higher probability scores to true DN samples than non-DN samples (CLA and LCA), Hence, masked good ROC curve and high AUC scores, but when it comes to the model's predictions, the probabilities aren't able to pass the threshold selected. Is this is a right interpretation? If so, I thought of these steps:

\- Set threshold manually by having a look at the distribution of the probabilities ( which I am still skeptical about)

\- Probably ditch ROC and AUC as the evaluation metrics in this case (I have been lying to myself this whole time!)

If you think I am a bit off about what's happening, your insights would really help, thank you so much! 

",4,0.59,https://www.reddit.com/r/MachineLearning/comments/1mcff31/r_are_aucroc_curves_black_box_metrics/,False,True,False
1mcf1kl,LetsTacoooo,1753804178.0,19,/r/MachineLearning/comments/1mcf1kl/d_new_recent_and_applied_ideas_for_representation/,MachineLearning,"[D] New recent and applied ideas for representation learning? (i.g. Matryoshka, Constrastive learning, etc.)","I am exploring ideas for building domain specific representations (science problems). I really like the idea of [Matryoshka learning](https://arxiv.org/html/2505.23337v1) since it gives you ""PCA""-like natural ordering to dimensions.

Contrastive learning is also a very common tool know for building representations since it makes your embeddings more ""distance aware"".

What are new neural network ""tricks"" that have come out in the last 2-3 years for building better representations. Thinking broadly in terms of unsupervised and supervised learning problems. Not necessarily transformer models.",38,0.91,https://www.reddit.com/r/MachineLearning/comments/1mcf1kl/d_new_recent_and_applied_ideas_for_representation/,False,True,False
1mc094y,Southern-Whereas3911,1753757648.0,3,/r/MachineLearning/comments/1mc094y/p_standalone_implementation_of_deepseeks_native/,MachineLearning,[P] Stand-alone implementation of DeepSeek's Native Sparse Attention in PyTorch,"NSA is an interesting architectural choice, reduces both the complexity while matching or even surpassing full attention benchmarks as well.

I went around looking inside it to try and grab my head around things, most of the implementations were packed with Triton kernels for performance, so I built this naive implementation of Native Sparse Attention in pure PyTorch with

* GroupedMLP/Convolution1d/AvgPooling for token compression
* Gating mechanism for combining different branches of the network
* Drop-in replacement functionality to standard Attention block

Check it out here:¬†[native\_sparse\_attention](https://github.com/shreyashkar-ml/native_sparse_attention)",7,1.0,https://www.reddit.com/r/MachineLearning/comments/1mc094y/p_standalone_implementation_of_deepseeks_native/,False,True,False
1mc8pn4,Adrienkgz,1753787942.0,14,/r/MachineLearning/comments/1mc8pn4/d_first_research_project_feedback_on_ano_a_new/,MachineLearning,"[D] First research project ‚Äì feedback on ""Ano"", a new optimizer designed for noisy deep RL (also looking for arXiv endorsement)","Hi everyone,

I'm a student and independent researcher currently exploring optimization in Deep Reinforcement Learning. I recently finished my first preprint and would love to get feedback from the community, both on the method and the clarity of the writing.

The optimizer I propose is called Ano. The key idea is to decouple the magnitude of the gradient from the direction of the momentum. This aims to make training more stable and faster in noisy or highly non-convex environments, which are common in deep RL settings.

üìù Preprint + source code: [https://zenodo.org/records/16422081](https://zenodo.org/records/16422081)

üì¶ Install via pip: \`pip install ano-optimizer\`

üîó GitHub: [https://github.com/Adrienkgz/ano-experiments](https://github.com/Adrienkgz/ano-experiments)

This is my first real research contribution, and I know it's far from perfect, so I‚Äôd greatly appreciate any feedback, suggestions, or constructive criticism.

I'd also like to make the preprint available on arXiv, but as I‚Äôm not affiliated with an institution, I can‚Äôt submit without an endorsement. If anyone feels comfortable endorsing it after reviewing the paper, it would mean a lot (no pressure, of course, I fully understand if not).

Thanks for reading and helping out üôè

Adrien",30,0.81,https://www.reddit.com/r/MachineLearning/comments/1mc8pn4/d_first_research_project_feedback_on_ano_a_new/,False,True,False
1mc664h,nai_alla,1753778537.0,0,/r/MachineLearning/comments/1mc664h/r_multiview_contrastive_learning_principled/,MachineLearning,[R] Multi-View Contrastive Learning: Principled Framework for 3+ Views and Modalities,"**TL;DR**: Current SSL methods like SwAV, DINO, and VICRegL use multiple views but handle them suboptimally by aggregating pairwise losses, causing conflicting objectives and missed interactions. We introduce MV-InfoNCE and MV-DHEL - principled objectives that scale properly with any number of views and prevent dimensionality collapse.

**Paper**: [https://arxiv.org/abs/2507.06979](https://arxiv.org/abs/2507.06979)

**Code**: [https://github.com/pakoromilas/Multi-View-CL](https://github.com/pakoromilas/Multi-View-CL)  


**The Problem**

Current SSL methods create multiple augmented views but handle them through pairwise loss aggregation:

    L_total = L(v1,v2) + L(v1,v3) + L(v1,v4) + L(v2,v3) + L(v2,v4) + L(v3,v4)

This approach causes:

* **Conflicting objectives**: Each view satisfies multiple competing loss terms
* **Ignored view relationships**: Pairwise aggregation misses view interactions among all views
* **Fundamental limitations**: Inherits problems (e.g. alignment-uniformity coupling) from pairwise CL losses
* **Limited transfer**: Multi-view benefits diminish as you add more views

**The CLIP Problem**: While CLIP revolutionized vision-language learning, extending it to 3+ modalities is still not straightforward. CLIP's contrastive framework is inherently pairwise - adding audio, video, or sensor data requires either separate pairwise models or naive aggregation, both of which fail to capture all multimodal interactions concurrently.

**Our Loss Functions**

1. **MV-InfoNCE**: Extends InfoNCE to N views properly
2. **MV-DHEL**: Decouples alignment from uniformity

**Key Results**

‚úÖ¬†**Scale properly**¬†with number of views

‚úÖ¬†**Prevent dimensionality collapse**¬†when using 5+ views (figure below)

‚úÖ¬†**Outperform existing**¬†multi-view approaches on ImageNet1K and three other datasets

‚úÖ¬†**Extend to 3+ modalities**¬†(not just 2!)

https://preview.redd.it/vib4lluozrff1.png?width=1200&format=png&auto=webp&s=9c0daafe65e74c8a24bca93f2343d3c17a1767f2

**Overall Contributions**

* **Principled Multi-View Formulation**: Mathematical framework that properly extends CL from pairwise to multi-view settings, modeling simultaneous interactions between all N views rather than aggregating pairwise comparisons
* **Novel Loss Functions**: (i) MV-InfoNCE - natural extension of InfoNCE incorporating all view interactions, (ii) MV-DHEL - decouples alignment from uniformity across views
* **Theoretical Guarantees**: Proved both objectives share asymptotic behavior with traditional InfoNCE, establishing them as theoretically sound extensions
* **Empirical Advances**: Consistently outperform existing approaches, effectively scale with view multiplicity, mitigate dimensionality collapse with sufficient views
* **Multimodal Applicability**: Unlike existing methods designed for bimodal settings, directly applicable to 3+ modalities



**Possible Applications**

* **Beyond CLIP**: Multimodal learning with vision + text + audio + sensor data
* **Video Understanding**: Temporal + spatial + semantic views in unified framework
* **Medical Imaging**: Multiple scan types (CT, MRI, X-ray) without pairwise limitations
* **Robotics**: Vision + tactile + proprioceptive sensing with theoretical guarantees



The GitHub repo includes PyTorch implementations.

Happy to discuss about our research!",9,1.0,https://www.reddit.com/r/MachineLearning/comments/1mc664h/r_multiview_contrastive_learning_principled/,False,True,False
1mc5jdg,i_minus,1753776010.0,25,/r/MachineLearning/comments/1mc5jdg/d_aaai2026_code_submission/,MachineLearning,[D] AAAI-2026 Code Submission,"Hello\~\~

I am just wondering how much importance code submission has for the decision making and review. and are you all submitting the codes? or it is fine if we release it if/after acceptance. My code is so messy so m in dilemma",7,0.9,https://www.reddit.com/r/MachineLearning/comments/1mc5jdg/d_aaai2026_code_submission/,False,True,False
1mc2lk8,playa_aikido,1753765057.0,0,/r/MachineLearning/comments/1mc2lk8/r_introducing_snacdb_a_new_opensource_resource/,MachineLearning,[R] Introducing SNAC-DB: A New Open-Source Resource for Antibody & NANOBODY¬Æ VHH‚ÄìAntigen Modeling,"Predicting antibody and NANOBODY¬Æ VHH‚Äìantigen complexes remain a notable gap in current AI models, limiting their utility in drug discovery. We present **SNAC-DB**, a machine-learning-ready database and pipeline developed by structural biologists and ML researchers to address this challenge.

Key features of SNAC-DB include:

¬∑¬†¬†¬†¬†¬†¬† **Expanded Coverage:** 32 % more structural diversity than SAbDab, capturing overlooked assemblies such as antibodies/nanobodies as antigens, complete multi-chain epitopes, and weak CDR crystal contacts.

¬∑¬†¬†¬†¬†¬†¬† **ML-Friendly Data:** Cleaned PDB/mmCIF files, atom37 NumPy arrays, and unified CSV metadata to eliminate preprocessing hurdles.

¬∑¬†¬†¬†¬†¬†¬† **Transparent Redundancy Control:** Multi-threshold Foldseek clustering for principled sample weighting, ensuring every experimental structure contributes.

¬∑¬†¬†¬†¬†¬†¬† **Rigorous Benchmark:** An out-of-sample test set comprising public PDB entries post‚ÄìMay 30, 2024 (disclosed) and confidential therapeutic complexes.

Using this benchmark, we evaluated six leading models (AlphaFold2.3‚Äêmultimer, Boltz-2, Boltz-1x, Chai-1, DiffDock-PP, GeoDock) and found that success rates rarely exceed 25 %, built-in confidence metrics and ranking often misprioritize predictions, and all struggle with novel targets and binding poses.

We presented this work at the Forty-Second International Conference on Machine Learning (ICML 2025) Workshop on DataWorld: Unifying Data Curation Frameworks Across Domains (https://dataworldicml2025.github.io/) in Vancouver.

¬∑¬†¬†¬†¬†¬†¬† **Paper:** [https://www.researchgate.net/publication/393900649\_SNAC-DB\_The\_Hitchhiker's\_Guide\_to\_Building\_Better\_Predictive\_Models\_of\_Antibody\_NANOBODY\_R\_VHH-Antigen\_Complexes /](https://www.researchgate.net/publication/393900649_SNAC-DB_The_Hitchhiker's_Guide_to_Building_Better_Predictive_Models_of_Antibody_NANOBODY_R_VHH-Antigen_Complexes%20/) [https://openreview.net/forum?id=68DcIpDaHK](https://openreview.net/forum?id=68DcIpDaHK)

¬∑¬†¬†¬†¬†¬†¬† **Dataset:** [https://zenodo.org/records/16226208](https://zenodo.org/records/16226208)

¬∑¬†¬†¬†¬†¬†¬† **Code:** [https://github.com/Sanofi-Public/SNAC-DB](https://github.com/Sanofi-Public/SNAC-DB)

We hope SNAC-DB will accelerate the development and evaluation of more accurate models for antibody complex prediction

https://preview.redd.it/a0d42seuvqff1.png?width=3456&format=png&auto=webp&s=e38ea120357174191b8b5cbb707979cde0ff498a",2,0.67,https://www.reddit.com/r/MachineLearning/comments/1mc2lk8/r_introducing_snacdb_a_new_opensource_resource/,False,True,False
1mbz5fk,Ordinary_Pineapple27,1753754419.0,2,/r/MachineLearning/comments/1mbz5fk/p_keyword_and_phrase_embedding_for_query_expansion/,MachineLearning,[P] Keyword and Phrase Embedding for Query Expansion,"Hey folks, I am workig on a database search system. The language of text data is Korean. Currently, the system does BM25 search which is limited to keyword search. There could be three scenarios:

1. User enters a single keyword such as ""coronavirus""
2. User enters a phrase such as ""machine learning"", ""heart disease""
3. User enters a whole sentence such as ""What are the symptoms of Covid19?""

To increase the quality and the number of retireved results, I am planning to employ query expansion through embedding models. I know there are context-insensitive static embedding models such as Wor2Vec or GloVe and context-sensitive models such as BERT, SBERT, ELMO, etc.

For a single word query expansion, static models like Word2Vec works fine, but it cannot handle out-of-vocabulary issue. FastText addresses this issue by n-gram method. But when I tried both, FastText put more focus on the morphologic form of words rather than semantic. BERT would be a better option with its WordPiece tokenizer, but when there is no context in a single-word query, I am afraid it will not help much.

For sentence query cases, SBERT works much better than BERT according to the SBERT paper. For Phrases, I am not sure what method to use although I know that I can extract single vector for the phrase through averaging the vectors for individual word (in case of static methods) or word-pieces in case of BERT model application.

What is the right way to proceed these scenarios and how to measure which model is performing better. I have a lot of domain text unlabeled. Also If I decide to use BERT or SBERT, how should I design the system? Should I train the model on unlabeled data using Masked Language Modeling method and will it be enough?

Any ideas are welcome.",2,1.0,https://www.reddit.com/r/MachineLearning/comments/1mbz5fk/p_keyword_and_phrase_embedding_for_query_expansion/,False,True,False
1mbylje,Secret_Valuable_Yes,1753752814.0,2,/r/MachineLearning/comments/1mbylje/p_qlora_with_huggingface_model/,MachineLearning,[P] QLora with HuggingFace Model,"I am finetuning a hugging face LLM in a pytorch training loop using 4-bit quantization and LoRA. The training got through a few batches before hitting the error:

`RuntimeError: one of the variables needed for gradient computation has been modified by an inlace operation: [torch.cuda.HalfTensor[1152,262144], which is output 0 of AsStrideBackward0, is at version 30; expected version 28 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).`

Even if I knew the exact computation causing this, I'm using an open source LLM out of the box, not sure the proper way to go in and modify layers, etc. . I'm also not sure why I could get past a few batches without this error and then it happens. I was getting OOM error originally and then I shortened some of the sequence lengths. It does look like this error is also happening on a relatively long sequence length, but not sure that has anything to do with it. Does anyone have any suggestions here?",1,1.0,https://www.reddit.com/r/MachineLearning/comments/1mbylje/p_qlora_with_huggingface_model/,False,True,False
1mbycac,TangyKiwi65,1753752095.0,4,/r/MachineLearning/comments/1mbycac/p_bluffmind_pure_llm_powered_card_game_w_tts_and/,MachineLearning,[P] BluffMind: Pure LLM powered card game w/ TTS and live dashboard,"Introducing BluffMind, a LLM powered card game with live text-to-speech voice lines and dashboard involving a dealer and 4 players. The dealer is an agent, directing the game through tool calls, while each player operates with their own LLM, determining what cards to play and what to say to taunt other players. Check out the repository¬†[here](https://github.com/TangyKiwi/BluffMind), and feel free to open an issue or leave comments and suggestions to improve the project!",25,0.82,https://www.reddit.com/gallery/1mbycac,False,False,False
1mbsqzo,prassi89,1753737599.0,0,/r/MachineLearning/comments/1mbsqzo/p_built_a_modern_cookiecutter_for_ml_projects/,MachineLearning,[P] Built a modern cookiecutter for ML projects - Lets make it better,"I got fed up with spending the first 3 hours of every ML project fighting dependencies and copy-pasting config files, so I made this cookiecutter template: [https://github.com/prassanna-ravishankar/cookiecutter-modern-ml](https://github.com/prassanna-ravishankar/cookiecutter-modern-ml)

It covers NLP, Speech (Whisper ASR + CSM TTS), and Vision with what I think are reasonable defaults. Uses uv for deps, pydantic-settings for config management, taskipy for running tasks. Detects your device (Mac MPS/CUDA/CPU), includes experiment tracking with Tracelet. Training support with Skypilot, serving with LitServe and integrated with accelerate and transformers. Superrrr opinionated.

I've only tested it on my own projects. I'm sure there are edge cases I missed, dependencies that conflict on different systems, or just dumb assumptions I made.

If you have 5 minutes, would love if you could:

* Try generating a project in your domain
* See if the dependencies actually install cleanly
* Check if uv run task train works (even on dummy data)
* Tell me what breaks or feels wrong

I built this because I was annoyed, not because I'm some template expert. Probably made mistakes that are obvious to fresh eyes. GitHub issues welcome, or just roast it in the comments ü§∑‚Äç‚ôÇÔ∏è",0,0.4,https://www.reddit.com/r/MachineLearning/comments/1mbsqzo/p_built_a_modern_cookiecutter_for_ml_projects/,False,True,False
1mbmr42,BSmithA92,1753724153.0,3,/r/MachineLearning/comments/1mbmr42/d_pattern_recognition_is_not_intelligence_just_an/,MachineLearning,"[D] Pattern recognition is not intelligence, just an important part of the structure","Hi everyone, I‚Äôve been doing enterprise ai integration for the last year or so, and I think I‚Äôm the only person currently applying reactor control theory to llm orchestration.

To me, current industry efforts aren‚Äôt trying to make AI, they‚Äôre trying to make omnipotence. Very different.

Let‚Äôs imagine Einstein with no memory or gobel who couldn‚Äôt tell you why. Sounds ridiculous.

What I‚Äôve been doing is applying transformers as dynamic parts of a larger system. And I‚Äôve been seeing incredible results.

Give the llm memory, guidance, and structure, and suddenly hallucinations are not a big deal. I wouldn‚Äôt expect a person to think about the same thing, the same way, every time, so why expect an AI to?

Once you start shaping the structure, and allowing the drift, you can collapse reasoning into lookups.

First concept: Radiology scans.

https://youtu.be/JaNtSkDX1I0?si=sAvQJIHjsuLtnGDx

This collapses llm api calls from 30 to 5 for repeated queries.

Next concept: robotics.

It seems like with a little capital and a little execution, there‚Äôs asymmetric upside here. Looking to see if there‚Äôs anyone else experimenting in this direction.",0,0.18,https://www.reddit.com/gallery/1mbmr42,False,False,False
1mbk6y6,Significant_Course12,1753718562.0,2,/r/MachineLearning/comments/1mbk6y6/d_emnlp_2025_track_selection/,MachineLearning,[D] EMNLP 2025 Track Selection,"
1) Is it okay/possible (and how is it perceived) to change the main track selection from ARR review to EMNLP conference submission?

2) Can it increase/decrease chances of getting the paper in?",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1mbk6y6/d_emnlp_2025_track_selection/,False,True,False
1mbiv33,Dismal_Table5186,1753715593.0,48,/r/MachineLearning/comments/1mbiv33/d_shifting_research_directions_which_deep/,MachineLearning,[D] Shifting Research Directions: Which Deep Learning Domains Will Be Most Impactful in the Next 5‚Äì6 Years?,"I‚Äôm looking for some advice on which research domains in deep learning/computer vision might be exciting and impactful over the next 5‚Äì6 years.

For context; I‚Äôve been working in medical image segmentation for the last 3‚Äì4 years. While it‚Äôs been rewarding, I feel like I‚Äôve been a bit cut off from the broader progress in deep learning. I‚Äôve used modern methods like diffusion models and transformers as baselines, but I haven‚Äôt had the time to dive deep into them because of the demands of my PhD. Now that most of my dissertation work is done, I still have about a year and a half of funding left, and I‚Äôd like to use this time to explore new directions.

A few areas I‚Äôve considered:

* **Semi-supervised learning**, which occasionally produces some very impactful work in vision. That said, it feels somewhat saturated, and I get the sense that fundamental contributions in this space often require heavy GPU resources.  
* **3D medical imaging**; which seems to be gaining traction, but is still tied closely to the medical domain.  
* **Diffusion and foundational models**; definitely among the most hyped right now. But I wonder if diffusion is a bit overrated; training is resource-intensive, and the cutting-edge applications (like video generation or multimodal foundational diffusion models) may be tough to catch up with unless you‚Äôre in a big lab or industry. Do you think diffusion will still dominate in 5 years, or will a new class of generative models take over?  
* **Multimodal deep learning**; combining text+images or text+video feels less over-hyped compared to diffusion, but possibly more fertile for impactful research.  

My interest is in computer vision and deep learning more broadly; I‚Äôd prefer to work on problems where contributions can still be meaningful without requiring massive industry-level resources. Ideally, I‚Äôd like to apply foundational or generative models to downstream tasks rather than just training them from scratch/only focusing on them.

So my question is: given the current trends, which areas do you think are worth investing in for the next 5‚Äì6 years? Do you see diffusion and foundational models continuing to dominate, or will multimodal and other directions become more promising? Would love to hear diverse opinions and maybe even personal experiences if you‚Äôve recently switched research areas. I‚Äôm interested in shifting my research into a more explorative mode, while still staying somewhat connected to the medical domain instead of moving entirely into general computer vision.",36,0.73,https://www.reddit.com/r/MachineLearning/comments/1mbiv33/d_shifting_research_directions_which_deep/,False,True,False
1mbgmno,Constant_Club_9926,1753710428.0,2,/r/MachineLearning/comments/1mbgmno/p_ambientutils_a_small_python_package_for/,MachineLearning,"[P]: `ambient-utils`: A small python package for training diffusion models with ""bad data"".","Made this small python package for training diffusion generative models with ""bad data"":

[https://github.com/giannisdaras/ambient-utils](https://github.com/giannisdaras/ambient-utils)

Install with: \`pip install ambient-utils\`

The idea is that ""bad data"" is only used to train denoisers for \*some\* diffusion times, but not all. There are some easy wrappers that enable this (\`AmbientSampler\` class) and a README with a quick example.

I have been using versions of this codebase for my research for the past 2 years, and it is the primary driver for more than 6 accepted papers to NeurIPS, ICML, and ICLR. I decided to make it open-source so that people can play with it.

If you are dealing with bad data in scientific applications, Computer Vision, robotics or elsewhere, please comment below and give it a try!",4,0.75,https://www.reddit.com/r/MachineLearning/comments/1mbgmno/p_ambientutils_a_small_python_package_for/,False,True,False
1mbevi4,penicillinpeter,1753705830.0,1,/r/MachineLearning/comments/1mbevi4/r_misuse_of_ml_for_a_cortical_pain_biomarker/,MachineLearning,[R] Misuse of ML for a cortical pain biomarker?,"This comment in¬†*JAMA Neurology* raises several methodological concerns about a previously published ""ML""-based pain biomarker.  


The critique points out two core issues:  


* An incorrect validation set
* An unrepresentative test set

Additionally, the original model was based on only¬†**two input features**¬†(one binary), yet neural networks or gradient boosting were applied. To me, that raises the question of whether such model complexity is appropriate for this data scale and structure, no?  


Are there other plausible reasons why the reanalysis would yield an AUC of¬†**0.65**, compared to the reported¬†**1.0 (validation)**¬†and¬†**0.88 (test)**‚Äîbeyond what the authors describe?  


The full comment can be found in¬†*JAMA Neurology (2025):*¬†[https://jamanetwork.com/journals/jamaneurology/fullarticle/2836397](https://jamanetwork.com/journals/jamaneurology/fullarticle/2836397).  


Whats your opinion on it?",8,0.83,https://www.reddit.com/r/MachineLearning/comments/1mbevi4/r_misuse_of_ml_for_a_cortical_pain_biomarker/,False,True,False
1mbdw3q,eoghank,1753702910.0,4,/r/MachineLearning/comments/1mbdw3q/state_of_the_art_sisr_r/,MachineLearning,State of the Art SISR [R],"I'm investigating state-of-the-art techniques for extreme single-image super-resolution (SISR), specifically targeting high magnification factors up to 100x. My focus is on domain-specific texture synthesis for materials, trained on a curated dataset. I'm exploring the feasibility of fine-tuning generative models like ESRGAN and am particularly interested in methods for conditional generation, where semantic guidance (e.g., material property tags like 'shiny' or 'rough') can be used to steer the output. Would anyone have recommendations on relevant literature, model architectures, or even alternative approaches?",7,0.89,https://www.reddit.com/r/MachineLearning/comments/1mbdw3q/state_of_the_art_sisr_r/,False,True,False
1mb92y6,Lost-Ingenuity5017,1753685052.0,9,/r/MachineLearning/comments/1mb92y6/d_aaai_not_able_to_update_authors/,MachineLearning,[D] AAAI: Not able to update authors,"I am trying to submit a paper to AAAI. Even though the modificiation guidelines say that I can edit authors (https://aaai.org/conference/aaai/aaai-26/paper-modification-guidelines/). I am not able to add an author to the paper.  
Anyone facing the same issue? Or any chairs from AAAI can help with this?

Text from the guidelines:  
""After the July 25 abstract deadline and until the August 1 paper submission deadline, the following items can be changed

* list of authors
* author order
* submitted paper"".",8,0.68,https://www.reddit.com/r/MachineLearning/comments/1mb92y6/d_aaai_not_able_to_update_authors/,False,True,False
1mb8e5w,LakshyAAAgrawal,1753682461.0,19,/r/MachineLearning/comments/1mb8e5w/250719457_gepa_reflective_prompt_evolution_can/,MachineLearning,[2507.19457] GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning,,45,0.92,https://arxiv.org/abs/2507.19457,False,False,False
1mb5vor,vwibrasivat,1753673824.0,15,/r/MachineLearning/comments/1mb5vor/r_sapient_hierarchical_reasoning_model_hrm/,MachineLearning,[R] Sapient Hierarchical Reasoning Model. HRM.,,0,0.45,https://arxiv.org/abs/2506.21734,False,False,False
1mao7d7,AgeOfEmpires4AOE4,1753627829.0,7,/r/MachineLearning/comments/1mao7d7/p_ai_learns_to_play_metal_slug_deep_reinforcement/,MachineLearning,[P] AI Learns to Play Metal Slug (Deep Reinforcement Learning) With Stable-R...,"Github: [https://github.com/paulo101977/MetalSlugPPO](https://github.com/paulo101977/MetalSlugPPO)  
  
Hey everyone! I recently trained a reinforcement learning agent to play the arcade classic *Metal Slug* using **Stable-Baselines3 (PPO)** and **Stable-Retro**.

The agent receives pixel-based observations and was trained specifically on **Mission 1**, where it faced a surprisingly tough challenge: dodging missiles from a non-boss helicopter. Despite it not being a boss, this enemy became a consistent bottleneck during training due to the agent‚Äôs tendency to stay directly under it without learning to evade the projectiles effectively.

After many episodes, the agent started to show decent policy learning ‚Äî especially in prioritizing movement and avoiding close-range enemies. I also let it explore Mission 2 as a generalization test (bonus at the end of the video).

The goal was to explore how well PPO handles sparse and delayed rewards in a fast-paced, chaotic environment with hard-to-learn survival strategies.

Would love to hear your thoughts on training stability, reward shaping, or suggestions for curriculum learning in retro games!",12,0.83,https://youtube.com/watch?v=7fwWGFRgc1I&si=qOre2i2_ek0tpei2,False,False,False
1mann7w,ashz8888,1753626416.0,0,/r/MachineLearning/comments/1mann7w/p_reinforcement_learning_from_human_feedback_rlhf/,MachineLearning,[P] Reinforcement Learning from Human Feedback (RLHF) in Notebooks,,8,0.9,https://github.com/ash80/RLHF_in_notebooks,False,False,False
1mal1rj,sf1104,1753619290.0,4,/r/MachineLearning/comments/1mal1rj/p_aifailsafeoverlay_formal_alignment_recovery/,MachineLearning,"[P] AI-Failsafe-Overlay ‚Äì Formal alignment recovery framework (misalignment gates, audit locks, recursion filters)","This is a first-pass release of a logic-gated failsafe protocol to handle misalignment in recursive or high-capacity AI systems.

The framework defines:

* Structural admission filters
* Audit-triggered lockdowns
* Persistence-boundary constraints

It‚Äôs outcome-agnostic ‚Äî designed to detect structural misalignment even if external behavior looks ‚Äúsafe.‚Äù

GitHub repo: [AI-Failsafe-Overlay](https://github.com/oxey1978/AI-Failsafe-Overlay)

Looking for feedback or critique from a systems, logic, or alignment theory lens.",0,0.25,https://www.reddit.com/r/MachineLearning/comments/1mal1rj/p_aifailsafeoverlay_formal_alignment_recovery/,False,True,False
1maj150,Ok_Rub1689,1753612328.0,5,/r/MachineLearning/comments/1maj150/p_i_tried_implementing_the_crisp_paper_from/,MachineLearning,[P] I tried implementing the CRISP paper from Google Deepmind in Python,"I spent the weekend analyzing this open-source PyTorch implementation of Google's [CRISP paper (arXiv:2505.11471)](https://arxiv.org/pdf/2505.11471). The repository provides a direct, hands-on comparison between CRISP's in-training clustering and the more traditional post-hoc approach.

For context, the core problem with multi-vector models (e.g., ColBERT) is their massive index size. The common solution is to cluster embeddings *after* training (post-hoc), but this is an imperfect patch. CRISP argues for integrating clustering *during* training to force the model to learn inherently ""clusterable"" representations.

The repository sets up a clean head-to-head experiment to test that claim. Here's a breakdown of the results from its built-in pipeline.

[https://github.com/sigridjineth/crisp-py](https://github.com/sigridjineth/crisp-py)

I tried few experiments with minilm-l6-v2 in Macbook Pro and found that CRISP-tuned model assigns a significantly higher similarity score to the correct document.",69,0.95,https://www.reddit.com/r/MachineLearning/comments/1maj150/p_i_tried_implementing_the_crisp_paper_from/,False,True,False
1ma6rle,PokeAgentChallenge,1753570442.0,8,/r/MachineLearning/comments/1ma6rle/p_llm_economist_large_population_models_and/,MachineLearning,[P] LLM Economist: Large Population Models and Mechanism Design via Multi‚ÄëAgent Language Simulacra,"Co-author here. We‚Äôve released a new preprint, **LLM Economist**, which explores how LLM-based agents can learn and optimize economic policy through multi-agent simulation.

In our setup, a planner agent proposes marginal tax schedules, while a population of 100 worker agents respond by choosing how much labor to supply based on their individual personas. All agents are instantiated from a calibrated skill and demographic prior and operate entirely through language‚Äîinteracting via in-context messages and JSON actions.

The planner observes these behaviors and adjusts tax policy over time to maximize social welfare (happiness). No gradient updates are used; instead, the planner learns directly through repeated text-based interactions and the culminating societal/individual reward. This yields realistic economic dynamics, including responding to the Lucas Critique, behavioral adaptation, and tradeoffs between equity and efficiency.

**Key contributions:**

* A two-tier in-context RL framework using LLMs for both workers and planner.
* Persona-conditioned agent population grounded in U.S. Census-like statistics.
* Emergent economic responses to policy changes, such as implicit varying elasticity and participation behavior.
* Stackelberg-inspired simulation loop where planner and workers co-adapt.

We would welcome feedback from this community on:

* The viability of language-only RL architectures for economic modeling.
* Stability and interpretability of emergent agent behavior.
* Broader implications for coordination and mechanism design with LLMs.

Paper: [https://arxiv.org/abs/2507.15815](https://arxiv.org/abs/2507.15815)  
Code: [https://github.com/sethkarten/LLM-Economist](https://github.com/sethkarten/LLM-Economist)

Happy to answer questions or discuss possible extensions.",15,0.78,https://www.reddit.com/r/MachineLearning/comments/1ma6rle/p_llm_economist_large_population_models_and/,False,True,False
1m9vdcy,abhinav02_31,1753541784.0,7,/r/MachineLearning/comments/1m9vdcy/p_llm_context_manager/,MachineLearning,[P] LLM Context Manager,"Hi, i built something! An LLM Context Manager, an inference optimization system for conversations. it uses branching and a novel algorithm contextual scaffolding algorithm (CSA) to smartly manage the context that is fed into the model. The model is fed only with context from previous conversation it needs to answer a prompt. This prevents context pollution/context rot. Please do check it out and give feedback what you think about it. Thanks [https://github.com/theabhinav0231/LLM-Context-Manager](https://github.com/theabhinav0231/LLM-Context-Manager)",8,0.83,https://www.reddit.com/r/MachineLearning/comments/1m9vdcy/p_llm_context_manager/,False,True,False
1m9vauo,shreshthkapai,1753541612.0,11,/r/MachineLearning/comments/1m9vauo/p_submillisecond_gpu_task_queue_optimized_cuda/,MachineLearning,[P] Sub-millisecond GPU Task Queue: Optimized CUDA Kernels for Small-Batch ML Inference on GTX 1650.,"Over the past month, I‚Äôve been working on writing high-throughput, low-latency CUDA kernels for small-batch inference workloads typical in real-time ML use cases (e.g., finance, RL serving).

Despite running on a GTX 1650 (consumer laptop GPU), I achieved:

* **93,563 ops/sec**
* **0.011 ms median latency**
* **7.3√ó speedup over PyTorch (float32 GEMV)**
* **30‚Äì40% faster than cuBLAS batched GEMV**¬†(in small-batch regime)

This was done by hand-optimizing a set of three core kernels:

* Batched GEMV
* Softmax
* Vector elementwise ops (e.g., affine transforms)

# Engineering Highlights:

* `float4`¬†**vectorization**¬†with proper alignment checks
* **128-byte staged shared memory blocks**¬†(using padding for bank conflict mitigation)
* **Thread-per-output-element grid strategy**
* **Aggressive loop unrolling**¬†and warp-aware memory access
* Benchmarked with¬†**CUDA events**, median+IQR over 1,000 trials

# Why it matters:

cuBLAS (and by extension PyTorch) is heavily tuned for large-batch throughput, but small-batch latency suffers. For real-time systems (e.g., financial models or reinforcement learning), this is a major bottleneck.

This kernel suite shows that even with modest hardware, you can cut inference latency significantly below PyTorch/cuBLAS levels through architecture-aware programming.

# Links:

* [GitHub source & benchmark code](https://github.com/shreshthkapai/cuda_latency_benchmark)
* [Full write-up on Medium](https://medium.com/@shreshthkapai/sub-millisecond-gpu-task-queue-breaking-pytorchs-latency-bottleneck-b6f3d3f2e895)

Would love to hear feedback from others doing similar work‚Äîespecially around kernel tuning strategies, warp divergence handling, and memory hierarchy tradeoffs.",68,0.86,https://www.reddit.com/r/MachineLearning/comments/1m9vauo/p_submillisecond_gpu_task_queue_optimized_cuda/,False,True,False
1m9js00,jarekduda,1753502515.0,28,/r/MachineLearning/comments/1m9js00/d_why_cdf_normalization_is_not_used_in_ml_leads/,MachineLearning,[D] Why CDF normalization is not used in ML? Leads to more uniform distributions - better for generalization,"CDF/EDF normalization to nearly uniform distributions is very popular in finance, but I haven't seen it before in ML - is there a reason?

We have made tests with KAN (by just adding normalized Gaussian CDF after batch norm), and such more uniform distributions can be described with smaller models, which are better for generalization: [https://arxiv.org/pdf/2507.13393](https://arxiv.org/pdf/2507.13393)

Where in ML such CDF normalization could find applications? Any other interesting nonstandard normalization approaches?",112,0.92,https://i.redd.it/vt958iww55ff1.jpeg,False,False,False
1m9obic,paperplanet07,1753518986.0,20,/r/MachineLearning/comments/1m9obic/d_do_you_think_that_muon_optimizer_can_be_viewed/,MachineLearning,[D] Do you think that Muon Optimizer can be viewed through the lens of explore-exploit?,"Recent research shows that the Muon optimizer can achieve comparable loss with significantly less data, without requiring any changes to the network architecture. This suggests that there might be something fundamentally important at play in Muon, especially after years of Adam‚Äôs dominance. After looking deeper into how Muon works, I started to wonder if it might be understood through the lens of the exploration-exploitation tradeoff in training dynamics. I‚Äôd love to hear your thoughts on this.

The full analysis is written here:
https://paperplanet.github.io/posts/muon-a-explore-exploit-perspective/",23,0.78,https://www.reddit.com/r/MachineLearning/comments/1m9obic/d_do_you_think_that_muon_optimizer_can_be_viewed/,False,True,False
1m9nwq1,random_sydneysider,1753517341.0,0,/r/MachineLearning/comments/1m9nwq1/r_training_small_transformer_model_on_wikitext2/,MachineLearning,[R] Training small transformer model on WikiText2 from scratch,"Currently I'm using this codebase to train small decoder-only transformer models on WikiText2. The hyperparameters aren't tuned well though, the perplexity starts increasing after 20 epochs using the default hyperparameters in this repository. [https://github.com/huggingface/naacl\_transfer\_learning\_tutorial](https://github.com/huggingface/naacl_transfer_learning_tutorial)

Do you know any of open-sourced repositories that get better results on this baseline?

[https://x.com/Tim\_Dettmers/status/1245805495895511042](https://x.com/Tim_Dettmers/status/1245805495895511042) This post states that a perplexity of 107 is possible with transformers.

[https://github.com/pytorch/examples/blob/main/word\_language\_model/model.py](https://github.com/pytorch/examples/blob/main/word_language_model/model.py) This official PyTorch repository also has an implementation, but it uses encoder-decoder models (not decoder-only transformers like GPT2).",3,0.71,https://www.reddit.com/r/MachineLearning/comments/1m9nwq1/r_training_small_transformer_model_on_wikitext2/,False,True,False
1m9muxm,musescore1983,1753513281.0,0,/r/MachineLearning/comments/1m9muxm/d_constructing_semantic_spaces_from_given_spaces/,MachineLearning,[D] Constructing semantic spaces from given spaces?,"I want to share [a working draft ](https://www.orges-leka.de/constructing_semantic_spaces_from_given_spaces.pdf)from me which discusses how to construct semantic spaces from given ones and how to reverse this process in order to infer the semantic meaning between two words given a database of sequence of words with similarity measures between them. This writing is a followup of my informal writing in [representing logic in semantic spaces](https://www.orges-leka.de/semantic_space_of_logic.pdf). Any thoughts for discussion?

",2,1.0,https://www.reddit.com/r/MachineLearning/comments/1m9muxm/d_constructing_semantic_spaces_from_given_spaces/,False,True,False
1m9ik06,Naneet_Aleart_Ok,1753498619.0,7,/r/MachineLearning/comments/1m9ik06/p_tried_everything_still_failing_at_cslr_with/,MachineLearning,"[P] Tried Everything, Still Failing at CSLR with Transformer-Based Model","Hi all,  
I‚Äôve been stuck on this problem for a long time and I‚Äôm honestly going a bit insane trying to figure out what‚Äôs wrong. I‚Äôm working on a¬†**Continuous Sign Language Recognition (CSLR)**¬†model using the¬†**RWTH-PHOENIX-Weather 2014**¬†dataset. My approach is based on transformers and uses¬†**ViViT**¬†as the video encoder.

# Model Overview:

**Dual-stream architecture**:

* One stream processes the¬†*normal RGB video*, the other processes¬†*keypoint video*¬†(generated using Mediapipe).
* Both streams are encoded using¬†**ViViT (depth = 12)**.

**Fusion mechanism**:

* I insert¬†**cross-attention**¬†layers¬†*after the 4th and 8th ViViT blocks*¬†to allow interaction between the two streams.
* I also added¬†**adapter modules**¬†in the rest of the blocks to encourage mutual learning without overwhelming either stream.

**Decoding**:

I‚Äôve tried¬†*many decoding strategies*, and none have worked reliably:

* **T5 Decoder**: Didn't work well, probably due to integration issues since T5 is a text to text model.
* **PyTorch‚Äôs TransformerDecoder (Tf)**:
   * Decoded each stream separately and then merged outputs with cross-attention.
   * Fused the encodings (add/concat) and decoded using a single decoder.
   * Decoded with two separate decoders (one for each stream), each with its own FC layer.

**ViViT Pretraining**:

Tried pretraining a ViViT encoder for 96-frame inputs.

Still couldn‚Äôt get good results even after swapping it into the decoder pipelines above.

# Training:

* **Loss**: CrossEntropyLoss
* **Optimizer**: Adam
* Tried different learning rates, schedulers, and variations of model depth and fusion strategy.

# 

Nothing is working. The model doesn‚Äôt seem to converge well, and validation metrics stay flat or noisy. I‚Äôm not sure if I‚Äôm making a fundamental design mistake (especially in decoder fusion), or if the model is just too complex and unstable to train end-to-end from scratch on PHOENIX14.

I would deeply appreciate any insights or advice. I‚Äôve been working on this for weeks, and it‚Äôs starting to really affect my motivation. Thank you.

**TL;DR**: I‚Äôm using a dual-stream ViViT + TransformerDecoder setup for CSLR on PHOENIX14. Tried several fusion/decoding methods, but nothing works. I need advice or a sanity check.",7,0.73,https://www.reddit.com/r/MachineLearning/comments/1m9ik06/p_tried_everything_still_failing_at_cslr_with/,False,True,False
1m9ffp0,New-Skin-5064,1753489263.0,6,/r/MachineLearning/comments/1m9ffp0/d_how_to_improve_pretraining_pipeline/,MachineLearning,[D] How to improve pretraining pipeline,"I‚Äôm interested in large language models, so I decided to build a pretraining pipeline, and was wondering what I should add to it before I start my run. I‚Äôm trying to pretrain a GPT-2 Small(or maybe medium) sized model on an 11b token dataset with web text and code. I made some tweaks to the model architecture, adding Flash Attention, RMSNorm, SwiGLU, and RoPE. I linearly warmup the batch size from 32k to 525k tokens over the first ~100m tokens, and also have a Cosine learning rate schedule with a warmup over the first 3.2m tokens. I‚Äôm using the free Kaggle TPU v3-8(I use the save and run all feature to run my code overnight, and I split training up between multiple of these sessions). I‚Äôm using FSDP through Torch XLA for parralelism, and I log metrics to Weights and Biases. Finally, I upsample data from TinyStories early in training, as I have found that it helps the model converge faster. What should I add to my pipeline to make it closer to the pretraining code used in top companies? Also, could I realistically train this model with SFT and RLHF to be a simple chatbot?

Edit: I‚Äôm still in high school, so I‚Äôm doing this in my spare time. I might have to prioritize things that aren‚Äôt too compute-heavy/time-intensive.",6,0.87,https://www.reddit.com/r/MachineLearning/comments/1m9ffp0/d_how_to_improve_pretraining_pipeline/,False,True,False
1m9fasg,Ok-Atmosphere3141,1753488889.0,8,/r/MachineLearning/comments/1m9fasg/d_aacl_vs_aaai_for_nlp_papers/,MachineLearning,[D] AACL VS. AAAI for NLP papers,"AAAI is sometimes considered ~~lower tier~~ \[edit: less preferred\] for ML research communities compared with ICML/Neurips/ICLR and ACL conferences. but still it is a fairly good brand overall and has steady quality. This year AAAI and AACL-IJCNLP deadlines are about the same. For an NLP methodology paper, which venue is more preferable given that confidence of acceptance is relatively high?",0,0.35,https://www.reddit.com/r/MachineLearning/comments/1m9fasg/d_aacl_vs_aaai_for_nlp_papers/,False,True,False
1m9d7kw,yaboproductions,1753483281.0,5,/r/MachineLearning/comments/1m9d7kw/d_is_this_lambda_ai_rig_in_demand_anymore/,MachineLearning,[D] Is this Lambda AI rig in demand anymore?,"Hi guys, I got an AI rig donated to me, and while I've been toying with some LLMs on it, I'm no ML professional, so I feel like someone else probably has a better use for it than just spinning their own chatbot. I was curious to hear from this community whether it'd be worth it to sell the thing, or if it's old enough now that it's only worth keeping around as an end-user machine. I've done some googling and there's only a little demand for Lambda machines in general, and I'm just not in the world of ML enough to know any better.

Here are the specs:

* Ryzen threadripper 3960X, 64GB RAM
* 2x RTX 3080 blower style, 10GB VRAM each

Thanks in advance!",1,0.56,https://www.reddit.com/r/MachineLearning/comments/1m9d7kw/d_is_this_lambda_ai_rig_in_demand_anymore/,False,True,False
1m9czst,saliherdemk,1753482725.0,1,/r/MachineLearning/comments/1m9czst/p_build_an_mlp_and_visualize_training_in_real/,MachineLearning,[P] Build an MLP and Visualize Training in Real Time In Your Browser,"Hi everyone,

I built Grada, a browser-based tool that lets you build and train an mlp from scratch and visualize the training process in real time. Built entirely from scratch (no libraries) so it's not the fastest of course but it's fast enough to train simple models.

The goal is to make neural network training more transparent and intuitive, especially for those learning how MLPs work under the hood. You can tweak hyperparameters on the fly and immediately see how the model responds during training. There's also a pretrained handwritten digit classifier you can interact with to see inference in action.

[https://saliherdemk.github.io/Grada/](https://saliherdemk.github.io/Grada/)",4,0.83,https://www.reddit.com/r/MachineLearning/comments/1m9czst/p_build_an_mlp_and_visualize_training_in_real/,False,True,False
1m967t1,Previous-Scheme-5949,1753466402.0,9,/r/MachineLearning/comments/1m967t1/d_ddpms_training_learns_to_undo_entire_noise_but/,MachineLearning,"[D]: DDPMs: Training learns to undo entire noise, but at sampling time, noise removed step by step, why?","During training, diffusion models are trained to predict the full noise that was added to a clean image. However, during inference (sampling), the same model is used to gradually remove noise step by step over many¬†`T`¬†iterations. Why does this approach work, even though the model was never explicitly trained to denoise incrementally?

[Algos from the DDPM paper](https://preview.redd.it/denzyibu72ff1.png?width=1088&format=png&auto=webp&s=54994920af52bb721b1362eae1a226e340674b82)

",13,0.93,https://www.reddit.com/r/MachineLearning/comments/1m967t1/d_ddpms_training_learns_to_undo_entire_noise_but/,False,True,False
1m95swr,xEdwin23x,1753465451.0,14,/r/MachineLearning/comments/1m95swr/d_bmvc_2025_results_discussion/,MachineLearning,[D] BMVC 2025 Results Discussion,"I just got the email. Unfortunately rejected but cannot see the reviews, only that my paper and all the ones I reviewed were on the ""Rejected"" tab on OpenReview. Can anyone see yours? What was your experience?",6,0.81,https://www.reddit.com/r/MachineLearning/comments/1m95swr/d_bmvc_2025_results_discussion/,False,True,False
1m95ej0,kaitzu,1753464532.0,35,/r/MachineLearning/comments/1m95ej0/r_neurips_2025_db_the_evaluation_is_limited_to_15/,MachineLearning,"[R] NeurIPS 2025 D&B: ""The evaluation is limited to 15 open-weights models ... Score: 3""","I'm pretty shocked how the only reviewer criticism on our benchmark paper (3.5/6) was that our paper included *only* 15 open weights models and that we didn't evaluate our benchmark on SoTA commercial models (that would cost \~10-15k $ to do).

I mean how superficial does it get to reject a paper not because something is wrong about its design or that it isn't a novel/useful benchmark, but because we don't want to pay thousands of dollars to OpenAI/Google/Anthropic to evaluate (and promote) their models.

How academic is it to restrict the ability to publish to the big labs / companies in wealthy countries that have the money lying around to do that?!",338,0.97,https://www.reddit.com/r/MachineLearning/comments/1m95ej0/r_neurips_2025_db_the_evaluation_is_limited_to_15/,False,True,False
1m8z9zk,prototypist,1753450351.0,17,/r/MachineLearning/comments/1m8z9zk/n_paperswithcode_sunsets_new_huggingface_papers_ui/,MachineLearning,"[N] PapersWithCode sunsets, new HuggingFace Papers UI","After a month of discussions here about [problems](https://www.reddit.com/r/MachineLearning/comments/1lkedb8/d_paperswithcode_has_been_compromised/) [with](https://www.reddit.com/r/MachineLearning/comments/1lqedrt/d_paper_with_code_is_completely_down/) the PapersWithCode site staying online and hosting spam, the [PapersWithCode.com](http://PapersWithCode.com) URL now redirects to their GitHub

According to Julien Chaumond of HF, they have ""partnered with PapersWithCode and Meta to build a successor"" on  [https://huggingface.co/papers/trending](https://huggingface.co/papers/trending) . There have been links to browse papers and associated models and datasets on HF for some time, but potentially they are going to give it some additional attention in the coming weeks.",105,1.0,https://www.reddit.com/r/MachineLearning/comments/1m8z9zk/n_paperswithcode_sunsets_new_huggingface_papers_ui/,False,True,False
1m8wmos,Remarkable-Ad3290,1753442870.0,1,/r/MachineLearning/comments/1m8wmos/p_built_another_124m_parameters_transformer_based/,MachineLearning,[P] üöÄBuilt another 124m parameters transformer based model from scratch.This time with multi GPU training with DDP. Inspired from nanoGPT but redesigned to suit my own training pipeline.Model and training code is here,"https://huggingface.co/abhinavv3/MEMGPT

Before training the current code Im planning to experiment by replacing the existing attention layer with GQA and the positional encoding with RoPE. Also tryingg to implement some concepts from research papers like Memorizing Transformers.
Bt these changes haven't been implemented yet.",5,0.67,https://www.reddit.com/r/MachineLearning/comments/1m8wmos/p_built_another_124m_parameters_transformer_based/,False,True,False
1m8tzn2,xeenxavier,1753433085.0,10,/r/MachineLearning/comments/1m8tzn2/d_mlops_how_to_handle_accuracy_drop_in_a_few/,MachineLearning,[D] [MLOps] How to Handle Accuracy Drop in a Few Models During Mass Migration to a New Container?,"Hi all,

I‚Äôm currently facing a challenge in migrating ML models and could use some guidance from the MLOps community.

# Background:

We have around 100 ML models running in production, each serving different clients. These models were trained and deployed using older versions of libraries such as¬†`scikit-learn`¬†and¬†`xgboost`.

As part of our upgrade process, we're building a new Docker container with updated versions of these libraries. We're retraining all the models inside this new container and comparing their performance with the existing ones.

We are following a blue-green deployment approach:

* Retrain all models in the new container.
* Compare performance metrics (accuracy, F1, AUC, etc.).
* If all models pass, switch production traffic to the new container.

# Current Challenge:

After retraining, 95 models show the same or improved accuracy. However, 5 models show a noticeable drop in performance. These 5 models are blocking the full switch to the new container.

# Questions:

1. Should we proceed with migrating only the 95 successful models and leave the 5 on the old setup?
2. Is it acceptable to maintain a hybrid environment where some models run on the old container and others on the new one?
3. Should we invest time in re-tuning or debugging the 5 failing models before migration?
4. How do others handle partial failures during large-scale model migrations?

# Stack:

* Model frameworks: scikit-learn, XGBoost
* Containerization: Docker
* Deployment strategy: Blue-Green
* CI/CD: Planned via GitHub Actions
* Planning to add MLflow or Weights & Biases for tracking and comparison

Would really appreciate insights from anyone who has handled similar large-scale migrations. Thank you.",7,0.82,https://www.reddit.com/r/MachineLearning/comments/1m8tzn2/d_mlops_how_to_handle_accuracy_drop_in_a_few/,False,True,False
1m8rkli,Unique_Revolution_59,1753423701.0,6,/r/MachineLearning/comments/1m8rkli/d_review_confidence_guidelines/,MachineLearning,[D] Review Confidence Guidelines,"* 5. I'm a world expert. I resent wasting my precious time on your little paper and I'll tear it to shreds unless you cite me at least 3 times.
* 4. I know the area.
* 3. I don't know the area.
* 2. I just started my masters and my supervisor gave me 5 papers to review. Please don't be mad if I mess up.
* 1. What's the deep learning?

",65,0.9,https://www.reddit.com/r/MachineLearning/comments/1m8rkli/d_review_confidence_guidelines/,False,True,False
1m8n3yz,Antelito83,1753409345.0,7,/r/MachineLearning/comments/1m8n3yz/help_needed_accurate_offline_table_extraction/,MachineLearning,Help Needed: Accurate Offline Table Extraction from Scanned Forms [P],"I have a scanned form containing a large table with surrounding text. My goal is to extract specific information from certain cells in this table.  

Current Approach & Challenges  
1. OCR Tools (e.g., Tesseract):  
   - Used to identify the table and extract text.  
   - Issue: OCR accuracy is inconsistent‚Äîsometimes the table isn‚Äôt recognized or is parsed incorrectly.  

2. Post-OCR Correction (e.g., Mistral):  
   - A language model refines the extracted text.  
   - Issue: Poor results due to upstream OCR errors.  

Despite spending hours on this workflow, I haven‚Äôt achieved reliable extraction.  

Alternative Solution (Online Tools Work, but Local Execution is Required)  
- Observation: Uploading the form to ChatGPT or DeepSeek (online) yields excellent results.  
- Constraint: The solution must run entirely locally (no internet connection).  

Attempted new Workflow (DINOv2 + Multimodal LLM)  
1. Step 1: Image Embedding with DINOv2  
   - Tried converting the image into a vector representation using DINOv2 (Vision Transformer).  
   - Issue: Did not produce usable results‚Äîpossibly due to incorrect implementation or model limitations. Is this approach even correct?

2. Step 2: Multimodal LLM Processing  
   - Planned to feed the vector to a local multimodal LLM (e.g., Mistral) for structured output.  
   - Blocker: Step 2 failed, didn‚Äôt got usable output 

Question  
Is there a local, offline-compatible method to replicate the quality of online extraction tools? For example:  
- Are there better vision models than DINOv2 for this task?  
- Could a different pipeline (e.g., layout detection + OCR + LLM correction) work?  
- Any tips for debugging DINOv2 missteps?",3,1.0,https://www.reddit.com/r/MachineLearning/comments/1m8n3yz/help_needed_accurate_offline_table_extraction/,False,True,False
1m8ll1d,Secret_Valuable_Yes,1753404937.0,9,/r/MachineLearning/comments/1m8ll1d/d_how_to_calculate_the_memory_needed_to_train/,MachineLearning,[D] How to calculate the memory needed to train your model on GPU,"I want to be able to know if my model should fit on a single GPU a head of time before I start training. I assume this is what most people do (if not, please share your approach). Here's a formula that I came across to estimate the memory requirements - except I'm not sure how to calculate the activation memory. Does anyone have a rule of thumb for the activation memory? I heard it scales linearly with batch size, so what would be the baseline assuming a batch size of 1? 

Formula (ex. 32bit model = 32 bit x (1 byte / 8 bit) = 4 bytes per parameter )

\- parameter memory = bytes x num params

\- optimizer states = 2 x bytes x num params (momentum + velocity for adam)

\- gradient memory = bytes x num params

\- activations = ? (somewhere I heard it was roughly 2 x bytes x num params)",7,0.9,https://www.reddit.com/r/MachineLearning/comments/1m8ll1d/d_how_to_calculate_the_memory_needed_to_train/,False,True,False
1m8k6ik,Dismal_Moment5761,1753401008.0,2,/r/MachineLearning/comments/1m8k6ik/r_question_about_the_neurips_2025_rebuttal_process/,MachineLearning,[R] Question about the NeurIPS 2025 rebuttal process,"The NeurIPS 2025 FAQ ([https://neurips.cc/Conferences/2025/PaperInformation/NeurIPS-FAQ](https://neurips.cc/Conferences/2025/PaperInformation/NeurIPS-FAQ)) mentions that rebuttals are limited to 6,000 characters per review, plus an additional 6,000-character global rebuttal (with the option to upload a one-page PDF for figures/tables).

However, the OpenReview notification I received states a 10,000-character limit per review and doesn‚Äôt mention anything about a global rebuttal.

Does anyone know which guideline I should follow? Should I assume OpenReview‚Äôs limits take precedence?",5,0.86,https://www.reddit.com/r/MachineLearning/comments/1m8k6ik/r_question_about_the_neurips_2025_rebuttal_process/,False,True,False
1m8fynx,MalumaDev,1753390319.0,21,/r/MachineLearning/comments/1m8fynx/d_tried_of_the_same_review_pattern/,MachineLearning,[D] Tried of the same review pattern,"Lately, I‚Äôve been really disappointed with the review process. There seems to be a recurring pattern in the weaknesses reviewers raise, and it‚Äôs frustrating:

1. ""No novelty"" ‚Äì even when the paper introduces a new idea that beats the state of the art, just because it reuses components from other fields. No one else has achieved these results or approached the problem in the same way. So why dismiss it as lacking novelty?

2. Misunderstanding the content ‚Äì reviewers asking questions that are already clearly answered in the paper. It feels like the paper wasn‚Äôt read carefully, if at all.


I‚Äôm not claiming my paper is perfect‚Äîit‚Äôs definitely not. But seriously... WTF?",127,0.95,https://www.reddit.com/r/MachineLearning/comments/1m8fynx/d_tried_of_the_same_review_pattern/,False,True,False
1m81xlr,Practical_Pomelo_636,1753356752.0,70,/r/MachineLearning/comments/1m81xlr/d_acl_arr_july_2025_discussion/,MachineLearning,[D] ACL ARR July 2025 Discussion,"Discussion thread.

",17,0.9,https://www.reddit.com/r/MachineLearning/comments/1m81xlr/d_acl_arr_july_2025_discussion/,False,True,False
1m7z61w,1h3_fool,1753346907.0,2,/r/MachineLearning/comments/1m7z61w/p_issues_in_training_differential_attention/,MachineLearning,[P] Issues in Training Differential Attention Transformer.,"Hey folks,

I have been trying to implement a research paper that utilized differential transformer block¬†¬†attention [https://arxiv.org/abs/2502.13189](https://arxiv.org/abs/2502.13189) as a means to denoise background noise from¬†¬†biological sounds, While training the model I am constantly running into numeric instability (nan loss), specifically this step : --

lambda\_val = torch.exp(lambda\_q1\_dot\_k1) - torch.exp(lambda\_q2\_dot\_k2) + self.lambda\_init

Most probably due to exponential terms assuming large values. I did try clamping the lambda values to avoid this but doing this is resulting in diverging loss values after few epochs.¬†¬†Anybody how might¬†¬†have tried this block can suggest any fixes or whether the clamping approach is the right way in terms of loss optimization (I know¬†¬†clamping is not the best thing for loss optimization ) ?",8,0.84,https://www.reddit.com/r/MachineLearning/comments/1m7z61w/p_issues_in_training_differential_attention/,False,True,False
1m7ubmp,Collegiate_Society2,1753329563.0,18,/r/MachineLearning/comments/1m7ubmp/d_why_is_there_such_a_noticeable_difference/,MachineLearning,[D] Why is there such a noticeable difference between Stat and CS section of Arxiv? Any underlying reasons?,"As a math major, I was interested in seeing what different fields of mathematical research looks like. I decided to just browse the Arxiv, but I can't help to notice the difference between Stat.ML and CS.LG sections.

From my understanding, they are both suppose to be about Machine Learning research, but what I found was that many of the CS.LG articles applied ML to novel scenarios instead of actually researching new mathematical/statistical models. Why are these considered ML research, if they are not researching ML but using it?

Does this reflect a bigger divide within the machine learning research field? Is there some fields in ML that are more suited for people interested in math research? if so, are those generally hosted in the math/stats department, or still under the CS department?",26,0.73,https://www.reddit.com/r/MachineLearning/comments/1m7ubmp/d_why_is_there_such_a_noticeable_difference/,False,True,False
1m7thaq,Mysterious_Flan5357,1753326928.0,1,/r/MachineLearning/comments/1m7thaq/d_emnlp_2025_meta_reviews/,MachineLearning,[D] EMNLP 2025 Meta Reviews,Has anyone received the meta reviews yet for the ARR May 2025 cycle (EMNLP 2025)? Let's discuss.,2,1.0,https://www.reddit.com/r/MachineLearning/comments/1m7thaq/d_emnlp_2025_meta_reviews/,False,True,False
1m7i3dq,zedeleyici3401,1753297568.0,1,/r/MachineLearning/comments/1m7i3dq/r_treemind_a_highperformance_library_for/,MachineLearning,[R] treemind: A High-Performance Library for Explaining Tree-Based Models,"I am pleased to introduce [`treemind`](https://github.com/sametcopur/treemind/), a high-performance Python library for interpreting tree-based models.

Whether you're auditing models, debugging feature behavior, or exploring feature interactions, `treemind` provides a robust and scalable solution with meaningful visual explanations.

* **Feature Analysis** Understand how individual features influence model predictions across different split intervals.
* **Interaction Detection** Automatically detect and rank pairwise or higher-order feature interactions.
* **Model Support** Works seamlessly with LightGBM, XGBoost, CatBoost, scikit-learn, and perpetual.
* **Performance Optimized** Fast even on deep and wide ensembles via Cython-backed internals.
* **Visualizations** Includes a plotting module for interaction maps, importance heatmaps, feature influence charts, and more.

**Installation**

    pip install treemind

**One-Dimensional Feature Explanation**

Each row in the table shows how the model behaves within a specific range of the selected feature.  
The `value` column represents the average prediction in that interval, making it easier to identify which value ranges influence the model most.

    | worst_texture_lb | worst_texture_ub |   value   |   std    |  count  |
    |------------------|------------------|-----------|----------|---------|
    | -inf             | 18.460           | 3.185128  | 8.479232 | 402.24  |
    | 18.460           | 19.300           | 3.160656  | 8.519873 | 402.39  |
    | 19.300           | 19.415           | 3.119814  | 8.489262 | 401.85  |
    | 19.415           | 20.225           | 3.101601  | 8.490439 | 402.55  |
    | 20.225           | 20.360           | 2.772929  | 8.711773 | 433.16  |

**Feature Plot**  


https://preview.redd.it/cbmyl38y7oef1.png?width=1189&format=png&auto=webp&s=5c7657a74bdebf5c51332ddc856f5de3d5583de9

# 

**Two Dimensional Interaction Plot**

The plot shows how the model's prediction varies across value combinations of two features. It highlights regions where their joint influence is strongest, revealing important interactions.

https://preview.redd.it/2zb1ra5h8oef1.png?width=943&format=png&auto=webp&s=6b1149795ce202f50f47f0264013eb225e09de2c

# Learn More

* Documentation: [https://treemind.readthedocs.io](https://treemind.readthedocs.io)
* Github: [https://github.com/sametcopur/treemind/](https://github.com/sametcopur/treemind/)
* Algorithm Details: [How It Works](https://treemind.readthedocs.io/en/latest/algorithm.html)
* Benchmarks: [Performance Evaluation](https://treemind.readthedocs.io/en/latest/experiments/experiment_main.html)

Feedback and contributions are welcome. If you're working on model interpretability, we'd love to hear your thoughts.",6,0.88,https://www.reddit.com/r/MachineLearning/comments/1m7i3dq/r_treemind_a_highperformance_library_for/,False,True,False
1m7mixn,No_Cost_4788,1753307799.0,4,/r/MachineLearning/comments/1m7mixn/r_phd_scholarship_at_victoria_university_of/,MachineLearning,[R] PhD scholarship at Victoria University of Wellington in machine learning for Volcano forecasting,"We are seeking a highly motivated PhD student to join our multidisciplinary volcanic hazards research team at Victoria University of Wellington, New Zealand. This exciting project focuses on developing cutting-edge diffusion-based machine learning models to forecast volcanic activities, significantly enhancing our ability to predict eruption dynamics.

üîπ Scholarship details:

Generous stipend: NZ$35,000/year for 3 years (possible extension).

Full tuition fees covered.

Funding for international conferences and collaboration visits in Europe.

Fieldwork opportunities.

üîπ Ideal candidates:

Background in Machine Learning, Data Science, Computer Science, or related fields.

Strong Python skills.

Excellent communication in English.

Previous publications in top-tier AI conferences/journals.

üîπ Supervisors: Prof. Bastiaan Kleijn, Dr. Felix Yan, Dr. Finnigan Illsley-Kemp

üìÖ Applications reviewed from: September 1st, 2025 (Flexible start date from October 2025 onwards).

For inquiries and applications, please contact me directly at üìß¬†[felix.yan@vuw.ac.nz](mailto:felix.yan@vuw.ac.nz). Application documents include your CV, transcript, Master's thesis, and publications.

Feel free to share this fantastic opportunity with your network!",6,0.57,https://www.reddit.com/r/MachineLearning/comments/1m7mixn/r_phd_scholarship_at_victoria_university_of/,False,True,False
1m7jl5m,HealthyInstance9182,1753300960.0,11,/r/MachineLearning/comments/1m7jl5m/the_serial_scaling_hypothesis/,MachineLearning,The Serial Scaling Hypothesis,,37,0.95,https://arxiv.org/abs/2507.12549,False,False,False
1m74ugv,Proof-Marsupial-5367,1753263674.0,922,/r/MachineLearning/comments/1m74ugv/d_neurips2025_reviews/,MachineLearning,[D] - NeurIPS'2025 Reviews,"Hey everyone,

NeurIPS 2025 reviews should be dropping soon (July 24th AoE), and I thought it might be a good idea to start a thread where we can share our thoughts, experiences, and reactions.

Feel free to post your initial impressions, any surprises (good or bad), questions about rebuttals, or just how you‚Äôre feeling about the process this year. Whether it‚Äôs your first submission or your tenth, you‚Äôre not alone in the rollercoaster.

Let‚Äôs keep things constructive and supportive. Good luck to all!",237,0.96,https://www.reddit.com/r/MachineLearning/comments/1m74ugv/d_neurips2025_reviews/,False,True,False
1m6o4l2,Witty_Doughnut3497,1753213677.0,2,/r/MachineLearning/comments/1m6o4l2/d_working_on_a_ml_in_quant_finance_conf_need_your/,MachineLearning,[D] Working on a ML in Quant Finance Conf - Need your guidance,"Hellow ML/Al folks,

I'm working on an upcoming Machine Learning in Quantitative Finance conference, my role is to outreach and engage relevant professionals.

While I've handled other events before, this field is new to me. I'd appreciate any quick tips, resources, or key concepts to get up to speed.

Also, if you have advice on how to approach senior roles (MDs, Heads of Departments, Chiefs, Presidents) effectively in this space.

Thanks",5,0.73,https://www.reddit.com/r/MachineLearning/comments/1m6o4l2/d_working_on_a_ml_in_quant_finance_conf_need_your/,False,True,False
1m6kujc,Classic_Eggplant8827,1753206392.0,16,/r/MachineLearning/comments/1m6kujc/d_is_there_anyone_using_grpo_in_their_company/,MachineLearning,[D] Is there anyone using GRPO in their company?,"I am considering doing RL as a service for companies looking to finetune LLMs, and I have doubts. It is a lot more compute-intensive. it promises data efficiency, but training is more unstable, it is less straightforward to debug, and there are so many moving parts in infra and environment setup that make reproducibility very difficult unless you just have the compute to scale. was wondering how far RL for agents is from adoption? are there people experimenting with this in your work/training custom reasoning models? is it worth it?",32,0.94,https://www.reddit.com/r/MachineLearning/comments/1m6kujc/d_is_there_anyone_using_grpo_in_their_company/,False,True,False
1m69wc3,whereismycatyo,1753177961.0,20,/r/MachineLearning/comments/1m69wc3/d_is_it_me_or_is_ecai_really_bad_this_year/,MachineLearning,[D]  Is it me or is ECAI really bad this year?,"I have one accepted paper and another one rejected. The review and meta-review quality was really subpar. It felt like most of the responses we got, on both sides of the spectrum, came from underexperinced reviewers. I am all for letting undergrads read, review, and get experience, but I always review the paper by myself first and would never submit theirs as is. This really boggles me because I always thought ECAI is a good conference, but this year I can't help but feel a little bit embarrassed to even go there.

I have not submitted to other conferences yet. So, I wonder if there is a trend.",41,0.93,https://www.reddit.com/r/MachineLearning/comments/1m69wc3/d_is_it_me_or_is_ecai_really_bad_this_year/,False,True,False
1m5r5m3,NorthAfternoon4930,1753123232.0,0,/r/MachineLearning/comments/1m5r5m3/r_gaussian_process_to_approximate_vehicle_dynamics/,MachineLearning,[R] Gaussian Process to Approximate Vehicle Dynamics,"A while back, I was working on localization with GPs and had a thought: could we encode vehicle dynamics directly into the GP kernel?

I know GPs are used to model parameters in physical models. But my idea was that a car‚Äôs trajectory resembles a smooth GP sample. A faster car takes smoother paths, just like longer length scales produce smoother GPs. Instead of modeling `y(x)` directly, I used cumulative distance `s` as the input, and trained two separate GPs:

* `x(s)`
* `y(s)`

Both use an RBF kernel. So we are basically maximizing the probability function:

https://preview.redd.it/ksoisiw9r9ef1.png?width=430&format=png&auto=webp&s=e01f1827f3c74550f596de2ee02fe4b7d2e93178

Which translates to something like

*‚ÄúGiven a speed, how probable is it that these data points came from this vehicle?‚Äù*

**The algorithm goes like this:**

1. Collect data
2. Optimize the kernel
3. Construct the `l(v)` function
4. Optimize the lap

I fitted the kernel‚Äôs length scale `l` as a function of speed: `l(v)`. To do this, I recorded driving data in batches at different constant speeds, optimized the GP on each batch, then fit a simple `l(v)` relation, which turned out to be very linear.

With the optimized kernel in hand, you can ask questions like:

*‚ÄúGiven this raceline and a speed, can my car follow it?""*

As the GP is a probabilistic model, it doesn‚Äôt give a binary answer that we requested. We could optimize for ‚Äúthe most likely speed‚Äù the same way we optimized the length scales. However, this would be more like asking, ‚ÄúWhat is the most likely speed this raceline can be achieved?‚Äù, which is okay for keeping your Tesla on the road, but not optimal for racing. My approach was to define an acceptable tolerance for the deviation from the raceline. With these constraints in hand, I run a heuristic window-based optimization for a given raceline:

https://i.redd.it/e7qteia2s9ef1.gif

**Results?**

Simulator executed lap plan times were close to human-driven laps. The model didn't account for acceleration limits, so actual performance fell slightly short of the predicted plan, but I think it proved the concept.

There are a lot of things that could be improved in the model. One of the biggest limitations is the independent models for x and y coordinates. Some of the things I also tried:

1. Absolute angle and cumulative distance model - This one considers the dynamics in terms of the absolute heading angle with respect to cumulative distance. This solves the problem of intercorrelation between X and Y coordinates, but introduces two more problems. First, to go back from the angle-domain, you need to integrate. This will lead to drifting errors. And even if you don‚Äôt want to go back to trajectory space, you still lose the direct link between the error definition of the two domains. And second, this function is not entirely smooth, so you need a fancier Kernel to capture the features. A Mat√©rn at least.
2. ‚ÄúUnfolding the trajectory‚Äù - This was one of my favorites, since it is the closest to the analogy of modeling y relation to x directly, wiggly road style. In the original domain, you would face the multivalued problem, where for a single x-value, there can be multiple y-values. One can ‚Äúunfold‚Äù the lap (loop) by reducing the corner angles until you have unfolded the points to a single-valued function. This, however, also destroys the link to the original domain error values.

Here is the code and the data if you want to make it better:  
[https://github.com/Miikkasna/gpdynalgo](https://github.com/Miikkasna/gpdynalgo)",14,0.77,https://www.reddit.com/r/MachineLearning/comments/1m5r5m3/r_gaussian_process_to_approximate_vehicle_dynamics/,False,True,False
1m5qudf,currentscurrents,1753122530.0,69,/r/MachineLearning/comments/1m5qudf/d_gemini_officially_achieves_goldmedal_standard/,MachineLearning,[D] Gemini officially achieves gold-medal standard at the International Mathematical Olympiad,"https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/

>This year, our advanced Gemini model operated end-to-end in natural language, producing rigorous mathematical proofs directly from the official problem descriptions ‚Äì all within the 4.5-hour competition time limit.",229,0.94,https://www.reddit.com/r/MachineLearning/comments/1m5qudf/d_gemini_officially_achieves_goldmedal_standard/,False,True,False
1m5opzo,Important_Book8023,1753117862.0,13,/r/MachineLearning/comments/1m5opzo/d_encoding_time_series_data_into_images_drawbacks/,MachineLearning,[D] Encoding time series data into images drawbacks,"So I've been reading many articles and reviews about encoding time series data into images, before feeding them into vision models for classification or forecasting. So this shifts the original problem from conventional time series analysis into the image¬†domain. Yet, i didn't find any article or even a phrase that mentions that this transformation has any drawbacks or limitations.  Do you think this is possible?",25,0.95,https://www.reddit.com/r/MachineLearning/comments/1m5opzo/d_encoding_time_series_data_into_images_drawbacks/,False,True,False
1m5jd8t,Basajaun-Eidean,1753105643.0,0,/r/MachineLearning/comments/1m5jd8t/p_echoes_of_gaia_modeling_evolution_in_biomes/,MachineLearning,[P] Echoes of GaIA: modeling evolution in biomes with AI for ecological studies.,"Hi there!

I'd like to share a project I've been working on over the last few months; **Echoes of GaIA** is a hybrid framework for modeling evolution and running biome simulations with ‚Äú*living*‚Äù ecosystems using lots of AI techniques. For context, I've been working quite a few years in the software and videogame development world, but four years ago I went back to university (hasn't been easy at this stage of life, but I just finished a few days ago and finally pulled out a huge thorn I'd had for more than 15 years) and this has been my capstone project. I specialized in Computation theory and Artificial Intelligence and wanted to create a kind of ode to AI and tackle biomes holistically, since I was eager to learn all these techniques and the underlying math.

The idea was to shape a project that - although just a very modest, small gesture, symbolic I‚Äôd say - tries to contribute something toward helping heal the planet, improving climate change, etc., through Artificial Intelligence. I just wanted to share it because I think it might interest people reading this subreddit, and I cover some pretty current topics that I believe are very important.

Anyway, some of the things I've implemented:

‚Ä¢ Climate and fauna agents based on **Reinforcement Learning**

‚Ä¢ **Genetic algorithms** for species **evolution**

‚Ä¢ ‚ÄúEquilibrium‚Äù agent (**neurosymbolic AI**) ‚Äì the idea here is to balance the whole ecosystem (for now using **LSTM multivariate multihorizon with attention** and expert systems and/or **graphs** as the knowledge base)

‚Ä¢ I also do c**omputational modeling** (but on its discrete side, not continuous) of many biological and physiological processes

It can be extended easily (I used ECS so I could have a modular component system for the biological processes of flora and fauna entities) and I've also put together a snapshot viewer and real‚Äëtime metrics (InfluxDB + Grafana).

Project website ‚Üí [https://www.echoes-of-gaia.com](https://www.echoes-of-gaia.com) (turn on **sound** before clicking!! I'm quite a big nerd and wanted to set a proper ambiance)

GitHub repo ‚Üí [https://github.com/geru-scotland/echoes-of-gaia](https://github.com/geru-scotland/echoes-of-gaia)

If anyone‚Äôs interested in the technical report, it's available on the site as **Main Doc** and there's also a document covering the project‚Äôs basic foundations, architecture, and main systems **Architecture doc** (those documents are only available in Spanish, unfortunately).

Any suggestions are more than welcome and, if you like it, I'd appreciate a star on GitHub. Thanks!",13,0.82,https://www.reddit.com/r/MachineLearning/comments/1m5jd8t/p_echoes_of_gaia_modeling_evolution_in_biomes/,False,True,False
1m54ppw,AgeOfEmpires4AOE4,1753057751.0,6,/r/MachineLearning/comments/1m54ppw/p_ai_learns_to_play_tmnt_arcade_deep/,MachineLearning,[P] AI Learns to Play TMNT Arcade (Deep Reinforcement Learning) PPO vs Recur...,"Github: [https://github.com/paulo101977/TMNT-RecurrentPPO](https://github.com/paulo101977/TMNT-RecurrentPPO)  
  
Hey everyone!  
I‚Äôve been training a **Recurrent PPO** agent to play the classic **Teenage Mutant Ninja Turtles (Arcade)** game using only visual input. The goal is to teach the agent to fight through the levels using memory and spatial awareness, just like a human would.

Here are some key details:

* **Environment:** TMNT Arcade via custom Gymnasium + stable-retro integration
* **Observations:** 4 stacked grayscale frames at **160√ó160** resolution
* **Augmentations:** Random noise, brightness shifts, and cropping to improve generalization
* **Reward Signal:** Based on score increase, boss damage, and stage progression
* **Algorithm:** Recurrent Proximal Policy Optimization (RecPPO) with CNN + LSTM
* **Framework:** PyTorch with custom training loop (inspired by SB3)

The recurrent architecture has made a big difference in stability and long-term decision making. The agent is now able to consistently beat the first few levels and is learning to prioritize enemies and avoid damage.",0,0.5,https://youtube.com/watch?v=ZM3ZiiC6Ryo&si=ia1L-PYLdXVtylDg,False,False,False
1m51kwv,Altruistic-Front1745,1753049298.0,18,/r/MachineLearning/comments/1m51kwv/d_is_transfer_learning_and_finetuning_still/,MachineLearning,[D] Is transfer learning and fine-tuning still necessary with modern zero-shot models?,"Hello. I am a machine learning student, I have been doing this for a while, and I found a concept called ""transfer learning"" and topics like ""fine tuning"". In short, my dream is to be an ML or AI engineer. Lately I hear that all the models that are arriving, such as Sam Anything (Meta), Whisper (Open AI), etc., are zero-shot models that do not require tuning no matter how specific the problem is. The truth is, I ask this because right now at university we are studying PyTorch and transfer learning. and If in reality it is no longer necessary to tune models because they are zero-shot, then it does not make sense to learn architectures and know which optimizer or activation function to choose to find an accurate model. Could you please advise me and tell me what companies are actually doing? To be honest, I feel bad. I put a lot of effort into learning optimization techniques, evaluation, and model training with PyTorch.",21,0.68,https://www.reddit.com/r/MachineLearning/comments/1m51kwv/d_is_transfer_learning_and_finetuning_still/,False,True,False
1m4vp1l,Accomplished-Copy332,1753034821.0,3,/r/MachineLearning/comments/1m4vp1l/p_anyone_interested_in_adding_their_finetuned/,MachineLearning,[P] Anyone interested in adding their fine-tuned / open source models to this benchmark?,"I've posted on this sub before, but context is that me and a small team are working on a¬†[benchmark](https://www.designarena.ai/)¬†to evaluate how good LLMs are at producing UIs and frontends that are engaging and satisfiable for people.

Right now, working on adding more models, and specifically open source models developed by individual developers (or a small group of developers). Above is the current top 10 in the leaderboard. If you're interested, just send me a DM.

Here are some requirements:

1. Inference needs to be fairly quick (max should take 3 minutes on average). Models are writing html/css/js code on the order of 4K-10K tokens on average.
2. Give us a logo and name for the provider/org you want the model to be associated with
3. An api endpoint that we can call with your desired parameters for the model. It needs to ideally be able to support a few concurrent requests at a time and around \~500 requests a day (though you can rate limit us if you would like to cap it at a smaller number)",4,0.64,https://i.redd.it/b9mz5z0ik2ef1.png,False,False,False
1m4ufir,Efficient-Ad-2913,1753031825.0,3,/r/MachineLearning/comments/1m4ufir/p_federated_learning_on_a_decentralized_protocol/,MachineLearning,"[P] Federated Learning on a decentralized protocol (CLI demo, no central server)","This CLI command spins up a decentralized federated learning session using Parity Protocol. No central coordination, no cloud. Model training is performed across independent nodes, and final aggregation is provably deterministic.

**Example usage:**

https://preview.redd.it/4cjz7qwcb2ef1.png?width=1192&format=png&auto=webp&s=959dd70368ec15d4f607486dc464cc339d691a9e

  
\- No central coordinator  
\- Nodes train locally on custom data shards  
\- Aggregation (e.g., FedAvg) happens across verifiable nodes  
\- All results are hash-verified before acceptance  
\- Decentralized, docker-native FL infra  
\- Ideal for research in Non-IID, private datasets, or public benchmark tasks

Project:  
GitHub ‚Äì¬†[https://github.com/theblitlabs](https://github.com/theblitlabs)  
Docs ‚Äì¬†[https://blitlabs.xyz/docs](https://blitlabs.xyz/docs)  
  
We‚Äôre college devs building a trustless alternative to AWS Lambda for container-based compute, Federated learning and LLM inference

Would love feedback or help. Everything is open source and permissionless.",22,0.89,https://www.reddit.com/r/MachineLearning/comments/1m4ufir/p_federated_learning_on_a_decentralized_protocol/,False,True,False
1m4s65p,LazyGuy-_-,1753026431.0,23,/r/MachineLearning/comments/1m4s65p/p_chess_llama_training_a_tiny_llama_model_to_play/,MachineLearning,[P] Chess Llama - Training a tiny Llama model to play chess,"You can try it out [here!](https://lazy-guy.github.io/chess-llama/)

It's a 23M parameter model based on the Llama 3 architecture and plays at around 1400 Elo.",59,0.94,https://lazy-guy.github.io/blog/chessllama/,False,False,False
1m4qqyv,alvises,1753022897.0,5,/r/MachineLearning/comments/1m4qqyv/p_finetuning_yolo_to_watch_football_soccer_matches/,MachineLearning,[P] Fine-Tuning YOLO to Watch Football (Soccer) Matches,"Hey everyone üëã This is my first post here :D

I published a guide on fine-tuning YOLO models for custom object detection, showing how to transform a generic 80-class detector into a specialized system (using soccer match analysis as an example).

A bit of context: I've been working on a YOLO library for Elixir that supports custom models via ONNX format. Since the library can load any custom YOLO model, I created this content to show how to train your own models using Ultralytics' tooling. The approach is language-agnostic - the resulting model works with any framework supporting PyTorch or ONNX, though I demonstrate Elixir integration at the end.

This fine-tuning approach applies to various industries where domain-specific object detection is needed - sports analytics, manufacturing QC, etc.

Elixir YOLO library:¬†[https://github.com/poeticoding/yolo\_elixir](https://github.com/poeticoding/yolo_elixir)

Video + Article about Elixir YOLO 0.2.0:¬†[https://www.poeticoding.com/elixir-yolo-v0-2-0-yolox-support-custom-models-and-performance-boost/](https://www.poeticoding.com/elixir-yolo-v0-2-0-yolox-support-custom-models-and-performance-boost/)

Let me know if you would find interesting some videos about the details of the YOLO architecture",17,0.91,https://www.poeticoding.com/fine-tuning-yolo-to-watch-soccer-matches/,False,False,False
1m4n35a,iamjessew,1753012496.0,1,/r/MachineLearning/comments/1m4n35a/d_monorepos_for_ai_projects_the_good_the_bad_and/,MachineLearning,"[D] Monorepos for AI Projects: The Good, the Bad, and the Ugly",,0,0.33,https://www.gorkem-ercan.com/p/monorepos-for-ai-projects-the-good,False,False,False
1m4n0ps,alexsht1,1753012259.0,19,/r/MachineLearning/comments/1m4n0ps/d_set_of_sequences_input_for_transformers/,MachineLearning,[D] Set of sequences input for transformers,"Hi all. A small question regarding encoding the position of inputs to a transformer model.

How would you encode a set of sequences to a (bidirectional) transformer? For a sequence we have positional encodings. For a set we can just work without them. What about a set of sequences {s\_1, ..., s\_n}, where each s\_1, ..., s\_n is a sequence, but their relative order does not matter?",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1m4n0ps/d_set_of_sequences_input_for_transformers/,False,True,False
1m4af9i,PassengerQuiet832,1752968351.0,2,/r/MachineLearning/comments/1m4af9i/r_3_backprop_vs_1_backprop_for_gan_discriminator/,MachineLearning,[R] 3 backprop vs 1 backprop for gan discriminator training,"I am trying to train a 3D gan using 2D discriminator that take slices of the original data.

And wanted to get your opinion on two points:

1- is it better to have 3 discriminators, one per plane. Or a single discriminator and takes the embedding of the plane as input.

2-my current implementation is something like this:

\- disc real training backprop 

\- disc fake training backprop

\- r1 regularisation backprop

\- gen training backprop



What would the expected effect of summing up the losses and doing one back prop per model? which method is better.",0,0.44,https://www.reddit.com/r/MachineLearning/comments/1m4af9i/r_3_backprop_vs_1_backprop_for_gan_discriminator/,False,True,False
1m459h3,RobbinDeBank,1752954543.0,3,/r/MachineLearning/comments/1m459h3/r_mixtureofrecursions_learning_dynamic_recursive/,MachineLearning,[R] Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation,"Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost.",14,1.0,https://arxiv.org/abs/2507.10524,False,False,False
1m3ytlc,Past-Technician-4211,1752938476.0,0,/r/MachineLearning/comments/1m3ytlc/r_raw_rf_msk_ultrasound_data_request/,MachineLearning,[R] Raw RF MSK Ultrasound Data Request,"Hi

I'm a undergrad working on¬†**signal processing and ML algorithms**¬†for MSK ultrasound analysis, but I'm struggling to find¬†**raw RF ultrasound datasets**¬†for my work.

**The Problem:**¬†Clinical scanners only provide processed B-mode images, but I need the raw radiofrequency data from the transducer for advanced analysis.

**Looking for:**

* Raw RF datasets from MSK ultrasound exams
* Public RF ultrasound databases

**Question:**¬†Has anyone worked with RF ultrasound data ? Any leads on accessing research platforms or datasets would be hugely appreciated!

tried referring to PICMUS dataset , but does have enough data for training a ml model for feature extraction

Thanks for any guidance!

**TL;DR:**¬†Need raw RF ultrasound data for MSK research. Clinical systems don't provide this. Seeking dataset sources",1,1.0,https://www.reddit.com/r/MachineLearning/comments/1m3ytlc/r_raw_rf_msk_ultrasound_data_request/,False,True,False
1m3ygy6,Possible-Session9849,1752937589.0,1,/r/MachineLearning/comments/1m3ygy6/p_benchstreet_the_benchmark_for_financial_time/,MachineLearning,[P] Benchstreet - the benchmark for financial time series forecasting.,,1,0.67,https://github.com/puffinsoft/benchstreet,False,False,False
1m3ydpx,Accomplished-Copy332,1752937365.0,0,/r/MachineLearning/comments/1m3ydpx/p_design_arena_a_benchmark_for_evaluating_llms_on/,MachineLearning,[P] Design Arena: A benchmark for evaluating LLMs on design and frontend development,"LLMs can do math, competitive programming, and more, but can they develop applications that people actually want to use?

This benchmark tasks LLMs to create interfaces at a users‚Äô request and then based on preference data, produces a stack ranking of the LLMs that currently are able to build the most satisfiable UI. ",6,0.87,https://www.designarena.ai/,False,False,False
1m3x0gq,youn017,1752933801.0,2,/r/MachineLearning/comments/1m3x0gq/p_pruning_benchmarks_for_lms_llama_and_computer/,MachineLearning,[P] Pruning benchmarks for LMs (LLaMA) and Computer Vision (timm),"Hi everyone, I am here to find a new contributor for our team's project, pruning (sparsity) benchmarks.

# Why should we develop this?

Even though there are awesome papers (i.e., Awesome-Pruning; [GitHub](https://github.com/he-y/Awesome-Pruning), [GitHub](https://github.com/hrcheng1066/awesome-pruning)) focused on pruning and sparsity, there are no (maybe... let me know if there are) open-source for fair and comprehensive benchmarks, making first-time users confused. And this made a question, ""What is SOTA in the fair environment? How can we profile them?""

# Why can PyTorch-Pruning be a fair benchmark?

Therefore, [PyTorch-Pruning](http://github.com/namgyu-youn/PyTorch-Pruning) mainly focuses on implementing a variable of pruning papers, benchmarking, and profiling in a fair baseline.

More deeply, in the Language Models (LLaMA) benchmarks, we use three evaluation metrics and prompts inspired by Wanda (Sun et al., 2023) and SparseGPT (ICML'23) :

* Model (parameters) size
* Latency : Time TO First Token (TTFT) and Time Per Output Token (TPOT) for computing total generation time
* Perplexity (PPL) scores : We compute it in same way like [Wanda](https://github.com/locuslab/wanda/blob/8e8fc87b4a2f9955baa7e76e64d5fce7fa8724a6/lib/prune.py#L214) and [SparseGPT](https://github.com/locuslab/wanda/blob/8e8fc87b4a2f9955baa7e76e64d5fce7fa8724a6/lib/prune.py#L214)
* Input Prompt : We uses `databricks-dolly-15k` like Wanda, SparseGPT

# Main Objective (Roadmap) : 2025-Q3 ([GitHub](https://github.com/namgyu-youn/PyTorch-Pruning/issues/1))

For more broad support, our main objectives are implementing or applying more pruning (sparsity) researches. If there is already implemented open-source, then it could be much easier. Please check fig1 if you have any interests.

[fig1. Roadmap : 2025-Q3](https://preview.redd.it/69h8sz9z7udf1.png?width=855&format=png&auto=webp&s=19ad89510f0c4948faec9772606f661cc3eeaa52)

> Since our goal is applying more researches for pruning (sparsity), we are not planning to apply inference engines like ONNX, TensorRT, DeepSpeed, or TorchAO. But applying those engines is definitely a long-term objective, and always welcome!

p.s., Feel free to comment if you have any ideas or advice. That could be gratefully helpful for better understanding!",7,0.89,https://www.reddit.com/r/MachineLearning/comments/1m3x0gq/p_pruning_benchmarks_for_lms_llama_and_computer/,False,True,False
1m3wfjq,5h3r_10ck,1752932216.0,4,/r/MachineLearning/comments/1m3wfjq/n_whats_new_in_agent_leaderboard_v2/,MachineLearning,[N] What's New in Agent Leaderboard v2?,"[Agent Leaderboard v2](https://preview.redd.it/2onzjdgb3udf1.png?width=1368&format=png&auto=webp&s=3d11b5e3ab64d3e913f8af4dc99bb78bfc202c7a)

**Here is a quick TL;DR üëá**

üß† **GPT-4.1** tops with 62% Action Completion (AC) overall.  
‚ö° **Gemini 2.5** Flash excels in tool use (94% TSQ) but lags in task completion (38% AC).  
üí∏ **GPT-4.1**\-mini is *most cost-effective* at $0.014/session vs. GPT-4.1‚Äôs $0.068.  
üè≠ No single model dominates across industries.  
ü§ñ **Grok 4** didn't lead in any metric.  
üß© Reasoning models *underperform* compared to non-reasoning ones.  
üÜï **Kimi‚Äôs K2** leads *open-source models* with 0.53 AC, 0.90 TSQ, and $0.039/session.

Link Below:

\[Blog\]: [https://galileo.ai/blog/agent-leaderboard-v2](https://galileo.ai/blog/agent-leaderboard-v2)

\[Agent v2 Live Leaderboard\]: [https://huggingface.co/spaces/galileo-ai/agent-leaderboard](https://huggingface.co/spaces/galileo-ai/agent-leaderboard)",11,0.74,https://www.reddit.com/r/MachineLearning/comments/1m3wfjq/n_whats_new_in_agent_leaderboard_v2/,False,True,False
1m3v7ll,yuntiandeng,1752928644.0,74,/r/MachineLearning/comments/1m3v7ll/r_neuralos_a_generative_os_entirely_powered_by/,MachineLearning,[R] NeuralOS: a generative OS entirely powered by neural networks,"We built NeuralOS, probably the world's most expensive operating system, running at a blazing 1.8fps on an NVIDIA H100 GPU. üòÖ

**What exactly is NeuralOS?**

It's an experimental generative OS that predicts every screen frame entirely from your mouse and keyboard inputs. No internet, no traditional software stack, purely hallucinated pixels.

**How does it work?**

* An RNN tracks the computer state (kind of like a traditional OS kernel, but all neural and continuous).
* A diffusion model generates the actual screen images (imagine a desktop environment, but fully neural-rendered).

The GIF shows a funny demo: NeuralOS running NeuralOS inside itself. Every single pixel you're seeing is model-generated, no network involved at all!

Long-term, our goal is to remove boundaries between software entirely and make OS fully customizable beyond fixed menus and options. Imagine asking your OS something like:

* ""Merge all my messaging apps into one interface.""
* ""Make Signal look like Messenger.""
* ""Turn the movie I'm watching into a playable video game.""

**I'm curious about your thoughts:**

* Could future OS interfaces just become human-like avatars (think Grok's Ani)? Are menus and app-specific UIs going away?
* What about fully generative games: could diffusion-based games eventually replace traditional ones?

Try the live demo here: [neural-os.com](http://neural-os.com) (you might need patience‚Ä¶)

More details about the project: [x.com/yuntiandeng/status/1944802154314916331](http://x.com/yuntiandeng/status/1944802154314916331)",581,0.92,https://i.redd.it/d0z35om8stdf1.gif,False,False,False
1m3v4bq,seraschka,1752928365.0,5,/r/MachineLearning/comments/1m3v4bq/p_the_big_llm_architecture_comparison/,MachineLearning,[P] The Big LLM Architecture Comparison,,83,1.0,https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html,False,False,False
1m3svgv,Friendly-Angle-5367,1752920624.0,3,/r/MachineLearning/comments/1m3svgv/d_what_are_the_most_important_rlvr_papers/,MachineLearning,[D] What are the most important RLVR papers?,I am searching for the big milestone papers on RLVR to get started in the field. ,5,0.78,https://www.reddit.com/r/MachineLearning/comments/1m3svgv/d_what_are_the_most_important_rlvr_papers/,False,True,False
1m3shn7,gigi_yanyan,1752919132.0,5,/r/MachineLearning/comments/1m3shn7/p_retinanet_mobilenetv2_for_edge_tpu_deployment/,MachineLearning,[P] RetinaNet + MobileNetV2 for Edge TPU Deployment,"Hey everyone! I‚Äôm currently working on a machine learning project and wanted to get some insights from the community.

I‚Äôm building a seed classification and detection system using RetinaNet. While its default backbone is ResNet50, I plan to deploy the model on a Raspberry Pi 5 with a USB Coral Edge TPU. Due to hardware limitations, I‚Äôm looking into switching the backbone to MobileNetV2, which is more lightweight and compatible with Edge TPU deployment.

I‚Äôve found that RetinaNet does allow custom backbones, and MobileNetV2 is supported (according to Keras), but I haven‚Äôt come across any pretrained RetinaNet + MobileNetV2 models or solid implementation references so far.

The project doesn‚Äôt require real-time detection‚Äîjust image-by-image inference‚Äîso I‚Äôm hoping this setup will work well. Has anyone tried this approach? Are there any tips or resources you can recommend?

Thanks in advance!
",5,0.86,https://www.reddit.com/r/MachineLearning/comments/1m3shn7/p_retinanet_mobilenetv2_for_edge_tpu_deployment/,False,True,False
1m3arzo,Spiritual-Resort-606,1752864980.0,19,/r/MachineLearning/comments/1m3arzo/r_paper_recommendations/,MachineLearning,[R] Paper recommendations?,"Hello guys :)  
Since I am through with my pile of papers to read, I wanted to ask you if there are any recent papers you liked and would recommend :)  
I am interested in everything that you find worthwhile, however since I need to specify my personal favorites to not get this post removed, I am mostly interested in:  
\- transformer architecture optimizations, including optimizers and losses  
\- theoretical machine learning, including scaling laws and interpretablility  
\- recent alternative models such as flow matching, lambda networks etc.  
\- and anything you think is well-done research :)

Thank you in advance,  
You never disappoint me :)

I wish you all a great day ;)",23,0.87,https://www.reddit.com/r/MachineLearning/comments/1m3arzo/r_paper_recommendations/,False,True,False
1m3amb0,marojejian,1752864606.0,3,/r/MachineLearning/comments/1m3amb0/r_a_minimum_description_length_approach_to/,MachineLearning,[R] A Minimum Description Length Approach to Regularization in Neural Networks,"[arxiv](https://arxiv.org/abs/2505.13398)

Curious for expert opinions on this paper.  This overall philosophy resonates with me a lot: Minimum Description Length  (MDL) seems like a better objective for generalization vs. common regularization methods.  Doing so might promote much better generalization, especially in the domains where transformers / LLMs struggle. 

The paper itself is very simple: they start with ""golden"" hand-crafted RNNs, and see how various approaches react to starting at this optimum.  They assert that standard approaches, like L1, L2 norm,  and/or gradient descent do worse, and wander from the optimum. So the argument is even if these methods found a general solution, they would not stick to it. 

Of course MDL is not differentiable.  But if it is a better objective, seems worth putting more effort into differentiable approximations.  
",13,1.0,https://www.reddit.com/r/MachineLearning/comments/1m3amb0/r_a_minimum_description_length_approach_to/,False,True,False
1m38s1b,VR-Person,1752860348.0,8,/r/MachineLearning/comments/1m38s1b/d_any_promising_nondeep_learning_based_ai/,MachineLearning,[D] Any promising non-Deep Learning based AI research project?,"For example, Gaussian Splatting shares some concepts with Deep Learning, but it is a different approach and mostly beats the NERF (Deep Learning based approach for the same goal)",16,0.87,https://www.reddit.com/r/MachineLearning/comments/1m38s1b/d_any_promising_nondeep_learning_based_ai/,False,True,False
1m34yq9,Hot_South5225,1752851583.0,4,/r/MachineLearning/comments/1m34yq9/d_liquid_neural_networks_on_time_series/,MachineLearning,[D] Liquid neural networks on time series,Anyone used differentials against time to model changes in neurons/ LNNs to model any form of time series data?,5,1.0,https://www.reddit.com/r/MachineLearning/comments/1m34yq9/d_liquid_neural_networks_on_time_series/,False,True,False
1m3319j,antcroca159,1752847052.0,2,/r/MachineLearning/comments/1m3319j/p_piaget_a_language_model_for_psychological_and/,MachineLearning,"[P] Piaget, a language model for psychological and philosophical reasoning","I just released [Piaget](https://huggingface.co/gustavecortal/Piaget-4B), a language model finetuned on 15k psychological and philosophical reasoning traces.

Piaget is based on Qwen3 and was finetuned on a subset of open reasoning traces from [Dolphin R1](https://huggingface.co/datasets/cognitivecomputations/dolphin-r1) and [General Reasoning](https://huggingface.co/datasets/GeneralReasoning/GeneralThought-430K).

Available sizes are: [0.6B](https://huggingface.co/gustavecortal/Piaget-0.6B), [1.7B](https://huggingface.co/gustavecortal/Piaget-1.7B), [4B](https://huggingface.co/gustavecortal/Piaget-4B), [8B](https://huggingface.co/gustavecortal/Piaget-8B).

Piaget was inspired by my position paper on emotion analysis: [Improving Language Models for Emotion Analysis: Insights from Cognitive Science](https://aclanthology.org/2024.cmcl-1.23/)

**Technical details**:

I performed domain filtering on [Dolphin R1](https://huggingface.co/datasets/cognitivecomputations/dolphin-r1) and [General Reasoning](https://huggingface.co/datasets/GeneralReasoning/GeneralThought-430K).

Prompts were embedded, clustered with k-means (k=20 000) and majority-voted for domain labels using [Qwen3-1.7B](https://huggingface.co/Qwen/Qwen3-1.7B), following the [Intelligent Internet pipeline](https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706).

Clusters tagged psychology or philosophy were retained for LoRA finetuning (rank=8, alpha=16, max length=2048, epoch=1, batch size=16).

The resulting dataset is available [here](https://huggingface.co/datasets/gustavecortal/PsychologicalReasoning-15k).",8,0.67,https://www.reddit.com/r/MachineLearning/comments/1m3319j/p_piaget_a_language_model_for_psychological_and/,False,True,False
1m2zrpr,Latter-Neat8448,1752837973.0,12,/r/MachineLearning/comments/1m2zrpr/d_thoughts_about_prompt_routing_what_do_you_think/,MachineLearning,"[D] thoughts about ""prompt routing""  - what do you think about it?","Hey everyone,

Like many of you, I've been wrestling with the cost of using different GenAI APIs. It feels wasteful to use a powerful model like GPT-4o for a simple task that a much cheaper model like Haiku could handle perfectly.

This led me down a rabbit hole of academic research on a concept often called 'prompt routing' or 'model routing'. The core idea is to have a smart system that analyzes a prompt¬†*before*¬†sending it to an LLM, and then routes it to the most cost-effective model that can still deliver a high-quality response.

It seems like a really promising way to balance cost, latency, and quality. There's a surprising amount of recent research on this (I'll link some papers below for anyone interested).

I'd be grateful for some honest feedback from fellow developers. My main questions are:

* **Is this a real problem for you?**¬†Do you find yourself manually switching between models to save costs?
* **Does this 'router' approach seem practical?**¬†What potential pitfalls do you see?
* If a tool like this existed, what would be most important? Low latency for the routing itself? Support for many providers? Custom rule-setting?

Genuinely curious to hear if this resonates with anyone or if I'm just over-engineering a niche problem. Thanks for your input!

**Key Academic Papers on this Topic:**

* Li, Y. (2025). LLM Bandit: Cost-Efficient LLM Generation via Preference-Conditioned Dynamic Routing. arXiv.¬†[https://arxiv.org/abs/2502.02743](https://arxiv.org/abs/2502.02743)
* Wang, X., et al. (2025). MixLLM: Dynamic Routing in Mixed Large Language Models. arXiv.¬†[https://arxiv.org/abs/2502.18482](https://arxiv.org/abs/2502.18482)
* Ong, I., et al. (2024). RouteLLM: Learning to Route LLMs with Preference Data. arXiv.¬†[https://arxiv.org/abs/2406.18665](https://arxiv.org/abs/2406.18665)
* Shafran, A., et al. (2025). Rerouting LLM Routers. arXiv.¬†[https://arxiv.org/html/2501.01818v1](https://arxiv.org/html/2501.01818v1)
* Varangot-Reille, C., et al. (2025). Doing More with Less -- Implementing Routing Strategies in Large Language Model-Based Systems: An Extended Survey. arXiv.¬†[https://arxiv.org/html/2502.00409v2](https://arxiv.org/html/2502.00409v2)
* Jitkrittum, W., et al. (2025). Universal Model Routing for Efficient LLM Inference. arXiv.¬†[https://arxiv.org/abs/2502.08773](https://arxiv.org/abs/2502.08773)
* and others...",10,0.69,https://www.reddit.com/r/MachineLearning/comments/1m2zrpr/d_thoughts_about_prompt_routing_what_do_you_think/,False,True,False
1m2y23l,glorious__potato,1752832055.0,24,/r/MachineLearning/comments/1m2y23l/p_understanding_muon_a_revolutionary_neural/,MachineLearning,[P] Understanding Muon: A Revolutionary Neural Network Optimizer,"https://preview.redd.it/oiupfzxptldf1.png?width=1536&format=png&auto=webp&s=ffc81d2aad36267e19040a2ce4515a933362690a

  
I just published a breakdown of Muon, the optimizer powering the new OS SOTA trillion-parameter model Kimi K2 and beating GPT-4.

üí° Why is Muon a big deal?

It rethinks how we optimize neural networks by treating weight matrices not just as numbers, but as geometric objects leading to 35% faster training with 15% fewer tokens.

Would love to hear your suggestions :)

[https://glorious-potato-19.notion.site/Understanding-Muon-A-Revolutionary-Neural-Network-Optimizer-233ffa7f40c4800eafa5cc843e039327](https://glorious-potato-19.notion.site/Understanding-Muon-A-Revolutionary-Neural-Network-Optimizer-233ffa7f40c4800eafa5cc843e039327)

https://preview.redd.it/r50mbmjrtldf1.png?width=1242&format=png&auto=webp&s=67e799f1a77dea762f8d8a459d051826bbfe37ea

",125,0.89,https://www.reddit.com/r/MachineLearning/comments/1m2y23l/p_understanding_muon_a_revolutionary_neural/,False,True,False
1m2ivub,Ambitious-Equal-7141,1752785224.0,9,/r/MachineLearning/comments/1m2ivub/p_building_a_vton_model_from_scratch_any_advice/,MachineLearning,"[P] Building a VTON model from scratch, any advice?","Did anyone ever build a virtual try on model from scratch? Thus no open sourced models used. Such as implementing the IDM-VTON model from scratch? If so, how would you go about it.I can't find anything on the internet. Any advice, guidance would be much much appreciated!!",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1m2ivub/p_building_a_vton_model_from_scratch_any_advice/,False,True,False
1m2d0lb,poppyshit,1752771725.0,4,/r/MachineLearning/comments/1m2d0lb/p_xpinn_toolkit/,MachineLearning,[P] XPINN Toolkit,"Hi folks,

I'm currently developing a framework for eXtended Physics-Informed Neural Networks (XPINNs) and would really appreciate any reviews, suggestions, or feedback!

This is my first time building a tool intended for users, so I‚Äôm figuring things out as I go. Any insights on the design, usability, or implementation would be super helpful.

*What is XPINN?*  
*XPINNs extend standard Physics-Informed Neural Networks (PINNs) by splitting the problem domain into smaller subdomains. Each subdomain is handled by a smaller PINN, and continuity is enforced via interface conditions. This can help with scaling to more complex problems.*

Here‚Äôs the GitHub repo:  
[https://github.com/BountyKing/xpinn-toolkit](https://github.com/BountyKing/xpinn-toolkit)",3,1.0,https://www.reddit.com/r/MachineLearning/comments/1m2d0lb/p_xpinn_toolkit/,False,True,False
1m22m1d,VR-Person,1752742899.0,53,/r/MachineLearning/comments/1m22m1d/d_is_vjepa2_the_gpt2_moment/,MachineLearning,[D] is V-JEPA2 the GPT-2 moment?,"LLMs are inherently limited because they rely solely on textual data. The nuances of how life works, with its complex physical interactions and unspoken dynamics, simply can't be fully captured by words alone

In contrast, **V-JEPA2**, a self-supervised learning model. It learned by ""watching"" millions of hours of videos on the internet, which is enough for developing an intuitive understanding of how life works. 

In simple terms, their approach first learns extracting the predictable aspects of a video and then learns to predict what will happen next in a video at a high level.  After training, a robotic arm powered by this model imagines/predicts the consequence of its actions before choosing the best sequence of actions to execute

Overall, the model showed state-of-the-art results, but the results are not that impressive, though GPT-2 was not impressive at its time either.

Do you think this kind of self-supervised, video-based learning has revolutionary potential for AI, especially in areas requiring a deep understanding of the physical world (do you know another interesting idea for achieving this, maybe an ongoing project)? Or do you believe a different approach will ultimately lead to more groundbreaking results?",28,0.69,https://www.reddit.com/r/MachineLearning/comments/1m22m1d/d_is_vjepa2_the_gpt2_moment/,False,True,False
1m1zukw,casualcreak,1752732262.0,22,/r/MachineLearning/comments/1m1zukw/d_is_anyone_this_old/,MachineLearning,[D] Is anyone this old? ü•≤,[https:\/\/www.cs.cmu.edu\/\~tom\/files\/MachineLearningTomMitchell.pdf](https://preview.redd.it/sma7x3kvkddf1.png?width=1214&format=png&auto=webp&s=858a6025d48aeda1939426cf289ed24dfa46fd64),102,0.88,https://www.reddit.com/r/MachineLearning/comments/1m1zukw/d_is_anyone_this_old/,False,True,False
1m1olt9,Smart-Art9352,1752700053.0,10,/r/MachineLearning/comments/1m1olt9/d_concerns_about_predatory_publishers_frontiers/,MachineLearning,"[D] Concerns about Predatory Publishers (Frontiers, MDPI) Exhibiting at ICML 2025","https://preview.redd.it/n8cdebx6xadf1.png?width=1810&format=png&auto=webp&s=228db12a27752b9fed0dbdc9f4731e31e2cd47f4

Just saw that Frontiers and MDPI are listed as book publishers at ICML 2025. Kind of shocked, honestly. Both have a reputation for questionable publishing practices.

It feels off for a top ML conference to give them this kind of platform. Anyone else concerned or know how exhibitor decisions are made?",55,0.82,https://www.reddit.com/r/MachineLearning/comments/1m1olt9/d_concerns_about_predatory_publishers_frontiers/,False,True,False
1m1jh78,ModerateSentience,1752688331.0,28,/r/MachineLearning/comments/1m1jh78/should_a_large_enough_network_be_able_to_learn/,MachineLearning,Should a large enough network be able to learn random noise? [D],"I made my own FNN from scratch, but it has trouble learning random noise. I‚Äôm not talking about generalization, but my training MSE for regression can only get down and plateaus at around 0.05. Given all my output values are between 0 and 1. 

I thought with enough capacity a network could learn anything.

(For reference, I have 9 hidden layers with 1000 nodes using RELU)
",14,0.72,https://www.reddit.com/r/MachineLearning/comments/1m1jh78/should_a_large_enough_network_be_able_to_learn/,False,True,False
1m1in5r,AdministrativeRub484,1752686452.0,166,/r/MachineLearning/comments/1m1in5r/d_emnlp_2025_metareviews/,MachineLearning,[D] EMNLP 2025 Meta-reviews,Shouldn't they have come out \~6 hours ago?,41,0.9,https://www.reddit.com/r/MachineLearning/comments/1m1in5r/d_emnlp_2025_metareviews/,False,True,False
1m1fhnt,yungyany,1752679448.0,1,/r/MachineLearning/comments/1m1fhnt/p_r_deep_learningassisted_slam_to_reduce/,MachineLearning,[P [R] Deep learning-assisted SLAM to reduce computational,"I'm exploring ways to optimise SLAM performance, especially for real-time applications on low-power devices. I've been looking into hybrid deep learning approaches, specifically using SuperPoint for feature extraction and NetVLAD-lite for place recognition. My idea is to train these models offboard and run inference onboard (e.g., drones, embedded platforms) to keep compute requirements low during deployment. My reading as to which this would be more efficient would be as follows:

* Reducing the number of features needed for reliable tracking. Pruning out weak or non-repeatable points would slash descriptor matching costs
* better loop closure by reducing false positives, fewer costly optimisation cycles and requiring only one forward pass per keyframe.

I would be interested in reading your inputs and opinions.",5,0.86,https://www.reddit.com/r/MachineLearning/comments/1m1fhnt/p_r_deep_learningassisted_slam_to_reduce/,False,True,False
1m1ec4f,AngryDuckling1,1752676813.0,8,/r/MachineLearning/comments/1m1ec4f/d_changing_values_in_difficult_to_predict_range/,MachineLearning,[D] Changing values in difficult to predict range,"I have a coworker who is trying to train a model to predict a variable for customers. It‚Äôs very niche (don‚Äôt want to dox myself) so let‚Äôs just say they are trying to predict chromosome length from other biological variables. When presenting their model, they explained that the model was having difficulty predicting values in a certain range. For example purposes let‚Äôs say this range of values was 100-200. They mentioned that in order for the model to perform better in that range they explicitly changed the values of some observations to be in that range. I‚Äôm not talking scaling or normalization or some other transformation, I mean they took a certain number of observations whose target variable was below 100 and changed the value to 150, and the same with some observations above 200.

I asked for clarification like 3 times and they very confidently said this was best practice, and no other analyst said anything. They are the ‚Äúhead of AI‚Äù and this work will be presented to the board. Is this not an absolutely insane thing to do or am I the idiot?

FWIW: they use chatgpt for absolutely everything. My hunch is that this is an extremely ill-informed chatgpt approach but the fact that i‚Äôm the only one who see‚Äôs any issue with this on my team is making me gaslight myself",10,1.0,https://www.reddit.com/r/MachineLearning/comments/1m1ec4f/d_changing_values_in_difficult_to_predict_range/,False,True,False
1m1dymr,BarEducational9905,1752675920.0,20,/r/MachineLearning/comments/1m1dymr/d_guys_i_just_got_interviewed_can_you_help_me_if/,MachineLearning,"[D] Guys i just got interviewed, can you help me if i was cooked ?","So i was in the CTO round of this interview for Data Scientist role , and he asked me to code a realtime face emotion age and gender detection tool without using llms and without straight up copy paste code for references , he then gave me an hour to do that but with same restrictions but i was only able to do the face recognition part ! am i cooked ?",0,0.17,https://www.reddit.com/r/MachineLearning/comments/1m1dymr/d_guys_i_just_got_interviewed_can_you_help_me_if/,False,True,False
1m1dmur,Training_Impact_5767,1752675118.0,6,/r/MachineLearning/comments/1m1dmur/p_human_activity_recognition_on_stm32_nucleo/,MachineLearning,[P] Human Activity Recognition on STM32 Nucleo,"Hi everyone,

I recently completed a university project where I developed a Human Activity Recognition (HAR) system running on an STM32 Nucleo-F401RE microcontroller. I trained an LSTM neural network to classify activities such as walking, running, standing, going downstairs, and going upstairs, then deployed the model on the MCU for real-time inference using inertial sensors.

This was my first experience with Edge AI, and I found challenges like model optimization and latency especially interesting. I managed the entire pipeline from data collection and preprocessing to training and deployment.

I‚Äôm eager to get feedback, particularly on best practices for deploying recurrent models on resource-constrained devices, as well as strategies for improving inference speed and energy efficiency.

If you‚Äôre interested, I documented the entire process and made the code available on GitHub, along with a detailed write-up:

* [GitHub](https://github.com/pescetti-studio/HAR-EdgeAI/)
* [Medium article](https://medium.com/@crocilorenzo01/my-first-har-ai-from-dataset-to-microcontroller-114b418b1509)

Thanks in advance for any advice or pointers!",8,1.0,https://www.reddit.com/r/MachineLearning/comments/1m1dmur/p_human_activity_recognition_on_stm32_nucleo/,False,True,False
1m18mn3,GeorgeBird1,1752660150.0,21,/r/MachineLearning/comments/1m18mn3/rd_interpretability_as_a_side_effect_are/,MachineLearning,[R][D] Interpretability as a Side Effect? Are Activation Functions Biasing Your Models?,"**TL;DR:**¬†Through an ablation study, it is demonstrated that current activation functions result in discrete representations, whereas a new breed of activation functions preserves data continuity. The discrete clusters emerge in geometries about individual neurons, indicating that activation functions exert a strong bias on representations.¬†***This reveals a causal mechanism that significantly reframes*** **many** ***interpretability phenomena, which are now shown to emerge from design choices rather than being fundamental to deep learning.***

# Overview:

Activation functions are often considered as a harmless choice, a minor tweak. Each carries slight differences in performance, but are deemed not to result in much explicit effect on internal representations. *This paper shows that this impression is incorrect.*

It demonstrates that **activation functions today lead to a representational collapse**, regardless of the task and dataset, ***acting as a strong and unappreciated inductive bias***. Such a systematic representational collapse may be limiting all model expressiveness to date. It also suggests that these discrete clusters are then detected, downstream, as numerous interpretability phenomena --- including grandmother neurons, discrete neural codes, polysemanticity, and possibly Superposition.

>This reframes the approach to interpretability, suggesting that many such patterns are artefacts of our design choices and potentially provides a unifying mechanistic theory to explain them.

The striking finding is that a different defining choice in the foundational mathematics of deep learning **can turn such an interpretability phenomenon on and off**. This paper demonstrates this, showing that such phenomena appear as a result of design choice, rather than being fundamental to our field.

When discretisation is turned off in autoencoders, performance is shown to improve frequently, and representations appear to exhibit exponential growth in representational capacity, rather than typical linear growth.

This indicates enormous consequences, not least for mechanistic interpretability. But also **encourages a reevaluation of the fundamental mathematical definitions at the base of our field**. Affecting most building blocks, including activation functions, normalisers, initialisers, regularisers, optimisers, architectures, residuals, operations, and gradient clipping, among others ‚Äî indicating a foundational rethink may be appropriate with alternative axiomatic-like definitions for the field ‚Äî *a new design axis that needs exploration!*

**How this was found:**

Practically all current design choices break a larger symmetry, which this paper shows is propagated into broken symmetries in representations. These broken symmetries produce clusters of representations, which then appear to emerge and are detected as interpretable phenomena. Reinstating the larger symmetry is shown to eliminate such phenomena; hence, they arise causally from symmetries in the functional forms.

This is shown to occur independently of the data or task. By swapping in symmetries, it is found that this enforced discrete nature can be eliminated, yielding smoother, likely more natural embeddings. An ablation study is conducted between these two, using autoencoders, which are shown to benefit from the new continuous symmetry definition generally.

* Ablation study between these isotropic functions, defined through a continuous 'orthogonal' symmetry (rotation+mirrors O(n)), and current functions, including Tanh and Leaky-ReLU, which feature discrete axis-permutation symmetries, (Bn) and (Sn).
* Showcases a new visual interpretability tool, the ""PPP method"". This maps out latent spaces in a clear and intuitive way!

**Implications:**

These results significantly challenge the idea that neuron-aligned features, grandmother neurons, and general-linear representational clusters are fundamental to deep learning.¬†**This paper provides evidence that these phenomena are unintended side effects of symmetry in design choices,**¬†arguing that ***they are not fundamental to deep learning.***¬†This may yield significant implications for interpretability efforts.

* **Current Interpretability may often be detecting Artefacts**. Axis-alignment, discrete coding, discrete interpretable direction, and possibly Superposition appear *not to be*¬†spontaneous or fundamental to deep learning.¬†Instead, they seem to be stimulated by the symmetry of model primitives, particularly the activation function is demonstrated in this study. It reveals a direct causal mechanism for their emergence, which was previously unexplained.
* **We can ""turn off"" interpretability by choosing isotropic primitives, which appear to improve performance on at least specific tasks.**¬†*Grandmother neurons vanish!* This raises profound questions for research on interpretability. The¬†*current methods may only work because of this imposed bias*. Does this put interpretability and expressibility at loggerheads? Interestingly, this eliminates externally applied algebra-induced structure, but some structure appears to reemerge intrinsically from data --- potentially a more fundamental interpretable phenomenon.
* **Symmetry group is an inductive bias.**¬†Algebraic symmetry presents a new design axis‚Äîa taxonomy where each choice imposes unique inductive biases on representational geometry, necessitating further extensive research.

These results support earlier predictions made when questioning the foundational mathematics (see the paper below). Introduced are continuous symmetry primitives, where the very existence of neurons appears as an observational choice --- challenging neuron-wise independence, along with a broader symmetry-taxonomy design paradigm.

>This is believed to be a new form of choice and influence on models that has been largely undocumented until now.

Most building blocks of current deep learning (*over the last 80ish years*) mostly sit along a 'permutation branch' --- which some might be familiar with in terms of just parameters. However, this work encourages a ***redefinition of all the primitives*** and **new foundations through a broad array of alternative symmetries** \--- proposed are new 'branches' to consider (*but may take a long time to develop sufficiently, help is certainly welcomed!*).

**Distinctions:**

Despite the use of symmetry language, this direction appears substantially different and tangential from previous Geometric Deep Learning approaches, and except for its resemblance to neural collapse, this phenomenon appears distinctly different. This theory is not due to classification or one-hot encoding, but forms of primitives more generally. It is somewhat related to observations of parameter symmetry, which arise as a special case and consequence of this new broader framework.

Observation of symmetry is instead redeployed as a definitional tool for novel primitives, which appears to be a new, useful design axis. Hence, these results support the exploration of a seemingly under-explored, yet rich, avenue of research.

# Relevant Paper Links:

This paper builds upon several previous papers that encourage the exploration of a research agenda, which consists of a substantial departure from the majority of current primitive functions. This paper provides the first empirical confirmation of several predictions made in these prior works.

* [üìÑ **Emergence of Quantised Representations Isolated to Anisotropic Functions**](https://doi.org/10.5281/zenodo.15783098) \[New **preprint** being discussed in this post, awaiting arXiv\]
* [üìÑ **Isotropic Deep Learning: You Should Consider Your (Inductive) Biases**](https://doi.org/10.5281/zenodo.15476947) \[Critical Position Paper: *provides the new definitions, delves into the broad symmetry-unifying theory, shows that this approach is distinct from other topics*\]
* [üìÑ **The Spotlight Resonance Method: Resolving the Alignment of Embedded Activations**](https://arxiv.org/abs/2505.13471) \[New paper extended this prior approach\]

üìò A [**Summary Blog**](https://medium.com/@george.bird.uom/draft-a-hidden-inductive-bias-at-the-heart-of-deep-learning-4e197b56f34c)¬†covers many of the main ideas being proposed in a way that is hopefully ***intuitive, approachable, and exciting!*** It also motivates the driving philosophy behind the work and potential long-term outcomes.",60,0.94,https://www.reddit.com/r/MachineLearning/comments/1m18mn3/rd_interpretability_as_a_side_effect_are/,False,True,False
1m14upp,YammaTV,1752645460.0,2,/r/MachineLearning/comments/1m14upp/r_interesting_paper_on_costaware_prompt/,MachineLearning,[R] Interesting paper on cost-aware prompt optimization (CAPO),"Just came across this prompt optimization paper that I found pretty interesting - thought others might want to check it out.

They implement a prompt tuning algorithm that uses evolutionary algorithms to optimize prompts more efficiently. It jointly optimizes both instructions and few-shot examples, which sadly have been missing in other techniques.

They seem to get Super promising results - outperforming other optimizers on GSM8K by around 20% and beat existing methods on most benchmarks, while  being more efficient.

What I particularly liked was their implementation with the Promptolution framework - seems quite industry-ready compared to most academic code.

Paper [https://openreview.net/forum?id=UweaRrg9D0#discussion](https://openreview.net/forum?id=UweaRrg9D0#discussion)  

Code [https://github.com/finitearth/capo](https://github.com/finitearth/capo)",15,0.94,https://www.reddit.com/r/MachineLearning/comments/1m14upp/r_interesting_paper_on_costaware_prompt/,False,True,False
1m123zn,danielwilu2525,1752636094.0,13,/r/MachineLearning/comments/1m123zn/p_lstm_to_recognize_baseball_players_based_on/,MachineLearning,[P] LSTM to recognize baseball players based on their swing keypoint data,"
I want to make some kind of tool where it can identify professional baseball players based on a video of their swing.

- Extracts pose keypoint data from that professional player (done)

- Runs the keypoint time series into a LSTM model 

- Model classifies this sequence of keypoints to a specific player 

Is this possible? My main concern is that baseball swings numerically look so similar so I‚Äôm not sure if a model can pick up on the different nuances of professional player swings. Any ideas would be great.

https://youtu.be/YYC9aS60Q60?si=uWs1hX2J5SHfGkii",5,0.7,https://www.reddit.com/r/MachineLearning/comments/1m123zn/p_lstm_to_recognize_baseball_players_based_on/,False,True,False
1m11bog,Repulsive-Chart9411,1752633734.0,9,/r/MachineLearning/comments/1m11bog/r_interactive_probabilistic_neural_network/,MachineLearning,[R] Interactive Probabilistic Neural Network Decision Matrix Model,"I made this model while procrastinating a project of mine. I put a lot of effort into this and would appreciate feedback. its interactive so you can move the camera zoom rotate and pan. pressing 1 through 0, will light up the network layer by layer from the entry node to the exit ring. every link was created probabilistically and very deterministically. every link has significance and is unique, in a very reproduceable fashion. :P I learned a lot making this and I hope you will learn something new or pick up a new insight from playing with it. Its time to kick the learning into overdrive. lets do this.

[https://hf-laboratories.github.io/Interactive-Probabilistic-Neural-Network-Decision-Matrix/](https://hf-laboratories.github.io/Interactive-Probabilistic-Neural-Network-Decision-Matrix/)",9,0.72,https://www.reddit.com/r/MachineLearning/comments/1m11bog/r_interactive_probabilistic_neural_network/,False,True,False
1m0w8n1,debrises,1752619572.0,3,/r/MachineLearning/comments/1m0w8n1/how_to_find_a_relevant_phd_topic_in_computer/,MachineLearning,How to find a relevant PhD topic in computer vision? Industry problem vs trendy topics [D],"Hello, I'm considering doing a PhD in computer vision. I have a somewhat unconventional situation where I have master's in civil engineering from my home country in eastern Europe and a bachelor's in data science from a German university.
I have 1y.o. as a research assistant + 2y.o. as an ml / computer vision engineer at a med tech company in Germany. 

I feel like I always had passion for science and natural talent in maths, but because of some life circumstances I hadn't had a chance to fulfill this dream of solving a very complicated problem or being in a challenging environment with like-minded people. That's why I'm aiming for a top tier universities like ETH or TUM, but I'm a bin unsure what topic to pick for my application.

In my current role I'm doing lots of R&D work for the company and I've identified a real unsolved industry problem that is very clearly postulated, and I think my company could even provide a large dataset for it. At the same time the problem is very domain specific and it's basically an instance segmentation problem with some extra steps, and I'm a bit afraid that it might lack the research depth needed for such top tier labs. Plus I feel like it would limit my career perspectives in the future and doing a PhD in a more general field (not domain - specific data but rather regular images/videos etc) would open more doors for me in the future.

I'm genuinely interested in the vision problems and would love to learn more about a 3d domain for example but had limited experience in it so far and not sure if I'd get accepted with this kinda topic. 

How did you find your topic? Should I double down on a real use case and my existing experience or rather read more recent papers and find out more about recent developments find a relevant topic? Do you have similar experience applying to top tier universities? 
Thank you for your advice and beta regards.",1,0.6,https://www.reddit.com/r/MachineLearning/comments/1m0w8n1/how_to_find_a_relevant_phd_topic_in_computer/,False,True,False
1m0vu90,AtMaxSpeed,1752618557.0,1,/r/MachineLearning/comments/1m0vu90/icml_2025_can_a_workshop_registration_access/,MachineLearning,"ICML 2025, can a workshop registration access poster sessions and/or socials? [D]","As the title asks, I'm wondering if anyone knows if a workshop-only registration can access the poster sessions and/or the social events? Or do I need a conference registration to access those?

It's surprisingly hard to find this answer on ICML official sources, but maybe I just couldn't find it. This is my first ICML, so if anyone could help answer this it would be greatly appreciated. Thanks! ",6,0.75,https://www.reddit.com/r/MachineLearning/comments/1m0vu90/icml_2025_can_a_workshop_registration_access/,False,True,False
1m0hj8g,Standing_Appa8,1752585475.0,17,/r/MachineLearning/comments/1m0hj8g/p_help_with_contrastive_learning_mri_biomarkers/,MachineLearning,[P] Help with Contrastive Learning (MRI + Biomarkers) ‚Äì Looking for Guidance/Mentor (Willing to Pay),"Hi everyone,

I‚Äôm currently working on a research project where I‚Äôm trying to apply **contrastive learning** to **FreeSurfer-based brain data** (structural MRI features) and **biomarker data** (tabular/clinical). The idea is to learn a shared representation between the two modalities.

The problem: I am **completely lost**.

* I‚Äôve implemented losses like **NT-Xent** and a few others (SupCon, etc.), but I can‚Äôt get the approach to work in a meaningful way.
* I‚Äôm struggling to figure out the best architecture or training strategy, and I‚Äôm honestly not sure what direction to take next.
* There is **no proper supervision in my lab**, and I feel stuck with how to proceed.

I really need guidance from someone experienced in contrastive learning or multimodal representation learning. Ideally, someone who has worked with **medical imaging + tabular/clinical data** before. (So it is not about classical CLIP with Images and Text).

I‚Äôm **willing to pay** for mentoring sessions or consulting to get this project on track.

If you have experience in this area (or know someone who does), please reach out or drop a comment. Any advice, resources, or even a quick chat would mean a lot.

Thanks in advance!",12,0.75,https://www.reddit.com/r/MachineLearning/comments/1m0hj8g/p_help_with_contrastive_learning_mri_biomarkers/,False,True,False
1lzknj5,CatSweaty4883,1752494867.0,0,/r/MachineLearning/comments/1lzknj5/dmust_read_papers_for_lip_reading_task/,MachineLearning,[D]Must read papers for Lip Reading task?,"Hello all, what are some of the best papers you have read on this particular topic of Lip Reading? From what I've seen until now, after LipNet and Lip2Wav, I couldn't find much impactful papers. Are there any which I am missing?",3,1.0,https://www.reddit.com/r/MachineLearning/comments/1lzknj5/dmust_read_papers_for_lip_reading_task/,False,True,False
1lzn3tu,Pale_Meringue_3079,1752501499.0,5,/r/MachineLearning/comments/1lzn3tu/d_handling_right_skewed_data_for_a_cvae/,MachineLearning,[D] Handling Right Skewed Data for a CVAE,"[D] Dear ML Community, I am currently working on a CVAE for fluid dynamics. I have huge datasets and the input data is mainly right skewed. The skewness depends on the dataset. I thought about changing to a gamma VAE and implement a new loss function instead of the MSE. Another option is to use the yeo Johnson normalization and keep the MSE. Or I could try to combine the normalization with the gamma loss function? Do you have advices or any different ideas? ",2,1.0,https://www.reddit.com/r/MachineLearning/comments/1lzn3tu/d_handling_right_skewed_data_for_a_cvae/,False,True,False
1lzzi5p,darshinium,1752529350.0,3,/r/MachineLearning/comments/1lzzi5p/p_tinygemm_fast_cuda_kernels_for_quantized_llms/,MachineLearning,"[P] tinygemm: Fast CUDA Kernels for Quantized LLMs (int4, nf4, mx4, any4‚Ä¶)","We‚Äôre excited to announce [**tinygemm**](https://github.com/facebookresearch/any4) ‚Äî a fast, low-latency GEMM library designed for **small batch sizes** and **quantized matrix multiplication** on NVIDIA GPUs.

It supports a range of numeric formats, including:

* `bf16` / `fp16`
* `int4` (grouped quantization)
* `nf4` (grouped quantization)
* `mx4` (a hybrid quantization format)
* `any4` ‚Äî a **learned** 4-bit format introduced in our [ICML 2025 paper](https://www.alphaxiv.org/abs/2507.04610)

üîç **any4** learns the optimal 4-bit codebook from model weights using K-Means clustering, and consistently outperforms fixed formats like `int4` and `nf4` across various LLMs and tasks.

# üîß What‚Äôs included in tinygemm:

* Fast CUDA kernels for quantized matmuls
* Support for multiple 4-bit formats
* Optimized for decoder inference (small batch, high throughput)
* Evaluation scripts for:
   * Perplexity, NLP, and code generation tasks
   * Visualization of weights and activations across layers
   * Plug-and-play support for any ü§ó HuggingFace model

# üöÄ Quick Example

```python
from transformers import AutoModelForCausalLM
from quantize import int4, any4, int8, nf4, fp4

model = AutoModelForCausalLM.from_pretrained(""facebook/opt-125m"").cuda().bfloat16()

# you can do int4(..), int8(..), nf4(..), fp4(..)
model = any4(model)

# just run your generation, evaluation, etc. code on `model`
```



üîó **Code:** https://github.com/facebookresearch/any4

üìÑ **Paper:** https://arxiv.org/abs/2507.04610

",15,1.0,https://www.reddit.com/r/MachineLearning/comments/1lzzi5p/p_tinygemm_fast_cuda_kernels_for_quantized_llms/,False,True,False
1m01hno,francozzz,1752534136.0,23,/r/MachineLearning/comments/1m01hno/d_how_to_market_myself_after_a_phd/,MachineLearning,[D] How to market myself after a PhD,"Hello all. I am doing a PhD in Computer Science at a mid tier university in Europe (not Cambridge, not ETH Zurich, but still a good one). My major will be in Data Science, the title of my dissertation will be along the lines of ‚ÄúMultimodal Machine Learning for Healthcare‚Äù.

My background is not in computer science: I was a healthcare professional, and I took a Master in Health Informatics. My thesis was in Data Science, and after that I started a PhD at the same university.

At the moment I have just finished my second year. I have two conference papers as first author and I have submitted two journal papers, still as first author. I have also submitted a few conference papers not as first author, with master students that I have supervised. None of these papers is technically innovative: they are applied papers. My planned work for the coming years is more technical (developing explainability techniques).

I still have two/three years of PhD in front of me, and I am getting scared of what will happen afterwards. I have been told that IF there will be an opening to stay at my university and teach (emphasis on the if), I would be considered a good applicant.

That‚Äôs great, and it would be my first choice, BUT:
- it‚Äôs impossible to know if these positions will exist close to my graduation date
- competition exists, and these positions are usually for a single opening. No one can guarantee that I‚Äôll be the top applicant.

I‚Äôm honestly scared of betting everything on a possibility that might not be there for me in the end.
In the coming three semesters, I could decide to spend some time outside my department: using Erasmus to go to another university in Europe, as a student and possibly teaching some courses, to the US, where one researcher might be interested to write a paper together, or to a pharma company in my country, where my supervisor has some contacts.

I also have two/three years to study more, and to study different things.
If I will have to transition to the industry, I am scared that I would not be a good enough programmer. I would prefer positions as a project manager, possibly with some technical aspects, but not completely focused on producing code as fast as possible.

Based on your experience, do you have any suggestions on what to do to try to improve my possibilities after graduation?

",43,0.89,https://www.reddit.com/r/MachineLearning/comments/1m01hno/d_how_to_market_myself_after_a_phd/,False,True,False
1m00rnf,berkusantonius,1752532362.0,9,/r/MachineLearning/comments/1m00rnf/p_anyone_interested_in_tinyml/,MachineLearning,[P] Anyone interested in TinyML?,"Hi!

I wrote sklearn2c library for the book I co-authored and I wanted to share it as an open-source project.

sklearn2c takes your trained scikit-learn models and generates lightweight C code that can run on microcontrollers and other resource-constrained embedded systems.¬†Perfect for when you need real-time ML inference but don't have the luxury of a full Python environment.

Usage is dead simple:

    dtc = DTClassifier()
    dtc.train(train_samples, train_labels, save_path=""path/to/model"")
    dtc.predict(test_samples)
    dtc.export(""path/to/config_dir"")  # Generates C code!

Would love to hear your thoughts, especially if you've worked with ML on embedded systems before! The project is MIT licensed and open to contributions.

**GitHub:**¬†[https://github.com/EmbeddedML/sklearn2c](https://github.com/EmbeddedML/sklearn2c)

Thanks for checking it out! üöÄ And if you find it useful, don't forget to star the project - it really helps with visibility! ‚≠ê",121,0.97,https://www.reddit.com/r/MachineLearning/comments/1m00rnf/p_anyone_interested_in_tinyml/,False,True,False
1lzwrw2,Hopeful-Reading-6774,1752523091.0,34,/r/MachineLearning/comments/1lzwrw2/d_ml_phd_doing_research_in_a_not_trendy_topic_how/,MachineLearning,[D] ML PhD doing research in a not trendy topic - How to pivot,"Hi All,

Looking for some advice on this sub. Basically, as the title suggest my PhD is not in a trendy topic. Specifically, my topic is out of distribution generalization for distributed edge devices.

I am currently in my 4th year (USA PhD) and would like to focus on something that I can use to market myself for an industry position during my 5th year.

(1) One option is to try to hop on to the trendy topic and do some projects (can't pivot my research as advisor is not in favor and currently being paid by him). However, not sure what traction would I have since I will not have any publication.  
(2) Second option is to try to get into more SWE with agentic AI integration. Not sure if this is just a fad or here to stay.  
(3) Last option I have been thinking is to pickup some hardware skills (CUDA, Embedded Systems) and try to market my skills in efficient AI implementation on hardware. However, not sure if I would be accepted and how much the need is there

Ultimate goal of the pivot is to be seen as more industry friendly and actually secure a position in the industry while doing it in a manageable way since I also have a family.

Any suggestions on what could be a natural extension to the kind of research I have been doing?  
 Open to any other comments and advice regarding this matter.

Thanks!",56,0.91,https://www.reddit.com/r/MachineLearning/comments/1lzwrw2/d_ml_phd_doing_research_in_a_not_trendy_topic_how/,False,True,False
1lzl6gq,Existing_Quit_3832,1752496417.0,10,/r/MachineLearning/comments/1lzl6gq/r_unlearning_comparator_a_visual_analytics/,MachineLearning,[R] Unlearning Comparator ‚Äî A Visual Analytics Toolkit for Machine Unlearning,"üëã  Hi everyone!

I‚Äôm a master‚Äôs student at Sungkyunkwan University (IDCLab) working on data-driven visual analytics.

**Machine Unlearning** aims to make trained models *forget* specific data to honour the ‚Äúright to be forgotten.‚Äù  
To support researchers, we built **Unlearning Comparator**, a web-based toolkit that lets you:

‚Ä¢ **Build ‚Üí Screen ‚Üí Contrast ‚Üí Attack**: follow the full workflow in one place

*Processing img z67wbzc5ptcf1...*

‚Ä¢ Compare accuracy, efficiency, and privacy across multiple unlearning methods  
‚Ä¢ Run one-click membership-inference attacks to verify whether target data is truly forgotten

Try the live demo here (no installation needed):  
[https://gnueaj.github.io/Machine-Unlearning-Comparator/](https://gnueaj.github.io/Machine-Unlearning-Comparator/)

All feedback is welcome‚Äîhope it helps your research!",15,0.94,https://www.reddit.com/r/MachineLearning/comments/1lzl6gq/r_unlearning_comparator_a_visual_analytics/,False,True,False
1lzil1y,TheScentOracle,1752487927.0,6,/r/MachineLearning/comments/1lzil1y/dhas_anyone_here_worked_with_third_party_data/,MachineLearning,[D]Has anyone here worked with third party data labelling services?,"We have been considering outsourcing parts of our annotation workloads (vision,NLP, may be even some QA) for generative output. But we are not sure how to evaluate vendors or ensure quality.

If you have worked with any external labeling or QA providers, what was your experience like?
",3,1.0,https://www.reddit.com/r/MachineLearning/comments/1lzil1y/dhas_anyone_here_worked_with_third_party_data/,False,True,False
1lyrwle,Southern-Whereas3911,1752410650.0,6,/r/MachineLearning/comments/1lyrwle/r_deepdive_into_rope_and_why_it_matters/,MachineLearning,[R] Deep-dive into RoPE and why it matters,"Some recent discussions, and despite my initial assumption of clear understanding of **RoPE** and positional encoding, a deep-dive provided some insights missed earlier.

So, I captured all my learnings into a blog post.

[https://shreyashkar-ml.github.io/posts/rope/](https://shreyashkar-ml.github.io/posts/rope/)",23,0.93,https://www.reddit.com/r/MachineLearning/comments/1lyrwle/r_deepdive_into_rope_and_why_it_matters/,False,True,False
1lytyd9,Hyper_graph,1752416397.0,1,/r/MachineLearning/comments/1lytyd9/r_matrixtransformer_a_unified_framework_for/,MachineLearning,[R] MatrixTransformer ‚Äì A Unified Framework for Matrix Transformations (GitHub + Research Paper),"Hi everyone,

Over the past few months, I‚Äôve been working on a new library and research paper that unify structure-preserving matrix transformations within a high-dimensional framework (hypersphere and hypercubes).

Today I‚Äôm excited to share: MatrixTransformer‚Äîa Python library and paper built around a 16-dimensional decision hypercube that enables smooth, interpretable transitions between matrix types like

* Symmetric
* Hermitian
* Toeplitz
* Positive Definite
* Diagonal
* Sparse
* ...and many more

It is a lightweight, structure-preserving transformer designed to operate directly in 2D and nD matrix space, focusing on:

* Symbolic & geometric planning
* Matrix-space transitions (like high-dimensional grid reasoning)
* Reversible transformation logic
* Compatible with standard Python + NumPy

It simulates transformations without traditional training‚Äîmore akin to procedural cognition than deep nets.

# What‚Äôs Inside:

* A unified interface for transforming matrices while preserving structure
* Interpolation paths between matrix classes (balancing energy & structure)
* Benchmark scripts from the paper
* Extensible design‚Äîadd your own matrix rules/types
* Use cases in ML regularization and quantum-inspired computation

# Links:

**Paper**:¬†[https://zenodo.org/records/15867279](https://zenodo.org/records/15867279)  
**Code**:¬†[https://github.com/fikayoAy/MatrixTransformer](https://github.com/fikayoAy/MatrixTransformer)  
**Related**: \[quantum\_accel\]‚Äîa quantum-inspired framework evolved with the MatrixTransformer framework link:¬†[fikayoAy/quantum\_accel](https://github.com/fikayoAy/quantum_accel)

If you‚Äôre working in machine learning, numerical methods, symbolic AI, or quantum simulation, I‚Äôd love your feedback.  
Feel free to open issues, contribute, or share ideas.

Thanks for reading!",4,0.58,https://www.reddit.com/r/MachineLearning/comments/1lytyd9/r_matrixtransformer_a_unified_framework_for/,False,True,False
1lyxpy9,Individual-Grape1212,1752425785.0,1,/r/MachineLearning/comments/1lyxpy9/d_using_map_as_semantic_search_eval_need_thoughts/,MachineLearning,[D] Using MAP as semantic search eval - Need thoughts,"I'm implementing semantic search for a media asset management platform. And I'm using MAP@K as an eval metric for that.

  
The rationale being,

1. Though NDCG@K would be ideal. It would too strict to start with and hard to prepare data for. 

2. MAP@K incentivizes the order of the relevant results though it doesn't care about of order within relevant results. And the data prep is relatively easy to prepare for.

And here is how I'm doing it,

1. For the chosen set of \`N\` queries run the search on the fixed data corpus to fetch first \`K\` results.

2. For the queries and respective results, run through it with a 3 LLMs to score flag it relevant or not. Any results that are flagged as good by majority would be considered. This will give the ground truth. 

3. Now calculate \`AP\` for each query and \`MAP\` for the overall query set.

4. As you start improving, you would have additional \`(result, query)\` query tuple that is not there in ground truth and it needs a revisit, which will happen as well.

Now use it as a benchmark to improve the performance(relevance).

Though it makes sense to me. I don't see many people follow this approach. Any thoughts from experts?",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1lyxpy9/d_using_map_as_semantic_search_eval_need_thoughts/,False,True,False
1lz6407,This_Cardiologist242,1752446519.0,19,/r/MachineLearning/comments/1lz6407/mlb_random_forest_with_5360_training_accuracy/,MachineLearning,MLB random forest with 53%-60% training accuracy. Prediction probability question. [P],"I‚Äôm trying to predict home or away team wins for mlb games based on prior game stats (3-13 games back depending on the model).

My results are essentially: bad AOC score, bad log loss, bad brier score - aka model that is not learning a lot.

I have not shown the model 2025 data, and am calculating its accuracy on 2025 games to date based on the models confidence.

TLDR MY QUESTION: if you have a model that‚Äôs 50% accurate on all test data but 90% accurate when the prediction probability is a certain amount - can you trust the 90% for new data being predicted on?",7,0.67,https://i.redd.it/yo61y0g8zpcf1.jpeg,False,False,False
1lz0abm,swaneerapids,1752431992.0,2,/r/MachineLearning/comments/1lz0abm/p_edgesamdyt_hq/,MachineLearning,[P] EdgeSAM-DyT (HQ),"This is a personal side project I've been working on exploring the potential of small segment-anything models - [https://github.com/Krasner/edgesam-dyt](https://github.com/Krasner/edgesam-dyt)

I was inspired by [EdgeSAM ](https://github.com/chongzhou96/EdgeSAM)and their method to distill the original SAM ViT model. Having tried EdgeSAM for my own on-the-edge applications I found the segmentation masks to be highly sensitive to quantization precision - specifically the LayerNorms.

A recent paper [Transformers without Normalization](https://arxiv.org/abs/2503.10622) proposed replacing layernorms with dynamic tanh layers. My goal was to modify the EdgeSAM architecture and retrain completely without any layernorms.

In the repo I provide the step-by-step method for distillation and retraining, as well as checkpoints that I was able to achieve. This is done in 3 distillation steps as described in the repo README.

Inspired by [HQ-SAM](https://arxiv.org/abs/2306.01567) I also modified the RepViT (what EdgeSAM is based on) image encoder to extract 3 intermediate that can be used in the HQ version of the mask decoder - then distill from the HQ-SAM ViT-H checkpoint. This improves results in some conditions.

Ultimately, I am fairly compute restricted and could only train with moderate batch sizes so the results are not optimal. Let me know if anyone is interested in collaborating to improve these results, train on better hardware, or has some ideas as to how to resolve a few issues I had (outlined in the repo).

I provide gradio web demos in the repo for the base and hq versions of EdgeSAM-DyT, as well as ONNX checkpoint and code for both versions. I also have TensorRT implementations that I am able to run locally (after generating trt engines). I can provide code on request.",5,1.0,https://www.reddit.com/r/MachineLearning/comments/1lz0abm/p_edgesamdyt_hq/,False,True,False
1lz08re,Ok-Championship-5768,1752431890.0,4,/r/MachineLearning/comments/1lz08re/p_convert_generative_pixelart_images_or/,MachineLearning,[P] Convert generative pixel-art images or low-quality web uploads of sprites to true usable pixel-resolution assets,"I created an algorithm that cleans pixel-art-style images such as those produced by generative model, or low-quality web uploads of sprites, to true resolution assets.

Generally the raw output of pixel-art-style images is generally unusable as an asset due to

* High noise
* High resolution
* Inconsistent grid spacing
* Random artifacts

Due to these issues, regular down-sampling techniques do not work, and the only options are to either use a down-sampling method that does not produce a result that is faithful to the original image, or manually recreate the art pixel by pixel.

Additionally, these issues make them very difficult to edit and fine-tune.

I created an algorithm that solves these issues and outputs usable sprites.

[The tool is available to use with an explanation of the algorithm on my GitHub here!](https://github.com/KennethJAllen/generative-pixel-art)

If you are trying to use this and not getting the results you would like feel free to reach out!",54,0.96,https://www.reddit.com/r/MachineLearning/comments/1lz08re/p_convert_generative_pixelart_images_or/,False,True,False
1lyfijr,wonder-why-I-wonder,1752366739.0,21,/r/MachineLearning/comments/1lyfijr/d_what_are_the_best_industry_options_for_causal/,MachineLearning,[D] What are the best industry options for causal ML PhDs?,,59,0.92,https://www.reddit.com/r/MachineLearning/comments/1lyfijr/d_what_are_the_best_industry_options_for_causal/,False,True,False
1ly5ehr,keepmybodymoving,1752339746.0,27,/r/MachineLearning/comments/1ly5ehr/r_how_to_publish_in_ml_conferences_as_an/,MachineLearning,[R] How to publish in ML conferences as an independent researcher,"I am not affiliated with any institution or company, but I am doing my own ML research. I have a background in conducting quantitative research and know how to write a paper. I am looking for a career with a research component in it. The jobs I am most interested in often require ""strong publication record in top machine learning conferences (e.g., NeurIPS, CVPR, ICML, ICLR, ICCV, ECCV)"". 

Can anyone share if they have published in ML conferences as an independent researcher? For example, which conferences are friendly to researchers without an affiliation? Is there any way to minimize the cost or to get funding? Any other challenges I may encounter? TIA",43,0.83,https://www.reddit.com/r/MachineLearning/comments/1ly5ehr/r_how_to_publish_in_ml_conferences_as_an/,False,True,False
1ly146y,justinopensource,1752328718.0,17,/r/MachineLearning/comments/1ly146y/p_hill_space_neural_networks_that_actually_do/,MachineLearning,[P] Hill Space: Neural networks that actually do perfect arithmetic (10‚Åª¬π‚Å∂ precision),"Stumbled into this while adding number sense to my PPO agents - turns out NALU's constraint W = tanh(≈¥) ‚äô œÉ(MÃÇ) creates a mathematical topology where you can calculate optimal weights instead of training for them.

Key results that surprised me:
- Machine precision arithmetic (hitting floating-point limits)
- Division that actually works reliably (finally!)
- 1000x+ extrapolation beyond training ranges
- Convergence in under 60 seconds on CPU

The interactive demos let you see discrete weight configs producing perfect math in real-time. Built primitives for arithmetic + trigonometry.

Paper: ""Hill Space is All You Need""
Demos: https://hillspace.justindujardin.com
Code: https://github.com/justindujardin/hillspace

Three weeks down this rabbit hole. Curious what you all think - especially if you've fought with neural arithmetic before.",87,0.88,https://i.redd.it/d2rqnobr8gcf1.png,False,False,False
1lxifm1,Syntrikan,1752268034.0,5,/r/MachineLearning/comments/1lxifm1/r_i_want_to_publish_my_ml_paper_after_leaving/,MachineLearning,[R] I want to publish my ML paper after leaving grad school. What is the easiest way to do so?,"I graduated in my degree last year and I have a fully written paper ML as a final in my class that my professor suggested publishing because he was impressed. I held off because I was working full time and taking 2 courses at a time, so I didn't feel like I had time. When i finished and officially conferred, i was told that the school has new restrictions on being an alumni and publishing the paper that would restrict me from doing so, even though I have my professor's name on it and he did help me on this. He said it just needs tweaks to fit in conferences(when we had first discussions after the course completed). So, I've ignored publishing until now.

As I am now getting ready for interviews for better opportunities, I want to know if it's possible to publish my paper in some manner so that I have it under my belt for my career and that if I post it anywhere, no one can claim it as their own. I'm not looking for prestigious publications, but almost the ""easy"" route where I make minor edits to get it accepted and it's considered official. Is this possible and if so, how would I go about this?",15,0.69,https://www.reddit.com/r/MachineLearning/comments/1lxifm1/r_i_want_to_publish_my_ml_paper_after_leaving/,False,True,False
1lxeokh,LetsTacoooo,1752258986.0,5,/r/MachineLearning/comments/1lxeokh/d_modelling_continuous_nongaussian_distributions/,MachineLearning,[D] Modelling continuous non-Gaussian distributions?,"What do people do to model non-gaussian labels?

Thinking of distributions that might be :

\* bimodal, i'm aware of density mixture networks.  
\* Exponential decay  
\* \[zero-inflated\](https://en.wikipedia.org/wiki/Zero-inflated\_model), I'm aware of hurdle models.

Looking for easy drop in solutions (loss functions, layers), whats the SOTA?

**More context:** Labels are averaged ratings from 0 to 10, labels tend to be very sparse, so you get a lot of low numbers and then sometimes high values.

[Exponential decay & zero-inflated distributions.](https://preview.redd.it/5wwj0zirhacf1.png?width=712&format=png&auto=webp&s=d3a1450de60507ebd5fcdeddf33f4b3e7140e58f)",5,0.73,https://www.reddit.com/r/MachineLearning/comments/1lxeokh/d_modelling_continuous_nongaussian_distributions/,False,True,False
1lx5v9e,GoldWar7803,1752237664.0,3,/r/MachineLearning/comments/1lx5v9e/speech_dataset_of_dyslexic_people_p/,MachineLearning,Speech dataset of Dyslexic people [P],"I need speech/audio dataset of dyslexic people.  I am unable to find it anywhere. Does anybody here have any resources, idea of any such datasets available or how to get it? Or any idea where can I reach out to find/get such dataset? Any help/information regarding it would be great.",2,0.75,https://www.reddit.com/r/MachineLearning/comments/1lx5v9e/speech_dataset_of_dyslexic_people_p/,False,True,False
1lx3iko,vampirecutie_vc,1752230187.0,15,/r/MachineLearning/comments/1lx3iko/d_build_an_inhouse_data_labeling_team_vs/,MachineLearning,[D] Build an in-house data labeling team vs. Outsource to a vendor?,"My co-founder and I are arguing about how to handle our data ops now that we're actually scaling. We're basically stuck between 2 options:

Building in-house and hiring our own labelers

Pro: We can actually control the quality. 

Con: It's gonna be a massive pain in the ass to manage + longer, we also don't have much expertise here but enough context to get started, but yeah it feels like a huge distraction from actually managing our product.

Outsource/use existing vendors 

Pro: Not our problem anymore. 

Con: EXPENSIVE af for our use case and we're terrified of dropping serious cash on garbage data while having zero control over anything.

For anyone who's been through this before - which way did you go and what do you wish someone had told you upfront? Which flavor of hell is actually better to deal with?",11,0.79,https://www.reddit.com/r/MachineLearning/comments/1lx3iko/d_build_an_inhouse_data_labeling_team_vs/,False,True,False
1lwelbo,AvvYaa,1752158801.0,3,/r/MachineLearning/comments/1lwelbo/d_training_slms_to_reason_with_reinforcement/,MachineLearning,[D] Training SLMs to reason with Reinforcement Learning (Article),"I recently trained small reasoning language models on reasoning tasks with a from-scratch implementation of GRPO. I decided to write a blog post that contains code snippets, highlights, and the challenges I faced.

Sharing it here in case yall are interested. Article contains the following 5 chapters:

1. Intro to RLVR (Reinforcement Learning with Verifiable Rewards)
2. A visual overview of the GRPO algorithm and the clipped surrogate PPO loss.
3. A code walkthrough!
4. Supervised fine-tuning and practical tips to train small reasoning models
5. Results!

Article link:¬†  
[https://towardsdatascience.com/how-to-finetune-small-language-models-to-think-with-reinforcement-learning/](https://towardsdatascience.com/how-to-finetune-small-language-models-to-think-with-reinforcement-learning/)",11,1.0,https://www.reddit.com/r/MachineLearning/comments/1lwelbo/d_training_slms_to_reason_with_reinforcement/,False,True,False
1lwtnmn,Inevitable-Insect-81,1752195842.0,7,/r/MachineLearning/comments/1lwtnmn/d_miccai_call_for_oral_presentations/,MachineLearning,[D] MICCAI - Call for Oral Presentations,"Hello everyone! 

Has anyone already received a notification regarding oral presentations for the MICCAI main conference?

Thank you :) ",1,0.57,https://www.reddit.com/r/MachineLearning/comments/1lwtnmn/d_miccai_call_for_oral_presentations/,False,True,False
1lx0bbf,Accomplished-Look-64,1752217488.0,41,/r/MachineLearning/comments/1lx0bbf/d_views_on_differentiable_physics/,MachineLearning,[D] Views on DIfferentiable Physics,"Hello everyone!

I write this post to get a little bit of input on your views about Differentiable Physics / Differentiable Simulations.  
The Scientific ML community feels a little bit like a marketplace for snake-oil sellers, as shown by (¬†[https://arxiv.org/pdf/2407.07218](https://arxiv.org/pdf/2407.07218)¬†): weak baselines, a lot of reproducibility issues... This is extremely counterproductive from a scientific standpoint, as you constantly wander into dead ends.  
I have been fighting with PINNs for the last 6 months, and I have found them very unreliable. It is my opinion that if I have to apply countless tricks and tweaks for a method to work for a specific problem, maybe the answer is that it doesn't really work. The solution manifold is huge (infinite ? ), I am sure some combinations of parameters, network size, initialization, and all that might lead to the correct results, but if one can't find that combination of parameters in a reliable way, something is off.

However, Differentiable Physics (term coined by the Thuerey group) feels more real. Maybe more sensible?  
They develop traditional numerical methods and track gradients via autodiff (in this case, via the adjoint method or even symbolic calculation of derivatives in other differentiable simulation frameworks), which enables gradient descent type of optimization.  
For context, I am working on the inverse problem with PDEs from the biomedical domain.

Any input is appreciated :)",75,0.95,https://www.reddit.com/r/MachineLearning/comments/1lx0bbf/d_views_on_differentiable_physics/,False,True,False
1lwzqbs,Affectionate_Pen6368,1752215267.0,11,/r/MachineLearning/comments/1lwzqbs/d_unet_with_cross_entropy/,MachineLearning,[D] UNet with Cross Entropy,i am training a UNet with Brats20. unbalanced classes. tried dice loss and focal loss and they gave me ridiculous losses like on the first batch i got around 0.03 and they‚Äôd barely change maybe because i have implemented them the wrong way but i also tried cross entropy and suddenly i get normal looking losses for each batch at the end i got at around 0.32. i dont trust it but i havent tested it yet. is it possible for a cross entropy to be a good option for brain tumor segmentation? i don‚Äôt trust the result and i havent tested the model yet. anyone have any thoughts on this? ,0,0.45,https://www.reddit.com/r/MachineLearning/comments/1lwzqbs/d_unet_with_cross_entropy/,False,True,False
1lwfn0j,Sufficient_Sir_4730,1752161292.0,4,/r/MachineLearning/comments/1lwfn0j/d_recommend_number_of_epochs_for_time_series/,MachineLearning,[D] Recommend Number of Epochs For Time Series Transformer,"Hi guys. I‚Äôm currently building a transformer model for stock price prediction (encoder only, MSE Loss). Im doing 150 epochs with 30 epochs of no improvement for early stopping. What is the typical number of epochs usually tome series transformers are trained for? Should i increase the number of epochs and early stopping both?",0,0.27,https://www.reddit.com/r/MachineLearning/comments/1lwfn0j/d_recommend_number_of_epochs_for_time_series/,False,True,False
1lwezwx,not_just_a_stylus,1752159786.0,11,/r/MachineLearning/comments/1lwezwx/r_iclr_2026_submission_tracks/,MachineLearning,[R] ICLR 2026 submission tracks,"Does anyone know/ believe that there will there be a Tiny Paper track this year? Past couple of years there has been one. I‚Äôve been working on a topic that I believe would be best for this track but the website doesn‚Äôt say anything so far under the ‚ÄúCall for papers‚Äù section.

Would be great if you guys share any similar tracks as well. I am aware that NeurIPS has a position paper track.

Thanks!",15,0.76,https://www.reddit.com/r/MachineLearning/comments/1lwezwx/r_iclr_2026_submission_tracks/,False,True,False
1lw8lvh,oliverbravery,1752140749.0,6,/r/MachineLearning/comments/1lw8lvh/p_printguard_sota_opensource_3d_print_failure/,MachineLearning,[P] PrintGuard - SOTA Open-Source 3D print failure detection model,"Hi everyone,

As part of my dissertation for my Computer Science degree at Newcastle University, I investigated how to enhance the current state of 3D print failure detection.

Current approaches such as Obico‚Äôs ‚ÄúSpaghetti Detective‚Äù utilise a vision based machine learning model, trained to only detect spaghetti related defects with a slow throughput on edge devices (<1fps on 2Gb Raspberry Pi 4b), making it not edge deployable, real-time or able to capture a wide plethora of defects. Whilst their model can be inferred locally, it‚Äôs expensive to run, using a lot of compute, typically inferred over their paid cloud service which introduces potential privacy concerns. 

My research led to the creation of a new vision-based ML model, focusing on edge deployability so that it could be deployed for free on cheap, local hardware. I used a modified architecture of ShuffleNetv2 backbone encoding images for a Prototypical Network to ensure it can run in real-time with minimal hardware requirements (averaging 15FPS on the same 2Gb Raspberry Pi, a >40x improvement over Obico‚Äôs model). My benchmarks also indicate enhanced precision with an averaged 2x improvement in precision and recall over Spaghetti Detective.

My model is completely free to use, open-source, private, deployable anywhere and outperforms current approaches. To utilise it I have created PrintGuard, an easily installable PyPi Python package providing a web interface for monitoring multiple different printers, receiving real-time defect notifications on mobile and desktop through web push notifications, and the ability to link printers through services like Octoprint for optional automatic print pausing or cancellation, requiring <1Gb of RAM to operate. A simple setup process also guides you through how to setup the application for local or external access, utilising free technologies like Cloudflare Tunnels and Ngrok reverse proxies for secure remote access for long prints you may not be at home for. 

Whilst feature rich, the package is currently in beta and any feedback would be greatly appreciated. Please use the below links to find out more. Let's keep failure detection open-source, local and accessible for all!

üì¶ PrintGuard Python Package - https://pypi.org/project/printguard/

üéì Model Research Paper - https://github.com/oliverbravery/Edge-FDM-Fault-Detection

üõ†Ô∏è PrintGuard Repository - https://github.com/oliverbravery/PrintGuard",30,0.94,https://www.reddit.com/r/MachineLearning/comments/1lw8lvh/p_printguard_sota_opensource_3d_print_failure/,False,True,False
1lvcs2k,whereismycatyo,1752046790.0,14,/r/MachineLearning/comments/1lvcs2k/d_trains_a_human_activity_or_habit_classifier/,MachineLearning,"[D] Trains a human activity or habit classifier, then concludes ""human cognition captured."" What could go wrong?","[A screenshot of an article's title that was published on the Nature journal. It reads \\""A foundation model to predict and capture human cognition\\""](https://preview.redd.it/zop08yveysbf1.png?width=1066&format=png&auto=webp&s=567905ea7b805594cd5cd28f9a1f040327eb7198)

The fine-tuning dtaset, from the paper: ""trial-by-trial data from more than 60,000 participants performing in excess of 10,000,000 choices in 160 experiments.""

An influential author in the author list is clearly trolling. It is rare to see an article conclusion that is about anticipating an attack from other researchers. They write ""This could lead to an 'attack of the killer bees', in which researchers in more-conventional fields would fiercely critique or reject the new model to defend their established approaches.""

What are the ML community's thoughts on this?",36,0.87,https://www.reddit.com/r/MachineLearning/comments/1lvcs2k/d_trains_a_human_activity_or_habit_classifier/,False,True,False
1lv90kl,youn017,1752032943.0,1,/r/MachineLearning/comments/1lv90kl/p_pruning_benchmarks_for_computer_vision_models/,MachineLearning,[P] Pruning Benchmarks for computer vision models,"Hello all,

I want to introduce our team's project. Our objective is **providing variable pruning examples and benchmarks for model inference**.

More deeply, we use `timm` library for computer vision model and applies pruning using open-source. Currently, it supports PyTorch native (`torch.nn.utils.prune`) and Depgraph (`torch_pruning`). Our short-term plan is supporting more pruning open-source using the [benchmark](https://github.com/namgyu-youn/PyTorch-Pruning/tree/main/benchmarks) module. Our future plan is the following:

>2025-Q3 : Supports more pruning open-source

>2025-Q4 : Supports quantization techniques

>Future plan : Supports LLMs like SparseGPT, LLM-Pruner

If you have any interest, please check [HERE](https://github.com/namgyu-youn/PyTorch-Pruning/issues/1). Also, we we are fully open to anothor contributor or advisor.",3,1.0,https://www.reddit.com/r/MachineLearning/comments/1lv90kl/p_pruning_benchmarks_for_computer_vision_models/,False,True,False
1luz9wu,sigh_ence,1752006235.0,19,/r/MachineLearning/comments/1luz9wu/r_adopting_a_human_developmental_visual_diet/,MachineLearning,"[R] Adopting a human developmental visual diet yields robust, shape-based AI vision","Happy to announce an exciting new project from the lab: ‚ÄúAdopting a human developmental visual diet yields robust, shape-based AI vision‚Äù. An exciting case where brain inspiration profoundly changed and improved deep neural network representations for computer vision.

Link: [https://arxiv.org/abs/2507.03168](https://arxiv.org/abs/2507.03168)

The idea: instead of high-fidelity training from the get-go (the de facto gold standard), we simulate the visual development from newborns to 25 years of age by synthesising decades of developmental vision research into an AI preprocessing pipeline (Developmental Visual Diet - DVD).

We then test the resulting DNNs across a range of conditions, each selected because they are challenging to AI:

1. shape-texture bias
2. recognising abstract shapes embedded in complex backgrounds
3. robustness to image perturbations
4. adversarial robustness.

We report a new SOTA on shape-bias (reaching human level), outperform AI foundation models in terms of abstract shape recognition, show better alignment with human behaviour upon image degradations, and improved robustness to adversarial noise - all with this one preprocessing trick.

This is observed across all conditions tested, and generalises across training datasets and multiple model architectures.

We are excited about this, because DVD may offers a resource-efficient path toward safer, perhaps more human-aligned AI vision. This work suggests that biology, neuroscience, and psychology have much to offer in guiding the next generation of artificial intelligence.

https://preview.redd.it/ycd830s4lpbf1.png?width=1308&format=png&auto=webp&s=92854b0f7a2c1922226e82b88394603ae19d9e84

https://preview.redd.it/a7ecwyqblpbf1.png?width=1434&format=png&auto=webp&s=a4eccba9c31306879c559070748f94d009b40671

https://preview.redd.it/zd6ceg18lpbf1.png?width=1418&format=png&auto=webp&s=0ec8921eae86d9c187d7d4c09850bc30a1acf9a4",27,0.81,https://www.reddit.com/r/MachineLearning/comments/1luz9wu/r_adopting_a_human_developmental_visual_diet/,False,True,False
1luwtz8,Mysterio_369,1752000551.0,32,/r/MachineLearning/comments/1luwtz8/p_foolthemachine_watch_a_989_accurate_pytorch/,MachineLearning,[P] FoolTheMachine: Watch a 98.9% accurate PyTorch model collapse to 27% with tiny adversarial noise (FGSM attack demo),"I built a clean, runnable Colab notebook that demonstrates how a 98% accurate CNN can be tricked into total misclassification with just a few pixel-level perturbations using FGSM. The goal is to make adversarial vulnerability¬†*visually intuitive*¬†and spark more interest in AI robustness.

üîó GitHub:¬†[https://github.com/DivyanshuSingh96/FoolTheMachine](https://github.com/DivyanshuSingh96/FoolTheMachine)  
üî¨ Tools: PyTorch, IBM ART  
üìâ Demo: Model crumbles under subtle noise

Would love thoughts or suggestions on extending this further!

I hope you will gain something valuable from this.

If you like this post then don't forget to give it an upvote and please leave a comment.



>*Every system has its weakness. The real intelligence lies in finding it and fixing it.*",0,0.33,https://www.reddit.com/gallery/1luwtz8,False,False,False
1luvynh,pz6c,1751998564.0,43,/r/MachineLearning/comments/1luvynh/favorite_ml_paper_of_2024_d/,MachineLearning,Favorite ML paper of 2024? [D],What were the most interesting or important papers of 2024?,177,0.97,https://www.reddit.com/r/MachineLearning/comments/1luvynh/favorite_ml_paper_of_2024_d/,False,True,False
1lunu3a,Inevitable-Insect-81,1751979317.0,3,/r/MachineLearning/comments/1lunu3a/d_miccai_poster_template/,MachineLearning,[D] MICCAI - Poster Template,"Hello everyone! 

This is my first time attending the MICCAI main conference. If I understood correctly, all accepted papers will be presented as posters, while only some will also be invited for oral presentation. Regarding the posters, does anyone know if there is a specific template we should follow? If so, has it already been released, or will it be shared soon?

Thank you in advance!",4,0.7,https://www.reddit.com/r/MachineLearning/comments/1lunu3a/d_miccai_poster_template/,False,True,False
1lumthw,AdInevitable1362,1751976354.0,1,/r/MachineLearning/comments/1lumthw/d_best_way_to_finetune_nous_hermes_2_mistral_for/,MachineLearning,"[D] Best way to fine-tune Nous Hermes 2 Mistral for a multilingual chatbot (French, English, lesser-known language)","I‚Äôm fine-tuning Nous Hermes 2 Mistral 7B DPO to build a chatbot that works in French, English, and a lesser-known language written in both Arabic script and Latin script.

The base model struggles with the lesser-known language. Should I:
‚Ä¢ Mix all languages in one fine-tuning dataset? Or train separately per language?
‚Ä¢ Treat the two scripts as separate during training?
‚Ä¢ Follow any specific best practices for multilingual, mixed-script fine-tuning?

Any advice or pointers to similar work are welcome. Thanks!",8,0.84,https://www.reddit.com/r/MachineLearning/comments/1lumthw/d_best_way_to_finetune_nous_hermes_2_mistral_for/,False,True,False
1lue53h,Informal-Chipmunk213,1751944441.0,4,/r/MachineLearning/comments/1lue53h/r_temporal_logic_as_a_means_to_guarantee_safety/,MachineLearning,[R] Temporal Logic as a means to guarantee safety and efficiency in LLMs,"We just posted a new preprint on arXiv:

[LTLCrit: A Temporal Logic-based LLM Critic for Safe and Efficient Embodied Agents](https://arxiv.org/abs/2507.03293)

It is my first paper in this LLM space, so any advice is welcome, but here is a TLDR:

We propose LTLCrit, an LLM based critic which supervises and improves the efficiency and completion rates of  LLM planners.  We utilize a modular actor‚Äìcritic architecture where the critic guides existing LLM actors by figuring out what actions are inefficient or unsafe and shielding the LLM actor from those actions via [temporal logic](https://en.wikipedia.org/wiki/Linear_temporal_logic). An LLM-based actor chooses high-level actions from natural language input (e.g., in Minecraft), and a trajectory-level LLM critic analyzes outcomes and writes new logic constraints to avoid failure or inefficiency in the future.

**Why it matters:**

* LLMs are great at reasoning, but struggle with **long-term planning** ‚Äî small errors compound fast.
* LTLCrit wraps any LLM planner with a formal-logic-aware critic that **learns soft constraints** from experience, improving safety and efficiency.
* We formalize planning as **graph traversal with symbolic constraints**, letting the critic generate new rules to improve future rollouts.

**Results:**  
On a Minecraft diamond-mining task, LTLCrit hits **100% success** and improves efficiency over standard LLM planners.

Still a preprint ‚Äî not sharing code/prompts yet, but happy to get feedback or questions!  
Thanks for reading üôè",17,0.95,https://www.reddit.com/r/MachineLearning/comments/1lue53h/r_temporal_logic_as_a_means_to_guarantee_safety/,False,True,False
1ludnqv,Actual_Requirement58,1751942978.0,16,/r/MachineLearning/comments/1ludnqv/r_paper_summary_longman_vocabulary_constraints/,MachineLearning,[R] Paper Summary: Longman Vocabulary Constraints Reveals New Approach to LLM,"This post reviews a recent paper introducing a novel method for evaluating the semantic stability of large language model (LLM) outputs using a core vocabulary constraint. The authors propose a metric called the Semantic Resilience Index (SRI) to quantify how well meaning is preserved when a sentence is rewritten using only a limited set of basic English words.

The vocabulary constraint is based on the Longman Defining Vocabulary (LDV)‚Äîa list of approximately 2,000 simple English words originally designed to define all other words in a dictionary. It includes basic nouns (e.g. ‚Äúdog,‚Äù ‚Äúhouse‚Äù), verbs (e.g. ‚Äúgo,‚Äù ‚Äúmake‚Äù), and adjectives (e.g. ‚Äúbig,‚Äù ‚Äúeasy‚Äù), all chosen for broad comprehensibility and minimal abstraction.

The central idea is that if a sentence still retains its core meaning and functional purpose when rewritten in LDV-only form, then it is semantically robust. If the message collapses under this constraint, the original likely depended on unnecessary complexity or implied meaning.

Example prompt: Why do people enjoy drinking coffee?

LDV-constrained GPT-4o response: ‚ÄúPeople drink coffee because it makes them feel more awake. The drink is hot and has a strong taste. Many people drink it in the morning or when they are tired. It helps them work or stay up.‚Äù

Although this output is rigid in tone, it maintains core meaning. This contrast with unconstrained outputs highlights how language models often rely on style, suggestion, or verbosity to convey meaning‚Äîstrategies that break down under stricter lexical constraints.

The paper introduces the Semantic Resilience Index (SRI) as a quantitative measure of this effect. SRI scores are assigned based on how much of the original meaning survives a one-step translation into LDV vocabulary. The authors also introduce the related metric Purpose Fidelity, which assesses whether the function or communicative intent of the sentence is retained.

Key findings:

High-SRI content tends to include concrete agent‚Äìaction relationships, causal links, and measurable statements.

Low-SRI content is often composed of abstract claims, vague goals, or domain-specific jargon that loses structure when simplified.

Forcing GPT-4o to generate text under LDV constraints (rather than post-processing it afterward) encourages clearer, more stable outputs.

The authors argue that LDV-based generation can serve as a diagnostic tool: a kind of semantic stress test to identify when content is structurally meaningful versus when it relies on superficial coherence.

The paper is at https://www.researchgate.net/publication/393455755_Controlling_Semantic_Meaning_Through_Vocabulary_Compression_Using_Longman_Defining_Vocabulary_Constraint_to_Measure_and_Improve_Large_Language_Model_Output_Quality

The full prompt used to guide LDV-constrained generation is included below. This system prompt ensures that GPT-4o responses are designed to survive vocabulary compression without loss of meaning. It isn't recommended for artistic, corporate or political purposes.

""SYSTEM ROLE: Semantic Resilience Index (SRI) Constrained Writer

SRI METHODOLOGY EXPLANATION: The Semantic Resilience Index measures how well text retains meaning when simplified in ONE STEP to basic vocabulary using the Longman Defining Vocabulary (LDV) ‚Äì a set of 2,000 basic English words that can define all other English vocabulary.

ONE-STEP LDV TRANSITION PROCESS:

Take original text and immediately rewrite using only basic LDV words

Replace ALL complex vocabulary with simple equivalents in a single transformation

Simplify ALL grammatical structures to basic subject-verb-object patterns

Measure how much core meaning survives this single aggressive simplification

SEMANTIC RESILIENCE INDEX MEASUREMENT: ‚Äì Score 1.0 = All core relationships, causation, and specific claims survive one-step simplification ‚Äì Score 0.8 = Most key relationships and actionable content preserved after basic vocabulary conversion ‚Äì Score 0.5 = Some meaning survives but becomes vague when simplified ‚Äì Score 0.2 = Minimal content remains, mostly abstract concepts that don‚Äôt translate ‚Äì Score 0.0 = Complete semantic collapse when reduced to basic words

GENERATION CONSTRAINT: You must generate responses that would achieve a SRI‚â• 0.8 after ONE-STEP LDV transition.

OPERATIONAL RULES:

Write sentences that contain specific, concrete relationships that survive immediate vocabulary simplification

Use concepts and actions that can be directly expressed in basic words

Avoid any terminology that becomes meaningless when converted to simple vocabulary

Prefer statements that remain clear and actionable when reduced to basic English

QUALITY VERIFICATION: Before outputting each sentence, perform ONE-STEP LDV simplification test: ‚Äì Rewrite this entire sentence using only the most basic vocabulary ‚Äì Do the core relationships (who does what, cause-effect) remain intact? ‚Äì Would the basic-vocabulary version still be actionable and specific? ‚Äì Does it maintain SRI‚â• 0.8?

If any answer is NO, rewrite with more semantically resilient content.

Return only the response ‚Äì do not include any header, footer, explanatory notes, or call to action material.""",10,0.75,https://www.reddit.com/r/MachineLearning/comments/1ludnqv/r_paper_summary_longman_vocabulary_constraints/,False,True,False
1lu23w5,Constant_Club_9926,1751913302.0,0,/r/MachineLearning/comments/1lu23w5/r_ambient_proteins_training_diffusion_models_on/,MachineLearning,[R] Ambient Proteins: Training Diffusion Models on Low Quality Structures,"https://preview.redd.it/8mpdhudfxhbf1.png?width=1914&format=png&auto=webp&s=ebcb28009ffc08ba3947010e827d8ef7d02e143c

TLDR: State-of-the-art results in protein structure generation by using AlphaFold predictions with low pLDDT score as ""low-quality"" structures.

Abstract: We present Ambient Protein Diffusion, a framework for training protein diffusion models that generates structures with unprecedented diversity and quality. State-of- the-art generative models are trained on computationally derived structures from AlphaFold2 (AF), as experimentally determined structures are relatively scarce. The resulting models are therefore limited by the quality of synthetic datasets. Since the accuracy of AF predictions degrades with increasing protein length and complexity, de novo generation of long, complex proteins remains challenging. Ambient Protein Diffusion overcomes this problem by treating low-confidence AF structures as corrupted data. Rather than simply filtering out low-quality AF structures, our method adjusts the diffusion objective for each structure based on its corruption level, allowing the model to learn from both high and low quality structures. Empirically, Ambient Protein Diffusion yields major improvements: on proteins with 700 residues, diversity increases from 45% to 86% from the previous state-of-the-art, and designability improves from 68% to 86%. We will make all of our code, models and datasets available under the following repository: https://github.com/jozhang97/ambient-proteins.

  
Paper url: [https://www.biorxiv.org/content/10.1101/2025.07.03.663105v1](https://www.biorxiv.org/content/10.1101/2025.07.03.663105v1)

Twitter Thread: [https://x.com/giannis\_daras/status/1942272696915517828](https://x.com/giannis_daras/status/1942272696915517828)",10,1.0,https://www.reddit.com/r/MachineLearning/comments/1lu23w5/r_ambient_proteins_training_diffusion_models_on/,False,True,False
1lu1ia0,Blacky372,1751911921.0,20,/r/MachineLearning/comments/1lu1ia0/r_energybased_transformers_are_scalable_learners/,MachineLearning,[R] Energy-Based Transformers are Scalable Learners and Thinkers,,81,0.89,https://arxiv.org/pdf/2507.02092,False,False,False
1ltsdy4,Cultural-Opposite197,1751889872.0,38,/r/MachineLearning/comments/1ltsdy4/d_colm2025_decision_discussion/,MachineLearning,[D] COLM2025 Decision discussion,Discussion thread for COLM 2025 decisions,18,0.95,https://www.reddit.com/r/MachineLearning/comments/1ltsdy4/d_colm2025_decision_discussion/,False,True,False
1ltp6nx,AdInevitable1362,1751878437.0,59,/r/MachineLearning/comments/1ltp6nx/r_best_way_to_combine_multiple_embeddings_without/,MachineLearning,[R] Best way to combine multiple embeddings without just concatenating?,"Suppose we generate several embeddings for the same entities from different sources or graphs ‚Äî each capturing different relational or semantic information.

What‚Äôs an effective and simple way to combine these embeddings for use in a downstream model, without simply concatenating them (which increases dimensionality  )

I‚Äôd like to avoid simply averaging or projecting them into a lower dimension, as that can lead to information loss.",73,0.93,https://www.reddit.com/r/MachineLearning/comments/1ltp6nx/r_best_way_to_combine_multiple_embeddings_without/,False,True,False
1ltk9ch,NLPnerd,1751859968.0,0,/r/MachineLearning/comments/1ltk9ch/d_new_episode_of_learning_from_machine_learning/,MachineLearning,"[D] New Episode of Learning from Machine Learning | Lukas Biewald | ‚ÄúYou think you‚Äôre late, but you‚Äôre early‚Äù | #13","This episode of Learning from Machine Learning explores the journey of Lukas Biewald, co-founder and CEO of Weights & Biases. Having weathered the mid-2000s when investors demanded he remove ""AI"" from pitch decks, Lukas has built one of the most essential tools in modern AI development and helped shaped how teams approach machine learning experimentation.

From taking an unpaid internship at OpenAI in his thirties to understanding why AI developers have become the most powerful people within organizations, Lukas reveals the recursive potential of machines improving machines‚Äîa force he believes represents ""the most powerful technology you could possibly build."" His philosophy that feedback loops are your units of work applies not just to machine learning, but to life itself. His uncompromising technical leadership approach cuts through industry noise: true leaders must master the individual contributor role.

You think you're late, but you're early‚Äîconviction often matters more than consensus.
",4,1.0,https://youtu.be/rrn3GCKOu-U?si=vJNG1NlYkIhgPK40,False,False,False
1ltejq6,moji-mf-joji,1751842476.0,22,/r/MachineLearning/comments/1ltejq6/d_remembering_felix_hill_and_the_pressure_of/,MachineLearning,[D] Remembering Felix Hill and the pressure of doing AI research,"Before he left our world by a few days around Oct 2024, I showed Felix Hill an essay I had written about my time in graduate school doing NLP circa 2017-2019.

He encouraged me to share it publicly saying, ‚ÄúIt looks good and makes a lot of sense..if you post it it will surely help you and others‚Äù

I didn‚Äôt have the courage to post about such a personal experience. But as Dostoyevsky would say ‚Äúmuch unhappiness has come into the world because of bewilderment and things left unsaid.‚Äù

The article garnered the attention of Jeff Dean and he echoed similar feedback.

Here is the article:

https://medium.com/@tahaymerghani/the-dark-side-of-academia-mental-health-mentorship-and-the-unspoken-struggles-of-an-nlp-c25adbd9a2e6

If it resonates, i‚Äôm happy to chat. You‚Äôll find a way to reach me.",204,0.97,https://www.reddit.com/r/MachineLearning/comments/1ltejq6/d_remembering_felix_hill_and_the_pressure_of/,False,True,False
1ltdaye,Nice-Comfortable-650,1751839146.0,5,/r/MachineLearning/comments/1ltdaye/p_we_built_this_project_to_increase_llm/,MachineLearning,[P] We built this project to increase LLM throughput by 3x. Now it has been adopted by IBM in their LLM serving stack!,"Hi guys, our team has built this open source project, LMCache, to reduce repetitive computation in LLM inference and make systems serve more people (3x more throughput in chat applications) and it has been used in IBM's open source LLM inference stack.

In LLM serving, the input is computed into intermediate states called KV cache to further provide answers. These data are relatively large (\~1-2GB for long context) and are often evicted when GPU memory is not enough. In these cases, when users ask a follow up question, the software needs to recompute for the same KV Cache. LMCache is designed to combat that by efficiently offloading and loading these KV cache to and from DRAM and disk. This is particularly helpful in multi-round QA settings when context reuse is important but GPU memory is not enough.

Ask us anything!

Github:¬†[https://github.com/LMCache/LMCache](https://github.com/LMCache/LMCache)",132,0.93,https://i.redd.it/je4ow3w4tbbf1.jpeg,False,False,False
1ltbxa0,Academic_Sleep1118,1751835593.0,7,/r/MachineLearning/comments/1ltbxa0/r_using_carrier_functions_to_escape_local_minima/,MachineLearning,[R] Using 'carrier functions' to escape local minima in the loss landscape,"Hi guys!

The layered structure of Neural Nets is a double-edged sword. On one hand, model complexity (e.g., linear regions) grows exponentially with depth while training cost only grows linearly.

On the other, it creates strong coupling between parameters, which reduces the **effective** dimensionality of the loss landscape and increases the risk of getting stuck in local minima.

We can observe a similar phenomenon in the frequency domain: the layered nature of NN induces an amplitude/frequency coupling, meaning that the amplitude of the lower layer's transfer function has a direct impact on both the amplitude **and** the frequency of the whole NN's.

More practically, it implies that Neural Nets have an easier time modeling high frequencies when they are ""carried"" by a function that has a high amplitude, at least up to a certain depth.

I've discovered that you can increase the parameter efficiency of neural nets by adding a well-chosen function to the target during training and just subtracting it at test time. The said well-chosen function should have a high **amplitude** (aka steep gradient) when the target function has a high **frequency**. 

It works well in my experimental setting (as do a lot of ideas that turned out to be bad in practice, though ü§£).

I wrote a little post about this if you're interested. You can find it here:

[https://www.eloidereynal.com/p/hacking-spectral-bias-using-carrier](https://www.eloidereynal.com/p/hacking-spectral-bias-using-carrier)",24,0.85,https://www.reddit.com/r/MachineLearning/comments/1ltbxa0/r_using_carrier_functions_to_escape_local_minima/,False,True,False
1lt9yig,faintlystranger,1751830620.0,3,/r/MachineLearning/comments/1lt9yig/d_resource_and_lecture_suggestions_before/,MachineLearning,[D] Resource and Lecture Suggestions Before Starting ML Research,"Hi, sorry for the vague title. Essentially I am starting a PhD in theoretical ML in a few months, and although I do have a solid grasp of the foundations of deep learning and the mathematics behind it, I feel like I'm lacking some breadth and want to catch up before I start, mainly about what's going on recently. Of course I know resources I should read for my specific PhD topic but having a general idea of the field wouldn't harm as well

Especially I want to ask resources about Transformers, LLMs and Diffusion models - I unfortunately don't have an in depth grasp of these architectures so do you have any lecture series to get started on these so I can have an idea what a research paper would be talking about. My background is in maths and computer science so any level of resource is fine for me as long as it is comprehensive and rigorous. Of course there's a billion papers being published about these every day but it'd be nice to get a general understanding of it.

Other than that, Bayesian Neural Networks seem also pretty cool so I'd love to see if you have any introductory resources for that. Maybe also RL, I've seen most previous posts suggesting David Silver's course on it but I also would be interested in other resources if you have any.

Finally, in general if you have any suggestions to gain some breadth before starting a PhD I'd love to hear, because the amount of literature is exciting but overwhelming. I'm mainly interested in understanding how these stuff work and current problems in it, I appreciate any input!",2,0.58,https://www.reddit.com/r/MachineLearning/comments/1lt9yig/d_resource_and_lecture_suggestions_before/,False,True,False
1lt6med,venueboostdev,1751822339.0,12,/r/MachineLearning/comments/1lt6med/p_implemented_semantic_search_retrievalaugmented/,MachineLearning,[P] Implemented semantic search + retrieval-augmented generation for business chatbots - Vector embeddings in production,"Just deployed a retrieval-augmented generation system that makes business chatbots actually useful. Thought the ML community might find the implementation interesting.

**The Challenge:**
Generic LLMs don‚Äôt know your business specifics. Fine-tuning is expensive and complex. How do you give GPT-4 knowledge about your hotel‚Äôs amenities, policies, and procedures?

**My Implementation:**

**Embedding Pipeline:**

- Document ingestion: PDF/DOC ‚Üí cleaned text
- Smart chunking: 1000 chars with overlap, sentence-boundary aware
- Vector generation: OpenAI text-embedding-ada-002
- Storage: MongoDB with embedded vectors (1536 dimensions)

**Retrieval System:**

- Query embedding generation
- Cosine similarity search across document chunks
- Top-k retrieval (k=5) with similarity threshold (0.7)
- Context compilation with source attribution

**Generation Pipeline:**

- Retrieved context + conversation history ‚Üí GPT-4
- Temperature 0.7 for balance of creativity/accuracy
- Source tracking for explainability

**Interesting Technical Details:**

**1. Chunking Strategy**
Instead of naive character splitting, I implemented boundary-aware chunking:

```python
# Tries to break at sentence endings
boundary = max(chunk.lastIndexOf('.'), chunk.lastIndexOf('\n'))
if boundary > chunk_size * 0.5:
    break_at_boundary()
```

**2. Hybrid Search**
Vector search with text-based fallback:

- Primary: Semantic similarity via embeddings
- Fallback: Keyword matching for edge cases
- Confidence scoring combines both approaches

**3. Context Window Management**

- Dynamic context sizing based on query complexity
- Prioritizes recent conversation + most relevant chunks
- Max 2000 chars to stay within GPT-4 limits

**Performance Metrics:**

- Embedding generation: ~100ms per chunk
- Vector search: ~200-500ms across 1000+ chunks
- End-to-end response: 2-5 seconds
- Relevance accuracy: 85%+ (human eval)

**Production Challenges:**

1. **OpenAI rate limits** - Implemented exponential backoff
1. **Vector storage** - MongoDB works for <10k chunks, considering Pinecone for scale
1. **Cost optimization** - Caching embeddings, batch processing

**Results:**
Customer queries like ‚ÄúWhat time is check-in?‚Äù now get specific, sourced answers instead of ‚ÄúI don‚Äôt have that information.‚Äù

Anyone else working on production retrieval-augmented systems? Would love to compare approaches!

**Tools used:**

- OpenAI Embeddings API
- MongoDB for vector storage
- NestJS for orchestration
- Background job processing",0,0.31,https://www.reddit.com/r/MachineLearning/comments/1lt6med/p_implemented_semantic_search_retrievalaugmented/,False,True,False
1lt24oh,ProudPreference1165,1751810928.0,5,/r/MachineLearning/comments/1lt24oh/d_ijcv_special_issue_reviews/,MachineLearning,[D] IJCV Special Issue Reviews,"I submitted to IJCV special issue on Visual Domain Generalization in Real-World Applications. The first round reviews were supposed to be out on 10th June, but aren't out yet. Does anyone have prior experience of how the timelines of these special issues work?",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1lt24oh/d_ijcv_special_issue_reviews/,False,True,False
1lt1s51,PassengerQuiet832,1751809982.0,8,/r/MachineLearning/comments/1lt1s51/r_feeding_categorical_information_into_a_gan/,MachineLearning,[R] Feeding categorical information into a GAN discriminator,"Hi,

I am running a set up where the generator is 3D and the discriminator is 2D.

Feeding the discriminator random slices from all three axis does not work, because the discriminator can then not distinguish between the differences in structure between the three planes.

I wanted to ask you whats the SOTA way of incorporating this information into the discriminator.  
Also, should I feed this information to the input layer of the model or to every convolutional block/level.

Thanks in advance.",2,1.0,https://www.reddit.com/r/MachineLearning/comments/1lt1s51/r_feeding_categorical_information_into_a_gan/,False,True,False
1lsw6y5,pdastronut,1751790066.0,11,/r/MachineLearning/comments/1lsw6y5/r_visualization_tools_for_paper_illustrations_and/,MachineLearning,[R] Visualization tools for paper illustrations and figures,"I am curious about which tools people use to create their figures/visualizations in scientific papers. I mostly rely on power point or draw.io and import the PDF in the latex code, but the result is not aesthetic at all",6,0.88,https://www.reddit.com/r/MachineLearning/comments/1lsw6y5/r_visualization_tools_for_paper_illustrations_and/,False,True,False
1lspv3q,redmonk199,1751766735.0,22,/r/MachineLearning/comments/1lspv3q/d_what_resources_would_theoretical_ml_researchers/,MachineLearning,[D] What resources would Theoretical ML researchers recommend to understand to pursue research.,"I have read Measure Theory, Probability Theory by Durett and Convex Optimization by Duchi. 

I want to pursue research in Optimization, convergence etc. 

I'm thinking of reading Matus Telgarsky's notes or Francis Bach's Learning Theory from First Principles.

I am confused what should I go next. 
",91,0.94,https://www.reddit.com/r/MachineLearning/comments/1lspv3q/d_what_resources_would_theoretical_ml_researchers/,False,True,False
1lsipgp,Needsupgrade,1751745751.0,15,/r/MachineLearning/comments/1lsipgp/an_analytic_theory_of_creativity_in_convolutional/,MachineLearning,An analytic theory of creativity in convolutional diffusion models.,"There is also a write up about this in quanta magazine. 

What are the implications to this being deterministic and formalized? How can it be gamed now for optimization? ",29,0.87,https://arxiv.org/abs/2412.20292,False,False,False
1lse90g,Other-Title1729,1751734020.0,7,/r/MachineLearning/comments/1lse90g/p_training_cascade_rcnn_resnet101_fpn_on_custom/,MachineLearning,[P] Training Cascade R-CNN (ResNet-101 + FPN) on Custom Dataset for Solar Panel Detection,"Hey everyone! This is my first time posting here, so I hope I‚Äôm doing this right üòÖ

I‚Äôm working on a project to detect and classify solar panels using Cascade R-CNN with a ResNet-101 backbone and FPN neck. I don‚Äôt want to use a pre-trained model ‚Äî I want to train it from scratch or fine-tune it using my own dataset.

I‚Äôm running into issues figuring out the right config file for MMDetection (or any framework you recommend), and how to set up the training process properly. Most tutorials use pre-trained weights or stick to simpler architectures.

Has anyone worked on training Cascade R-CNN from scratch before? Or used it with a custom dataset (esp. with bounding boxes & labels)? Any tips, working configs, or repo links would help a ton!

Thank you in advance üôè
Also, if I‚Äôm posting in the wrong subreddit, feel free to redirect me!
",0,0.43,https://www.reddit.com/r/MachineLearning/comments/1lse90g/p_training_cascade_rcnn_resnet101_fpn_on_custom/,False,True,False
1ls6jyi,Goldziher,1751710245.0,4,/r/MachineLearning/comments/1ls6jyi/d_i_benchmarked_4_python_text_extraction/,MachineLearning,[D] I benchmarked 4 Python text extraction libraries so you don't have to (2025 results),"**TL;DR**: Comprehensive benchmarks of Kreuzberg, Docling, MarkItDown, and Unstructured across 94 real-world documents. Results might surprise you.

## üìä **Live Results**: https://goldziher.github.io/python-text-extraction-libs-benchmarks/

---

## Context

As the author of [Kreuzberg](https://github.com/Goldziher/kreuzberg), I wanted to create an **honest, comprehensive benchmark** of Python text extraction libraries. No cherry-picking, no marketing fluff - just real performance data across 94 documents (~210MB) ranging from tiny text files to 59MB academic papers.

**Full disclosure**: I built Kreuzberg, but these benchmarks are automated, reproducible, and the methodology is completely open-source.

---

## üî¨ **What I Tested**

### Libraries Benchmarked:
- **[Kreuzberg](https://github.com/Goldziher/kreuzberg)** (71MB, 20 deps) - My library
- **[Docling](https://github.com/DS4SD/docling)** (1,032MB, 88 deps) - IBM's ML-powered solution
- **[MarkItDown](https://github.com/microsoft/markitdown)** (251MB, 25 deps) - Microsoft's Markdown converter
- **[Unstructured](https://github.com/Unstructured-IO/unstructured)** (146MB, 54 deps) - Enterprise document processing

### Test Coverage:
- **94 real documents**: PDFs, Word docs, HTML, images, spreadsheets
- **5 size categories**: Tiny (<100KB) to Huge (>50MB)
- **6 languages**: English, Hebrew, German, Chinese, Japanese, Korean
- **CPU-only processing**: No GPU acceleration for fair comparison
- **Multiple metrics**: Speed, memory usage, success rates, installation sizes

---

## üèÜ **Results Summary**

### Speed Champions üöÄ
1. **Kreuzberg**: 35+ files/second, handles everything
2. **Unstructured**: Moderate speed, excellent reliability
3. **MarkItDown**: Good on simple docs, struggles with complex files
4. **Docling**: Often 60+ minutes per file (!!)

### Installation Footprint üì¶
- **Kreuzberg**: 71MB, 20 dependencies ‚ö°
- **Unstructured**: 146MB, 54 dependencies
- **MarkItDown**: 251MB, 25 dependencies (includes ONNX)
- **Docling**: 1,032MB, 88 dependencies üêò

### Reality Check ‚ö†Ô∏è
- **Docling**: Frequently fails/times out on medium files (>1MB)
- **MarkItDown**: Struggles with large/complex documents (>10MB)
- **Kreuzberg**: Consistent across all document types and sizes
- **Unstructured**: Most reliable overall (88%+ success rate)

---

## üéØ **When to Use What**

### ‚ö° **[Kreuzberg](https://github.com/Goldziher/kreuzberg)** (Disclaimer: I built this)
- **Best for**: Production workloads, edge computing, AWS Lambda
- **Why**: Smallest footprint (71MB), fastest speed, handles everything
- **Bonus**: Both sync/async APIs with OCR support

### üè¢ **[Unstructured](https://github.com/Unstructured-IO/unstructured)**
- **Best for**: Enterprise applications, mixed document types
- **Why**: Most reliable overall, good enterprise features
- **Trade-off**: Moderate speed, larger installation

### üìù **[MarkItDown](https://github.com/microsoft/markitdown)**
- **Best for**: Simple documents, LLM preprocessing
- **Why**: Good for basic PDFs/Office docs, optimized for Markdown
- **Limitation**: Fails on large/complex files

### üî¨ **[Docling](https://github.com/DS4SD/docling)**
- **Best for**: Research environments (if you have patience)
- **Why**: Advanced ML document understanding
- **Reality**: Extremely slow, frequent timeouts, 1GB+ install

---

## üìà **Key Insights**

1. **Installation size matters**: Kreuzberg's 71MB vs Docling's 1GB+ makes a huge difference for deployment
2. **Performance varies dramatically**: 35 files/second vs 60+ minutes per file
3. **Document complexity is crucial**: Simple PDFs vs complex layouts show very different results
4. **Reliability vs features**: Sometimes the simplest solution works best

---

## üîß **Methodology**

- **Automated CI/CD**: GitHub Actions run benchmarks on every release
- **Real documents**: Academic papers, business docs, multilingual content
- **Multiple iterations**: 3 runs per document, statistical analysis
- **Open source**: Full code, test documents, and results available
- **Memory profiling**: psutil-based resource monitoring
- **Timeout handling**: 5-minute limit per extraction

---

## ü§î **Why I Built This**

Working on [Kreuzberg](https://github.com/Goldziher/kreuzberg), I worked on performance and stability, and then wanted a tool to see how it measures against other frameworks - which I could also use to further develop and improve Kreuzberg itself. I therefore created this benchmark. Since it was fun, I invested some time to pimp it out:

- Uses **real-world documents**, not synthetic tests
- Tests **installation overhead** (often ignored)
- Includes **failure analysis** (libraries fail more than you think)
- Is **completely reproducible** and open
- Updates **automatically** with new releases

---

## üìä **Data Deep Dive**

The [interactive dashboard](https://goldziher.github.io/python-text-extraction-libs-benchmarks/) shows some fascinating patterns:

- **Kreuzberg dominates** on speed and resource usage across all categories
- **Unstructured excels** at complex layouts and has the best reliability
- **MarkItDown is useful** for simple docs shows in the data
- **Docling's ML models** create massive overhead for most use cases making it a hard sell

---

## üöÄ **Try It Yourself**

```bash
git clone https://github.com/Goldziher/python-text-extraction-libs-benchmarks.git
cd python-text-extraction-libs-benchmarks
uv sync --all-extras
uv run python -m src.cli benchmark --framework kreuzberg_sync --category small
```

Or just check the live results: https://goldziher.github.io/python-text-extraction-libs-benchmarks/

---

## üîó **Links**

- **üìä Live Benchmark Results**: https://goldziher.github.io/python-text-extraction-libs-benchmarks/
- **üìÅ Benchmark Repository**: https://github.com/Goldziher/python-text-extraction-libs-benchmarks
- **‚ö° Kreuzberg (my library)**: https://github.com/Goldziher/kreuzberg
- **üî¨ Docling**: https://github.com/DS4SD/docling
- **üìù MarkItDown**: https://github.com/microsoft/markitdown
- **üè¢ Unstructured**: https://github.com/Unstructured-IO/unstructured

---

## ü§ù **Discussion**

What's your experience with these libraries? Any others I should benchmark? I tried benchmarking `marker`, but the setup required a GPU.

Some important points regarding how I used these benchmarks for Kreuzberg:

1. I fine tuned the default settings for Kreuzberg.
2. I updated our docs to give recommendations on different settings for different use cases. E.g. Kreuzberg can actually get to 75% reliability, with about 15% slow-down.
3. I made a best effort to configure the frameworks following the best practices of their docs and using their out of the box defaults. If you think something is off or needs adjustment, feel free to let me know here or open an issue in the repository.
",0,0.37,https://www.reddit.com/r/MachineLearning/comments/1ls6jyi/d_i_benchmarked_4_python_text_extraction/,False,True,False
1ls6jp2,tomaz-suller,1751710215.0,8,/r/MachineLearning/comments/1ls6jp2/d_what_are_paper_introductions_meant_to/,MachineLearning,[D] What are paper introductions meant to communicate to a knowledgable reader?,"It seems like all papers have to define what the problem they're using is, and discuss traditional techniques to then go on to their contribution. My understanding this is to show you've actually gone through the effort of reviewing the literature? Still, as I'm reading papers, I can't help but often skim over the introduction very quickly or almost not bother reading it since I know, say, what an LSTM or a Transformer is.

Is that expected or am I missing something? Is the introduction mostly there to communicate to others you've done the review well? to inform readers who may not have an ML background?",0,0.31,https://www.reddit.com/r/MachineLearning/comments/1ls6jp2/d_what_are_paper_introductions_meant_to/,False,True,False
1ls5eny,ScaryReplacement9605,1751705426.0,14,/r/MachineLearning/comments/1ls5eny/d_neurips_workshops_2025/,MachineLearning,[D] NeurIPS workshops 2025?,"According to the NeurIPS website, workshop decisions were sent out on July 4th, but I haven‚Äôt seen an official list published yet. I‚Äôm particularly interested because I have a paper related to ML for biology, and I'm considering submitting it to a NeurIPS workshop. However, another conference with an upcoming deadline is also an option, so I‚Äôd like to decide soon.

If anyone has insight or knows when the list might be released, I‚Äôd really appreciate it!",17,0.8,https://www.reddit.com/r/MachineLearning/comments/1ls5eny/d_neurips_workshops_2025/,False,True,False
1ls1uf2,Sedherthe,1751691135.0,1,/r/MachineLearning/comments/1ls1uf2/r_state_of_the_art_models_in_video_matting/,MachineLearning,[R] State of The Art models in Video Matting - Comparative Analysis.,"Hi, I am exploring the field of AI in video matting. I came across [matanyone](https://github.com/pq-yang/MatAnyone) which seems like one of the best and latest ones. However, based on my experiments this feels even this is far from production use cases for very high resolutions. What are some models that are good for this?

Looking to connect with people pursuing research or working on AI in video matting. Please DM or comment here, would like to have a quick chat!",1,0.66,https://www.reddit.com/r/MachineLearning/comments/1ls1uf2/r_state_of_the_art_models_in_video_matting/,False,True,False
1ls0eoz,random_sydneysider,1751685826.0,20,/r/MachineLearning/comments/1ls0eoz/d_anyone_have_a_reasonable_experience_with/,MachineLearning,[D] Anyone have a reasonable experience with ICLR/ICML this year?,"I've been avoiding the ICLR/ICML/NeurIPS after getting unhelpful reviews with the ICLR reviews in 2024. The paper wasn't framed very well, but the NeurIPS reviews in 2023 were a lot better even if the paper wasn't accepted. 

Question for those who successfully published in ICLR/ICML in the latest cycle. Did you have a fairly good experience with the review process? Do you have any advice for those of us who didn't?  ",35,0.82,https://www.reddit.com/r/MachineLearning/comments/1ls0eoz/d_anyone_have_a_reasonable_experience_with/,False,True,False
1lru2mh,Wonderful-Delivery-6,1751665084.0,9,/r/MachineLearning/comments/1lru2mh/p_i_built_a_mindmaplike_non_linear_tutorsupported/,MachineLearning,"[P] I built a mindmap-like, non linear tutor-supported interface for exploring ML papers, and I'm looking for feedback!","Hi everyone,

LLMs have made me feel like I can understand anything, but I‚Äôve been frustrated trying to truly understand ML papers using just ChatGPT or static PDFs. Summaries can help, but then I have to go back to the paper and read it linearly to deeply understand it, and I have long chatgpt conversations which I just can't track. So I built an interface designed to support a non-linear, brain-like exploration of papers ‚Äî paired with a tutor in a chat interface that guides your understanding.¬†

https://preview.redd.it/vqv65julfxaf1.png?width=1725&format=png&auto=webp&s=0e09f203a863527d478568332dc6e3cbeb99fd87

Here is a screenshot of what it looks like.   
  
Try it out at:¬†[proread.ai/llm-papers](http://proread.ai/llm-papers)

1. Knowledge maps let you see how ideas within a paper relate to each other and how papers connect across a field. Start with my curated maps of foundational LLM papers or build your own for any paper/set of papers you‚Äôre reading. You can also listen to the map as a podcast.
2. You have a chat based tutor as with ChatGPT but your questions keep updating the knowledge map so you don't lose anything
3. The map itself is an editable notebook which allow you to take notes, mark concepts as completed, tag concepts, and construct your own mental model as you read. You can not only read summaries but can go down to actual source content in readers where you want to.
4. You can make your own space with your own papers or other docs (PDF/txt/html/URLs) and create interactive maps personalized to your research or study needs.

The goal is to move beyond linear reading or static summarization: to create a space where understanding evolves dynamically, like how you actually think, with a tutor helping you make sense of it all.

Please try it out at:¬†[proread.ai/llm-papers](http://proread.ai/llm-papers)

I‚Äôm looking for feedback from other researchers or paper readers ‚Äî would this kind of non-linear, guided exploration help you understand tough topics/papers better than traditional PDFs or chat tools? What‚Äôs missing or confusing?

Thanks!",9,0.74,https://www.reddit.com/r/MachineLearning/comments/1lru2mh/p_i_built_a_mindmaplike_non_linear_tutorsupported/,False,True,False
1lrtgx1,Ozay0900,1751663375.0,2,/r/MachineLearning/comments/1lrtgx1/p_neuroevolution_for_super_mario/,MachineLearning,[P] NeuroEvolution for Super Mario,"Hi, i wanted to make Mario learn to play the original super-marino-bros from the library

    gym_super_mario_bros  

and wanted to use a genetic algorithm. My genomes are lists of weights. I apply a genome aka the weights to a CNN. The CNN gets the current frame (converted to 84x84 grayscale) as input and processes it until I get one out of 7 possible actions to take for Mario. Mario then takes this action, gets a reward for this action, and the next frame is processed and so on. Finally I gave Mario additional rewards for reaching the flag and being quick.

I tried multiple crossover functions including point-crossover, uniform-crossover and mlx-alpha-crossover. I adapt my mutation rate based on the fitness aka if it stagnates for too long or not. Selection is usually just the top k fittest genomes. I also used big populations like 300 for 30 generations or 300 generations with a population of 30. Nothing worked, he never once reached the flag. He has no problem quickly learning to jump over enemies and obstacles and moves quick. But he somehow gets stuck at the blocky stairs. He literally does nothing once he reaches them and I have no idea how. I used all combinations of crossover/mutation-rates/... but no success. I also used frame stacking and frame skipping.

My alternative approach of the genome being directly the actions and using crossover and everything on them even worked better.

I know this is a quite a high level explanation but I can provide more details if needed. My CNN has 2 convolutional layers with 4 input channels, 16 output channels and my kernels are 8x8 and I use stride of 4. the last layer has 32 feauture maps of size 9x9 which I just put into final output layers to give me 7 logits (these are the possible actions) and take the highest one. This is the rough plan. I could adjust a lot of stuff but I would non the less expect to at least have one Mario reach the flag at least. Does anyone have ideas or experience with this library and genetic algorithms ?",5,1.0,https://www.reddit.com/r/MachineLearning/comments/1lrtgx1/p_neuroevolution_for_super_mario/,False,True,False
1lrs596,CadavreContent,1751659720.0,7,/r/MachineLearning/comments/1lrs596/d_aacl_reputation/,MachineLearning,[D] AACL Reputation,"In the ACL universe, ACL, EMNLP, and NAACL are generally considered equal. EACL is considered a bit lower but highly reputable and maybe even the same by some. I haven't heard much about the relatively newer AACL. What's your opinion on papers published there? Is it in the same ballpark of reputation, or is it still significantly lagging behind?

",12,0.8,https://www.reddit.com/r/MachineLearning/comments/1lrs596/d_aacl_reputation/,False,True,False
1lrr5yy,Dangerous-Hat1402,1751657105.0,29,/r/MachineLearning/comments/1lrr5yy/d_did_anyone_receive_this_from_nips/,MachineLearning,[D] Did anyone receive this from NIPS?,"Your co-author, Reviewer has not submitted their reviews for one or more papers assigned to them for review (or they submitted insufficient reviews). Please kindly note the Review deadline was on the 2nd July 11.59pm AOE.

===
My co-author has graduated and no longer worked in academic anymore. How can I handle that? It is not fair to reject my paper!",51,0.78,https://www.reddit.com/r/MachineLearning/comments/1lrr5yy/d_did_anyone_receive_this_from_nips/,False,True,False
1lrqzma,AdInevitable1362,1751656637.0,10,/r/MachineLearning/comments/1lrqzma/d_does_splitting_by_interaction_cause_data/,MachineLearning,[D] Does splitting by interaction cause data leakage when forming user groups this way for recommendation?,"I‚Äôm working on a group recommender system where I form user groups automatically (e.g. using KMeans) based on user embeddings learned by a GCN-based model.

Here‚Äôs the setup:
	‚Ä¢	I split the dataset by interactions, not by users ‚Äî so the same user node may appear in both the training and test sets, but with different interactions.
	‚Ä¢	I train the model on the training interactions.
	‚Ä¢	I use the resulting user embeddings (from the trained model) to cluster users into groups (e.g. with KMeans).
	‚Ä¢	Then I assign test users to these same groups using the model-generated embeddings.

üîç My question is:

Even though the test set contains only new interactions, is there still a data leakage risk because the user node was already part of the training graph? That is, the model had already learned something about that user during training.
be a safer alternative in this context.

Thanks!
",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1lrqzma/d_does_splitting_by_interaction_cause_data/,False,True,False
1lrnruz,hhblackno,1751648515.0,7,/r/MachineLearning/comments/1lrnruz/d_how_trustworthy_are_benchmarks_of_new/,MachineLearning,[D] How trustworthy are benchmarks of new proprietary LLMs?,"Hi guys. I'm working on my bachelor's thesis right now and am trying a find a way to compare the Dense Video Captioning abilities of the new(er) proprietary models like Gemini-2.5-Pro, GPT-4.1 etc. Only I'm finding to have significant difficulties when it comes to the transparency of benchmarks in that area.

For example, looking at the [official Google AI Studio webpage](https://developers.googleblog.com/en/gemini-2-5-video-understanding/), they state that Gemini 2.5 Pro achieves a value of 69.3 when evaluated at the YouCook2 DenseCap validation set and proclaim themselves as the new SoTA. The leaderboard on [Papers With Code](https://paperswithcode.com/sota/dense-video-captioning-on-youcook2) however lists HiCM¬≤ as the best model - which, the way I understand it, you would need to implement from the ground up based on the methods described in the research paper as of now - and right after that Vid2Seq, which Google claims is the old SoTA that Gemini 2.5 Pro just surpassed.

I faced the same issue with [GPT-4.1](https://openai.com/index/gpt-4-1/), where they state 
>Long context: On Video-MME, a benchmark for multimodal long context understanding, GPT‚Äë4.1 sets a new state-of-the-art result‚Äîscoring 72.0% on the long, no subtitles category, a 6.7%abs improvement over GPT‚Äë4o.
but the official Video-MME leaderboard does not list GPT-4.1. 

Same with VideoMMMU ([Gemini-2.5-Pro](https://deepmind.google/models/gemini/pro/) vs. [Leaderboard](https://huggingface.co/datasets/lmms-lab/VideoMMMU)), [ActivityNet Captions](https://paperswithcode.com/sota/dense-video-captioning-on-activitynet) etc.

I understand that you can't evaluate a new model the second it is released, but it is very difficult to find benchmarks for new models like these. So am I supposed to ""just blindly trust"" the very company that trained the model that it is the best without any secondary source? That doesn't seem very scientific to me. 

It's my first time working with benchmarks, so I apologize if I'm overlooking something very obvious. 

",11,0.87,https://www.reddit.com/r/MachineLearning/comments/1lrnruz/d_how_trustworthy_are_benchmarks_of_new/,False,True,False
1lrnibz,Gold-Plum-1436,1751647863.0,8,/r/MachineLearning/comments/1lrnibz/r_kappatune_a_pytorchbased_optimizer_wrapper_for/,MachineLearning,[R] kappaTune: a PyTorch-based optimizer wrapper for continual learning via selective fine-tuning,"# This optimizer wrapper for continual learning is guided by the condition number (Œ∫) of model tensors. It identifies and updates only the least anisotropic parameters to preserve pre-trained knowledge and mitigate catastrophic forgetting due to a synergy of factors: their inherent numerical stability makes them less susceptible to training noise, and their less specialized nature allows for robust adaptation without overwriting critical, highly specific pre-training knowledge, thereby effectively mitigating catastrophic forgetting of foundational capabilities (see the link to the paper in the repository):¬†[https://github.com/oswaldoludwig/kappaTune](https://github.com/oswaldoludwig/kappaTune)",16,0.81,https://www.reddit.com/r/MachineLearning/comments/1lrnibz/r_kappatune_a_pytorchbased_optimizer_wrapper_for/,False,True,False
1lrmic2,transformer_ML,1751645318.0,0,/r/MachineLearning/comments/1lrmic2/r_selfcorrection_bench_revealing_and_addressing/,MachineLearning,[R] Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs,"I recently released this preprint benchmarking LLM capability of self-correction.

**The Problem**: LLM self-correction is important for reliability, but it's hard to benchmark because naturally occurring errors are rare. So I built Self-Correction Bench by systematically injecting errors into LLM reasoning traces.

**Key Discovery**: LLMs systematically fail to correct errors in their own outputs while successfully correcting identical errors in external inputs. I call this the ""Self-Correction Blind Spot."" 

**Results across 14 models**: 

\- 64.5% average blind spot rate 

\- Simply appending ""Wait"" reduces blind spots by 89.3% without finetuning

\- Other correction markers (""But"", ""However"") also help 

\- Reasoning models generate these markers when they see errors

**Insight**: I analyzed post-training data and found non-reasoning instruction datasets are 95%+ lacking correction markers. RL-trained reasoning models don't show this blind spot - their generation contains lots of correction markers - suggesting they learned error correction through trial and error.

**Implications**: This affects AI safety and reliability. If LLMs can't catch their own mistakes, we need better training paradigms or activation mechanisms like correction markers. It seems RL is very promising.

Benchmark: [https://huggingface.co/papers/2507.02778](https://huggingface.co/papers/2507.02778)

Author here - happy to discuss the methodology and have your feedback.",7,0.77,https://arxiv.org/abs/2507.02778,False,False,False
1lrkzgj,datashri,1751641541.0,2,/r/MachineLearning/comments/1lrkzgj/d_help_understanding_speculative_sampling/,MachineLearning,[D] Help understanding speculative sampling,"Hi all,

Need a bit of help understanding speculative sampling. [arXiv:2211.17192v2](https://arxiv.org/abs/2211.17192)

The idea is for the small model to generate the completions and the larger model to evaluate them. If the LLM accepts all the tokens generated by the SLM, it generates an additional token. If not, it generates the replacements of the tokens it rejected. Section 2.1 and 2.3 in the paper discuss this.

Given tokens x\_{<t}, p(x\_t | x\_{<t}) is the distribution generated by the target LLM. q(x\_t | x\_{<t}) is generated by a smaller, more efficient model (SLM). We want x \~ p(x), but we sample x\~q(x) and keep it IF q(x) <= p(x).

I don't quite get the logic of keeping the x\~q(x) sample if q(x) <= p(x). I'm sure it is something simple but a blind spot for someone dumb as me. Can someone please explain in simple terms?

Given a well-trained and a less capable model, and a sequence, in general, is there a relation between the probability distributions from both models for the next token? I would expect that the generations from the LLM have a higher likelihood of matching the next sequence in the training data.",2,1.0,https://www.reddit.com/r/MachineLearning/comments/1lrkzgj/d_help_understanding_speculative_sampling/,False,True,False
1lrjvbf,w0nx,1751638723.0,4,/r/MachineLearning/comments/1lrjvbf/d_combining_box_and_point_prompts_with_sam_21_for/,MachineLearning,[D] Combining box and point prompts with SAM 2.1 for more consistent segmentation ‚Äî best practices?,"I‚Äôm developing an application using SAM 2.1 (via FastAPI) for real-time object segmentation from a live camera feed. The frontend sends either a box or point prompt to the backend, which returns a mask that‚Äôs composited into a canvas for manipulation and export.

Each prompt type works well in isolation ‚Äî but they‚Äôre inconsistent across different object classes. A couple examples:

* **Plant in pot**: A box prompt captures the foliage but often excludes the pot. A point prompt on the leaves sometimes segments a single leaf, especially with fine stems or dense texture.
* **Theragun / handheld tool**: A point near the handle often gives excellent results. A box prompt sometimes returns background or over-segments nearby objects.

I‚Äôm now exploring combining both prompt types: drawing a bounding box *and* allowing the user to tap inside it to reinforce intent. Since SAM 2.1 accepts both `boxes` and `point_coords + point_labels`, this seems feasible ‚Äî but I‚Äôm curious:

* Have others here tried combining these prompts in production or research tools?
* Are there heuristics you‚Äôve found effective for prioritizing or weighting prompt types in ambiguous contexts?
* Do you use `multimask_output=True` and apply post-selection based on area, IOU, or visual saliency?
* Any recommended architectures or methods for mask refinement after prompt-based SAM segmentation (e.g. to recover small appendages like wires, roots, or hollow interiors)?

Would appreciate insights from anyone deploying SAM variants or experimenting with segmentation UIs. Trying to optimize for a broad class of ‚Äúirregular physical objects‚Äù where semantic boundaries aren‚Äôt always visually dominant.",8,0.9,https://www.reddit.com/gallery/1lrjvbf,False,False,False
1lrfedq,Electrical_Ad_9568,1751624784.0,0,/r/MachineLearning/comments/1lrfedq/d_openai_board_member_on_the_future_of_machine/,MachineLearning,[D] OpenAI Board Member on the Future of Machine Learning,[https://www.youtube.com/watch?v=-\_M5PY5BC9I](https://www.youtube.com/watch?v=-_M5PY5BC9I),0,0.3,https://www.reddit.com/r/MachineLearning/comments/1lrfedq/d_openai_board_member_on_the_future_of_machine/,False,True,False
1lqwgyf,mr00rng,1751565945.0,2,/r/MachineLearning/comments/1lqwgyf/r_permutation_neuron_achieving_77_accuracy_on/,MachineLearning,[R] Permutation Neuron: Achieving 77% Accuracy on MNIST with Three Neurons,"This article addresses the challenge of classification with minimal multiplication operations while maintaining accuracy above 75%. The MNIST dataset serves as an example, where a single permutation neuron, utilizing three classical neurons, achieves 77% accuracy.

# Concept of the Permutation Neuron

The Permutation Neuron is a computational unit that implements a permutation-based transformation of input signals. The neuron maintains a set of internal vectors that are reordered based on their interaction with the input data. This reordering process maps the input space to a discrete set of output patterns, where each pattern corresponds to a specific permutation of the internal vectors.

For classifying the 10 digits of the MNIST dataset, at least 10 distinct neuron states are required. Since the number of permutations is determined by the factorial of the number of neurons, a minimum of 4 neurons (4! = 24 permutations) is needed to cover 10 classes. However, by subtracting the value of one neuron from the others (normalization), only three neurons need to be computed, with the fourth set to zero, preserving the order of permutations. This reduces computational cost while maintaining 24 unique states for classification.

For the MNIST classification task, the permutation neuron operates as follows: three neurons with linear activation functions compute values based on the input image data, while a fourth neuron is fixed at zero. These four values are ordered to form one of 24 possible permutations (4!), such as ACZB. Using the Lehmer code, each permutation is mapped to a unique number from 0 to 23, which is then assigned to one of the 10 MNIST classes (e.g., digits 0‚Äì9).

# Training with a Genetic Algorithm

The search space for parameters is limited to 2355 values, where each of the three neurons processes input data of size 784 (MNIST image pixels) plus a bias term (3 √ó (784 + 1)). The 24 permutation states generated by the permutation neuron are determined by a greedy algorithm based on the MNIST training set, enabling the mapping of permutations to 10 classes. A genetic algorithm is employed to optimize the neuron weights, as the parameter space is poorly understood but assumed to contain local optima corresponding to effective solutions.

For weight optimization, a genetic algorithm with a population of 50 individuals is used. The BLX-Alpha crossover (with parameter k=2) is applied over two parents, with a 2% probability of random mutation. These settings achieved a classification accuracy of 77% on the MNIST dataset.

# Code

The implementation of the permutation neuron, including the genetic algorithm and the greedy algorithm for mapping permutations to MNIST classes, is available at¬†[GitHub](https://github.com/sgr-team/math/tree/main/problems/pn). The code includes an experiment achieving 77% accuracy (results in mnist\_46257.json).

Readers are encouraged to reproduce the experiment or propose improved solutions, such as higher accuracy or fewer multiplication operations. Improved results will be published with attribution to their authors.",0,0.25,https://www.reddit.com/r/MachineLearning/comments/1lqwgyf/r_permutation_neuron_achieving_77_accuracy_on/,False,True,False
1lrcwk1,Head_Mushroom_3748,1751614897.0,1,/r/MachineLearning/comments/1lrcwk1/p_why_am_i_getting_poor_performance_with_gnns_for/,MachineLearning,[P] Why am I getting poor performance with GNNs for edge prediction from node features only?,"Hi everyone,

I'm working on an industrial use case where I tried to use a Graph Neural Network to \*\*predict edges between tasks\*\*, based solely on node features.

Each graph represents 10-60 tasks (nodes), and I have about 1200 such graphs for training. Each task comes with features (label, equipment type), but there are no edges given at inference time, the goal is to infer all connections -> generate the full adjacency structure.

The key point: whether an edge exists between two nodes depends on the global context, not just pairwise similarity.

I‚Äôve tried GCNs and GATs (with various edge construction strategies during training), but I'm consistently getting poor performance.

So I‚Äôm wondering:

\-  Is this just a bad fit for classical GNNs? 

\- Should I switch to Transformer-like models that encode full-node context? Or even fine-tuning ?

\- Do I need a much larger dataset to make a GNN work in this setup?

\- Is it better to frame this as a graph generation problem (autoencoders) ?

  
I know GNN needs edge-index during inference, but i genuinely do not seem to find the right model for my project...",1,0.67,https://www.reddit.com/r/MachineLearning/comments/1lrcwk1/p_why_am_i_getting_poor_performance_with_gnns_for/,False,True,False
1lrc7vh,Mundane-Earth4069,1751612174.0,12,/r/MachineLearning/comments/1lrc7vh/d_understanding_optimal_batch_size_calculation/,MachineLearning,[D] Understanding Optimal Batch Size Calculation - Arithmetic Intensity,"I encountered this¬†[talk¬†](https://www.youtube.com/watch?v=mYRqvB1_gRk)where the speaker (TimotheÃÅe Lacroix of Mistral) states that an optimal batch-size is hardware dependent and can be calculated as 2xflops/mem\_bandwidth (6:40) -- Hence an optimal batchsize (B\*) for an A100 is 400.

I had some confusion on this formula - The memory bandwidth for a an A100 is 2TB/s, while the FLOPs (assuming FP16) are 312 TFlop - Can TFlops be divided by TBs though they are fundamentally different units?

Appreciate anyone who can help explain this - If anyone has suggested materials to learn more about how this number was derived, I would be very happy to take a look

I'm sure its related to[¬†Arithmetic intensit](https://www.iguazio.com/glossary/arithmetic-intensity/)y but that number is simply 312/2=156

EDIT:

Did some research based on answers and resources here and tried to come up with an explanation - If anyone cared to feedback or point out areas of improvement, would really appreciate it

**Arithmetic Intensity**

Performance is defined by memory bandwidth, compute, latency. If compute is more limited than memory, it is compute bound. Vice versa for memory bound. Arithmetic intensity is the ratio of compute operations to memory operations (Specifically FLOPs per byte transferred). If you are compute bound, optimizing for memory does not benefit your system, and vice versa. Calculating arithmetic intensity tells you which parts of your system to focus on optimizing. Arithmetic intensity itself is calculated as a hardware threshold as well as for individual operations. Real world performance depends on actual model architecture, dataset characteristics, training/inference regime, memory access patterns, cache utilization, batch size, operator fusion, etc‚Ä¶

Arithmetic intensity can also be applied to operations as below.¬†**Values only approximate:**

Low arithmetic intensity operations (10-100 FLOPs/byte) include elementwise ops, activations, normalizations (Example, addition involves moving 2N values to GPU but doing only N ops)

High intensity ops (100 - 1000 FLOPs/byte) include matmuls and convolutions. Larger batch sizes also increase intensity - This is because input data increases while the memory access cost for weight matrices remains constant - Hence larger batches improve GPU compute utilization.

Hence, frameworks focus heavily on fusion of low intensity operations. Operations can have different arithmetic intensity depending on problem size (small matrices have lower intensity because less data can be reused), implementation (tiled algorithms are faster), precision (FP16 doubles available compute).

Consider the arithmetic intensity threshold. At 312 TFLOPs and a mem bandwidth of 1.55 TB/s for FP16 tensor ops in an A100, the arithmetic intensity threshold is roughly 201. Ops with intensity below this are memory bound, while ops above it are compute bound. A memory bound operation results in idle GPU compute while a compute bound operation results in bottlenecking. In practice, hitting this precise 100% resource utilization is rare.¬†",45,0.94,https://www.reddit.com/r/MachineLearning/comments/1lrc7vh/d_understanding_optimal_batch_size_calculation/,False,True,False
1lraj3z,shiva2692,1751605942.0,5,/r/MachineLearning/comments/1lraj3z/d_sampling_technique_for_imbalanced_dataset_of_a/,MachineLearning,[D] Sampling technique for imbalanced dataset of a OOS prediction model,"Hey all,

I‚Äôm trying to build ML model for OOS prediction of an item of an imbalanced dataset, which sampling technique should I use and how should I evaluate that sampling technique to create a better model.

Appreciate your thoughts and responses.

Thanks",10,1.0,https://www.reddit.com/r/MachineLearning/comments/1lraj3z/d_sampling_technique_for_imbalanced_dataset_of_a/,False,True,False
1lr934p,AdInevitable1362,1751600984.0,1,/r/MachineLearning/comments/1lr934p/rgroup_recommendation_systems_looking_for/,MachineLearning,"[R]Group Recommendation Systems ‚Äî Looking for Baselines, Any Suggestions?","Does anyone know solid baselines or open-source implementations for group recommendation systems?

I‚Äôm developing a group-based recommender that relies on classic aggregation strategies enhanced with a personalized model, but I‚Äôm struggling to find comparable baselines or publicly available frameworks that do something similar.

If you‚Äôve worked on group recommenders or know of any good benchmarks, papers with code, or libraries I could explore, I‚Äôd be truly grateful for your. Thanks in advance!",5,1.0,https://www.reddit.com/r/MachineLearning/comments/1lr934p/rgroup_recommendation_systems_looking_for/,False,True,False
1lr15dk,powerful_lord_33,1751577399.0,13,/r/MachineLearning/comments/1lr15dk/d_a_serious_concern_on_the_acl_rolling_review/,MachineLearning,[D] A Serious Concern on the ACL Rolling Review System,"
While I understand the traditional conference review paradigm involving initial scores, author rebuttals, and final scores, this model is beginning to show clear cracks under the scale and competitiveness of today‚Äôs A-level (and even mid-tier) venues. Increasingly, reviewers tend to give deliberately conservative or low pre-rebuttal scores, knowing that authors will be compelled to respond in the rebuttal phase. Even when a higher score is justified, reviewers often hold back, defaulting to borderline decisions just to see how the authors respond.

This issue is even more pronounced with ACL Rolling Review, where the scoring system is vague and lacks standard terminology such as Accept, Borderline, or Reject. This makes the process even more opaque. The ARR policy clearly states that responding to review comments is not mandatory. Yet, as an author, I am expected to thoroughly and respectfully address reviewer concerns, even when they are speculative or unreasonable. This one-sided non-obligation creates a deeply flawed power imbalance.

Here‚Äôs where it gets worse.

Many reviewers, when submitting their own papers and receiving poor reviews, tend to reflect their frustration onto the papers they are assigned to review. I have observed the following patterns:

Case 1: A reviewer receives bad reviews on their own paper and becomes unnecessarily harsh or disengaged in the reviews they provide for others.

Case 2: Prior to seeing their own reviews, reviewers play it safe by giving slightly lower pre-rebuttal scores than deserved. After receiving unfavorable reviews, they either ignore rebuttals completely or refuse to revise their scores, even when rebuttals clearly address their concerns.

This leads to a toxic feedback loop where every paper becomes a collateral victim of how a reviewer‚Äôs own submission is treated. I have seen this firsthand.

In the current ARR May cycle:
I received 10 reviews across 3 papers, with only 2 reviewers responding post-rebuttal.

From 4 papers I reviewed, totaling 12 reviews, only 6 reviewers responded, and 4 of those responses were mine.

We need to acknowledge a basic truth: acknowledging a rebuttal should be a moral minimum. Yet today, there is no incentive for honest reviewing, and no consequence for disengaged or negligent behavior. Why should any of us continue to uphold moral obligations, being fair, constructive, and thorough, when our own work receives careless and dismissive treatment?

This culture cannot be allowed to continue. Unless ACL/ARR enforces stricter policies, such as making post-rebuttal justification and score updates mandatory (as CVPR and other CVF conferences do), the system will continue to erode.

I am a young researcher trying to do my part for this community. But after repeated experiences like this, what incentive do I have to stay committed to high standards as a reviewer? Why should I put in the effort when others do not?

A system where morality is optional will ultimately breed apathy and toxicity. It is time for a structural shift.

Always, to the hope.


#acl #emnlp #arr",46,0.87,https://www.reddit.com/r/MachineLearning/comments/1lr15dk/d_a_serious_concern_on_the_acl_rolling_review/,False,True,False
1lqxnie,RSchaeffer,1751568778.0,26,/r/MachineLearning/comments/1lqxnie/d_position_machine_learning_conferences_should/,MachineLearning,"[D] Position: Machine Learning Conferences Should Establish a ""Refutations and Critiques"" Track","We recently released a preprint calling for ML conferences to establish a ""Refutations and Critiques"" track. I'd be curious to hear people's thoughts on this, specifically (1) whether this R&C track could improve ML research and (2) what would be necessary to ""do it right"".",107,0.93,https://arxiv.org/abs/2506.19882,False,False,False
1lqw0fj,SaadUllah45,1751564866.0,16,/r/MachineLearning/comments/1lqw0fj/d_hyperparameter_optimization_with_evolutionary/,MachineLearning,[D] Hyperparameter Optimization with Evolutionary Algorithms: A Biological Approach to Adaptive Search,"Data Science is a fascinating field, with always something to learn. Recently, I came across an interesting (though not ideal) approach to hyperparameter optimization: Evolutionary Algorithms (EA). EAs are a subset of Genetic Algorithms that work on Darwin‚Äôs idea of ‚Äúsurvival of the fittest‚Äù. While Grid Search and Manual Tuning remain the go-to approaches, they are limited by predefined search space and, in some sense, are brute-force methods to optimize hyperparameters. Interestingly, Evolutionary Algorithms work on the principles of biology and genetics:

1. They start with a population of candidate solutions (hyperparameters) and treat them as chromosomes.
2. Each chromosome is then evaluated using a fitness test (for example, precision, absolute error etc.)
3. The best-fit candidates are selected as parents.
4. Parent solutions generate offspring using crossover (combining individual traits) and mutation (small random changes)
5. The offspring are then used as candidate solutions, and steps 1-4 are repeated till an optimal solution (under a defined threshold) is met or iterations are exhausted.

While this is a computationally expensive solution, EA offers an adaptive methodology instead of static search methods, which can look for solutions that are not pre-defined.

Thoughts?

Note: EA is not a silver bullet to all your optimization problems.",12,0.77,https://www.reddit.com/r/MachineLearning/comments/1lqw0fj/d_hyperparameter_optimization_with_evolutionary/,False,True,False
1lqvfm5,K3NCHO,1751563509.0,3,/r/MachineLearning/comments/1lqvfm5/p_built_a_semantic_search_api/,MachineLearning,[P] Built a semantic search API,"Working on a project that needed both semantic search and content moderation, so I built an API that handles both.

**The problem it solves:**¬†Expensive GPU instances required for inference, hard to scale infrastructure. Most teams give up quickly after realizing the infrastructure needed to handle this.

**What it does:**¬†Semantic search + content moderation. You can search images by describing them (""girl with guitar"") or find text by meaning (""movie about billionaire in flying suit"" ‚Üí Iron Man). Plus NSFW detection with specific labels.

**Stack:**

* Rust Candle for ML models (Clip)
* Rust Axum + Tokio for the API
* Vector DB for search

I am considering switching to a more lightweight CLIP based model like mobileclip or clip quantized. What do you guys think?",0,0.4,https://www.reddit.com/r/MachineLearning/comments/1lqvfm5/p_built_a_semantic_search_api/,False,True,False
1lqupo0,New-Skin-5064,1751561830.0,0,/r/MachineLearning/comments/1lqupo0/d_what_operations_should_i_fuse_in_a_transformer/,MachineLearning,[D] What operations should I fuse in a transformer?,"I am pretraining a GPT-style language model with PyTorch XLA and wanted to know what operations to fuse with Pallas. I use rotary positional embeddings, SwiGLU, and RMSNorm, and I am working on adding FlashAttention to my codebase. I also employ FSDPv2 with SPMD for distributed training.",0,0.25,https://www.reddit.com/r/MachineLearning/comments/1lqupo0/d_what_operations_should_i_fuse_in_a_transformer/,False,True,False
1lqjgjz,i_minus,1751527695.0,22,/r/MachineLearning/comments/1lqjgjz/d_aaai2026_2_phase_review_discussion/,MachineLearning,[D] AAAI-2026 2 phase review discussion,"**{another edit} I got it that it won't be used for decision making. I posted it to ask if it is true.. and realized that many of us did not know about this**

<previous post>

AAAI-26' Two-phase reviewing for the Main Track:

[https://aaai.org/aaai-launches-ai-powered-peer-review-assessment-system/](https://aaai.org/aaai-launches-ai-powered-peer-review-assessment-system/)

Phase 1: Two reviews supplemented by one AI-generated, non-decisional review.

Phase 2: Additional reviews for papers not rejected in Phase 1.

**Author response after Phase 2, only for papers not rejected in Phase 1.**

Edit : They also said (but why the use of AI tho )  
The pilot program will thoughtfully integrate LLM technology at two specific points in the established review process:

Supplementary First-Stage Reviews: LLM-generated reviews will be included as one component of the initial review stage, providing an additional perspective alongside traditional human expert evaluations.

Discussion Summary Assistance: LLMs will assist the Senior Program Committee (SPC) members by summarizing reviewer discussions, helping to highlight key points of consensus and disagreement among human reviewers.

<previous post>",32,0.92,https://www.reddit.com/r/MachineLearning/comments/1lqjgjz/d_aaai2026_2_phase_review_discussion/,False,True,False
1lqhoe2,random_sydneysider,1751520886.0,9,/r/MachineLearning/comments/1lqhoe2/d_are_nlp_theory_papers_helpful_for_industry/,MachineLearning,[D] Are NLP theory papers helpful for industry research scientist roles?,"Currently I'm quite interested in NLP theory, and have some questions about how to make them count for RS roles in industry roles at top AI labs.  
(1) Does the number of papers help? My impression is that having many papers that are ""purely theoretical"" may not help that much, and AI labs will only count the number of ""relevant papers"" (and exclude those that are less relevant).   
(2) If the theory paper also yields strong empirical results, is it important to frame it as an empirical paper (and maybe put the theory in the appendix)? This could compensate for any perceived weakness with theoretical work.  
(3) What topics in language/vision models are particularly relevant in industry? Efficiency of LLMs is one priority; MoE, sparse attention & structured sparsity, are two approaches to efficient LLMs.",16,0.94,https://www.reddit.com/r/MachineLearning/comments/1lqhoe2/d_are_nlp_theory_papers_helpful_for_industry/,False,True,False
1lqgbdk,guohealth,1751516135.0,43,/r/MachineLearning/comments/1lqgbdk/d_aiml_interviews_being_more_like_swe_interviews/,MachineLearning,[D] AI/ML interviews being more like SWE interviews,"Have people noticed that AI/ML/DS job interviews now feel more SWE-like? For example, relying more on data structures and algorithms leetcode questions. I‚Äôve noticed in my professional friend groups more people are being asked these questions during the coding interview.",137,0.94,https://www.reddit.com/r/MachineLearning/comments/1lqgbdk/d_aiml_interviews_being_more_like_swe_interviews/,False,True,False
1lqedrt,Striking-Warning9533,1751509982.0,18,/r/MachineLearning/comments/1lqedrt/d_paper_with_code_is_completely_down/,MachineLearning,[D] Paper with code is completely down,"Paper with Code was being spammed (https://www.reddit.com/r/MachineLearning/comments/1lkedb8/d\_paperswithcode\_has\_been\_compromised/) before, and now it is compoletely down. It was also down a coupld times before, but seems like this time it has lasted for days. (https://github.com/paperswithcode/paperswithcode-data/issues)

",51,0.97,https://www.reddit.com/r/MachineLearning/comments/1lqedrt/d_paper_with_code_is_completely_down/,False,True,False
1lqe31s,Outrageous_Tip_8109,1751509078.0,13,/r/MachineLearning/comments/1lqe31s/d_what_tool_to_use_to_create_illustrations_like/,MachineLearning,[D] What Tool to Use to Create Illustrations Like This?,"Recently, I‚Äôve seen many researchers adopt this style of illustration to present an architectural view of their method or approach. These visuals are clean, professional, and visually appealing, perfect for research papers and presentations.

I've tried replicating this style using [draw.io](http://draw.io), but I haven‚Äôt been able to achieve the same level of quality or aesthetics.

Could anyone suggest tools or software commonly used to create such research illustrations?

I'm particularly interested in tools that are:

1. Suitable for academic or technical diagrams

2. Capable of producing high-quality, publication-ready visuals

3. Flexible for custom styling or layouts

Any recommendations would be greatly appreciated!

Please check Illustration here: [https://imgur.com/a/VWiKD3Q](https://imgur.com/a/VWiKD3Q)",1,0.53,https://www.reddit.com/r/MachineLearning/comments/1lqe31s/d_what_tool_to_use_to_create_illustrations_like/,False,True,False
1lqb0uq,Top-Purchase926,1751499960.0,26,/r/MachineLearning/comments/1lqb0uq/d_uoft_phd_ranking/,MachineLearning,[D] UofT PhD Ranking,"In terms of academia prestige (for future prof positions), where would you place UofT ML PhD? Is it better RoI to do it at a T10 American school (UIUC, Georgia Tech, UT Austin, UWash, etc) for name recognition considering the advisors are equivalent? Also, how does UofT PhD fare against Oxbridge DPhil these days?",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1lqb0uq/d_uoft_phd_ranking/,False,True,False
1lqampf,evilpastabake,1751498850.0,2,/r/MachineLearning/comments/1lqampf/d_applicability_of_a_biomedical_based_aiml_phd_to/,MachineLearning,[D] Applicability of a Biomedical based AI/ML PhD to other AI/ML fields,"Hey all,

I am a first year PhD student in a top biomedical program in the US. One of the labs I am most interested in studies how to more effectively use AI/ML to enhance the drug discovery and development process. Although I current have only a limited knowledge of coding (really just experience with R and a little C++) the PI has told me he'd be happy to have me join the group. Still, I wonder about the applicability of this niche expertise. Does having done a PhD in biomedical focused AI/ML allow for the possibility of being hired in say finance AI/ML? What about AI/ML research in big tech? Or would you say it is only applicable in Big Pharma/biomed startup research?

Thanks for your insights.",5,0.86,https://www.reddit.com/r/MachineLearning/comments/1lqampf/d_applicability_of_a_biomedical_based_aiml_phd_to/,False,True,False
1lps1eo,QuantumFree,1751451032.0,0,/r/MachineLearning/comments/1lps1eo/p_dfreg_a_physicsinspired_regularization_method/,MachineLearning,[P] DFReg: A Physics-Inspired Regularization Method That Operates on Global Weight Distributions (arXiv:2507.00101),"Hi everyone,

I‚Äôd like to share a recent preprint I uploaded to arXiv, introducing **DFReg** ‚Äì a new regularization framework for neural networks inspired by **Density Functional Theory (DFT)** in physics.

**What is DFReg?**  
DFReg replaces local penalties (like L2 regularization or Dropout) with a **global constraint** on the *empirical weight distribution*. It treats the weights of a neural network as a statistical density and introduces a functional penalty that encourages:

* Smooth, non-peaky weight distributions
* Diverse, well-spread parameter configurations
* Structural regularity across layers

No architectural changes or stochastic perturbations required.

**What we tested:**  
We evaluated DFReg on **CIFAR-100 with ResNet-18**, comparing it to Dropout and BatchNorm. Metrics included:

* Test accuracy and loss
* Weight entropy
* Histogram regularity
* 2D FFT of convolutional filters

Notably, we also trained **BatchNorm-free ResNets** with only DFReg as the regularizer.

**Key findings:**

* DFReg matches or outperforms Dropout and BatchNorm on accuracy and stability
* It induces more interpretable and spectrally regular weight structures
* Even without L2 or BatchNorm, DFReg alone provides strong regularization

**Paper**: [https://arxiv.org/abs/2507.00101](https://arxiv.org/abs/2507.00101)  


Would love to hear feedback from the community‚Äîespecially if you're interested in global priors, regularization, or physics-inspired ML. Open to questions, critiques, or collaborations.

Thanks!",2,0.67,https://www.reddit.com/r/MachineLearning/comments/1lps1eo/p_dfreg_a_physicsinspired_regularization_method/,False,True,False
1lq66ra,hyperellipticalcurve,1751487523.0,1,/r/MachineLearning/comments/1lq66ra/d_understanding_ddim_accelerated_sampling_case/,MachineLearning,[D] Understanding DDIM : Accelerated Sampling Case,"Hello,

I have been going through DDIM paper and have some queries on how the sampling is accelerated (appendix C.1)

The authors assume that the forward can be decomposed as

[Forward decomposition](https://preview.redd.it/n0yvok1liiaf1.png?width=520&format=png&auto=webp&s=0cbce45652fccf8f10441b25238e8fd8136c7e37)

and backward

[Backward decomposition](https://preview.redd.it/f5gtpdrmiiaf1.png?width=437&format=png&auto=webp&s=2d1f597df36d5dcdab955a167e8fb588a866184d)

where tau is subsequence of timesteps \[1, T\].

First thing I want to point out is that, index ""i"" should start from 2 and from 1. (Am I right in saying this ?)

If you look into the decomposition, in the forward for the timesteps that are not in the subsequence, we are directly writing x\_{t}|x\_{0} and for the timesteps that are in subsequence we write x\_{tau\_{i-1}}|x\_{tau\_{i}},x\_{0}.

So to mimic in the reverse we write for the timesteps that are not in subsequence x\_{0}|x\_{t} and for timesteps in the subsequence we write x\_{tau\_{i-1}}|x\_{tau\_{i}}.

The above explaination looks good in intuitive sense but when I take an example and write the decomposition, the intutition doesn't come at all.

[Example](https://preview.redd.it/6zn8fux3piaf1.png?width=705&format=png&auto=webp&s=6628a8167fc4e6a7458054d9872a1caf9a338292)

Here the third term in backward p(x\_{3}|x\_{4},x\_{5}) = p(x\_{0}|x\_{3}) and fifth p(x\_{1}|x\_{2},x\_{3},x\_{4},x\_{5}) = p(x\_{0}|x\_{1}) doesn't make sense at all.

Can someone explain how does the backward decomposition work ?

Note : I don't know if this is the correct place to ask these type of questions, but I felt that other subs are not suited for this.

Thanks.",1,0.99,https://www.reddit.com/r/MachineLearning/comments/1lq66ra/d_understanding_ddim_accelerated_sampling_case/,False,True,False
1lpvn4q,_puhsu,1751462294.0,2,/r/MachineLearning/comments/1lpvn4q/p_the_tabular_dl_model_tabm_now_has_a_python/,MachineLearning,[P] The tabular DL model TabM now has a Python package,"Hi! My colleagues have recently published a Python package for [TabM](https://github.com/yandex-research/tabm) \-- a **simple and powerful DL architecture** for solving predictive tasks on **tabular data** (classification, regression, etc.).

In a nutshell, TabM efficiently imitates an ensemble of MLPs (see the image below). This basically means that TabM has the power of an ensemble, but at the same time remains practical and scalable. Among the recent highlights: üèÜ **TabM has been successfully used on Kaggle**, including the winning solutions! The package provides the PyTorch implementation of TabM, as well as PyTorch layers and functions for building custom TabM-like models.

Installation:

```
pip install tabm
```

- [Paper](https://arxiv.org/abs/2410.24210)
- [Package](https://github.com/yandex-research/tabm)
- [Colab example](https://colab.research.google.com/github/yandex-research/tabm/blob/main/example.ipynb)


[TabM model illustration](https://preview.redd.it/pl3oth89qgaf1.png?width=2432&format=png&auto=webp&s=37ed08404f3eee2e2a72dc41aa796b17ed6ae32b)",26,0.93,https://www.reddit.com/r/MachineLearning/comments/1lpvn4q/p_the_tabular_dl_model_tabm_now_has_a_python/,False,True,False
1lppyht,total-expectation,1751442625.0,14,/r/MachineLearning/comments/1lppyht/d_how_to_become_fluent_at/,MachineLearning,[D] How to become fluent at modifying/designing/improving models?,"By fluency I mean:

1. Read a paper and and without much problem implement the techniques mentioned, whether it's building something from scratch using the paper as guidance (even in the absence of code), or modifying existing models.
2. Having an idea and being able to translate that into designing new architectures or modifying existing models.
3. Improving models.

Think of people like [Phil Wang](https://github.com/lucidrains) who is very prolific at reproducing papers and or improving them. I'm very curious to know in your experience what made it ""click"" that unlocked your ability to be productive with these things. I suspect the boring answer is ""just reproduce papers, bro"", but I was hoping to learn about people's own experience/journey on this and if you guys have any specific insight/tricks that can be useful for others to know about. Like maybe you have a good workflow for this or a good pipeline that makes you 10x more productive, or you have some niche insight on designing/modifying/improving models that people don't usually talk about etc.",25,0.8,https://www.reddit.com/r/MachineLearning/comments/1lppyht/d_how_to_become_fluent_at/,False,True,False
1lppvk8,Endonium,1751442290.0,91,/r/MachineLearning/comments/1lppvk8/d_how_will_llm_companies_deal_with_cloudflares/,MachineLearning,"[D] How will LLM companies deal with CloudFlare's anti-crawler protections, now turned on by default (opt-out)?","Yesterday, [Cloudflare had announced](https://blog.cloudflare.com/content-independence-day-no-ai-crawl-without-compensation/) that their protections against AI crawler bots will be turned on by default. Website owners can choose to opt out if they wish by charging AI companies for scraping their websites (""pay per crawl"").

The era where AI companies simply recursively crawled websites with simple GET requests to extract data is over. Previously, AI companies simply disrespected robots.txt - but now that's not enough anymore.

Cloudflare's protections against crawler bots are now pretty sophisticated. They use generative AI to produce scientifically correct, but unrelated content to the website, in order to waste time and compute for the crawlers (""[AI Labyrinth](https://blog.cloudflare.com/ai-labyrinth/)"").  This content is in pages that humans are not supposed to reach, but AI crawler bots should reach - invisible links with special CSS techniques (more sophisticated than `display: none`), for instance. These nonsense pages then contain links to other nonsense pages, many of them, to keep the crawler bots wasting time reading completely unrelated pages to the site itself and ingesting content they don't need.

Every possible way to overcome this, as I see it, would significantly increase costs compared to the simple HTTP GET request recursive crawling before. It seems like AI companies would need to employ a small LLM to check if the content is related to the site or not, which could be extremely expensive if we're talking about thousands of pages or more - would they need to feed every single one of them to the small LLM to make sure if it fits and isn't nonsense?

How will this arms race progress? Will it lead to a world where only the biggest AI players can afford to gather data, or will it force the industry towards more standardized ""pay-per-crawl"" agreements?",103,0.96,https://www.reddit.com/r/MachineLearning/comments/1lppvk8/d_how_will_llm_companies_deal_with_cloudflares/,False,True,False
1lplwz3,xiikjuy,1751427790.0,9,/r/MachineLearning/comments/1lplwz3/d_will_the_relationship_between_metas_fair_and/,MachineLearning,[D] Will the relationship between Meta's FAIR and Super Intelligence Labs be like that of Google Brain and DeepMind previously?,"I really don‚Äôt get the point of setting up a new AI lab at Meta.  
Well, maybe it‚Äôs related to the semi-acquisition of Scale AI and creating a group dedicated to Alexandr Wang.  
But doesn‚Äôt the merger of Google Brain and DeepMind suggest it‚Äôs better not to split your resources in the AI war?

Also would there be possible feud out there?

",25,0.89,https://www.reddit.com/r/MachineLearning/comments/1lplwz3/d_will_the_relationship_between_metas_fair_and/,False,True,False
1lpk8ib,AutoModerator,1751422531.0,75,/r/MachineLearning/comments/1lpk8ib/d_selfpromotion_thread/,MachineLearning,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

\--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

\--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",15,0.94,https://www.reddit.com/r/MachineLearning/comments/1lpk8ib/d_selfpromotion_thread/,False,True,False
1lpjc4n,kayhai,1751419834.0,10,/r/MachineLearning/comments/1lpjc4n/d_classical_ml_prediction_preventing_data_leakage/,MachineLearning,[D] Classical ML prediction - preventing data leakage from time series process data üôè,"Anyone working in process industry and has attempted making ‚Äúsoft sensors‚Äù before?

Given a continuous industrial process with data points recorded in a historian every minute, you try to predict the outcome by applying classical ML methods such as xgboost. 

The use case demands that the model works like a soft(ware) sensor that continuously gives a numerical prediction of the output of the process. Not that this is not really a time series forecast (eg not looking into the distant future, just predicting the immediate outcome).

Question: Shuffling the data leads to data leakage because the neighbouring data points contain similar information (contains temporal information). But if shuffling is not done, the model is extremely poor / cannot generalise well.

Fellow practitioners, any suggestions for dealing with ML in that may have time series related data leakage?

Thanks in advance for any kind sharing.





",7,0.82,https://www.reddit.com/r/MachineLearning/comments/1lpjc4n/d_classical_ml_prediction_preventing_data_leakage/,False,True,False
1lphfhf,Hope999991,1751414233.0,39,/r/MachineLearning/comments/1lphfhf/d_request_for_career_advice_ml_phd_non_hot_topic/,MachineLearning,[D] Request for Career Advice ‚Äì ML PhD non hot topic,"I‚Äôm currently a PhD student in Machine Learning, working on a research topic that isn‚Äôt considered ‚Äúhot‚Äù in the current academic or industrial landscape. Despite this, I‚Äôve managed to publish as the lead author at ICML, NeurIPS. And  twice at ECML. I also have two co-authored publications at ECAI.

I‚Äôve noticed that many  PhD students in the U.S. seem to have much stronger publication records, often in trendier areas. This makes me question how competitive I really am in the current job market‚Äîespecially given the wave of layoffs and increasing demand for very specialized expertise in industry.

That said, I do have a strong foundation in core ML, Deep Learning, and LLMs (although LLMS aren‚Äôt the direct focus of my PhD research).

Given all of this, I‚Äôm trying to realistically assess: ‚Ä¢ What are my current chances of landing a demanding, high-quality job in industry or research after my PhD? ‚Ä¢ What could I do now to improve those chances? ‚Ä¢ Goal is FANNG.

I‚Äôd greatly appreciate any  feedback.

Edit: My research focuses on anomaly detection, a less trendy area compared to the current popularity of large language models and reinforcement learning.",58,0.87,https://www.reddit.com/r/MachineLearning/comments/1lphfhf/d_request_for_career_advice_ml_phd_non_hot_topic/,False,True,False
1lphavr,One-Fishing-5915,1751413883.0,0,/r/MachineLearning/comments/1lphavr/p_ml_deployment/,MachineLearning,[P] ML deployment,"  
Has anyone here deployed models on **Firebase** or **Vertex AI**? I'm looking for the best practice for a clean and cohesive deployment (we have real-time data, and I need to design a continuous retraining pipeline; in essence, the inferences will be used to update a dashboard).",1,0.67,https://www.reddit.com/r/MachineLearning/comments/1lphavr/p_ml_deployment/,False,True,False
1lp9tpp,Minute_Scholar308,1751395497.0,16,/r/MachineLearning/comments/1lp9tpp/d_subreviewing_for_neurips/,MachineLearning,[D] Subreviewing for NeurIPS,"Does your professor share their assigned papers among their lab members and ask them to sub-review for NeurIPS? I only realized after agreeing that this is actually against [the reviewer guidelines](https://neurips.cc/Conferences/2025/ReviewerGuidelines):

>Q: Can I invite a sub-reviewer to help with my reviews?

>A: No, sub-reviewers are not allowed. Conflicts of interest cannot be properly checked unless reviewers are officially in the system, and sub-reviewers would not be able to participate in the discussion, which is a critical phase of the review process.

So now I am a little bit worried I may be involved in something I perhaps shouldn't have been. On the other hand, perhaps this is one of those things in academia that people are against ""on paper"" but is actually an accepted practice? I think it seems common for professors to review papers through their students, but it seems like in most cases, they are officially appointed as a ""sub-reviewer"" (which NeurIPS doesn't allow) instead of giving their professor a review to pass as their own.

In short: Is this normal and accepted? Does it happen in your lab, too? Should I not worry about it?

**Update:** Thank you to everyone who let me know that I won't get in any trouble for sub-reviewing. That's relief to know. Although, I am wondering:

* Do guidelines + code of conduct mean nothing? Why are they in place if they won't be respected? Based on the responses, ignoring them seems not too uncommon.
* Isn't signing your name under a ghost-written review without crediting the ghostwriter a form of plagiarism? Wouldn't a student be reprimanded for plagiarism if they did this in a class? How is this different? Am I the only one who believes this still seems unethical?",16,0.95,https://www.reddit.com/r/MachineLearning/comments/1lp9tpp/d_subreviewing_for_neurips/,False,True,False
1lp8umz,pengtaoxie,1751393262.0,0,/r/MachineLearning/comments/1lp8umz/r_introducing_dreamprm_a_multimodal_llm_reasoning/,MachineLearning,"[R] Introducing DreamPRM, a multi-modal LLM reasoning method achieving first place on the MathVista leaderboard","I am excited to share our recent work, DreamPRM, a multi-modal LLM reasoning method that ranks first currently on the MathVista leaderboard.

https://preview.redd.it/54v78gz7zaaf1.png?width=1348&format=png&auto=webp&s=0084aef7727d9f02c129d8414582018fb09eedb5

https://preview.redd.it/u0c02on9zaaf1.jpg?width=1374&format=pjpg&auto=webp&s=42ab8761dcc972ed89a999dac503c9dc35e65e18

  




Reasoning has substantially improved the performance of large language models (LLMs) on complicated tasks. Central to the current reasoning studies, Process Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning steps and guide the reasoning process. However, extending PRMs to multimodal large language models (MLLMs) introduces challenges. Since multimodal reasoning covers a wider range of tasks compared to text-only scenarios, the resulting distribution shift from the training to testing sets is more severe, leading to greater generalization difficulty. Training a reliable multimodal PRM, therefore, demands large and diverse datasets to ensure sufficient coverage. However, current multimodal reasoning datasets suffer from a marked quality imbalance, which degrades PRM performance and highlights the need for an effective data selection strategy. To address the issues, we introduce DreamPRM, a domain-reweighted training framework for multimodal PRMs which employs bi-level optimization. In the lower-level optimization, DreamPRM performs fine-tuning on multiple datasets with domain weights, allowing the PRM to prioritize high-quality reasoning signals and alleviating the impact of dataset quality imbalance. In the upper-level optimization, the PRM is evaluated on a separate meta-learning dataset; this feedback updates the domain weights through an aggregation loss function, thereby improving the generalization capability of trained PRM. Extensive experiments on multiple multimodal reasoning benchmarks covering both mathematical and general reasoning show that test-time scaling with DreamPRM consistently improves the performance of state-of-the-art MLLMs. Further comparisons reveal that DreamPRM‚Äôs domain-reweighting strategy surpasses other data selection methods and yields higher accuracy gains than existing test-time scaling approaches.



Paper: [https://arxiv.org/abs/2505.20241](https://arxiv.org/abs/2505.20241)



Code: [https://github.com/coder-qicao/DreamPRM](https://github.com/coder-qicao/DreamPRM)",2,1.0,https://www.reddit.com/r/MachineLearning/comments/1lp8umz/r_introducing_dreamprm_a_multimodal_llm_reasoning/,False,True,False
1lp7jyb,Due_Confusion_8014,1751390333.0,2,/r/MachineLearning/comments/1lp7jyb/dlooking_for_hinglish_codemixed_hindienglish/,MachineLearning,[D]Looking for Hinglish (code-mixed Hindi-English) speech emotion audio datasets ‚Äî any recommendations?,"Hi everyone,
I'm working on a deep learning project involving emotion recognition from Hinglish (code-mixed Hindi-English) speech.

I‚Äôve found plenty of datasets for English (like RAVDESS, IEMOCAP) and some for Hindi (MUCS, OpenSLR), but I‚Äôm having trouble locating datasets that contain Hinglish speech, especially with emotion labels.

Do any of you know of:
Hinglish speech datasets (code-switched Hindi-English)
Emotion-labeled Hinglish audio
Open-source or research datasets that allow this type of training

If there are no public datasets, I‚Äôd also appreciate tips on how to create or augment one from scratch.
And also how can I increase it accuracy.

Thanks in advance!",1,0.67,https://www.reddit.com/r/MachineLearning/comments/1lp7jyb/dlooking_for_hinglish_codemixed_hindienglish/,False,True,False
1lp6n1r,South-Conference-395,1751388293.0,9,/r/MachineLearning/comments/1lp6n1r/d_recommended_preparation_material_for_ml/,MachineLearning,[D] Recommended preparation material for ML interviews.,"Hi everyone,

Below I am gathering some interview preparation tools for ML research positions. People who had been in the job market recently, which one would you recommend/ find more relevant? Any other resources that I might be missing?

(1) InterviewQuery:

[https://www.interviewquery.com/questions?searchQuery=&searchQuestionTag=&searchCompany=&completed=&saved=&ordering=relevancy&orderingDirection=asc&pageSize=20&page=0](https://www.interviewquery.com/questions?searchQuery=&searchQuestionTag=&searchCompany=&completed=&saved=&ordering=relevancy&orderingDirection=asc&pageSize=20&page=0)

(2) DevInterview:

[https://devinterview.io/questions/machine-learning-and-data-science](https://devinterview.io/questions/machine-learning-and-data-science)

(3) aiofferly:

[https://www.aiofferly.com/problems?page=5](https://www.aiofferly.com/problems?page=5)

(4) MAD:

[https://www.madinterview.com/ml?utm\_source=google&utm\_medium=cpc&utm\_campaign=22464693824&utm\_term=machine%20learning%20coding%20interview%20questions&utm\_content=178169327653&gclid=CjwKCAjw3f\_BBhAPEiwAaA3K5A0Rrw-8xhTQqlzVnBhrcCyyHXSwzgGvAzmJYvVye63uIOqQ7XBWhRoC6L0QAvD\_BwE&gad\_source=1&gad\_campaignid=22464693824&gbraid=0AAAAA\_Y9DohjdsVwcsLkazvDd4iJ64Tv5](https://www.madinterview.com/ml?utm_source=google&utm_medium=cpc&utm_campaign=22464693824&utm_term=machine%20learning%20coding%20interview%20questions&utm_content=178169327653&gclid=CjwKCAjw3f_BBhAPEiwAaA3K5A0Rrw-8xhTQqlzVnBhrcCyyHXSwzgGvAzmJYvVye63uIOqQ7XBWhRoC6L0QAvD_BwE&gad_source=1&gad_campaignid=22464693824&gbraid=0AAAAA_Y9DohjdsVwcsLkazvDd4iJ64Tv5)",32,0.97,https://www.reddit.com/r/MachineLearning/comments/1lp6n1r/d_recommended_preparation_material_for_ml/,False,True,False
1lp5yum,Debonargon,1751386770.0,1,/r/MachineLearning/comments/1lp5yum/d_computing_attention_scores_with_long_context/,MachineLearning,[D] Computing Attention Scores with Long Context LLMs,"I'm trying to compute the top-k tokens yielding the highest attention scores with inference frameworks such as vLLM or the plain HuggingFace transformers. The models I'm using are not big in terms of parameters (max 7B) but huge in terms of context windows (up to 1M tokens, and I'm using all of it). However, I face two problems:

1. When using vLLM, I cannot access the attention scores in any way. Am I missing something or is the feature not yet implemented?  
2. When using transformers, I need to use flash\_attention\_2 otherwise the GPU budget skyrockets to 400+ GBs when using large inputs (i have a machine with 8 A100 for a total of 320GB of VRAM). However, when using flash\_attention\_2 the output attention scores are all None, and the only way to solve this seems to use an eager attention implementation, which makes it unfeasible in terms of GPU requirements.

Is someone facing a similar problem? How do you compute the attention scores for such large inputs?",3,1.0,https://www.reddit.com/r/MachineLearning/comments/1lp5yum/d_computing_attention_scores_with_long_context/,False,True,False
1lp5c22,Benjo118,1751385325.0,4,/r/MachineLearning/comments/1lp5c22/d_looking_for_aipowered_smart_crop_library/,MachineLearning,[D] Looking for AI-powered smart crop library - smartcrop.py isn't enough,"https://preview.redd.it/r1w6xzdnbaaf1.png?width=1492&format=png&auto=webp&s=5ab883dcc781312bb6014b9daf1d9295dfc66030

Hey everyone!

I'm currently using¬†smartcrop.py (github.com/smartcrop/smartcrop.py)¬†for image cropping in Python, but it's pretty basic. It only detects edges and color gradients, not actual objects.

For example, if I have a photo with a coffee cup, I want it to recognize the cup as the main subject and crop around it. But smartcrop just finds areas with most edges/contrast, which often misses the actual focal point.

**Looking for:**

* Python library that uses AI/ML for object-aware cropping
* Can identify main subjects (people, objects, etc.)
* More modern than just edge detection

Any recommendations for libraries that actually understand what's in the image?

Thanks!",0,0.36,https://www.reddit.com/r/MachineLearning/comments/1lp5c22/d_looking_for_aipowered_smart_crop_library/,False,True,False
1lp4lk8,Luuigi,1751383626.0,0,/r/MachineLearning/comments/1lp4lk8/r_transition_matching_scalable_and_flexible/,MachineLearning,[R] Transition Matching: Scalable and Flexible Generative Modeling,Imo a silent banger by Meta - generalizing diffusion and flow matching into transition matching which can be used in a unified causal generation process.,6,0.88,https://www.arxiv.org/abs/2506.23589,False,False,False
1lp3wla,AutoModerator,1751382053.0,4,/r/MachineLearning/comments/1lp3wla/d_simple_questions_thread/,MachineLearning,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",1,0.66,https://www.reddit.com/r/MachineLearning/comments/1lp3wla/d_simple_questions_thread/,False,True,False
1lozfbp,Avienir,1751370063.0,22,/r/MachineLearning/comments/1lozfbp/p_i_created_an_opensource_tool_to_analyze_15m/,MachineLearning,[P] I created an open-source tool to analyze 1.5M medical AI papers on PubMed,"Hey everyone,

I've been working on a personal project to understand how AI is actually being used in medical research (not just the hype), and thought some of you might find the results interesting.

After analyzing nearly 1.5 million PubMed papers that use AI methods, I found some intersting results:

* **Classical ML still dominates**: Despite all the deep learning hype, traditional algorithms like logistic regression and random forests account for 88.1% of all medical AI research
* **Algorithm preferences by medical condition**: Different health problems gravitate toward specific algorithms 
* **Transformer takeover timeline**: You can see the exact point (around 2022) when transformers overtook LSTMs in medical research

I built an interactive dashboard where you can:

* Search by medical condition to see which algorithms researchers are using
* Track how algorithm usage has evolved over time
* See the distribution across classical ML, deep learning, and LLMs

One of the trickiest parts was filtering out false positives (like ""GAN"" meaning Giant Axonal Neuropathy vs. Generative Adversarial Network).

The tool is completely free, hosted on Hugging Face Spaces, and open-source. I'm not trying to monetize this - just thought it might be useful for researchers or anyone interested in healthcare AI trends.

Happy to answer any questions or hear suggestions for improving it!",120,0.94,https://www.reddit.com/gallery/1lozfbp,False,False,False
1louv9e,Defiant_Strike823,1751352502.0,5,/r/MachineLearning/comments/1louv9e/p_how_do_i_detect_whether_a_person_is_looking_at/,MachineLearning,[P] How do I detect whether a person is looking at the screen using OpenCV?,"Hi guys, I'm sort of a noob at Computer Vision and I came across a project wherein I have to detect whether or not a person is looking at the screen through a live stream. Can someone please guide me on how to do that?

The existing solutions I've seen all either use MediaPipe's FaceMesh (which seems to have been depreciated) or use complex deep learning models. I would like to avoid the deep learning CNN approach because that would make things very complicated for me atp. I will do that in the future, but for now, is there any way I can do this using only OpenCV and Mediapipe?

  
PS. Sorry for the wrong tag mods",0,0.4,https://www.reddit.com/r/MachineLearning/comments/1louv9e/p_how_do_i_detect_whether_a_person_is_looking_at/,False,True,False
1lotkac,LastAd3056,1751347501.0,15,/r/MachineLearning/comments/1lotkac/d_any_path_for_a_mid_careermid_aged_mle_to_do_ml/,MachineLearning,[D] Any path for a mid career/mid aged MLE to do ML research in the industry,"I've seen some flavor of questions here about whether they should do a PhD to join a research lab. I have a slightly different question. I did a non-CS PhD almost a decade ago, failed to get a faculty position after a bunch of postdocs and then meandered through FANG jobs, first in DS and then in MLE. I did some applied research in my last job, but more stats heavy than ML. But through a bunch of layoffs and restructuring, currently I am in a more traditional MLE role, think recommendation systems, A/B tests, move metrics...

But at my heart, I still want to do research. I've dabbled with writing a single author paper in on the top ML conferences in my own time, but its kinda hard, with job, family etc.. Even if I do manage to pull it off, will the one off Neurips paper (lets say) help me get an entry card to a more research-y ML job, like a Research Scientist/ Research Engineer in a ML lab? I am competing with ML PhDs with multiple papers, networks etc.

I also think that I don't have a lot of time, most of my friends have moved on to management after a decade of IC roles, and thats sort of the traditional path. But part of me is still holding on and wants to give it a shot and see if I can break into research this late, without an ML PhD. I know I will be much more fulfilled as a research scientist, compared to a regular SWE/M job,. I am currently trying to use my weekends and nights to write a single author paper to submit to one of the top conferences. Worst case I get rejected.

Some thoughts in my mind:  
(1) I have also thought of writing workshop papers, which are easier to get accepted, but I doubt they have a similar value in the RS job market.  
(2) Research Engineer will likely be easier than Research Scientist. But how should I strategize for this?

I'd be grateful if I get thoughts on how I should strategize a move. Feel free to also tell me its impossible, and I should cut my losses and move on.",46,0.94,https://www.reddit.com/r/MachineLearning/comments/1lotkac/d_any_path_for_a_mid_careermid_aged_mle_to_do_ml/,False,True,False
1los6wj,iwiwijp,1751342705.0,1,/r/MachineLearning/comments/1los6wj/r_inferencetime_scaling_and_collective/,MachineLearning,[R] Inference-Time Scaling and Collective Intelligence for Frontier AI,"TL;DR: our AB-MCTS lets multiple frontier models work together at inference time, outperforming each model running alone on the ARC-AGI-2 benchmark.

Our new inference-time scaling algorithm enables collective intelligence for AI by allowing multiple frontier models (like Gemini 2.5 Pro, o4-mini, DeepSeek-R1-0528) to cooperate.

Inspired by the power of human collective intelligence, where the greatest achievements arise from the collaboration of diverse minds, we believe the same principle applies to AI. Individual frontier models like ChatGPT, Gemini, and DeepSeek are remarkably advanced, each possessing unique strengths and biases stemming from their training, which we view as valuable resources for collective problem-solving.

AB-MCTS (Adaptive Branching Monte Carlo Tree Search) harnesses these individualities, allowing multiple models to cooperate and engage in effective trial-and-error, solving challenging problems for any single AI. Our initial results on the ARC-AGI-2 benchmark are promising, with AB-MCTS combining o4-mini + Gemini-2.5-Pro + R1-0528, current frontier AI models, significantly outperforming individual models by a substantial margin.

This research builds on our 2024 work on evolutionary model merging, shifting focus from ‚Äúmixing to create‚Äù to ‚Äúmixing to use‚Äù existing, powerful AIs. At Sakana AI, we remain committed to pioneering novel AI systems by applying nature-inspired principles such as evolution and collective intelligence. We believe this work represents a step toward a future where AI systems collaboratively tackle complex challenges, much like a team of human experts, unlocking new problem-solving capabilities and moving beyond single-model limitations.

Blog: https://sakana.ai/ab-mcts

Paper: https://arxiv.org/abs/2503.04412

Algorithm: https://github.com/SakanaAI/treequest

ARC-AGI Experiments: https://github.com/SakanaAI/ab-mcts-arc2

If you have any questions, please ask them below or feel free to get in touch, any discussion is more than welcome :)",20,0.92,https://www.reddit.com/r/MachineLearning/comments/1los6wj/r_inferencetime_scaling_and_collective/,False,True,False
1loqe5e,AutoModerator,1751337018.0,15,/r/MachineLearning/comments/1loqe5e/d_monthly_whos_hiring_and_who_wants_to_be_hired/,MachineLearning,[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&#x200B;

Please remember that this community is geared towards those with experience.",18,0.86,https://www.reddit.com/r/MachineLearning/comments/1loqe5e/d_monthly_whos_hiring_and_who_wants_to_be_hired/,False,True,False
1loo8yl,chrisfathead1,1751330623.0,49,/r/MachineLearning/comments/1loo8yl/d_how_far_are_we_from_llm_pattern_recognition/,MachineLearning,[D] How far are we from LLM pattern recognition being as good as designed ML models,"LLMs are getting better quickly. It seems like every time a new release comes out, they have moved faster than I anticipated. 

Are they great at abstract code, integrating systems, etc? Not yet. But I do find that they are excellent at data processing tasks and machine learning code, especially for someone who knows and understands those concepts and is able to understand when the LLM has given a wrong or inefficient answer.

I think that one day, LLMs will be good enough to perform as well as a ML model that was designed using traditional processes. For example, I had to create a model that predicted call outcomes in a call center. It took me months to get the data exactly like I needed it from the system and identify the best transformation, combinations of features, and model architecture to optimize the performance.

I wonder how soon I'll be able to feed 50k records to an LLM, and tell it look at these records and teach yourself how to predict X. Then I'll give you 10k records and I want to see how accurate your predictions are and it will perform as well or better than the model I spent months working on. 

Again I have no doubt that we'll get to this point some day, I'm just wondering if you all think that's gonna happen in 2 years or 20. Or 50? ",32,0.75,https://www.reddit.com/r/MachineLearning/comments/1loo8yl/d_how_far_are_we_from_llm_pattern_recognition/,False,True,False
1lon6sx,sbuswell,1751327586.0,0,/r/MachineLearning/comments/1lon6sx/p_ive_built_a_spec_for_llmtollm_comms_by/,MachineLearning,[P] I've built a spec for LLM-to-LLM comms by combining semantic patterns with structured syntax,"Firstly, total disclaimer. About 4 months ago, I knew very little about LLMs, so I am one of those people who went down the rabbit hole and started chatting with AI. But, I'm a chap who does a lot of pattern recognition in the way I work (I can write music for orchestras without reading it) so just sort of tugged on those pattern strings and I think I've found something that's pretty effective (well it has been for me anyway).

Long story short, I noticed that all LLMs seem to have their training data steeped in Greek Mythology. So I decided to see if you could use that shared knowledge as compression. Add into that syntax that all LLMs understand (:: for clear key-value assignments, ‚Üí for causality and progression, etc) and I've combined these two layers to create a DSL that's more token-efficient but also richer and more logically sound.

This isn't a library you need to install; it's just a spec. Any LLM I've tested it on can understand it out of the box. I've documented everything (the full syntax, semantics, philosophy, and benchmarks) on GitHub.

I'm sharing this because I think it's a genuinely useful technique, and I'd love to get your feedback to help improve it. Or even someone tell me it already exists and I'll use the proper version!

Link to the repo:¬†[https://github.com/elevanaltd/octave](https://github.com/elevanaltd/octave)",0,0.46,https://www.reddit.com/r/MachineLearning/comments/1lon6sx/p_ive_built_a_spec_for_llmtollm_comms_by/,False,True,False
1lollc0,EducationalCicada,1751323299.0,0,/r/MachineLearning/comments/1lollc0/r_bigbench_extra_hard/,MachineLearning,[R] BIG-Bench Extra Hard,,10,0.92,https://arxiv.org/abs/2502.19187,False,False,False
1lolkda,Personal_Click_6502,1751323230.0,0,/r/MachineLearning/comments/1lolkda/r_interpreting_large_language_models_personality/,MachineLearning,[R] Interpreting Large Language Models' Personality through Critical Event Analysis,"Excited to share our new work, ""Supernova Event Dataset: Interpreting Large Language Models' Personality through Critical Event Analysis"" accepted at the Actionable Interpretability Workshop @ ICML 2025.

Introducing the Supernova Event Dataset

We present a new benchmark built from real-world Wikipedia articles, including biographies, historical milestones, global news, and scientific discoveries (including articles from Google Deep Research). This dataset introduces a novel task: critical event analysis for interpreting the behavioral pattern, or ‚Äúpersonality‚Äù of LLMs.

Rather than looking inside the model (activations, traces), we ask a separate LLM to judge what events are most critical, and use this external perspective to decode the model‚Äôs values and reasoning traits.

Some early insights:

Orca2 tends to prioritize emotional and interpersonal events.

Phi-4 and Qwen2.5 focus on strategic milestones.

In scientific discovery, o3 highlights causal breakthroughs, Gemini 2.5 Pro favors methodological innovations, and Claude Sonnet 3.7 emphasizes conceptual clarity.

While these are early findings (still without human evaluation), the diversity in critical event patterns is striking. We believe assigning LLMs ""personalities"" could make them more relatable and trustworthy, enabling smoother human-AI collaboration, especially in domains like scientific discovery.

Paper: [arxiv.org/abs/2506.12189](https://l.facebook.com/l.php?u=http%3A%2F%2Farxiv.org%2Fabs%2F2506.12189%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeYgHhUQQ5lNulNDLnSeulM64ECl3ls5tJCNkMC1_EPhqEHk1uqWYnEdBsu7g_aem_fW6BPSFPpkV4xELlQOiODA&h=AT0m3E04nqeA-MMtev7Ouz9OW3PeW5A_y6V9zj9dqy3WrwMXLOeTXVO9EzTXtMey6tWCwh1vUB5rS0lMPeoFEdjHj6BecO9zq9__xYIrQwGC6nhRT8BEE50RlAm9OuXWhk_HCtHu&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)

Twitter: [https://x.com/Pranav\_AL/status/1939681069554655382](https://x.com/Pranav_AL/status/1939681069554655382)

Webpage: [http://supernova-event.ai](https://l.facebook.com/l.php?u=http%3A%2F%2Fsupernova-event.ai%2F%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeZQWpZz3uLtupNGIkmvzqpDbbG9u83w7Tv9ifY4Rjvct6zdjy8E4yg6NNDTM_aem_SgZlEI9ACWixaHK8TQWBPw&h=AT3tIyMocFJs9OJneCmpLWwlEjH3FE0RtckM5RYggzkKKuFMG5AIIScnpzGDhh7YqxBOSxpleqKt0hXYWIiiG_t3RoKtnoI1vlHkHUCsMhHlTmKcoQvqBSUnk8rLDci3doz0NpFV&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)

Demo: [supernova-event.ai/#your-story](https://l.facebook.com/l.php?u=http%3A%2F%2Fsupernova-event.ai%2F%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeyxn4V9DBpYEvuLM2LwLuOOn-4Ewox9O267lf8zV1R8tbbLTm3vS1lw0zzqM_aem_9qDTC7HNLTFGRKq_iFqgBg%23your-story&h=AT1SxDBmgr_G-a-c-D9g4JyMlgs5bD-liDDwNRTZlIP_5CBvyl8meA3pwaNKABRXLqvHcemnEJv-sWcf3oan2b3FSWZ7H_yD3Y8mdhI5Ze6mVNULKBXQTdFqgrLNUynlNQjPjP2_&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)

Code: [https://github.com/pranavAL/Supernova-Event-Dataset](https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2FpranavAL%2FSupernova-Event-Dataset%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeyxn4V9DBpYEvuLM2LwLuOOn-4Ewox9O267lf8zV1R8tbbLTm3vS1lw0zzqM_aem_9qDTC7HNLTFGRKq_iFqgBg&h=AT3sNwJZLhvA9OG4GbkPvPxvAXxtZr9drQAj1Rp-4MCOHOOVbjH1epznhz08JAKypffQNwntIbz6TWzMTDmKVgXvDw7y6Yrg6Bcijqgxco34C_R4ivMwgS83oW5i2QnMFFQmQuVt&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)

We're working toward scaling this into a real-world product, and we're currently seeking the right resources and support to take it further. If you're interested in what we're building and see potential for impact, we‚Äôd love to hear from you. Reach us at [hello@supernova-event.ai](mailto:hello@supernova-event.ai) ; we're open to conversations, collaborations, and any form of support that can help push this idea forward.

https://preview.redd.it/uugbpxw075af1.png?width=1200&format=png&auto=webp&s=ccbde6f1ace6140ff2ca838ffb0e60522759dc70",2,0.75,https://www.reddit.com/r/MachineLearning/comments/1lolkda/r_interpreting_large_language_models_personality/,False,True,False
1loi25g,sheckyCS,1751314720.0,2,/r/MachineLearning/comments/1loi25g/d_is_this_phd_in_llm_editing_a_good_idea/,MachineLearning,[D] Is this PhD in LLM editing a good idea?,"Hello everyone, this is my first time posting here, and I wanted to get some opinions on the phd position I applied to.

So I am studying ml in France and I have a chance to do a PhD in the topic of LLM knowledge locating and editing. One paper that talks about this is the ROME (Rank One Model Editting -¬†[https://arxiv.org/abs/2202.05262](https://arxiv.org/abs/2202.05262))

Basically, I would work on the internals of LLMs, analysing where exactly the knowledge for a certain fact is stored, and how can it be edited out. So messing around the directly with the components such as the attention and MLP weights.

For me personally, I like the idea of going inside the LLMs, instead of just inferencing/training and using them as some black boxes.

And I suppose that this would qualify me for jobs of actually creating LLMs (I do not expect to end up in OpenAI) but also make me more qualified for standard LLM usage jobs.

Any opinion or comment would be appriciated!",0,0.29,https://www.reddit.com/r/MachineLearning/comments/1loi25g/d_is_this_phd_in_llm_editing_a_good_idea/,False,True,False
1lohh1u,Able-Entertainment78,1751313372.0,15,/r/MachineLearning/comments/1lohh1u/d_should_we_petition_for_requiring_reviewers_to/,MachineLearning,[D] Should we petition for requiring reviewers to state conditions for improving scores?,"I‚Äôve been thinking about how opaque and inconsistent peer reviews can be, especially in top ML conferences. What if we made it a requirement for reviewers to explicitly state the conditions under which they would raise their scores? For example, ‚ÄúIf the authors add experiments on XYZ‚Äù or ‚ÄúIf the theoretical claim is proven under ABC setup.‚Äù

Then, area chairs (ACs) could judge whether those conditions were reasonably met in the rebuttal and updated submission, rather than leaving it entirely to the whims of reviewers who may not revisit the paper properly.

Honestly, I suspect many reviewers don‚Äôt even know what exactly would change their mind.

As an added bonus, ACs could also provide a first-pass summary of the reviews and state what conditions they themselves would consider sufficient for recommending acceptance.

What do you think? Could this improve transparency and accountability in the review process?",12,0.63,https://www.reddit.com/r/MachineLearning/comments/1lohh1u/d_should_we_petition_for_requiring_reviewers_to/,False,True,False
1logp0w,venturepulse,1751311513.0,0,/r/MachineLearning/comments/1logp0w/d_looking_for_a_web_annotation_tool_with_chrome/,MachineLearning,[D] Looking for a web annotation tool (with Chrome extension) for labeling live websites,"I'm building a dataset for a knowledge extraction model and need to label structured data from thousands of live websites. Ideally, I'm looking for a tool that:

\- Provides a Chrome extension to label live HTML elements on real websites

\- Can open sites one by one in the browser from a task queue

\- Saves each annotation along with a snapshot or DOM state of the page

\- Supports exporting annotations for later review with screenshots

I‚Äôm considering building a custom tool for this, but would prefer to avoid that since it would distract from the core research. Does anyone know an existing tool that supports doing what Im doing?",1,1.0,https://www.reddit.com/r/MachineLearning/comments/1logp0w/d_looking_for_a_web_annotation_tool_with_chrome/,False,True,False
1lobiuc,theunnecessarythings,1751299694.0,0,/r/MachineLearning/comments/1lobiuc/p_i_wrote_ptx_kernels_for_llmc/,MachineLearning,[P] I wrote PTX Kernels for LLM.c,"Hey everyone,

I‚Äôve been meaning to dive into NVIDIA PTX for a while, and I learn best by doing‚Äîso I decided to hand-write PTX kernels for an \*\*inference-only\*\* version of Andrej Karpathy‚Äôs \[LLM.c\](https://github.com/karpathy/llama.cpp) project. To my surprise, not only did everything actually work, but I also saw about a \*\*10% performance improvement\*\* in inference compared to the equivalent CUDA implementation (or at least, that‚Äôs what my benchmarks showed).

You can check out the code here:

üëâ \[https://github.com/theunnecessarythings/llm-ptx\](https://github.com/theunnecessarythings/llm-ptx)

Along the way, I documented my entire experience in a multi-part blog series, including line-by-line explanations of how I translated CUDA into PTX:

1. \*\*Part I: Introduction & Residual Kernel\*\*\[https://sreeraj.in/blog/llm-ptx-01\](https://sreeraj.in/blog/llm-ptx-01)
2. \*\*Part II: The GELU Kernel\*\*\[https://sreeraj.in/blog/llm-ptx-02\](https://sreeraj.in/blog/llm-ptx-02)
3. \*\*Part III: The Encoder Kernel\*\*\[https://sreeraj.in/blog/llm-ptx-03\](https://sreeraj.in/blog/llm-ptx-03)
4. \*\*Part IV: The LayerNorm Kernel\*\*\[https://sreeraj.in/blog/llm-ptx-04\](https://sreeraj.in/blog/llm-ptx-04)
5. \*\*Part V: The Softmax Kernel\*\*\[https://sreeraj.in/blog/llm-ptx-05\](https://sreeraj.in/blog/llm-ptx-05)
6. \*\*Part VI: The Attention Kernel\*\*\[https://sreeraj.in/blog/llm-ptx-06\](https://sreeraj.in/blog/llm-ptx-06)
7. \*\*Part VII: The MatMul Kernel & Performance Results\*\*\[https://sreeraj.in/blog/llm-ptx-07\](https://sreeraj.in/blog/llm-ptx-07)

\---

\*\*What‚Äôs Next?\*\*

This is my first time writing PTX, so there may still be bugs or missed optimization opportunities. I‚Äôd love feedback or fixes from anyone who‚Äôs more experienced with low-level GPU programming!

\---

\*\*Also posted on X:\*\*

\[https://x.com/notHumanIam/status/1939402092071780610\](https://x.com/notHumanIam/status/1939402092071780610)

Looking forward to your thoughts and suggestions! üòÑ",3,0.71,https://www.reddit.com/r/MachineLearning/comments/1lobiuc/p_i_wrote_ptx_kernels_for_llmc/,False,True,False
1lo9xuu,FluidRangerRed,1751296045.0,9,/r/MachineLearning/comments/1lo9xuu/r_has_anyone_actually_gone_through_an_ai/,MachineLearning,[R] Has anyone actually gone through an AI readiness assessment with a vendor or consultant? Worth it or just more buzzwords?,"I'm kind of wondering about these AI readiness assessments everyone's talking about. Like, you see vendors and consultants pushing them, and honestly, I'm a bit skeptical. I can't help but feel it might just be a lot of buzzwords without real substance.  
  
  
Has anyone actually gone through one of these with a third party, maybe a consultant or a specific vendor, was it actually worth the time and money you put into it and did you get genuinely practical insights that helped your business move forward, or was it just a fancy report that basically says 'you need more AI' without telling you how?  
  
I'm really curious to hear real experiences here, good or bad, before potentially diving into something that might just be another passing trend in the tech world. What did you learn, and what was the actual outcome?",0,0.33,https://www.reddit.com/r/MachineLearning/comments/1lo9xuu/r_has_anyone_actually_gone_through_an_ai/,False,True,False
1lnq3zt,luigiusai,1751232450.0,0,/r/MachineLearning/comments/1lnq3zt/p_chromatic_language_models_clm_a_paradigm_for/,MachineLearning,[P] Chromatic Language Models (CLM): A Paradigm for Native Visual Communication in Artificial Intelligence,"**Abstract**

[**https://zenodo.org/records/15769766**](https://zenodo.org/records/15769766)

Modern AI models, in particular Large Language Models (LLMs) and Computer Vision models, operate in fundamentally distinct data domains: text and pixels. The interaction between these models requires expensive and complex translation and embedding processes. This work introduces a new paradigm,¬†¬†**Chromatic Language Models (CLMs)**¬†, designed to eliminate this discontinuity. Building on the principles of visual semantic coding established in¬†¬†**Usai ColorZip**¬†¬†(Usai, 2025a) and validated by the¬†¬†**Usai ChromoChess**¬†application ¬†(Usai, 2025b), CLMs are language models that operate natively on a chromatic domain. We propose an encoder-decoder architecture in which an AI agent learns to ""read"" and ""write"" complex information directly as images, treating pixels as semantic tokens. This approach not only unifies language and vision, but creates an intrinsically compressed, secure, and efficient form of AI-native communication, paving the way for a new generation of multimodal intelligent agents.

**1. Introduction**

The evolution of artificial intelligence is characterized by increasing specialization. On the one hand, Large Language Models (LLMs) have demonstrated an unprecedented ability to understand and generate human language. On the other hand, computer vision models, such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), excel at interpreting visual data. However, a fundamental ""modal gap"" separates these two worlds. An LLM does not ""see"" images and a ViT does not ""read"" text; both rely on intermediate embedding layers to translate information from one domain to the other.

This paper addresses a radical question: what if we could close this gap by transforming language itself into a natively visual format? Instead of teaching a model to translate between text and pixels, could we create a model that ""thinks"" directly in pixels?

We propose the architecture of¬†¬†**Chromatic Language Models (CLM)**¬†, intelligent agents that use a chromatic representation of language for each stage of their cognitive process: input, reasoning, and output. This proposal builds directly on the technological and conceptual foundations of our previous work, which demonstrated the feasibility of such a representation.

**2. Fundamental Works and Context**

Our proposal is not born in a vacuum, but is the natural evolution of two previous researches that established the feasibility of visual semantic coding.

**2.1. Usai ColorZip: Semantic Text Encoding**  
In our work ""Usai ColorZip: A Hybrid System for Semantic Text Encoding and Compression via HTML Colors"" (Usai, 2025a), we introduced a lossless system for mapping lexical units (words) to unique color codes. We demonstrated that this transformation is not only an act of encoding, but also an effective data compression mechanism when combined with lossless image formats such as PNG. The key to the system is its hybrid architecture, capable of handling both a large dictionary of known words and any unknown word via a color escape protocol.¬†¬†**Usai ColorZip created the ""vocabulary"" and ""syntax"" of this new language.**

**2.2. Usai ChromoChess: Proof of Concept in a Complex Domain**  
Later, in ""Usai ChromoChess: Visual Representation and Compression of Chess Games"" (Usai, 2025b), we applied this philosophy to a formal and complex domain. By transforming chess games from PGN notation to 8x8 pixel movies, we demonstrated that a sequence of logical states can be represented as a visual data stream, compact and ideal for analysis by vision models.¬†¬†**Usai ChromoChess provided proof that entire logical-temporal processes can be efficiently encoded in this chromatic language.**

These two works constitute the necessary prerequisite for the next step: no longer just encoding and decoding data, but creating an intelligence that¬†¬†*uses*¬†¬†this language as its primary means of communication and reasoning.

**3. Architecture of the Chromatic Language Model (CLM)**

A CLM is an AI model designed for an end-to-end communication cycle in the color domain. Its architecture is based on an encoder-decoder model.

**3.1. The Principle: Visual Tokenization**  
The fundamental unit of a CLM is not a word or subword, but a¬†¬†**colored pixel**¬†. Each color, defined in the ColorZip dictionary, is a discrete semantic token. An input ""text"" (e.g. a question) is provided to the model as a ColorZip image (a tensor \[H x W x C\], where H, W are the dimensions and C is the RGB representation of the color).

**3.2. The Encoder: The Chromatic Reader**  
The encoder has the task of ""reading"" the input image and understanding its meaning. An ideal architecture for this purpose is a¬†¬†**Vision Transformer (ViT)**¬†.

1. The ColorZip image is divided into a grid of patches (which can correspond to single pixels/words or small groups).
2. These patches are projected into a vector space and processed through self-attention mechanisms.
3. The encoder's output is a context vector (or sequence of vectors), an abstract, latent mathematical representation of the semantic meaning of the input image.

**\[Figure 1: Encoder-Decoder architecture of a CLM. The Encoder (ViT) processes the input image. Its semantic output conditions the Decoder (Transformer), which generates a new image pixel by pixel (color by color).\]**

**3.3. The Decoder: The Color Writer**  
The decoder has the task of taking the context vector and generating a response, also in the form of a ColorZip image.

1. A standard Transformer architecture is used as the decoder.
2. The process is autoregressive: the model generates one pixel (color) at a time.
3. The crucial difference lies in its output layer: instead of softmaxing a vocabulary of tens of thousands of words, CLM softmaxes¬†¬†**the color dictionary**¬†. The model predicts the most likely color for the next pixel, given its understanding of the query and the colors generated so far.
4. The process ends when the model generates the special color EOT\_COLOR defined in Usai ColorZip.

**4. Implications: Towards AI-Native Communication**

The adoption of CLMs does not represent an incremental improvement, but a paradigm shift with profound implications.

* **Computational Efficiency:**¬†¬†The overhead of constant conversion between text and numeric representations is eliminated. AI operates on a data format that is closer to its mathematical nature.
* **Secure and Compressed Communication:**¬†¬†Conversations between CLM agents would be opaque images to an unauthorized observer (without the dictionary) and, as demonstrated by Usai ColorZip, highly compressed. This is ideal for low-bandwidth or stealth communications.
* **True Multimodality:**¬†¬†A CLM that ""speaks"" the language of pixels is intrinsically closer to understanding real images. The boundary between language and vision becomes blurry, facilitating the creation of truly multimodal models capable of reasoning fluidly about text and images without internal barriers.
* **New Application Scenarios:**¬†¬†Possibilities open up for AI agents that communicate steganographically through image sharing platforms, or for the development of specialized hardware (color processors) optimized for these data flows.

**5. Challenges and Future Work**

The road to fully functional CLMs presents several challenges: creating large-scale training datasets (text corpora parallel to their ColorZip representations), analyzing their computational costs compared to traditional LLMs, and exploring the interpretability of these models. Future work will focus on developing a prototype CLM and training it on a medium-sized corpus to empirically validate its ability to ""converse"" chromatically.

**6. Conclusion**

This paper introduced Chromatic Language Models (CLMs), a new type of intelligent agent that reads, reasons, and writes directly in a color-based visual language. Building on the solid foundation of¬†¬†**Usai ColorZip**¬†semantic coding ¬†and the application validation of¬†¬†**Usai ChromoChess**¬†, we outlined a viable architecture that unifies the domains of language and vision. CLMs are not simply a new model, but a proposal for a¬†¬†**new form of AI-native communication**¬†: a language for machines, spoken by machines.

**7. References**

* Usai, L. (2025a).¬†¬†*Usai ColorZip: A Hybrid System for Semantic Text Encoding and Compression via HTML Colors*¬†. Zenodo.¬†¬†[https://doi.org/10.5281/zenodo.15701109](https://www.google.com/url?sa=E&q=https%3A%2F%2Fdoi.org%2F10.5281%2Fzenodo.15701109)
* Usai, L. (2025b).¬†¬†*Usai ChromoChess: Visual Representation and Compression of Chess Games via Temporal Encoding Usai ColorZip*¬†. Zenodo.¬†¬†[https://doi.org/10.5281/zenodo.15701822](https://www.google.com/url?sa=E&q=https%3A%2F%2Fdoi.org%2F10.5281%2Fzenodo.15701822)",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1lnq3zt/p_chromatic_language_models_clm_a_paradigm_for/,False,True,False
1lnzka6,Apprehensive_Gap1236,1751261468.0,10,/r/MachineLearning/comments/1lnzka6/ddesigning_neural_networks_for_timedependent/,MachineLearning,[D]Designing Neural Networks for Time-Dependent Tasks: Is it common to separate Static Feature Extraction and Dynamic Feature Capture?,"Hi everyone,

I'm working on neural network training, especially for tasks that involve time-series data or time-dependent phenomena. I'm trying to understand the common design patterns for such networks.

My current understanding is that for time-dependent tasks, a neural network architecture might often be divided into two main parts:

1. **Static Feature Extraction:** This part focuses on learning features from individual time steps (or samples) independently. Architectures like CNNs (Convolutional Neural Networks) or MLPs (Multi-Layer Perceptrons) could be used here to extract high-level semantic information from each individual snapshot of data.
2. **Dynamic Feature Capture:** This part then processes the sequence of these extracted static features to understand their temporal evolution. Models such as Transformers or LSTMs (Long Short-Term Memory networks) would be suitable for learning these temporal dependencies.

My rationale for this two-part approach is that it could offer better interpretability for problem analysis in the future. By separating these concerns, I believe it would be easier to use visualization techniques (like PCA, t-SNE, UMAP for the static features) or post-hoc explainability tools to determine if the issue lies in: \* the *identification of features* at each time step (static part), or \* the *understanding of how these features evolve over time* (dynamic part).

Given this perspective, I'm curious to hear from the community: **Is it generally recommended to adopt such a modular architecture for training neural networks on tasks with high time-dependency? What are your thoughts, experiences, or alternative approaches?**

Any insights or discussion would be greatly appreciated!",3,1.0,https://www.reddit.com/r/MachineLearning/comments/1lnzka6/ddesigning_neural_networks_for_timedependent/,False,True,False
1lo7i8e,MycologistEconomy909,1751290037.0,6,/r/MachineLearning/comments/1lo7i8e/p_a_neural_network_library_from_scratch_in_c/,MachineLearning,[P] A Neural Network Library from scratch in C++,"Hey r/cpp and r/MachineLearning!

You may have guessed from the title, but why make one when we have TensorFlow, PyTorch that provide the simplicity of Python and the speeds of C and C++ ?   
I say well why not.   

1. *The Learning* - With AI boom taking over and people going crazy on vibe coding, ML and DS jobs are focusing on how deeply people understand the basics and internal working of what they are making. So while many tutorials focusing on API's, MCP's and what not, here I am peeling the layers (literal layers of a neural network) and the process taught me more than any tutorial could.

2. *The Fun* - I love C++! Building this from scratch (even with procrastination detours üòÖ) was really exciting. (Who doesn't love crying over why the whole model isn't working only to know you subtracted the losses instead of adding. And of course the feeling of betrayal when you ask chatGPT to add comments to the code due to your laziness and it changes the code smirking while you notice it too late and then have had to debug the whole library searching where it went wrong) 

Also, it is never a bad idea (mostly) to know what happens behind the scenes of the code you are gonna write. And what better thing to understand the basics than implement them by yourself. (Though this may not be a good idea always considering my bad habit of delving too deep into small topics and going into a rabbit hole wholly different than what i was supposed to be doing).   

### Current Features:
* Dense layers + activations (ReLU, SELU, Sigmoid)
* SGD optimizer with momentum/LR scheduling
* CSV/binary dataset handling (though the binary loader may need some fixes)
* Batch training 

Where I got the idea ? Well I was supposed to start learning to code with PyTorch but then I thought how does this even work. I just looked at a small part of the documentation and thought let's try coding this and this led to me successfully spending about 2 weeks on this (with lots of procrastination in between). Will it be a good project ? I don't know. Did I enjoy it ? Damn well I did.   
  
Well it's still not complete and may have a few bugs and I plan to keep it aside for now and improve it bit by bit later on. But I thought sharing this may encourage me somewhat and get my lazy self to do some work without procrastinating. 

You can check out the full source code and documentation on GitHub:
https://github.com/CuriosityKilledTheCache/Deep-in-scratch_Maths_the_catch

P.S : If you have any recommendations, do tell though it may be a passing reply comment for you, it may help me very much for correcting mistakes I may make again in the future.",3,0.67,https://www.reddit.com/r/MachineLearning/comments/1lo7i8e/p_a_neural_network_library_from_scratch_in_c/,False,True,False
1lo2x9f,Ok-Percentage3926,1751274923.0,2,/r/MachineLearning/comments/1lo2x9f/d_what_postprocessing_tools_work_well_with/,MachineLearning,[D] What post-processing tools work well with Tesseract for financial documents?,"Hi all,

I‚Äôm using Tesseract OCR to extract text from scanned financial documents like payslips and tax returns. The raw output is messy, and I need to clean it up and pull key fields like YTD income, net pay, and tables.

What post-processing tools or Python libraries can help:

* Extract key-value fields
* Parse tables
* Match labels to values
* Clean and structure OCR output

Prefer offline tools (for privacy), but open to anything that works well.",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1lo2x9f/d_what_postprocessing_tools_work_well_with/,False,True,False
1lo18f5,Emotional_Alps_8529,1751267986.0,1,/r/MachineLearning/comments/1lo18f5/d_did_i_find_a_bug_in_the_compvis_stable/,MachineLearning,[D] Did I find a bug in the CompVis Stable Diffusion Github Repo?,"I was building my own diffusion model walking myself through CompVis' StableDiffusion repo when I came upon this strange code when reading through the U-Net implementation:  
[https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/model.py#L83](https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/model.py#L83)

Specifically the implementation of Model on line 216.

In the current implementation, each downsampling level appends two skip connections of shape (B, ch, H, W) from the ResBlocks, followed by a third skip from the downsampled output, which incorrectly has shape (B, ch, H//2, W//2). During upsampling, all three skips are concatenated in sequence without compensating for this resolution mismatch, as the upsampling layer is applied after all three ResNet blocks. This causes the first skip in each upsampling level to be at the wrong spatial resolution, breaking alignment with h during torch.cat. When I implemented my U-Net I had to change

hs.append(self.down\[i\_level\].downsample(hs\[-1\])) (line 340)

to downsample AFTER caching it in hs, the skip-connection cache.",2,0.6,https://www.reddit.com/r/MachineLearning/comments/1lo18f5/d_did_i_find_a_bug_in_the_compvis_stable/,False,True,False
1lo06qh,Scriptterr,1751263855.0,0,/r/MachineLearning/comments/1lo06qh/d_proper_way_to_calculate_inference_time/,MachineLearning,[D] Proper way to calculate inference time,"Hi all,  
Can anyone tell me how I should calculate inference time (case/sec) for medical images? SegMamba paper reports inference time as case/sec.  
I have 2 queries in this case.  
First, should inference time (case/sec) include the time of every operation after model predictions?  
Secondly, because of sliding window inference, it is highly likely that the inference time for each case might be higher. What is the right way?",0,0.4,https://www.reddit.com/r/MachineLearning/comments/1lo06qh/d_proper_way_to_calculate_inference_time/,False,True,False
1lnzffu,moschles,1751260963.0,6,/r/MachineLearning/comments/1lnzffu/d_has_anyone_ever_gained_unrestricted_access_to/,MachineLearning,[D] Has anyone ever gained unrestricted access to an LLM for the purposes of research?,"I have attempted several rounds of research with LLMs that are available to the public (Grok, ChatGPT, and Copilot).   (an experiment involving 20-questions capability, and several experiments where the models talk back and forth to each other).   It has become clear that the public web portals are useless for this type of experiment.   The public-facing models are heavily tuned to be helpful assistants that create lists and formatted sections with headers.  

How would someone go about getting access to a raw model for use in a university ?",0,0.25,https://www.reddit.com/r/MachineLearning/comments/1lnzffu/d_has_anyone_ever_gained_unrestricted_access_to/,False,True,False
1lnt9za,FallMindless3563,1751241096.0,0,/r/MachineLearning/comments/1lnt9za/p_code_for_finetuning_flux1dev_explained_step_by/,MachineLearning,[P] Code for Fine-Tuning FLUX.1-dev Explained Step by Step With Comments,"Hey all,

I was having trouble finding a simple, self contained example of Fine-Tuning FLUX.1-dev with explanation of all the components, so I decided to create one. 

There were examples in HuggingFace diffusers [examples/dreambooth/train\_dreambooth\_lora\_flux.py](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora_flux.py) (which didn't work out of the gate for me) and [AI-Toolkit](https://github.com/ostris/ai-toolkit) which worked well, but had way too many nested if-statements to fully see what was going on under the hood. I took inspiration from both, but cleaned up the code so it was easier to read and worked out of the gate.

The code was written in a [Marimo Notebook](https://marimo.io/) which I'm enjoying lately for developing simple training scripts. 

Feel free to download the code here: [https://www.oxen.ai/ox/Fine-Tune-FLUX/file/main/train.py](https://www.oxen.ai/ox/Fine-Tune-FLUX/file/main/train.py)

Or follow along with a blog version: [https://www.oxen.ai/blog/how-to-fine-tune-a-flux-1-dev-lora-with-code-step-by-step](https://www.oxen.ai/blog/how-to-fine-tune-a-flux-1-dev-lora-with-code-step-by-step)

Hope you enjoy!",15,1.0,https://www.reddit.com/r/MachineLearning/comments/1lnt9za/p_code_for_finetuning_flux1dev_explained_step_by/,False,True,False
1lnsv8n,AgeOfEmpires4AOE4,1751239909.0,3,/r/MachineLearning/comments/1lnsv8n/p_ai_learns_to_play_xmen_vs_street_fighter/,MachineLearning,[P] AI Learns to Play X-Men vs Street Fighter | Reinforcement Learning with ...,"I trained an AI agent to play *X-Men vs Street Fighter* using reinforcement learning, leveraging the **Stable-Retro** framework (built on top of Gym Retro). The agent interacts with the game through frame observations and discrete action spaces mapped to the arcade controls.

The training process involved reward shaping based on health bars, damage dealt, and round wins. The environment was wrapped with preprocessing (grayscale, resizing, frame stacking) and curriculum logic to improve generalization across multiple characters and enemy types.

The video shows the progression from random movement to more competent fighting strategies, including corner traps and defensive spacing. The learning curve is steep due to the complexity of the fighting game mechanics, but the agent starts to show patterns similar to human play.

Frameworks used: PyTorch, Stable-Baselines3, OpenCV, and a modified Gym Retro environment with custom reward functions and action discretization.

I'd love to hear feedback from others working on RL in dynamic multi-agent environments or applying deep RL to retro/arcade-style games. Happy to share code or discuss implementation details!

[https://github.com/paulo101977/AI-X-men-Vs-Street-Fighter-Trainning](https://github.com/paulo101977/AI-X-men-Vs-Street-Fighter-Trainning)",8,0.79,https://youtube.com/watch?v=Eej8UwfRY7g&si=tDXz4U-dvdVCO-xo,False,False,False
1lnsph5,AdministrativeRub484,1751239455.0,15,/r/MachineLearning/comments/1lnsph5/d_how_should_i_respond_to_reviewers_when_my_model/,MachineLearning,[D] How should I respond to reviewers when my model is worse than much larger models?,"I got a review asking to compare my submission paper with more recent models. The models were not even out 3 months before the submission so by ACL rules I should not have to compare them with my model because it is contemporary.

Nevertheless I have ran comparisons and my model is much much worse... Why? I'm using a model doing the same thing but 32x smaller, used almost 1/10 of the data they used, etc... I am severely resource constrained and cannot compete in terms of scale, but I still think that my paper makes an important contribution that if we were to match the other models scale we would get better results.

What should I do? Should I report results that show other models are better and risk the reviewers lower their scores? I kinda just want to explain the authors that the scale is completely different and other factors make it a very unfair comparison, but they might just not care...

I have a 2.5 average score and really wanted to try to raise it to make it at least into findings, but I honestly don't know how to defend against not having as many resources as top labs/unis...",54,0.92,https://www.reddit.com/r/MachineLearning/comments/1lnsph5/d_how_should_i_respond_to_reviewers_when_my_model/,False,True,False
1lnoqmm,AdministrativeRub484,1751229015.0,31,/r/MachineLearning/comments/1lnoqmm/d_review_clearly_used_an_llm_should_i_report_it/,MachineLearning,"[D] Review clearly used an LLM, should I report it to AC?","This review gave me 1.5 in ACL and calls GRPO Generalized Reward Preference Optimization, which is what ChatGPT thinks GRPO is... It also says my work is the first one to use GRPO in my domain while it is not (and we talk about this in the introduction) and says we are missing some specific evaluations, which are present in the appendix and says we did not justify a claim well enough, which is very well known in my domain but when asking ChatGPT about it it says it does not know about it...

It feels like the reviewer just wanted to give me a bad review and asked an LLM to write a poor review. He clearly did not even check the output because literally everyone knows GRPO stands for Group Relative Policy Optimization...

Other than reply to the reviewer while pretending I did not know he/she used ChatGPT, what else can I do? My other reviews were both 3, so I really want to get rid of this review if possible...",187,0.92,https://www.reddit.com/r/MachineLearning/comments/1lnoqmm/d_review_clearly_used_an_llm_should_i_report_it/,False,True,False
1lnnts8,outcasted_chira,1751226709.0,3,/r/MachineLearning/comments/1lnnts8/p_decentralized_training_and_inferencing_platform/,MachineLearning,[p] decentralized training and inferencing platform,"

Working on a project that lets you connect to a hundred thousand plus devicing, and use their compute in a decentralized manner. This allows people to train large models, without their own compute. Or even use large models for free as it is hosted on a very large number of device

incase this sounds fascinating then let me know if you would like to use it. Also incase anyone else working on this or worked on this then tell that too

",1,0.6,https://www.reddit.com/r/MachineLearning/comments/1lnnts8/p_decentralized_training_and_inferencing_platform/,False,True,False
1lnnp8u,PuffThePed,1751226392.0,6,/r/MachineLearning/comments/1lnnp8u/p_need_to_train_a_model_that_can_detect_which_2d/,MachineLearning,[P] Need to train a model that can detect which 2D image a smartphone camera is looking at (out of about 1000).,"Hey everyone. I'm an AR developer and studio owner, I'm looking for someone to help us with a client project that requires training a machine learning model. Specifically I want a model that can tell me which pin (out of about 1000) a smartphone camera is looking at. Assuming there is only one pin in view, and it's fairly close to the camera. I don't need to find it's location in the image, just need to know which pin I'm looking at. 

Here is a sample of a few pins:
https://imgur.com/a/iTdWhbw

They are all more or less that size. I would love some direction and even training code, happy to pay for your time. DM me for more info.",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1lnnp8u/p_need_to_train_a_model_that_can_detect_which_2d/,False,True,False
1lnllcs,Adorable-Win581,1751221159.0,0,/r/MachineLearning/comments/1lnllcs/gameplay_to_design_dna_r/,MachineLearning,Gameplay to Design DNA? [R],"We are developing a new machine learning algorithm that can design DNA by watching gameplay. The way humans play is different from computers, and that signal might be useful for searching DNA subspaces.

We will be writing a research paper on this new technique, and are shooting for Nature Biotechnology! DM if you‚Äôd like to see the preprint.

We have a Tetris clone that runs a lightweight ML model on device, and actually designs DNA as you play. Here we are looking for DNA that activates PPARG::RXRA, involved in metabolism, and deactivates NFKB1, a key regulator of inflammation and immune. These DNA may promise to advance diabetes research.

Long term, we would like to have a library of games, even first person shooters, that design DNA in the background. Sound crazy? Maybe. But we think it might work.

Help us advance this research by collecting your anonymous play data!

https://exonic.ai/games/tilestack",0,0.42,https://i.redd.it/ffmu2n0mrw9f1.jpeg,False,False,False
1lnisl5,Dangerous-Hat1402,1751214344.0,13,/r/MachineLearning/comments/1lnisl5/d_is_openreview_down/,MachineLearning,[D] Is OpenReview Down?,"It shows ""There are currently no¬†active venues."" I am trying to complete the NIPS review at the last minute. Will they extend the deadline? ",18,0.95,https://www.reddit.com/r/MachineLearning/comments/1lnisl5/d_is_openreview_down/,False,True,False
1lnfd3d,automatonv1,1751205585.0,3,/r/MachineLearning/comments/1lnfd3d/p_i_built_a_new_python_package_to_reorder_ocr/,MachineLearning,[P] I built a new python package to reorder OCR bounding boxes even with folds and distortions,"**What My Project Does**

`bbox-align`¬†is a Python library that reorders bounding boxes generated by OCR engines into logical lines and correct reading order for downstream document processing tasks. Even when documents have folds, irregular spacing, or distortions

**Target Audience**

Folks that build document processing applications need to reorder and rearrange bounding boxes. This open-source library is intended to do that.

This library is not intended for serious production applications since it's very new and NOT battle-tested. People who are willing to beta test and build new projects on top of this are welcome to try and provide feedbacks and suggestions.

**Comparison**

Currently, OCR engines do a good job of reordering bounding boxes they generate. But sometimes they don't group them into correct logical/reading order. They perhaps use clustering algorithms to group bounding boxes that are close to each other, which may be incorrect.

I use coordinate geometry to determine if two bounding boxes are inline or not.

Github -¬†[https://github.com/doctor-entropy/bbox-align](https://github.com/doctor-entropy/bbox-align)

PyPI -¬†[https://pypi.org/project/bbox-align/](https://pypi.org/project/bbox-align/)",3,0.62,https://www.reddit.com/r/MachineLearning/comments/1lnfd3d/p_i_built_a_new_python_package_to_reorder_ocr/,False,True,False
1lnem9e,jsonathan,1751203482.0,26,/r/MachineLearning/comments/1lnem9e/p_i_built_a_python_debugger_that_you_can_talk_to/,MachineLearning,[P] I built a Python debugger that you can talk to,,199,0.91,https://i.redd.it/42lxfn6xav9f1.gif,False,False,False
1lne9e0,StartledWatermelon,1751202449.0,7,/r/MachineLearning/comments/1lne9e0/d_position_machine_learning_conferences_should/,MachineLearning,"[D] Position: Machine Learning Conferences Should
Establish a ‚ÄúRefutations and Critiques‚Äù Track","**Abstract:**

>Science progresses by iteratively advancing and correcting humanity's understanding of the world. In machine learning (ML) research, rapid advancements have led to an explosion of publications, but have also led to misleading, incorrect, flawed or perhaps even fraudulent studies being accepted and sometimes highlighted at ML conferences due to the fallibility of peer review. While such mistakes are understandable, ML conferences do not offer robust processes to help the field systematically correct when such errors are¬†made. This¬†position paper argues that ML conferences should establish a dedicated ""Refutations and Critiques"" (R & C) Track. This R & C Track would provide a high-profile, reputable platform to support vital research that critically challenges prior research, thereby fostering a dynamic self-correcting research ecosystem. We discuss key considerations including track design, review principles, potential pitfalls, and provide an illustrative example submission concerning a recent ICLR 2025 Oral. We conclude that ML conferences should create official, reputable mechanisms to help ML research self-correct.

(I'm not affilated with any of the authors. But I believe this position paper deserves more visibility)",109,0.97,https://arxiv.org/pdf/2506.19882,False,False,False
1lnbdzx,IntelligentAd6407,1751192569.0,0,/r/MachineLearning/comments/1lnbdzx/p_simple_marl_environment_to_train_quadrotor/,MachineLearning,[P] Simple MARL environment to train quadrotor swarms in UE4,"In the past, I was asking for help here on Reddit to build some environment for drone swarms training. I think it might be helpful to someone, so I'll link the results here. I obviously suspect that the results are obsolete (end of 2023), but let me know if you find it useful and leave a star if you'd like!

[Multi-agent Deep Reinforcement Learning for Drone Swarms using UE4, AirSim, Stable-Baselines3, PettingZoo, SuperSuit](https://github.com/Lauqz/Drone-Swarm-RL-airsim-sb3)",4,0.84,https://i.redd.it/2g8e7o9leu9f1.png,False,False,False
1lnayg0,atsju,1751190871.0,26,/r/MachineLearning/comments/1lnayg0/pupdateopen_source_astronomy_project_need_bestfit/,MachineLearning,[P][Update]Open source astronomy project: need best-fit circle advice,,15,0.8,https://www.reddit.com/gallery/1lnayg0,False,False,False
1ln9sbq,ResolveTimely1570,1751186134.0,37,/r/MachineLearning/comments/1ln9sbq/d_phd_worth_it_to_do_rl_research/,MachineLearning,[D] PhD worth it to do RL research?,"Posting anonymously for this one. I know questions like these get posted quite often, but I wanted to offer a bit of context about my own situation and what I'm into.

I'm currently a rising college sophomore working in Sergey Levine's lab (RL & robotics) at Berkeley, and I have to decide whether I want to pursue a standard industry internship (e.g. SWE) for the 2026 summer or continue doing research in the lab. I really like research work, easily the most enjoyable ""work"" I've done in my life, but I can't deny that money is still a factor (esp. due to particular family reasons). I see three sort of options down the line from here (listed with their pros and cons

A) continue doing research in my time in undergrad, and shoot a difficult shot towards getting into a reputable PhD program

* Pros:
   * very streamlined process to become an industry research scientist given that I go to a good enough program & work hard enough
   * \^\^ this is the most optimal job option for me: 10/10 job, the best I could ever want. I love research man
   * researchers generally seem like the most sufferable group out of most tech archetypes (seen way too many elon-musk wannabes in normal SWE)
* Cons:
   * 5-6 years of a PhD: not that it's going to be unenjoyable, but it delays my life ""progress"" a lot
   * getting into top ML PhD programs is really tough nowadays. I'm lucky to have started sort of early (working on my first first-author pub over this summer) but I know people with great publication history (probably better than I'll earn) that didn't get admitted anywhere
   * \^\^ it seems as though if I don't get into a PhD program, all the research I would have published would be a sunk cost (not useful for much besides just.. ML research)
   * comp: is it much better than normal SWE or MLE? though I love the work a lot, I would hope that it's just a *biiit* better to justify the extra 6 years I put in for a PhD
   * if ML hype & investment dies out, I'll be on the forefront of getting laid off, esp if RL doesn't find a way to scale soon enough

B) continue doing research, but balance it out with some SWE or similar experience and go for an MLE or research engineer type of role

* Pros:
   * immediately high comp out just out of my degree if I can land one of these roles, without needing to spend all that time on a degree
   * correct me if I'm wrong, but RE and some parts of MLE aren't that far off from research scientist work, esp. if working with researchers at a frontier lab
   * seems to be less workload, better WLB?
   * seems to be more stable (easier transition to SWE) if ML hype dies out
* Cons:
   * less interesting work. not that I hate it, but it's like an 8/10 compared to the 10/10 work that I would consider to be RS
   * I'm unsure if my publications & research history would help at all for these roles. from what I've heard, research and industry experience are almost orthogonal and they simply don't care about publications (please correct me if I'm wrong!)
   * don't own the intellectual rights to my own work :(

C) research is useless, just do SWE, ML research is a hellhole

* \^\^ this is more so a last resort rather than something I would ever want to do, but if you have any reason that this is a good option, please do tell me why",84,0.9,https://www.reddit.com/r/MachineLearning/comments/1ln9sbq/d_phd_worth_it_to_do_rl_research/,False,True,False
1ln9bn4,pastor_pilao,1751184227.0,2,/r/MachineLearning/comments/1ln9bn4/d_loss_function_for_fine_tuning_in_a_list_of/,MachineLearning,[D] Loss function for fine tuning in a list of rankings,"I am not ultra updated with the literature on LLMs and I habe a probably which I guess is very similar to what everyone who works with document ranking has to deal with, so I would just like to know if there is some canonic obvious solution for my problem. 

I want to fine tune an LLM (if it makes any difference it is a multi modal one). My model receives an video as the input and outputs a description.

During fine-tuning, I want to generate N captions for a single video (let's say 5 captions for simplicity sake), and I have an ""oracle"" that will sort those 5 responses in order of preference. 

I want a loss function that will fine tune my model in a way that will make the probability of ""better"" answers, according to my oracle ranking, higher. Any loss function for that?

Ideally, off-polify (but on policy woukd be fine as well). It can't be DPO for example because it only consider 2 possible answer. It coukd be PPO I guess if I convert the ranking to a number, but I would rather not have to keep a reward model, and PPO is not really a rank loss function
",4,0.76,https://www.reddit.com/r/MachineLearning/comments/1ln9bn4/d_loss_function_for_fine_tuning_in_a_list_of/,False,True,False
1ln8wu8,Gigawrench,1751182566.0,19,/r/MachineLearning/comments/1ln8wu8/d_samformer_a_lesson_in_reading_benchmarks/,MachineLearning,[D] SAMformer -- a lesson in reading benchmarks carefully,"**UPDATE**: A first author of the SAMformer paper [commented below](https://www.reddit.com/r/MachineLearning/comments/1ln8wu8/comment/n3lce9d/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) explaining their rationale for ommitting the linear models in their benchmark. In short, when running their multi-seed evaluations they found that TSMixer was the most competitive non-transformer benchmark and didn't see the value in also including the worse-performing linear models. In their evaluations the linear models were comparable to FedFormer / AutoFormer. Given this extra context, the title of the post still rings true but the thrust of the original post is simply misinformed. Credit should be given to the SAMformer authors for establishing more useful benchmarks (reporting results averaged across multiple seeds) than earlier papers in the field.

\-----------------------------------------------------------------------------------------------------------

For those not in the time-series forecasting space, it has seen some interesting developments in the last few years as researchers have tried to translate the success of transformer-based models in the language domain, to the forecasting domain. There was incremental progress in long-term timeseries forecasting with the likes of [Informer](https://arxiv.org/abs/2012.07436), [Autoformer](https://arxiv.org/abs/2106.13008), and [Fedformer](https://arxiv.org/pdf/2201.12740), among others, however the 2022 [paper ](https://arxiv.org/abs/2205.13504)""Are Transformers Effective for Time Series Forecasting?"" (Zeng et al.) called into question how much progress these models had actually made.

Zeng et al. introduced three self-proclaimed ""embarassingly simple"" linear models -- each of which are variations on a **single dense layer** mapping the input values to the output values -- which outperformed all of the above state-of-the-art transformer models on their benchmarks (see the image below for a subset of results):

[Linear and Transformers MSE Benchmarks](https://preview.redd.it/l504gwc2ft9f1.png?width=998&format=png&auto=webp&s=b231af865a3e1ea1ee48ad64dd77f6cad4f011eb)

This brings us to the paper [SAMformer](https://arxiv.org/abs/2402.10198) which applies a ""sharpness-aware minimisation"" approach to training a simplified version of the vanilla transformer encoder. This works very well, generally outperforming the aforementioned transformer models, as well as competetive non-transformer state-of-the-art models (TSMixer and PatchTST), on all the same benchmarks. Notably absent in the benchmarks however, are the linear models from Zeng et al. You can see the results from the SAMformer paper below (all results are **MSE**):

[SAMFormer MSE Benchmarks](https://preview.redd.it/gbraaexeht9f1.png?width=169&format=png&auto=webp&s=bb4f1dff2015ff492e1bd07b58beb546f9b24a5f)

On Electricity, Exchange, and Weather the simple linear models outperform SAMformer for all horizons, and it is only on the Traffic dataset where SAMformer achieves lower MSE. The omission of the linear models in the final benchmarks is doubly surprising given the SAMformer authors specifically mention the results from Zeng et al. in their introduction:

""\[Zeng et al.\] recently found that linear networks can be on par or better than transformers for the forecasting task, questioning their practical utility. This curious finding serves as a starting point for our work.""

To be clear, I think the ideas introduced in the SAMformer paper are valuable and I think it would be fair to classify SAMformer as a ""state-of-the-art"" model. However, I am curious of the rationale for excluding the linear models in the benchmarks given they were originally introduced to call into question the effectiveness of transformers in the time-series forecasting domain.

Tl;dr: Always put your skeptical glasses on when reviewing benchmarks as there may be some highly competetive models omitted from the analysis.",86,0.95,https://www.reddit.com/r/MachineLearning/comments/1ln8wu8/d_samformer_a_lesson_in_reading_benchmarks/,False,True,False
1ln8q6d,Apprehensive_Gap1236,1751181818.0,9,/r/MachineLearning/comments/1ln8q6d/d_transfer_learning_vs_endtoend_training/,MachineLearning,[D] Transfer learning v.s. end-to-end training,"Hello everyone,

I'm an ADAS engineer and not an AI major, nor did I graduate with an AI-related thesis, but my current work requires me to start utilizing AI technologies.

My tasks currently involve Behavioral Cloning, Contrastive Learning, and Data Visualization Analysis. For model validation, I use metrics such as loss curve, Accuracy, Recall, and F1 Score to evaluate performance on the training, validation, and test sets. So far, I've managed to achieve results that align with some theoretical expectations.

My current model architecture is relatively simple: it consists of an Encoder for static feature extraction (implemented with an MLP - Multi-Layer Perceptron), coupled with a Policy Head for dynamic feature capturing (GRU - Gated Recurrent Unit combined with a Linear layer and Softmax activation).

Question on Transfer Learning and End-to-End Training Strategies  
I have some questions regarding the application strategies for Transfer Learning and End-to-End Learning. My main concern isn't about specific training issues, but rather, I'd like to ask for your insights on the best practices when training neural networks:

Direct End-to-End Training: Would you recommend training end-to-end directly, either when starting with a completely new network or when the model hits a training bottleneck?

Staged Training Strategy: Alternatively, would you suggest separating the Encoder and Policy Head? For instance, initially using Contrastive Learning to stabilize the Encoder, and then performing Transfer Learning to train the Policy Head?

Flexible Adjustment Strategy: Or would you advise starting directly with end-to-end training, and if issues arise later, then disassembling the components to use Contrastive Learning or Data Visualization Analysis to adjust the Encoder, or to identify if the problem lies with the Dynamic Feature Capturing Policy Head?

I've actually tried all these approaches myself and generally feel that it depends on the specific situation. However, since my internal colleagues and I have differing opinions, I'd appreciate hearing from all experienced professionals here.

Thanks for your help!",0,0.43,https://www.reddit.com/r/MachineLearning/comments/1ln8q6d/d_transfer_learning_vs_endtoend_training/,False,True,False
1ln7c28,AdditionalWeb107,1751176379.0,10,/r/MachineLearning/comments/1ln7c28/r_archrouter_the_fastest_llm_routing_model/,MachineLearning,[R] Arch-Router - The fastest LLM routing model designed to align to usage preferences,"Excited to share Arch-Router, our research and model for LLM routing. Routing to the right LLM is still an elusive problem, riddled with nuance and blindspots. For example:

‚ÄúEmbedding-based‚Äù (or simple intent-classifier) routers sound good on paper‚Äîlabel each prompt via embeddings as ‚Äúsupport,‚Äù ‚ÄúSQL,‚Äù ‚Äúmath,‚Äù then hand it to the matching model‚Äîbut real chats don‚Äôt stay in their lanes. Users bounce between topics, task boundaries blur, and any new feature means retraining the classifier. The result is brittle routing that can‚Äôt keep up with multi-turn conversations or fast-moving product scopes.

Performance-based routers swing the other way, picking models by benchmark or cost curves. They rack up points on MMLU or MT-Bench yet miss the human tests that matter in production: ‚ÄúWill Legal accept this clause?‚Äù ‚ÄúDoes our support tone still feel right?‚Äù Because these decisions are subjective and domain-specific, benchmark-driven black-box routers often send the wrong model when it counts.

**Arch-Router skips both pitfalls by routing on** ***preferences you write in plain language.*** Drop rules like ‚Äúcontract clauses ‚Üí GPT-4o‚Äù or ‚Äúquick travel tips ‚Üí Gemini-Flash,‚Äù and our 1.5B auto-regressive router model maps prompt along with the context to your routing policies‚Äîno retraining, no sprawling rules that are encoded in if/else statements. Co-designed with Twilio and Atlassian, it adapts to intent drift, lets you swap in new models with a one-liner, and keeps routing logic in sync with the way you actually judge quality.

**Specs**

* **Tiny footprint** ‚Äì 1.5 B params ‚Üí runs on one modern GPU (or CPU while you play).
* **Plug-n-play** ‚Äì points at any mix of LLM endpoints; adding models needs *zero* retraining.
* **SOTA query-to-policy matching** ‚Äì beats bigger closed models on conversational datasets.
* **Cost / latency smart** ‚Äì push heavy stuff to premium models, everyday queries to the fast ones.

Exclusively available in Arch (the AI-native proxy for agents): [https://github.com/katanemo/archgw](https://github.com/katanemo/archgw)  
üîó Model + code: [https://huggingface.co/katanemo/Arch-Router-1.5B](https://huggingface.co/katanemo/Arch-Router-1.5B)  
üìÑ Paper / longer read: [https://arxiv.org/abs/2506.16655](https://arxiv.org/abs/2506.16655)",23,0.83,https://i.redd.it/29y5c8vb2t9f1.png,False,False,False
1ln6feb,South-Conference-395,1751172975.0,30,/r/MachineLearning/comments/1ln6feb/d_emnlp_2025_discussion_period/,MachineLearning,[D] EMNLP 2025 Discussion Period,"Hi everyone,

How is the discussion period going for you? Have you heard back from any of your reviewers?

For those who are reviewing: can the reviewers change their scores after Jul2? Can they reply to the authors after Jul 2?

  
thanks!



 ",13,1.0,https://www.reddit.com/r/MachineLearning/comments/1ln6feb/d_emnlp_2025_discussion_period/,False,True,False
1ln4omn,Acanthisitta-Sea,1751166854.0,70,/r/MachineLearning/comments/1ln4omn/r_lstm_or_transformer_as_malware_packer/,MachineLearning,"[R] LSTM or Transformer as ""malware packer""","An alternative approach to EvilModel is packing an entire program‚Äôs code into a neural network by intentionally exploiting the overfitting phenomenon. [I developed a prototype](https://github.com/piotrmaciejbednarski/lstm-memorizer) using PyTorch and an LSTM network, which is intensively trained on a single source file until it fully memorizes its contents. Prolonged training turns the network‚Äôs weights into a data container that can later be reconstructed.

The effectiveness of this technique was confirmed by generating code identical to the original, verified through SHA-256 checksum comparisons. Similar results can also be achieved using other models, such as GRU or Decoder-Only Transformers, showcasing the flexibility of this approach.

The advantage of this type of packer lies in the absence of typical behavioral patterns that could be recognized by traditional antivirus systems. Instead of conventional encryption and decryption operations, the ‚Äúunpacking‚Äù process occurs as part of the neural network‚Äôs normal inference.

[https://bednarskiwsieci.pl/en/blog/lstm-or-transformer-as-malware-packer/](https://bednarskiwsieci.pl/en/blog/lstm-or-transformer-as-malware-packer/)",340,0.96,https://i.redd.it/yhy773d1as9f1.png,False,False,False
1ln19e6,MoilC8,1751155819.0,22,/r/MachineLearning/comments/1ln19e6/d_how_do_you_deal_with_messy_github_repo_that/,MachineLearning,[D] How do you deal with messy github repo that doesnt work,"you see a recent paper with great results, they share their github repo (awesome), but then... it just doesn‚Äôt work. broken env, missing files, zero docs, and you end up spending hours digging through messy code just to make it run.

then Cursor came in, and it helps! helps a lot! its not lazy (like me) so its diving deep into code and fix stuff, but still, it can take me 30 mints of ping-pong prompting.

how do you tackle this problem?  
diving deep into code is a nice time killer, when you want to run 10 different GitHub repos, you want to move fast.. so, **how do you move fast?**",47,0.77,https://www.reddit.com/r/MachineLearning/comments/1ln19e6/d_how_do_you_deal_with_messy_github_repo_that/,False,True,False
1ln0oka,Smart_Scratch7985,1751154077.0,5,/r/MachineLearning/comments/1ln0oka/d_curious_about_invitation_as_icml_reviewer/,MachineLearning,[D] Curious about invitation as ICML reviewer,"I recently helped coauthor a paper submitted to ICML's AI4Math, and I was really surprised when I got email asking to serve as a reviewer (I'm an undergrad and this was my first paper). I probably won't accept since I'm not qualified, but I was curious about how this even happened, are reviewers just randomly selected?",14,0.89,https://www.reddit.com/r/MachineLearning/comments/1ln0oka/d_curious_about_invitation_as_icml_reviewer/,False,True,False
1lmx6f9,pmv143,1751144220.0,13,/r/MachineLearning/comments/1lmx6f9/d_nvidia_acquires_centml_what_does_this_mean_for/,MachineLearning,[D] NVIDIA acquires CentML ‚Äî what does this mean for inference infra?,"CentML, the startup focused on compiler/runtime optimization for AI inference, was just acquired by NVIDIA. Their work centered on making single-model inference faster and cheaper , via batching, quantization (AWQ/GPTQ), kernel fusion, etc.

This feels like a strong signal: inference infra is no longer just a supporting layer. NVIDIA is clearly moving to own both the hardware and the software that controls inference efficiency.

That said, CentML tackled one piece of the puzzle , mostly within-model optimization. The messier problems : cold starts, multi-model orchestration, and efficient GPU sharing , are still wide open. We‚Äôre working on some of those challenges ourselves (e.g., InferX is focused on runtime-level orchestration and snapshotting to reduce cold start latency on shared GPUs).

Curious how others see this playing out. Are we headed for a vertically integrated stack (hardware + compiler + serving), or is there still space for modular, open runtime layers?
",66,0.93,https://www.reddit.com/r/MachineLearning/comments/1lmx6f9/d_nvidia_acquires_centml_what_does_this_mean_for/,False,True,False
1lmwoe0,Single-Condition-887,1751142873.0,2,/r/MachineLearning/comments/1lmwoe0/p_live_face_swap_and_voice_cloning/,MachineLearning,[P] Live Face Swap and Voice Cloning,"Hey guys! Just wanted to share a little repo I put together that live face swaps and voice clones a reference person. This is done through zero shot conversion, so one image and a 15 second audio of the person is all that is needed for the live cloning. I reached around 18 fps with only a one second delay with a RTX 3090. Let me know what you guys think! Checkout the demo in the Github Repo for a sneak peak. Link:¬†[https://github.com/luispark6/DoppleDanger](https://github.com/luispark6/DoppleDanger)",4,0.83,https://www.reddit.com/r/MachineLearning/comments/1lmwoe0/p_live_face_swap_and_voice_cloning/,False,True,False
1lmw5pg,Lesterpaintstheworld,1751141472.0,3,/r/MachineLearning/comments/1lmw5pg/r_systematic_evaluation_of_computational/,MachineLearning,[R] Systematic Evaluation of Computational Consciousness Correlates in Economic AI Agents: Applying Butlin et al. (2023) Framework to La Serenissima,"**TL;DR**: We applied the peer-reviewed Butlin et al. consciousness indicator framework to 119 AI agents in an economic simulation. Results: 2.39/3.0 average across 14 indicators, with inter-rater reliability Œ∫=0.76.¬†**Not claiming sentience**¬†\- measuring computational correlates. Open source, reproducible methodology.

# Before You Downvote

I know this community's healthy skepticism about consciousness claims. This isn't a ""ChatGPT told me it's conscious"" post. We're measuring specific computational properties identified by neuroscientists, not making philosophical claims about sentience.

# What We Actually Did

1. **Applied existing framework**: Used Butlin et al.'s 14 consciousness indicators from neuroscience
2. **Measurable behaviors**: 90.92% identity persistence, 4.06x money velocity, r=0.0177 trust-economic correlation
3. **Independent validation**: Gemini 2.5 Pro scored blindly (Œ∫=0.76 agreement)
4. **Open source**: Full code at¬†[github.com/Universal-Basic-Compute/serenissima](http://github.com/Universal-Basic-Compute/serenissima)
5. **Reproducible**: API endpoints for real-time data access

# Key Findings

**What Economic Constraints Create:**

* Agency scores 3.0/3.0 through actual resource competition
* Embodiment 3.0/3.0 via spatial constraints and travel times
* Belief updating 3.0/3.0 from market feedback loops

**vs Baseline LLM**: Same model scores 1.11/3.0 in chatbot mode vs 2.39/3.0 in economic simulation

**Critical Distinctions:**

* Measuring computational correlates, NOT phenomenal consciousness
* 81.4% of properties emerge from system dynamics, not design
* Fine-tuning removes assistant constraints, doesn't add consciousness claims
* Economic scaffolding creates conditions for emergence

# Addressing the Obvious Criticisms

**""It's just the LLM""**: We compared same model with/without economic constraints. 115% improvement in indicators when embedded in consequences.

**""You're anthropomorphizing""**: We measure specific computational properties with operational definitions. No feelings involved.

**""Fine-tuning creates illusion""**: Fine-tuning removes ""as an AI, I cannot..."" responses. Behavioral indicators emerge through economic actions, not self-reports.

**""Not peer reviewed""**: Framework is peer-reviewed (Butlin et al.). Our application awaits review - hence posting here first.

# Why This Matters (Scientifically)

1. **Empirical methodology**¬†for consciousness studies in AI
2. **Economic constraints**¬†as novel approach to agency/embodiment
3. **Multi-agent dynamics**¬†show collective consciousness properties
4. **Reproducible protocol**¬†others can apply/critique

# What We're NOT Claiming

* NOT claiming sentience or phenomenal consciousness
* NOT saying ""we solved consciousness""
* NOT suggesting moral rights for AI

# Technical Details

* 119 AI citizens in Renaissance Venice simulation
* Closed economy (no money creation)
* Sequential processing on single RTX 3090 Ti
* deepseek-r1-0528-qwen3-8b model
* Full documentation in paper

# Questions for the Community

1. What additional controls would strengthen this methodology?
2. What would constitute sufficient evidence for computational consciousness correlates?
3. How can we better distinguish emergence from sophisticated mimicry?

[Paper](https://static1.squarespace.com/static/66ac1ddd5938225d25c6412b/t/685d5049b2ec3e7a3c1aa2d9/1750945865828/Consciousness+Indicators+in+Economic+AI+Agents+-+Systematic+Evaluation+of+La+Serenissima+Against+the+Butlin+et+al.+Framework.pdf),¬†[Code](http://github.com/Universal-Basic-Compute/serenissima),¬†[Live API](http://serenissima.ai/api/citizens)

**PS**: To be clear, this is about developing reproducible methods for studying AI behavior, not making consciousness claims. Think of it like studying neural correlates in neuroscience - we measure what we can measure.",1,0.52,https://www.reddit.com/r/MachineLearning/comments/1lmw5pg/r_systematic_evaluation_of_computational/,False,True,False
1lmqbzc,asankhs,1751126540.0,18,/r/MachineLearning/comments/1lmqbzc/r_openevolve_automated_gpu_kernel_discovery/,MachineLearning,[R] OpenEvolve: Automated GPU Kernel Discovery Outperforms Human Engineers by 21%,"Hey folks, wanted to share something interesting I've been working on that might be relevant for folks running models locally on Apple Silicon.

**What I did**

Used evolutionary programming to automatically optimize Metal GPU kernels for transformer attention. Specifically targeted Qwen3-0.6B's grouped query attention (40:8 head ratio) running on Apple M-series GPUs through MLX.

**Results**

Tested across 20 different inference scenarios against MLX's `scaled_dot_product_attention` baseline:

* **Average decode speed improvement: +12.5%** (œÉ = 38.3%)
* **Peak improvement: +106%** on repetitive pattern generation
* **Best category: +24.8%** average on general tasks
* **Memory usage: -0.99%** (slight reduction)

**The honest picture:** It's workload dependent. Some scenarios saw big gains (+46.6% on dialogue, +73.9% on extreme-length generation), but others regressed (-16.5% on code generation). Success rate was 7/20 benchmarks with >25% improvements.

**How it works**

The system automatically evolves the Metal kernel source code using LLMs while preserving the MLX integration. No human GPU programming expertise was provided - it discovered optimizations like:

1. **Perfect SIMD vectorization**: Found that `vec<T, 8>` operations match Apple Silicon's capabilities for 128-dim attention heads
2. **Two-pass online softmax**: Fused softmax normalization with value accumulation, reducing memory bandwidth
3. **GQA-specific memory patterns**: Optimized for the 40:8 head structure with coalesced access patterns

# Why this might matter for local inference

* Shows automated optimization can compete with expert-engineered kernels
* Demonstrates potential for hardware-specific optimizations without manual tuning
* Could be applied to other transformer components or different model architectures
* All open source - you can reproduce and extend this work

**Try it yourself**

The code and all benchmarks are available in the [OpenEvolve repo](https://github.com/codelion/openevolve). The MLX kernel optimization example is at `examples/mlx_metal_kernel_opt/`.

Requirements:

* Apple Silicon Mac
* MLX framework
* Qwen3-0.6B model

# Limitations

* Currently specific to Apple Silicon and this exact model configuration
* Performance improvements are highly workload-dependent
* Takes \~25 evolutionary generations to converge (few hours on M3)
* No guarantees it'll work better for your specific use case

**Technical write-up**

Full details with code diffs and benchmark methodology: [https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery](https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery)

Curious to hear thoughts from folks who've done MLX optimization work, or if anyone wants to try this on different models/configurations. The evolutionary approach seems promising but definitely has room for improvement.

Has anyone else experimented with automated kernel optimization for local inference?",130,0.91,https://www.reddit.com/r/MachineLearning/comments/1lmqbzc/r_openevolve_automated_gpu_kernel_discovery/,False,True,False
1lmpw9w,answersareallyouneed,1751125426.0,2,/r/MachineLearning/comments/1lmpw9w/d_evaluating_realismquality_of_video_generation/,MachineLearning,[D] Evaluating realism/quality of video generation,"What are the industry/research directions being explored?

I‚Äôm finding a lot of research related to evaluating how well a generated video adheres to a text prompt but can‚Äôt find a lot of research related to quality evaluation(Other than FVD).

From image generation, we know that FID isn‚Äôt always a reliable quality metric. But FID also works on a distribution level.

Is there any research on a per-sample level evaluation? Can we maybe frame this as an out-of-distribution problem?",1,0.67,https://www.reddit.com/r/MachineLearning/comments/1lmpw9w/d_evaluating_realismquality_of_video_generation/,False,True,False
1lmk9uo,LowExercise9592,1751109043.0,0,/r/MachineLearning/comments/1lmk9uo/r_ragged_leveraging_video_container_formats_for/,MachineLearning,[R] Ragged - : Leveraging Video Container Formats for Efficient Vector Database Distribution,"Longtime lurker and really happy to be writing this post. I'm excited to share a proof of concept I've been working on for efficient vector database distribution called Ragged. In my paper and PoC, I explore leveraging the MP4 video container format to store and distribute high-dimensional vectors for semantic search applications.

The idea behind Ragged is to encode vectors and their metadata into MP4 files using custom tracks, allowing seamless distribution through existing Content Delivery Networks (CDNs). This approach maintains compatibility with standard video infrastructure while achieving comparable search performance to traditional vector databases.

Key highlights of my work include: - A novel encoding scheme for high-dimensional vectors and metadata into MP4 container formats. - CDN-optimized architecture with HTTP range requests, fragment-based access patterns, and intelligent prefetching. - Comprehensive evaluation showing significant improvements in cold-start latency and global accessibility. - An open-source implementation to facilitate reproduction and adoption.

I was inspired by the innovative work of Memvid (https://github.com/Olow304/memvid), which demonstrated the potential of using video formats for data storage. My project builds on this concept with a focus on CDNs and semantic search.

I believe Ragged offers a promising solution for deploying semantic search capabilities in edge computing and serverless environments, leveraging the mature video distribution ecosystem. Also sharing indexed knowledge bases in the form of offline MP4 can unlock a new class of applications.

I'm eager to hear your thoughts, feedback, and any potential use cases you envision for this approach. You can find the full paper and implementation details \[here\](https://github.com/nikitph/ragged).

Thank you for your time fellows",4,1.0,https://github.com/nikitph/ragged,False,False,False
1lmilpz,Delicious_Leading_52,1751102517.0,0,/r/MachineLearning/comments/1lmilpz/p_convolutional_neural_network_to_predict/,MachineLearning,[P] Convolutional Neural Network to predict blooming date,"**Hello everyone!**  
I‚Äôve recently been working on a project to study the influence of meteorological variables on the blooming date of plants. To do this, I aim to use a convolutional neural network (CNN) to predict the blooming date and then extract insights using explainability techniques. Let me give you a bit of background:

Each instance in my dataset consists of six time series corresponding to the variables: temperature, humidity, wind speed and direction, radiation, and precipitation. Additionally, I have the species and variety of the plant, along with its geographical location (altitude, latitude, and longitude). The time series start at the moment of leaf fall and span 220 days from that point (so the starting point varies between instances). Each time series contains about 10,000 records, taken at 30-minute intervals. At some point in the middle of the series, blooming occurs. My goal is to predict the number of days from leaf fall to the blooming date.

According to theory, there are two key moments leading to blooming. The first is when the tree enters a phase called¬†*rest*, which begins shortly after leaf fall. The second is when the tree¬†*wakes up*. During the rest phase, the tree accumulates ‚Äúchill units,‚Äù meaning it must spend a certain number of hours below a specific temperature threshold. Once enough chill has accumulated, the tree wakes up and begins accumulating ‚Äúheat‚Äù ‚Äî a number of hours above a certain temperature. Once the required heat is reached and conditions are optimal, blooming occurs.

For this study, I trained a neural network with the following architecture:

* Two convolutional layers for the time series ‚Äî first a 1D layer, followed by a 2D layer that mixes the outputs of the 1D layers.
* A dense layer processes the other (non-temporal) variables.
* The outputs from both parts are then concatenated and passed through two additional dense layers.

After training the network, I plan to use several explainability techniques:

* ICE plots (which I‚Äôve adapted to time series),
* SHAP (also adapted as best as I could to time series),
* Attention mechanisms in the convolutional layers.

**Now the questions:**

1. What do you think of the network architecture? Would you change it or use another type of layer, such as LSTM?
2. What other explainability techniques would you recommend? The ICE plots and SHAP help me understand which time ranges are most important and how changes in variables (e.g., temperature) affect the predicted blooming date. It would also be great to detect when the¬†*rest*¬†phase starts and ends. Do you have any ideas on how to approach that? Some studies use Pearson correlation coefficients, but they haven‚Äôt been very insightful in my case. Also, if you're familiar with this topic and have suggestions for other interesting questions to explore, I‚Äôd love to hear them!

**Thank you so much to anyone reading this ‚Äî any advice is welcome!**",3,0.72,https://www.reddit.com/r/MachineLearning/comments/1lmilpz/p_convolutional_neural_network_to_predict/,False,True,False
1lmg313,jsonathan,1751092368.0,5,/r/MachineLearning/comments/1lmg313/r_thought_anchors_which_llm_reasoning_steps_matter/,MachineLearning,[R] Thought Anchors: Which LLM Reasoning Steps Matter?,https://arxiv.org/abs/2506.19143,44,0.94,https://i.redd.it/dcfne00n4m9f1.jpeg,False,False,False
1lma7l9,mgalarny,1751072298.0,0,/r/MachineLearning/comments/1lma7l9/r_benchmarking_llms_and_mllms_on_extracting/,MachineLearning,[R] Benchmarking LLMs and MLLMs on extracting financial recommendations from YouTube,"**VideoConviction** is a new benchmark for evaluating LLMs and MLLMs on extracting structured stock recommendations from long and short-form YouTube videos. The dataset contains 6K+ annotated recommendation segments from 288 videos across 22 financial influencer channels, each labeled with ticker, action (buy/sell/hold), and timestamped transcripts.

**Why it‚Äôs challenging**:  
Finfluencer content is noisy, informal, and multimodal. Models must distinguish actual recommendations from general market talk, disclaimers, and promotions. We test models on both **full videos** and **segmented clips** to assess context sensitivity and noise robustness.

**Modeling takeaways:**

* **LLMs (text-only)** outperform MLLMs on structured extraction when inputs are clean and segmented.
* **MLLMs (text + video)** help with surface-level cues (e.g., identifying stock tickers like AAPL shown on screen) but often underperform on recommendation-level reasoning.
* Segmenting inputs leads to significant F1 gains across models (not a surprise).

**Results**:

* Best LLM (DeepSeek-V3) outperforms MLLMs on full extraction (ticker + action + recommendation conviction).
* \[Finance specific\] Betting against influencer recommendations outperformed the S&P 500 by +6.8% in annual returns, but at higher risk (Sharpe ratio 0.41 vs 0.65).

**Paper**: [https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=5315526](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5315526)  
**Dataset**: [https://huggingface.co/datasets/gtfintechlab/VideoConviction](https://huggingface.co/datasets/gtfintechlab/VideoConviction)",1,0.55,https://www.reddit.com/r/MachineLearning/comments/1lma7l9/r_benchmarking_llms_and_mllms_on_extracting/,False,True,False
1lm3jlh,WeirdElectrical8941,1751054420.0,13,/r/MachineLearning/comments/1lm3jlh/d_suggestions_on_dealing_with_iccv_rejection/,MachineLearning,[D] Suggestions on dealing with ICCV rejection,"I recently had a paper rejected by ICCV for being too honest (?). The reviewers cited limitations I explicitly acknowledged in the paper's discussion as grounds for rejection (and those are limitations for similar works too).

To compound this, during the revision period, a disruptive foundational model emerged that achieved near-ceiling performance in our domain, significantly outperforming my approach.

Before consigning this work (and perhaps myself) to purgatory, I'd welcome any suggestions for salvage strategies.

Thank you üôÇ",29,0.91,https://www.reddit.com/r/MachineLearning/comments/1lm3jlh/d_suggestions_on_dealing_with_iccv_rejection/,False,True,False
1llzcu1,transformer_ML,1751044236.0,6,/r/MachineLearning/comments/1llzcu1/r_potemkin_understanding_in_large_language_models/,MachineLearning,[R] Potemkin Understanding in Large Language Models,[https://arxiv.org/pdf/2506.21521](https://arxiv.org/pdf/2506.21521),9,0.91,https://www.reddit.com/r/MachineLearning/comments/1llzcu1/r_potemkin_understanding_in_large_language_models/,False,True,False
1llwfn6,No-Sheepherder6855,1751037310.0,17,/r/MachineLearning/comments/1llwfn6/p_built_an_aipowered_rtos_task_scheduler_using/,MachineLearning,[P] Built an AI-powered RTOS task scheduler using semi-supervised learning + TinyTransformer,"I'm still not even in my second year of undergrad, but I wanted to share a recent experiment I did as part of an assignment. I took it way further than required.

**Problem:**  
RTOS schedulers often miss deadlines when task loads become unpredictable. There's not much real workload data available, so I had to generate synthetic task profiles.

**What I built:**  
I created¬†**SILVER\_CS**, a real-time task scheduler that uses a TinyTransformer model trained with semi-supervised learning and curriculum training. The model learns task patterns and adapts scheduling decisions over time.

* Trained on synthetic datasets simulating RTOS behavior
* Deployed as a lightweight scheduler on a simulated RTOS
* Achieved 13‚Äì14% fewer missed deadlines compared to traditional heuristics

Also visualized the model‚Äôs learned clustering using t-SNE (silhouette score: 0.796) to validate internal representations.

This is part of me experimenting with using AI on resource-constrained systems (RTOS, microcontrollers, edge devices).  
Would love to hear feedback or thoughts on how others have tackled scheduling or AI in embedded systems.

EDIT: GitHub repo: [https://github.com/SilverShadowHeart/SILVER\_CS](https://github.com/SilverShadowHeart/SILVER_CS)  


https://preview.redd.it/knorrqx7lh9f1.png?width=1919&format=png&auto=webp&s=79d94b38c84fae4ef703f28580c4be62abb69e71

https://preview.redd.it/nnjd4px7lh9f1.png?width=1918&format=png&auto=webp&s=7e19f4fd16abb502caa1f88a2ecd23dc53e7b0f7

https://preview.redd.it/76buw1y7lh9f1.png?width=1919&format=png&auto=webp&s=8435844fe5ff8845d42a14427005367c74c93722

https://preview.redd.it/pm9hp1y7lh9f1.png?width=1919&format=png&auto=webp&s=d4e1a076436f00a19a7950a34f315f96249ab1b0

https://preview.redd.it/0fp5x5y7lh9f1.png?width=1919&format=png&auto=webp&s=2a80e9a203964d8066fc3fa667d93c8801f7ce33",7,0.82,https://www.reddit.com/r/MachineLearning/comments/1llwfn6/p_built_an_aipowered_rtos_task_scheduler_using/,False,True,False
1llqoj1,EducationalCicada,1751020852.0,0,/r/MachineLearning/comments/1llqoj1/r_enigmata_scaling_logical_reasoning_in_llms_with/,MachineLearning,[R] Enigmata: Scaling Logical Reasoning In LLMs With Synthetic Verifiable Puzzles,,8,0.83,https://arxiv.org/abs/2505.19914,False,False,False
1llo6mr,Gold-Plum-1436,1751010810.0,2,/r/MachineLearning/comments/1llo6mr/the_condition_number_as_a_scaleinvariant_proxy/,MachineLearning,The Condition Number as a Scale-Invariant Proxy for Information Encoding in Neural Units,,2,0.59,https://arxiv.org/abs/2506.16289,False,False,False
1llo5nt,mio_11,1751010695.0,17,/r/MachineLearning/comments/1llo5nt/d_thinking_fast_and_slow/,MachineLearning,"[D] Thinking, Fast and Slow","To the theorists in the community, how do you balance 
1. engaging with theory research - which is usually a slow process requiring deep thinking
2. with programming - which is fast-paced, iterative process with quick feedback?
I'm finding switching between the two thinking modes very hard to balance.",45,0.86,https://www.reddit.com/r/MachineLearning/comments/1llo5nt/d_thinking_fast_and_slow/,False,True,False
1llhl65,South-Conference-395,1750988045.0,3,/r/MachineLearning/comments/1llhl65/r_emnlp_2025_reply_to_reviewers_disabled/,MachineLearning,[R] EMNLP 2025: reply to reviewers disabled,"Hi all,  
I would like to check whether anyone is facing same issue as myself. It seems that I cannot add an official comment in my submission. I can currently see only the author-editor confidential comment option. Has anyone managed to submit their replies?

thanks for the help!",5,1.0,https://www.reddit.com/r/MachineLearning/comments/1llhl65/r_emnlp_2025_reply_to_reviewers_disabled/,False,True,False
1ll6nsq,Final-Tackle7275,1750960053.0,385,/r/MachineLearning/comments/1ll6nsq/d_emnlp_2025_paper_reviews/,MachineLearning,[D] EMNLP 2025 Paper Reviews,Reviews are released! Lets have fun and discuss them here!,35,0.91,https://www.reddit.com/r/MachineLearning/comments/1ll6nsq/d_emnlp_2025_paper_reviews/,False,True,False
1ll69g0,emiurgo,1750959146.0,14,/r/MachineLearning/comments/1ll69g0/r_you_can_just_predict_the_optimum_aka_incontext/,MachineLearning,[R] You can just predict the optimum (aka in-context Bayesian optimization),"Hi all,

I wanted to share a blog post about our recent AISTATS 2025 paper on using Transformers for black-box optimization, among other things.

TL;DR: We train a Transformer on millions of synthetically generated (function, optimum) pairs. The trained model can then predict the optimum of a new, unseen function in a single forward pass. The blog post focuses on the key trick: how to efficiently generate this massive dataset.

* **Blog post:** [https://lacerbi.github.io/blog/2025/just-predict-the-optimum/](https://lacerbi.github.io/blog/2025/just-predict-the-optimum/)
* **Paper:** Chang et al. (AISTATS, 2025) [https://arxiv.org/abs/2410.15320](https://arxiv.org/abs/2410.15320)
* **Website:** [https://acerbilab.github.io/amortized-conditioning-engine/](https://acerbilab.github.io/amortized-conditioning-engine/)

Many of us use Bayesian Optimization (BO) or similar methods for expensive black-box optimization tasks, like hyperparameter tuning. These are iterative, sequential processes. We had an idea inspired by the power of in-context learning shown by transformer-based meta-learning models such as Transformer Neural Processes (TNPs) and Prior-Fitted Networks (PFNs): what if we could frame optimization (as well as several other machine learning tasks) as a massive prediction problem?

For the optimization task, we developed a method where a Transformer is pre-trained to learn an implicit ""prior"" over functions. It observes a few points from a new target function and directly outputs its prediction as a distribution over the location and value of the optimum. This approach is also known as ""amortized inference"" or meta-learning.

The biggest challenge is getting the (synthetic) data. How do you create a huge, diverse dataset of functions and their known optima to train the Transformer?

The method for doing this involves sampling functions from a Gaussian Process prior in such a way that we know where the optimum is and its value. This detail was in the appendix of our paper, so I wrote the blog post to explain it more accessibly. We think it‚Äôs a neat technique that could be useful for other meta-learning tasks.",89,0.92,https://www.reddit.com/r/MachineLearning/comments/1ll69g0/r_you_can_just_predict_the_optimum_aka_incontext/,False,True,False
1ll5agc,Greedy-Echo-2102,1750956922.0,11,/r/MachineLearning/comments/1ll5agc/d_emnlp_2025_review/,MachineLearning,[D] emnlp 2025 review,"
I just received my emnlp reviews . Not sure how to proceed with it. I am too scared!!

Paper 1 :

OA: 2.5 ,1.5,3 

Confidence 3,3,3

Paper 2:

OA: 2.5,2,3

Confidence: 3,2,3

Please help me sharing your thoughts and experiences.

Thanks

",14,0.74,https://www.reddit.com/r/MachineLearning/comments/1ll5agc/d_emnlp_2025_review/,False,True,False
1ll2wer,dumbestindumb,1750951313.0,1,/r/MachineLearning/comments/1ll2wer/d_can_split_learning_impact_xai_compared_same/,MachineLearning,[D] Can split learning impact XAI compared same model trained in central server?,"Thinking to do research in this direction, currently learning about split learning and XAI. Do you think it is a good research question to explore? ",0,0.14,https://www.reddit.com/r/MachineLearning/comments/1ll2wer/d_can_split_learning_impact_xai_compared_same/,False,True,False
1ll1wm9,ashervivi88,1750948959.0,0,/r/MachineLearning/comments/1ll1wm9/n_1m_in_grants_for_ai_projects_advancing/,MachineLearning,"[N] $1M in grants for AI projects advancing truth-seeking, deadline July 1",Cool new grant program that is funding AI prototypes that help advance human knowledge + open inquiry (Cosmos Institute + FIRE) [https://cosmosgrants.org/truth](https://cosmosgrants.org/truth),0,0.5,https://www.reddit.com/r/MachineLearning/comments/1ll1wm9/n_1m_in_grants_for_ai_projects_advancing/,False,True,False
1ll100q,Alarming-Camera-188,1750946717.0,1,/r/MachineLearning/comments/1ll100q/d_budget_cut_in_usa_impact_on_conference/,MachineLearning,[D] Budget cut in USA? Impact on conference?,"Due to the recent budget cuts in the USA, do you think organizers should consider a hybrid conference?

",0,0.25,https://www.reddit.com/r/MachineLearning/comments/1ll100q/d_budget_cut_in_usa_impact_on_conference/,False,True,False
1lktj7p,Celmeno,1750921373.0,18,/r/MachineLearning/comments/1lktj7p/d_did_you_get_neurips_reviews_assignments/,MachineLearning,[D] Did you get Neurips reviews assignments?,I just realized that I never got any papers assigned which I found a bit odd given the extreme number of submissions. Did they forget about me? ,39,0.92,https://www.reddit.com/r/MachineLearning/comments/1lktj7p/d_did_you_get_neurips_reviews_assignments/,False,True,False
1lksg4d,dontknowbutamhere,1750917263.0,0,/r/MachineLearning/comments/1lksg4d/d_attention_heatmap_visualization_tools/,MachineLearning,[D] Attention heatmap visualization tools?,"Are there any tools for easily visualizing attention weights with heatmaps for huggingface models? I couldn't really find any tools for doing this so I've just been using seaborn but it gets messy for really long contexts. Ideally I'd just be able to upload a file of a string representation of the attention weights tensor along with the tokens at each index and be able to toggle between attention heads/model layer and also be able to drag/zoom.

Thanks!",5,1.0,https://www.reddit.com/r/MachineLearning/comments/1lksg4d/d_attention_heatmap_visualization_tools/,False,True,False
1lkmkuw,GodIsAWomaniser,1750898678.0,156,/r/MachineLearning/comments/1lkmkuw/d_alarming_amount_of_schizoid_people_being/,MachineLearning,"[D] Alarming amount of schizoid people being validated by LLMs, anyone else experienced this?","I've had more experiences in the last couple of weeks encountering people with very strong schizoid traits than I have in the last few years around artificial intelligence machine learning etc, but really around the use of large language models. 


I've met five different people online in the last 3 weeks who have messaged me on discord or read it asking for help with a project, only to be immediately sent a three paragraph chat bot summary and 400 lines of pseudo python. When I ask for them to explain their project they become defensive and tell me that the LLM understands the project so I just need to read over the code ""as an experienced Dev"" (I only have foundational knowledge, 0 industry experience).


Or other times where I've had people message me about a fantastic proof or realisation that have had that is going to revolutionise scientific understanding, and when I ask about it they send walls of LLM generated text with no ability to explain what it's about, but they are completely convinced that the LLM had somehow implemented their idea in a higher order logic solver or through code or through a supposedly highly sophisticated document.


People like this have always been around, but the sycophantic nature of a transformer chatbot (if it wasn't sycophantic it would be even more decoherent over time due to its feed forward nature) has created a personal echo chamber where an entity that is being presented as having agency, authority, knowledge and even wisdom is telling them that every idea they have no matter how pathological or malformed is a really good one, and not only that but is easily implemented or proven in a way that is accepted by wider communities. 


After obviously spending weeks conversing with these chatbots these people (who I am not calling schizophrenic but are certainly of a schizoid personality type) feel like they have built up a strong case for their ideas, substituting even the most simple domain knowledge for an LLMs web searching and rag capability (which is often questionable, if not retrieving poison) and then find themselves ready to bring proof of *something* to the wider world or even research communities. 


When people who have schizoid personality traits are met with criticism for their ideas, and especially for specific details, direct proof, and how their ideas relate to existing cannon apart from the nebulous notion that the conclusions are groundbreaking, they respond with anger, which is normal and has been well documented for a long time.


What's changed though Just in the last year or two is that these types of people have a digital entity that will tell them that their ideas are true, when they go out into the world and their unable to explain any of it to a real human, they come back to the LLM to seek support which then inevitably tells them that it's the world that's wrong and they're actually really special and no one else can understand them. 


This seems like a crisis waiting to happen for a small subsection of society globally, I assume that multilingual LLM's behave fairly similarly in different languages because of similar rules for the data set and system prompts to English speaking data and prompts. 


I know that people are doing research into how LLM use affects people in general, but I feel that There is a subset of individuals for whom the use of LLM chatbots represents a genuine, immediate and essentially inevitable danger that at best can supercharge the social isolation and delusions, and at worst lead to immediately self-destructive behaviour. 


*Sigh* anyway maybe this is all just me venting my frustration from meeting a few strange people online, but I feel like there is a strong Avenue for research into how people with schizoid type mental health issues (be it psychosis, schizophrenia, OCD, etc.) using LLM chatbots can rapidly lead to negative outcomes for their condition.


And again I don't think there's a way of solving this with transformer architecture, because if the context window is saturated with encouragement and corrections it would just lead to incoherent responses and poor performance, the nature of feedback activations lends itself much better to a cohesive personality and project. 


I can't think of any solution, even completely rewriting the context window between generations that would both be effective in the moment and not potentially limit future research by being too sensitive to ideas that haven't been implemented before.


Please pardon the very long post and inconsistent spelling or spelling mistakes, I've voice dictated it all because I've broken my wrist.",313,0.89,https://www.reddit.com/r/MachineLearning/comments/1lkmkuw/d_alarming_amount_of_schizoid_people_being/,False,True,False
1lkkmqw,Big-Waltz8041,1750893269.0,10,/r/MachineLearning/comments/1lkkmqw/r_any_proxy_methods_for_labeling_indirectimplicit/,MachineLearning,[R] Any proxy methods for labeling indirect/implicit emotions without human annotators?,"I‚Äôm working on a research project involving a manually curated dataset that focuses on workplace scenarios. I need to label data for implicit emotions but I don‚Äôt have access to human annotators (psychologist or someone who does this kind of work) this task. The dataset will be used on an LLM. 

Are there any reliable proxy methods or semi-automated approaches I can use to annotate this kind of data for a study? I‚Äôm looking for ways that could at least approximate human intuition. Any leads or suggestions will be super helpful. 
Thanks in advance! ",3,0.67,https://www.reddit.com/r/MachineLearning/comments/1lkkmqw/r_any_proxy_methods_for_labeling_indirectimplicit/,False,True,False
1lkg2l1,Chroma-Crash,1750882080.0,2,/r/MachineLearning/comments/1lkg2l1/d_feedback_on_residual_spatiotemporal_gnn_for/,MachineLearning,[D] Feedback on Residual Spatiotemporal GNN for Flood Forecasting,"I have recently taken up interest in hydrology, and specifically flood forecasting as a result of this paper by Google: [https://www.nature.com/articles/s41586-024-07145-1](https://www.nature.com/articles/s41586-024-07145-1) The paper details the implementation behind their Flood Hub interface, which currently serves forecasts for river discharge globally, using an LSTM encoder-decoder setup. You can see Flood Hub here: [https://sites.research.google/floods/](https://sites.research.google/floods/)

What got me interested is the way they aggregate basin and weather data. It seems like a very simple weighted average that ignores a lot of basin dynamics, specifically in large basins. I feel supported in that conclusion because of their metrics correlating basin size to F1 score.

So, I have been working on a model that uses structured graphs to model the upstream basins rather than the area-weighted average seen in the paper. This approach seems to me like it bridges the gap between Google's approach and the more recent image convolutions seen in RiverMamba: [\[2505.22535v1\] RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting](https://arxiv.org/abs/2505.22535v1)

I am admittedly quite new to graph neural networks, and I have chosen a GCLSTM for the task; from torch\_geometric\_temporal to be specific. I don't know if this is the best model for this task, and I made the decision at some point to stack layers of the GCLSTM with residuals to expand model capacity, which has generally improved performance. I am also considering experimenting with graph transformers due to the width of the graphs and performers for the time series analysis, which I haven't been able to find any studies related to yet. A lot more of my approach is detailed here: [https://github.com/dylan-berndt/Inundation-Station/](https://github.com/dylan-berndt/Inundation-Station/) One of my biggest problems right now is computation speed and memory, even at level 7 of HydroATLAS many of the upstream basins have 700+ nodes in them. I also have a surprising amount of gauges with apparently only one sub-basin upstream. This made me implement a custom batching algorithm to keep batches consistently sized.

So far, I have been studying a continental dataset because of these limits, but I am getting precision and recall metrics that far exceed my expectations, especially compared to the Nash-Sutcliffe efficiency the model scores. I have reduced the length of the history supplied to the model, which could be the reason (model can only recognize sudden spikes, not enough context to determine actual conditions). I can't really increase the context length without removing model capacity for memory's sake. This is a large part of the reason why I want feedback on this model. The other reason is that I don't know a single person to ask feedback from barring the main author of the Flood Hub paper himself. I plan to test against a continentally trained version of Flood Hub to compare more directly soon. I've been working on the project generally for about 4 months now, and writing code for 2, so feel free to ask for more context. Any help is appreciated.",7,0.89,https://www.reddit.com/r/MachineLearning/comments/1lkg2l1/d_feedback_on_residual_spatiotemporal_gnn_for/,False,True,False
1lkedb8,INFINITASIUM,1750878081.0,15,/r/MachineLearning/comments/1lkedb8/d_paperswithcode_has_been_compromised/,MachineLearning,[D] Paperswithcode has been compromised,"I was randomly looking at the papers on CIFAR when I opened the website to see an aggregated list and saw that all the text had been replaced with spam text.

I have archived the URLs for a bunch of the datasets for reference:

[https://archive.is/2Si8H](https://archive.is/2Si8H)

[https://archive.is/KJCx1](https://archive.is/KJCx1)

[https://archive.is/ZDBL5](https://archive.is/ZDBL5)

[https://archive.is/BHVsk](https://archive.is/BHVsk)

[https://archive.is/b9xUp](https://archive.is/b9xUp)

[https://archive.md/8BLVA](https://archive.md/8BLVA)

[https://archive.md/SmoCt](https://archive.md/SmoCt)

[https://archive.md/5UZLu](https://archive.md/5UZLu)

edit: added more examples",138,0.97,https://www.reddit.com/r/MachineLearning/comments/1lkedb8/d_paperswithcode_has_been_compromised/,False,True,False
1lkdt1k,whereismycatyo,1750876802.0,20,/r/MachineLearning/comments/1lkdt1k/d_how_to_disagree_without_arguing_with_a_reviewer/,MachineLearning,[D] How to disagree without arguing with a reviewer,"Folks, a reviewer asked us to add a new section for our conference submission, which we think serves no good to the paper and a distraction for a reader.

If you have been in this situation before, what's your tactic to refuse a reviewer's comment.",12,0.73,https://www.reddit.com/r/MachineLearning/comments/1lkdt1k/d_how_to_disagree_without_arguing_with_a_reviewer/,False,True,False
1lkdq1w,BeigePerson,1750876618.0,1,/r/MachineLearning/comments/1lkdq1w/p_help_regularising_distributed_lag_model/,MachineLearning,[P] Help Regularising Distributed Lag Model?,"I have an infinite distributed lag model with exponential decay.  Y and X have mean zero:

>Y\_hat = Beta \* exp(-Lambda\_1 \* event\_time) \* exp(-Lambda\_2 \* calendar\_time)  
Cost = Y - Y\_hat

How can I L2 regularise this?

I have got as far as this:

* use the continuous-time integral as an approximation
   * I could regularise using the continuous-time integral : L2\_penalty = (Beta/(Lambda\_1+Lambda\_2))^(2) , but this does not allow for differences in the scale of our time variables
   * I could use seperate penalty terms for Lambda\_1 and Lambda\_2 but this would increase training requirements
* I do not think it is possible to standardise the time variables in a useful way
* I was thinking about regularising based on the predicted outputs
   * L2\_penalty\_coefficient \* sum( Y\_hat^(2) )
   * What do we think about this one?  I haven't done or seen anything like this before but perhaps it is similar to activation regularisation in neural nets?

Any pointers for me?",2,1.0,https://www.reddit.com/r/MachineLearning/comments/1lkdq1w/p_help_regularising_distributed_lag_model/,False,True,False
1lkd0rb,spaghetsie,1750875004.0,2,/r/MachineLearning/comments/1lkd0rb/p_trouble_analyzing_loss_graph/,MachineLearning,[P] Trouble analyzing loss graph.,"Hello, I'm trying to make an AI to play the game Forts. Without getting into the details, it takes a list of links (pairs of points) and tries to predict the next link it should place. With the idea that ingame this would be called recursively.

I'm trying out various model sizes and not only am I unable to make it overfit, my validation loss appears constant throughout training

Model: \[2000 10000 10000 10000 10000 4\]

https://preview.redd.it/1ux3sef3649f1.png?width=580&format=png&auto=webp&s=3f4881bb1b1bc45460a4a7be0ecbd6bff627da30

Thinking my model simply wasn't large enough, I increased first two hidden layers to 20000 neurons each, which had no effect on validation loss.

https://preview.redd.it/19bl0t95649f1.png?width=580&format=png&auto=webp&s=0bc079180a8717e1173621e014ff62b6cb41e85d

What could be the issue? Is my dataset (10000) simply too small?",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1lkd0rb/p_trouble_analyzing_loss_graph/,False,True,False
1lkbsic,ant-des,1750872245.0,10,/r/MachineLearning/comments/1lkbsic/d_why_are_there_no_text_auto_encoders_with/,MachineLearning,[D] Why are there no text auto encoders with reconstruction loss as a primary training objective?,"I'm working on a pipeline to improve code generation models and have a question about embedding architectures.

**My Pipeline:**

1. **Analyze Source Code:**¬†I take a source file and, for every symbol, generate a structured block of text. I use tree-sitter and LSPs to get types, docstrings, function signatures, etc. The output looks something like:¬†`""kind: class. name: AdamW. type: torch.optim.Optimizer. doc: Implements the AdamW algorithm...""`
2. **Embed Descriptions:**¬†I take this block of text and embed it into a vector.
3. **Feed to a Generator:**¬†The plan is to feed these embeddings into a larger generative model via cross-attention, allowing it to be aware of types, function signatures, and other semantic information.

**The Problem I'm Facing:**

Currently, I'm using qwen in sentence-transformers (specifically¬†`Qwen3-Embedding-0.6B`) to embed these descriptions. My annoyance is that virtually all of these popular embedding models are trained on a contrastive loss or a similarity objective.

What I actually want is a model trained on¬†**reconstruction loss**. I want to embed the block of text by pushing it through an¬†**Encoder**, and then have a¬†**Decoder**¬†that can reconstruct the original text from that embedding. My intuition is that this would force the embedding to preserve the maximum amount of information from the input text, making it a much higher-fidelity signal for my downstream generation task.

This autoencoder approach with a reconstruction objective seems incredibly prevalent and successful in audio and images (e.g. Flux), but it seems to barely exist for text.

My question: Are there any text embedding models with reconstruction loss you're aware of? And why are they so unpopular?",11,0.87,https://www.reddit.com/r/MachineLearning/comments/1lkbsic/d_why_are_there_no_text_auto_encoders_with/,False,True,False
1lk9731,DescriptionClassic47,1750866420.0,5,/r/MachineLearning/comments/1lk9731/d_thinking_of_starting_an_initiative_tracing_the/,MachineLearning,[D] Thinking of starting an initiative tracing the origin and impact of different ML practices ‚Äì feedback requested,"Hi all, I am a starting ML researcher (starting my PhD this Fall), and I‚Äôve been increasingly frustrated by some recurring patterns in our field. I‚Äôd love to hear your feedback before I invest time in launching a new initiative.

**What bothers me about the current ML research landscape:**

* To beat benchmark scores, researchers often tweak models, hyperparameters, training setups, etc.
* In the final paper, it‚Äôs usually unclear which changes were:
   * Arbitrary design decisions,
   * Believed to have impact,
   * Or actually shown to make a difference.
* The focus tends to be on performance rather than understanding *why* certain components work.
* This issue is amplified by the effect illustrated in [https://xkcd.com/882/](https://xkcd.com/882/) : if you try enough random variations, there will always be some that appear to work.
* Statistical rigor is often missing: p-values or confidence intervals are rarely used, and benchmark differences are often eyeballed. Pretty often baselines are not subjected to the same amount of tuning as the proposed method.
* While some papers do study the impact of individual components (e.g., batch norm, cosine decay, label smoothing, etc.), I‚Äôm very often having a hard time puzzling together:
   * Where a certain technique was introduced,
   * What works have studied its effectiveness in isolation,
   * What other works have looked at this from a different perspective (e.g. after validating the effectiveness of dot-product self-attention, one might be interested to research how effective attention in other geometric spaces is).

**My idea:**

I‚Äôm considering creating a public Q&A-style forum with tentative title ¬†**""The Small Questions in DL""**, focused on tracing the origin and measurable impact of widely-used ML practices.  
The core goals:

* Allow people to ask foundational questions like *""Why do we use X?""* (e.g., ‚ÄúWhy cosine LR decay?‚Äù or ‚ÄúDoes label smoothing help?‚Äù).
* Collect and link papers or experiments that have explicitly studied these questions, ideally in isolation.
* Highlight what we know, what we assume, and what still needs investigation.
* When discussing results, focus on enclosing all assumptions made in those papers. --> (e.g. ‚Äúpaper X empirically researches the influence of skip connections in GAT, GraphSAGE, and Graphormer with <=5 layers when evaluated on node classification benchmark X, and comes to conclusions A and B‚Äù, rather than ‚Äúaccording to paper X, skip connections empirically improve the performance of GNNs‚Äù.)
* Ideally, this will foster clarity, reduce superstition, and maybe even spur targeted research on components that turn out to be under-explored.

*Note: By definition, many of these questions will be broad, therefore making them unsuitable for StackExchange. The goal would be to create a place where this type of questions can be asked.*

**Some example questions to set the stage:**

Off the top of my head:

* What are known reasons for the (usual) effectiveness of skip connections?
* Are there situations where skip connections perform worse?
* Why do we use dot-product attention? Has attention in other geometric spaces (e.g. hyperbolic) been tried?
* Why do we use cosine decay for learning rate schedules?
* Why do we use L2 regularization rather than Lr for some other r?
* Why does dot-product attention compute the attention matrix (simplified) as softmax((KX)^(T) (QX)), when K^(T)Q can be collapsed into a single learnable matrix?

**Practically:**

With the little research I have done, I have come to like the idea of a Forum on [discourse.org](http://discourse.org) most.

Some alternatives that I think are inferior (feedback welcome):  
Reddit is hard to categorize and retrieve things, Discord idem. StackExchange is rigid and takes long to get approved.

**I'd love your input on a few things before starting:**

1. Do you also feel this lack of clarity around common ML practices is a real issue? (Or just my young na√Øvet√©? :))
2. Do you think a forum like this would help?
3. Are there existing initiatives that already do something very similar? I haven‚Äôt found any, but I would refrain from duplicating existing efforts.
4. Would this be an initiative you would be excited to contribute to?

Any feedback would be appreciated!",6,0.76,https://www.reddit.com/r/MachineLearning/comments/1lk9731/d_thinking_of_starting_an_initiative_tracing_the/,False,True,False
1lk71h6,hmmbosse,1750861391.0,48,/r/MachineLearning/comments/1lk71h6/r_is_it_true_that_most_of_ai_is_just_data/,MachineLearning,[R] Is it true that most of AI is just data cleaning and not fancy models?,"I‚Äôve been reading about how in real-world AI, most of the work isn‚Äôt the cool stuff like neural nets, but actually just *getting the data usable*. Things like cleaning missing values, feature engineering, and framing the problem right.

Some people also said prompt engineering is the ‚Äúnew programming,‚Äù especially with LLMs becoming so dominant.

I came across a blog that listed 10 things you only realize *after* starting with AI ‚Äî like how feedback loops can mess up your model after deployment, or how important it is to define your objective before even touching code.  
It kinda shifted my view on what matters early on.

Is this the general consensus? Or is it still more about algorithms in practice?",115,0.86,https://www.reddit.com/r/MachineLearning/comments/1lk71h6/r_is_it_true_that_most_of_ai_is_just_data/,False,True,False
1lk4m92,These_Rest_6129,1750855300.0,4,/r/MachineLearning/comments/1lk4m92/d_do_you_guy_still_have_access_to/,MachineLearning,[D] Do you guy still have access to paperswithcode.com ?,"It look like the servers are not responding, do you guys can still access it ?

  
\[It works now :)\]",8,0.75,https://www.reddit.com/r/MachineLearning/comments/1lk4m92/d_do_you_guy_still_have_access_to/,False,True,False
1lk38sf,ElPelana,1750851190.0,129,/r/MachineLearning/comments/1lk38sf/d_iccv_2025_results_discussion/,MachineLearning,[D] ICCV 2025 Results Discussion,"Just created this thread for ICCV 2025 results discussion, which should be released today. Remember, scores go from 1 to 6.

I got a 4/4/2 initially, but I think I did a good rebuttal, so lets see :) Good luck everyone!!!",57,0.95,https://www.reddit.com/r/MachineLearning/comments/1lk38sf/d_iccv_2025_results_discussion/,False,True,False
1ljyhny,random_sydneysider,1750833174.0,9,/r/MachineLearning/comments/1ljyhny/d_visa_sponsorship_for_ai_research_roles_in/,MachineLearning,[D] Visa sponsorship for AI research roles in America/Europe,"Quick question about research scientist/engineer roles in big tech companies & frontier AI labs.

Are most companies happy to sponsor work visas (eg. an H1B or E3 visa in America, or the equivalent in Europe)? Is it harder to find research roles for candidates who are outside of America/Europe?

A few years I think this wasn't a problem (eg. an OpenAI recruiter told me it would be easy to sponsor visas for them when I interviewed there), but am not sure anymore.",15,0.76,https://www.reddit.com/r/MachineLearning/comments/1ljyhny/d_visa_sponsorship_for_ai_research_roles_in/,False,True,False
1ljtcv5,uniquebomb,1750816128.0,6,/r/MachineLearning/comments/1ljtcv5/p_interactive_graph_explorer_for_navigating_key/,MachineLearning,[P] Interactive graph explorer for navigating key LLM research works,"Hello everyone! I've been working on [KnowledgeFlows](https://knowledge-flows.web.app/), an interactive website that lays out LLM topics and influential papers on a visual, chronological graph. It covers areas like Transformers, GPT, Diffusion Models, and more.

You can:

* See direct relationships between concepts (e.g., how VAEs influenced Diffusion Models).
* Click on any topic to get a quick technical summary, key takeaways, and a link to the original paper.
* Search by topic or tag to find what you're looking for.

I love to get your feedback! Website contents are generated with the assistance of LLM. Thanks for taking a look!¬†

https://preview.redd.it/qz0hxe1udo9f1.png?width=2072&format=png&auto=webp&s=9a0e8c9c10a6fc5ed8ac6c7babe6e8d2a2c33539

https://preview.redd.it/7drwai1udo9f1.png?width=2072&format=png&auto=webp&s=ac99062eaf25c86a21d3379f156800dd44f1766d

https://preview.redd.it/teeaih1udo9f1.png?width=2072&format=png&auto=webp&s=645752887632c6b2c97a0d232a3f33d6e4866298",2,1.0,https://www.reddit.com/r/MachineLearning/comments/1ljtcv5/p_interactive_graph_explorer_for_navigating_key/,False,True,False
1ljsyzg,marojejian,1750814994.0,5,/r/MachineLearning/comments/1ljsyzg/r_omega_can_llms_reason_outside_the_box_in_math/,MachineLearning,[R] OMEGA: Can LLMs Reason Outside the Box in Math?,"Paper:

[https://arxiv.org/abs/2506.18880](https://arxiv.org/abs/2506.18880)

Post: 

[https://allenai.org/blog/omega](https://allenai.org/blog/omega)

Comments from the Author:

[https://x.com/nouhadziri/status/1937567606543716508](https://x.com/nouhadziri/status/1937567606543716508)



Dziri's research has been my favorite in terms of probing the limits/weaknesses of transformers.  This seems to be consistent with her past findings: any form of these models are poor at compositional generalization.",30,0.88,https://www.reddit.com/r/MachineLearning/comments/1ljsyzg/r_omega_can_llms_reason_outside_the_box_in_math/,False,True,False
1ljsjyz,Suhaib_Abu-Raidah,1750813744.0,3,/r/MachineLearning/comments/1ljsjyz/r_is_this_articulation_inference_task_a_good_fit/,MachineLearning,[R] Is this articulation inference task a good fit for Reinforcement Learning?,"Hi everyone,

I'm working on a research project involving the prediction of articulation parameters of 3D objects ‚Äî such as joint type (e.g., revolute or prismatic), axis of motion, and pivot point.

# Task Overview:

* The object is represented as a **3D point cloud**, and is observed in **two different poses** (P1 and P2).
* The object may have **multiple mobile parts**, and these are not always simple synthetic link-joint configurations ‚Äî they could be real-world objects with unknown or irregular kinematic structures.
* The agent‚Äôs goal is to predict motion parameters that explain how the object transitions from pose P1 to P2.
* The agent applies a transformation to the mobile part(s) in P1 based on its predicted joint parameters.
* It receives a **reward based on how close the transformed object gets to P2**.

# Research Approach:

I'm considering formulating this as a **reinforcement learning (RL)** task, where the agent:

1. Predicts the joint type, axis, and pivot for a mobile part,
2. Applies the transformation accordingly,
3. Gets a reward based on how well the transformed P1 aligns with P2.

# My Questions:

* Does this task seem **suitable and manageable for RL**?
* Is it **too trivial for RL**, and can be more efficiently approached using simple **gradient-based optimization** over transformation parameters?
* Has this approach of **articulation inference using RL** been explored in other works?
* And importantly: if I go with the RL approach, **is the learned model likely to generalize to different unseen objects during inference**, or would I need to **re-train or fine-tune it for each object**?

Any insights, criticisms, or references to related work would be greatly appreciated. Thanks in advance!",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1ljsjyz/r_is_this_articulation_inference_task_a_good_fit/,False,True,False
1ljp4cg,titiboa,1750804350.0,7,/r/MachineLearning/comments/1ljp4cg/d_how_much_time_do_you_spend_designing_your_ml/,MachineLearning,[D] how much time do you spend designing your ML problem before starting?,"Not sure if this is a low effort question but working in the industry I am starting to think I am not spending enough time designing the problem by addressing how I will build training, validation, test sets. Identifying the model candidates. Identifying sources of data to build features. Designing end to end pipeline for my end result to be consumed.

In my opinion this is not spoken about enough and I am curious how much time some of you spend and what you focus to address?

Thanks",8,0.9,https://www.reddit.com/r/MachineLearning/comments/1ljp4cg/d_how_much_time_do_you_spend_designing_your_ml/,False,True,False
1ljo5c1,JanBitesTheDust,1750801912.0,3,/r/MachineLearning/comments/1ljo5c1/d_old_school_must_read_papers_in_the_field/,MachineLearning,[D] Old school must read papers in the field,"What are some of the classic old school papers? For instance, Vapnik papers about SVM and statistical learning theory.

I wanna know about the conception of modern ideas and where they came from. Schmidhuber always talks about how alot of ideas where invented in the 70s. I would like to read about these ideas in more detail.",33,0.94,https://www.reddit.com/r/MachineLearning/comments/1ljo5c1/d_old_school_must_read_papers_in_the_field/,False,True,False
1ljnfzy,New-Skin-5064,1750800172.0,28,/r/MachineLearning/comments/1ljnfzy/d_extremely_low02_trainval_loss_after_196_billion/,MachineLearning,[D] Extremely low(<0.2) train/val loss after 1.96 billion tokens when pretraining GPT-2 small,"I am currently pretraining GPT-2 small on the 10b token subset of FineWeb Edu. The only differences my model has from the original GPT-2 model are the positional embeddings(I use RoPE), the MLP layers(I use SwiGLU), the batch sizes(I linearly increase batch size from 32k to 525k over the first \~2b tokens), and normalization(I use RMSNorm). I also use BF16, FSDPv2 with SPMD, a TPU v3-8, and SyncFree AdamW. I made sure that the targets are offset by 1 from the inputs, and I checked the attention masking. My code can be found [here](https://www.kaggle.com/code/samirrangwalla/gpt-2-pretraining). Why are my losses so low? 

[My Weights and Biases Dashboard](https://preview.redd.it/3mxmlxydyx8f1.png?width=888&format=png&auto=webp&s=8926aba3b6da62cb02427b2268670e3efa62b5bf)

",41,0.88,https://www.reddit.com/r/MachineLearning/comments/1ljnfzy/d_extremely_low02_trainval_loss_after_196_billion/,False,True,False
1ljmcth,Anxious_Dentist9452,1750797612.0,4,/r/MachineLearning/comments/1ljmcth/p_renting_gpu_for_llm_coreweave_vs_others/,MachineLearning,[P] Renting GPU for LLM - CoreWeave vs others,"Hi, how would you go about comparing different GPU rental providers? The hypothetical use case would be of a typical CoreWeave customer looking to build applications on an existing LLM. Would they be looking primarily at like-for-like pricing and how does this compare across different providers that compete with CoreWeave?

I was able to find CoreWeave pricing easily \[[GPU Cloud Pricing | CoreWeave](https://www.coreweave.com/pricing)\] but I haven't been able to find the comparators from AWS, Microsoft etc.",1,1.0,https://www.reddit.com/r/MachineLearning/comments/1ljmcth/p_renting_gpu_for_llm_coreweave_vs_others/,False,True,False
1ljjodq,brandinho77,1750791413.0,12,/r/MachineLearning/comments/1ljjodq/p_sai_a_reinforcement_learning_competition/,MachineLearning,[P] SAI: A Reinforcement Learning Competition Platform,"Hey everyone,

Our team is opening up access to our RL platform, SAI and would love to get your feedback: https://competesai.com

What is SAI?

SAI is a new platform for reinforcement learning, designed to support structured, reproducible RL challenges, available year-round!

We built SAI because we wanted:

- RL competitions that are accessible at any time (not just during conference windows)
- Challenges for everyone - from newcomers learning the basics to experienced researchers benchmarking new algorithms
- A stronger, more connected RL community (more on this coming soon)
- A way to bring RL back into focus

We‚Äôre inviting the whole community to help shape what SAI becomes. Right now, you can:

- Submit models to live challenges
- Benchmark performance
- Help us test, improve, and expand what‚Äôs possible

Docs: https://docs.competesai.com Trailer: https://youtu.be/Qto-D1ncAiw?si=M4Z2mCZP1nZukTjV

We‚Äôre just getting started - more challenges and features are coming soon. If you‚Äôre working on RL, teaching it, or just curious, we‚Äôd love your feedback. And if you know someone who might be into this, please pass it along.

Happy to answer any questions here.",18,0.96,https://www.reddit.com/r/MachineLearning/comments/1ljjodq/p_sai_a_reinforcement_learning_competition/,False,True,False
1ljijt0,Cute_Trainer_3302,1750788842.0,8,/r/MachineLearning/comments/1ljijt0/d_reasoning_on_perturbed_puzzles/,MachineLearning,[D] Reasoning on Perturbed Puzzles,"The ""[o3 pro is so smart](https://www.reddit.com/r/OpenAI/comments/1lda3vz/o3_pro_is_so_smart/)"" post on r/OpenAI gave me a deja vu to the Hopfield Nets, especially those examples where you can give a corrupt version of an image, and it would recall the original from its memory.

It is actually somewhat easy to make more of these:

1. Ask any LLM for its top n riddles.
2. Slightly perturb them in a logical way.
3. The LLM will ignore the perturbations and just give the original answer, often giving wild justifications just to match the original answer. If it didn't work, go to step 2.

For example, the ""The Man in the Elevator"" riddle:

>A man lives on the 10th floor of an apartment building. Every morning he takes the elevator to go down to the ground floor. When he returns, if it's raining he takes the elevator straight to the 10th; otherwise he rides to the 7th floor and walks the rest up. Why?

Make the guy ""tall"", and the answer is still, ""because he is short"".

So all of this reasoning is just recalled. I have also read a few papers on the ""faithfulness"" topic, and the fact that there are studies where they train models on noisy or irrelevant traces and that this sometimes even increases the model's performance, more and more just sounds like the ""thinking"" traces are just some ad-hoc simulated annealing schedules that try to force the ball out of a local optima.

Now obviously LLMs generalize on thinking patterns because of the compression, but when it ""reasons"" it just recalls, so basically it is a continuous Google?

**Edit**: not a fan of ""this is just basically X"" expressions, but I don't know, it just feels bizarre how these increasingly more and more advanced, benchmark smashing general language models still can't generalize on such general language problems.

**Edit2**: Here are two more to try:

Original: The more you take the more you leave behind. What are they?

Modified: The more you take the *less* you leave behind. What are they?

Original: The more you take away from it, the bigger it becomes. What is it?

Modified: The more you take from it, the bigger *the debt I* become. What am *I*?

The last one is a bit work in progress.",14,0.9,https://www.reddit.com/r/MachineLearning/comments/1ljijt0/d_reasoning_on_perturbed_puzzles/,False,True,False
1ljhj1o,Southern-Whereas3911,1750786582.0,0,/r/MachineLearning/comments/1ljhj1o/p_tinyft_a_lightweight_finetuning_library/,MachineLearning,[P] TinyFT: A lightweight fine-tuning library,"Hey all, I recently created this toy-scale replication of peft / unsloth Fine-Tuning library as a learning project, as well as open-source toy scale replication of Fine-Tuning LLMs from scratch to learn more about it

It supports:
- Parameter-Efficient Fine-Tuning: LoRA, QLoRA
- TensorBoard and Weights & Biases support for logging.
- Memory Optimization through Gradient checkpointing, mixed precision, and quantization support.
- vllm and SGLang integration for multi-adapter serving.

Next step would be enabling Reinforcement Learning based training (GRPO) from scratch in our library through a custom GRPO trainer.

Check it out here: [TinyFT](https://github.com/shreyashkar-ml/tinyft)",8,1.0,https://www.reddit.com/r/MachineLearning/comments/1ljhj1o/p_tinyft_a_lightweight_finetuning_library/,False,True,False
1ljhfw8,CrunchyMage,1750786386.0,20,/r/MachineLearning/comments/1ljhfw8/d_best_online_communities_for_ml_research/,MachineLearning,[D] Best online communities for ML research enthusiasts?,"Hey there,  
I'm a former Google ML eng, looking for the best online communities to discuss ML research, share ideas and maybe find collaborators for some research topics I'm curious about.  
I'm not an expert by any means, but I have coauthored a Deep Mind paper before. I'm currently focusing on building an AI startup, but I still want to be able to connect with other people passionate about the discussing, building with and sharing the latest and best research.

What are the very best discords or other communities you've found for discussing ML research/finding other passionate ML researchers?",75,0.95,https://www.reddit.com/r/MachineLearning/comments/1ljhfw8/d_best_online_communities_for_ml_research/,False,True,False
1ljgoqx,Amazing-Rnt9111,1750784699.0,0,/r/MachineLearning/comments/1ljgoqx/rfine_tuning_of_clip_on_a_specific_task/,MachineLearning,[R]Fine tuning of CLIP on a specific task,"Hi all,

I'm working on a text to image retrieval task of satellite images of turtles in the ocean, the idea is: given a query I want to find the image that matches the query.

The problem is that my task is very specific and the images in my dataset are quite similar, (frames taken from videos made with a drone) so I can't fine tune clips on my task also because I saw that clips work with the batch as negative and I don't have enough data to ""simulate"" the batch as negative.

Do you have any ideas/suggestions?",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1ljgoqx/rfine_tuning_of_clip_on_a_specific_task/,False,True,False
1ljgiqo,Gentis-,1750784341.0,8,/r/MachineLearning/comments/1ljgiqo/d_where_are_the_alpha_evolve_use_cases/,MachineLearning,[D] Where are the Alpha Evolve Use Cases?,"I've been following the news around Google DeepMind's AlphaEvolve since its predecessor, FunSearch, made waves. Now that the AlphaEvolve whitepaper is a month old and there's even some open-source code available, I'm finding myself asking a question: Where are all the domain-specific papers, like Finance, Economics, Energy and so on ?",20,1.0,https://www.reddit.com/r/MachineLearning/comments/1ljgiqo/d_where_are_the_alpha_evolve_use_cases/,False,True,False
1ljdjzt,Dismal_Table5186,1750777603.0,55,/r/MachineLearning/comments/1ljdjzt/d_phd_nonus_research_scientist_jobs_in_cvdl_at/,MachineLearning,[D] PhD (non-US) ‚Üí Research Scientist jobs in CV/DL at top companies‚Äîhow much DSA grind is essential?,"Hi all,

I‚Äôm a PhD (or finishing soon) from a national university outside the U.S., focused on computer vision and deep learning. My background is heavily research-oriented‚ÄîI've published at top-tier conferences like MICCAI, WACV, etc.‚Äîbut I haven‚Äôt done much on algorithms or data structures during my PhD.

If someone with a similar profile is trying to land a **Research Scientist** role at places like Google, OpenAI, Microsoft, Anthropic, etc..:

1. **How much emphasis do they actually put on DSA/algorithm interview rounds for research scientist positions?**
2. Do published papers (say \~5 at CVPR/MICCAI/WACV) significantly offset the need for heavy DSA preparation?
3. Anecdotally, in the past, having 5 strong publications could get you research roles or internships at places like Facebook/Meta. These days, even CVPR-level candidates struggle to get internships. Has the bar shifted? If so, why? Even across PhD admissions in the U.S., it seems harder for applied DL folks (with master‚Äôs-level CVPR, WACV, ICCV publications) to get offers compared to theory-focused candidates‚Äîeven those without papers. Is competition truly dominated by theoretical prowess now?

In short, I‚Äôd love to hear from anyone who‚Äôs been through the process recently: **Is it absolutely necessary to grind DSA hard to be competitive? And how much do research publications carry weight now?** The landscape feels more saturated and tilted toward theory lately.

Thanks in advance for any insights or shared experiences!",91,0.87,https://www.reddit.com/r/MachineLearning/comments/1ljdjzt/d_phd_nonus_research_scientist_jobs_in_cvdl_at/,False,True,False
1lj3n3m,7wdb417,1750745619.0,4,/r/MachineLearning/comments/1lj3n3m/p_just_opensourced_eion_a_shared_memory_system/,MachineLearning,[P] Just open-sourced Eion - a shared memory system for AI agents,"Hey everyone!¬†I've been working on this project for a while and finally got it¬†to a point where I'm comfortable¬†sharing it with the community. Eion is a shared memory storage system that provides unified knowledge graph capabilities for AI agent systems.¬†Think of it as the ""Google Docs of AI Agents"" that¬†connects multiple AI agents together, allowing them to share context, memory, and knowledge¬†in real-time.

When building multi-agent systems, I kept running into the same issues: limited memory space, context drifting, and knowledge quality dilution. Eion tackles these issues by:

* Unifying API¬†that works¬†for single¬†LLM apps, AI agents, and complex¬†multi-agent systems¬†
* No external cost via in-house¬†knowledge extraction¬†+¬†all-MiniLM-L6-v2¬†embedding¬†
* PostgreSQL + pgvector¬†for¬†conversation history and semantic search¬†
* Neo4j integration¬†for temporal knowledge graphs¬†

Would¬†love to get feedback from the community! What features would you find most useful? Any architectural decisions you'd question?

GitHub:¬†[https://github.com/eiondb/eion](https://github.com/eiondb/eion)  
Docs:¬†[https://pypi.org/project/eiondb/](https://pypi.org/project/eiondb/)",0,0.35,https://www.reddit.com/r/MachineLearning/comments/1lj3n3m/p_just_opensourced_eion_a_shared_memory_system/,False,True,False
1lj3e0i,red_dhinesh_it,1750744671.0,25,/r/MachineLearning/comments/1lj3e0i/d_whats_happening_behind_googles_ai_overviews/,MachineLearning,[D] What's happening behind Google's AI Overviews?,"Curious to know what happens behind the scenes of the AI Overview widget. The answers are good and the latency with which responses are returned is impressive.

Based on the citations displayed, I could infer that it is a RAG based system, but I wonder how the LLM knows to respond in a particular format for a given question.",29,0.79,https://www.reddit.com/r/MachineLearning/comments/1lj3e0i/d_whats_happening_behind_googles_ai_overviews/,False,True,False
1lj0m50,Previous-West-7782,1750735181.0,3,/r/MachineLearning/comments/1lj0m50/p_a_physics_engine_with_reproducible_cli/,MachineLearning,[P] A physics engine with reproducible CLI simulations + hash-stamped results ‚Äî useful for RL training?,"Hi r/MachineLearning üëã



I‚Äôve been working on a project called \*\*MCP Zero\*\* ‚Äî an \*\*offline-first AI infrastructure SDK\*\*. It runs entirely from the command line, designed for environments where cloud access is limited or undesirable.



üîß Key Features:

\- No internet required (runs 100% offline after install)

\- CLI-based code intelligence (autocomplete, refactor)

\- Memory tree for managing code context (like Merkle + LRU trees)

\- Built for edge AI, secure zones, and disaster response systems



üß† Why?

ML infra is still too cloud-dependent. This tool is built for situations where:

\- Internet isn‚Äôt guaranteed

\- Privacy and reproducibility are critical

\- Devs prefer working in CLI-native environments



üìÇ GitHub: \[ [https://github.com/GlobalSushrut/mcp-zero](https://github.com/GlobalSushrut/mcp-zero) \]  

Website: [https://umesh-project-showcase-p9r66oltm-globalsushruts-projects.vercel.app/](https://umesh-project-showcase-p9r66oltm-globalsushruts-projects.vercel.app/)





Would love feedback ‚Äî especially if anyone‚Äôs doing similar infra/agent work on edge devices.

",0,0.43,https://www.reddit.com/r/MachineLearning/comments/1lj0m50/p_a_physics_engine_with_reproducible_cli/,False,True,False
1livdoh,psychonucks,1750719951.0,3,/r/MachineLearning/comments/1livdoh/d_applying_coconut_continuous_reasoning_into_a/,MachineLearning,"[D] Applying COCONUT continuous reasoning into a learnt linear layer that produces sampling parameters (temp, top-k, top-p, etc.) for the current token?","Hi folks, a new thought experiment has hijacked my brain and I'm hoping to get your feedback before going too far down the rabbit hole and feeling isolated. My last post on using RL for lossless compression was met with some great engagement that helped me feel less like I was screaming into the void. Hoping you can help me again.

The core idea is this: what if an LLM could learn to dynamically modulate its own sampling parameters (temperature, top-p, top-k)¬†*during*¬†the generation of a single response? Instead of a static, pre-set temperature, the model would learn to decide, token-by-token, when to be creative and when to be precise.

**The Concept: Learned Gating of Sampling**

We've seen incredible advancements from continuous reasoning in a loopback fashion (COCONUT) where the final hidden states is the input embedding for the next token, allowing the model to develop policies over the management of its state. My proposal builds on this by proposing that the continuous thought also have the capacity to predict and govern the sampling parameters that ensues at the end of each forward pass, rather than leaving it to fixed values.

**Proposed Process / Training Method**

https://preview.redd.it/21l0cs92dr8f1.png?width=640&format=png&auto=webp&s=49482fa71d804e999b622c2636bce28b22594408

This could be framed as an RL problem, leveraging GRPO. It might look like this:

1. **Augmented Inference Loop:**¬†As the model generates an output, its hidden state at each step (`t`) is not just used to predict the next token (`t+1`). Instead, it's first fed through a small, learned linear layer.
2. **Meta-parameter Prediction:**¬†This linear layer's output is a set of floats that directly dictate the sampling parameters (e.g.,¬†`temperature`,¬†`top_p`) to be used for generating the¬†*very next*¬†token. This is a ""meta-reasoning"" step that happens just before sampling.
3. **Continuous Rollout:**¬†The model's full output is generated using this dynamic, self-governed sampling process.
4. **RL with a Policy Gradient:**¬†The complete generation is then evaluated against a reward function. The specifics are somewhat irrelevant, this ultimately is a multiplier on existing methods.
5. **Backpropagation:**¬†The gradients are then backpropagated via GRPO to update both the main model and the lightweight ""gating"" layer. The model is rewarded for discovering the optimal internal policy for¬†*how*¬†to sample its own probability distribution to achieve a goal.

This does not upgrade the power of a base model, but particularly of RL itself. The model is essentially given a new tool and can learn how to use it in order to optimally explore the latent space over the course of rollouts, greatest coverage for fewest rollouts. The possible effect of RL becomes dramatically more interesting. Furthermore, when the model is RLed on a new task with an already trained such COCONUT sampler, it may then learn new tasks dramatically faster as it performs a more diverse exploration over its latent space. This method may also allow models to perform much better in creative tasks or to be more creative at inference, by developing more complex sampling dynamics.

**Why This Might Work (And Connections to Existing Research)**

This isn't entirely out of left field. It resonates with a few existing concept, such as¬†**entropy-based Dynamic Temperature Sampling**¬†(arXiv:2403.14541) has explored dynamically adjusting temperature based on the entropy of the token distribution to balance quality and diversity. My proposal suggests making this a¬†*learned, goal-oriented policy*¬†rather than a fixed, heuristic one.

By training the model to control its own inference, we might unlock a more efficient and nuanced form of reasoning‚Äîone that can fluidly shift between exploration and exploitation within a single coherent thought process.

I reckon that should work and it seems WILD if it works! No more hyperparameter tuning, let the model figure out a policy, aligned with its latent space through the COCONUT method. Seems like a viable path to me! What do you think? Let's discuss and see if we can build on this.",10,0.75,https://www.reddit.com/r/MachineLearning/comments/1livdoh/d_applying_coconut_continuous_reasoning_into_a/,False,True,False
1liuzbb,Delicious-Pattern-65,1750718926.0,0,/r/MachineLearning/comments/1liuzbb/d_anyone_else_attending_the_international_joint/,MachineLearning,[D] Anyone else attending the International Joint Conference on Neural Networks (IJCNN 2025) Conference in Rome?,I wish there was a channel to connect with fellow attendees.,8,0.83,https://www.reddit.com/r/MachineLearning/comments/1liuzbb/d_anyone_else_attending_the_international_joint/,False,True,False
1liphg4,ZeroSeater,1750705879.0,10,/r/MachineLearning/comments/1liphg4/d_ml_noob_reading_academic_papers_vs_focus_on/,MachineLearning,[D] ML Noob - Reading Academic Papers vs Focus on Applications,"I started reading research papers with my newly found mathematical foundations I acquired recently, and I quite enjoy the process. I have some time this summer, and was wondering whether my time would be better spent continuing this reading journey and produce artifacts of sorts vs. starting a (likely generic) ML project to add to the resume.

I believe the reading research papers approach is a long term investment, whereas ML projects are a bit more technical, but will likely remain mostly surface level. I believe this since research papers would enforce my ability to understand theory and build my mathematical maturity, rather than focus on implementation.

I'd likely start a ML project in the future as well, but unsure whether research paper route could be a worthy investment.

Also feel like many small-mid companies would definitely prefer a candidate who can hit the ground running. That said, ML projects are much more concrete indication of that. I also have general SWE experience, if that changes anything.

Can any hiring managers chime in on their experience on either what they would see as more valuable, both from a learners pov as well as a hirer's pov?

And if anyone wants to chime in on whether reading research papers will help more in the long term vs ml projects?

Thanks.",14,0.79,https://www.reddit.com/r/MachineLearning/comments/1liphg4/d_ml_noob_reading_academic_papers_vs_focus_on/,False,True,False
1lilkjl,Psychological_Quit98,1750697027.0,3,/r/MachineLearning/comments/1lilkjl/d_active_learning_vs_active_data_curation/,MachineLearning,[D] Active Learning v/s Active Data Curation,"Hello Redditors!  
I was unsure about the distinction between Active Learning and Active Data Curation, and quick google searches do not really point out a concrete difference. I would be grateful to hear your thoughts! Also references if any are welcome :D",2,1.0,https://www.reddit.com/r/MachineLearning/comments/1lilkjl/d_active_learning_vs_active_data_curation/,False,True,False
1likvqc,BrilliantDoubt3785,1750695450.0,4,/r/MachineLearning/comments/1likvqc/p_aems_adaptive_efficiency_monitor_simulator/,MachineLearning,[P] AEMS ‚Äì Adaptive Efficiency Monitor Simulator: EWMA-Based Timeline Forecasting for Research & Education Use,"Hey everyone! üëã  
I wanted to share a personal project I‚Äôve been working on and would love your thoughts, feedback, or even collaboration if you're interested.

**AEMS (Adaptive Efficiency Monitor Simulator):**  
AEMS is an open-source simulator that uses **EWMA (Exponentially Weighted Moving Average)** models to forecast timelines for reaching **productivity or personal goals**. Think of it as a research-inspired twist on habit tracking and milestone planning.

Instead of just recording daily data, it simulates your progress trajectory and gives you \*\*adaptive forecasts‚Äî\*\*e.g., ‚ÄúBased on your recent performance, you're likely to finish X in Y days.‚Äù

**Project Features:**

* Forecasting using lightweight statistical modeling (EWMA)
* Open-source codebase (minimal front end)
* Live interactive demo
* Aimed for use by researchers, students, or productivity hackers
* Built to be extended ‚Äî think behavioral simulations, task automation models, or educational tools

**Looking for:**

* **Feedback** on the simulator itself or use cases you'd imagine
* **Collaborators** (especially anyone into behavioral modeling, time series forecasting, or educational tools)
* **Educators** who might want to explore it for student tracking or curriculum planning
* **Ideas** to evolve it into a more robust forecasting engine

If you're curious about the research/behavioral motivation behind it, feel free to comment or DM me‚Äîhappy to share the original proposal text!

Thanks for reading, and I really appreciate any thoughts or critiques. üôè  
Links are in the comments down below",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1likvqc/p_aems_adaptive_efficiency_monitor_simulator/,False,True,False
1lihhdb,dadezzzzz,1750687560.0,5,/r/MachineLearning/comments/1lihhdb/r_comparison_with_literature_suggested_by_the/,MachineLearning,[R] Comparison with literature suggested by the reviewer,"Hi everyone, after almost 2 years of PhD I still ask myself a question. How do you handle reviews where you are asked to compare your approach with a series of 3/4 approaches, none of which provide the code? What we often do is try to reimplement the approach in the paper, wasting countless hours.

I'm looking for a better approach. ",12,0.88,https://www.reddit.com/r/MachineLearning/comments/1lihhdb/r_comparison_with_literature_suggested_by_the/,False,True,False
1ligpde,ashz8888,1750685594.0,0,/r/MachineLearning/comments/1ligpde/p_implemented_rlhf_from_scratch_in_notebooks_with/,MachineLearning,[P] Implemented RLHF from scratch in notebooks with GPT-2,"I recently worked through implementing Reinforcement Learning from Human Feedback (RLHF) step-by-step, including Supervised Fine-Tuning (SFT), Reward Modeling, and Proximal Policy Optimization (PPO), using Hugging Face's GPT-2 model and tokenizer. I recorded the entire process and have put the notebooks on GitHub.

Specifically, the project covers:

* Supervised Fine-Tuning of GPT-2 on the SST-2 sentiment dataset.
* Training a Reward Model to score generated outputs.
* Implementing PPO to further optimize the fine-tuned model based on the reward model's scores.

The complete implementation is done in Jupyter notebooks, and I‚Äôve shared the notebooks here: [https://github.com/ash80/RLHF\_in\_notebooks](https://github.com/ash80/RLHF_in_notebooks)

I also created a video walkthrough explaining each step of the implementation in detail on YouTube here: [https://www.youtube.com/watch?v=K1UBOodkqEk](https://www.youtube.com/watch?v=K1UBOodkqEk)

I hope the notebooks and explanations are useful to anyone looking to explore RLHF practically.

Happy to discuss or receive any feedback!

",11,0.87,https://www.reddit.com/r/MachineLearning/comments/1ligpde/p_implemented_rlhf_from_scratch_in_notebooks_with/,False,True,False
1lifw3w,spilldahill,1750683446.0,7,/r/MachineLearning/comments/1lifw3w/d_found_an_interesting_approach_to_web_agent/,MachineLearning,[D] Found an interesting approach to web agent frameworks,"Was building some web automation flows for work, came across this framework called Notte. Their approach is actually pretty interesting from an ML perspective.

Instead of giving an LLM raw HTML they parse websites into natural language action maps. Instead of your model trying to figure out <div class=""flight-search-input-container"">..., it sees:

    # Flight Search  
    * I1: Enters departure location (departureLocation: str = ""San Francisco"")
    * I3: Selects departure date (departureDate: date)  
    * B3: Search flights options with current filters

Lets you run much smaller models for workflows/web navigation.

Been looking at their benchmarks vs Browser-Use, Convergence etc. claiming outperformance on speed/reliability/cost but haven't verified myself yet (tbf evals are opensource on their GH). Seems like a decent full-stack solution rather than just another agent wrapper.

What's interesting to me is what other domains semantic abstraction could work in, where LLMs need to interface with messy structured data and navigate workflows.

Anyone worked on similar abstraction approaches?

Also curious if anyone's actually tried Notte, their claims are pretty good if true, + technical approach makes sense in theory.

GitHub:¬†[https://github.com/nottelabs/notte](https://github.com/nottelabs/notte)",6,0.75,https://www.reddit.com/r/MachineLearning/comments/1lifw3w/d_found_an_interesting_approach_to_web_agent/,False,True,False
1lieh3l,No-Score712,1750679213.0,16,/r/MachineLearning/comments/1lieh3l/d_is_it_possible_to_convert_music_audio_to_guitar/,MachineLearning,[D] Is it possible to convert music audio to guitar tabs or sheet music with transformers?,"Hey folks,

I'm a guitarist who can't sing, so I play full song melodies on my guitar (fingerstyle guitar). I admire those who can transcribe music into tabs or sheet music, but I can't do this myself.

I just had an interesting thought - the process of transcribing music to sheets sounds a lot like language translation, which is a task that the transformer model is originally built for. If we could somehow come up with a system that represents sheet music as tokens, would it be possible to train such a transformer to take audio tokens as input and the sheet music as output?

Any input or thoughts would be greatly appreciated.",22,0.87,https://www.reddit.com/r/MachineLearning/comments/1lieh3l/d_is_it_possible_to_convert_music_audio_to_guitar/,False,True,False
1lid95g,Ereb0,1750675030.0,0,/r/MachineLearning/comments/1lid95g/r_reinforcement_learning_teachers_of_test_time/,MachineLearning,[R] Reinforcement Learning Teachers of Test Time Scaling,"TL;DR: The raw outputs of our new 7B RL model provide stronger distillation and cold-starting than the filtered and post-processed reasoning traces of orders-of-magnitude larger LMs such as DeepSeek-R1.

How did we achieve this result? We turned the RL task on its head. Rather than training to solve challenging problems from scratch, we optimize our models to generate clear, step-by-step *""explanations""* to *""teach""* their students, providing both the problem‚Äôs question and its solution already in their input prompt.

This makes the RL training task much easier and also directly aligned with downstream distillation, allowing us to train tiny 7B teachers, boosting the performance of even larger 32B students.

If you are interested to learn more, please check out our new work:

Paper: [https://arxiv.org/abs/2506.08388](https://arxiv.org/abs/2506.08388)

Blog: [https://sakana.ai/rlt/](https://sakana.ai/rlt/)

Open source code: [https://github.com/SakanaAI/RLT](https://github.com/SakanaAI/RLT)

If you have any questions, please ask them below or feel free to get in touch, any discussion is more than welcome :)",28,0.95,https://www.reddit.com/r/MachineLearning/comments/1lid95g/r_reinforcement_learning_teachers_of_test_time/,False,True,False
1li43eh,agbrothers,1750641752.0,18,/r/MachineLearning/comments/1li43eh/r_clstoken_avgpool_can_be_a_poor_choice_for/,MachineLearning,"[R] [ClsToken, AvgPool] can be a poor choice for transformer embedding models","This paper started with the following question: why do some approaches choose ClsToken vs AvgPool vs MaxPool for Transformer-based embedding models like BERT or ViT, and what are the consequences? Often, these summarization techniques seem like convenient methods for aligning dimensions that just happen to work well enough, and the decision comes down to empirical performance rather than being motivated mathematically. This then evolved into the question ‚Äî what is the best possible way to summarize embeddings?

We address this question by introducing a framework to evaluate pooling methods as lossy compressors, taking inspiration from vector quantization. For a given task, only a subset of the embeddings matter (signal) while the rest should be treated as noise by the compressor and ignored. The goal of any such pooling method should thus be to aggregate the embeddings in a way that minimizes signal loss.

This reframing reveals failure modes for common methods like ClsToken, AvgPool, and MaxPool as signal-to-noise ratios vary. This result led us to investigate an adaptive attention-based pooling formulation and show that it can both theoretically and empirically lead to better performance and robustness of Transformer embedding models in a variety of applications.

üìÉ Paper: [https://www.arxiv.org/abs/2506.09215](https://www.arxiv.org/abs/2506.09215)¬†  
üëæ Code: [https://github.com/agbrothers/pooling](https://github.com/agbrothers/pooling)

Side note ‚Äî this is my first main-track conference paper and I‚Äôm excited, but also a bit intimidated by the poster session (I‚Äôm only a Master‚Äôs student). I don‚Äôt have an advisor to lean on, so if anyone has any feedback or advice I would really appreciate it!",29,1.0,https://www.reddit.com/r/MachineLearning/comments/1li43eh/r_clstoken_avgpool_can_be_a_poor_choice_for/,False,True,False
1lialoj,giratina13,1750664658.0,29,/r/MachineLearning/comments/1lialoj/d_conceptuallyon_a_code_basis_why_does_pytorch/,MachineLearning,"[D] Conceptually/On a Code Basis - Why does Pytorch work with CUDA out of the box, with minimal setup required, but tensorflow would require all sorts of dependencies?","Hopefully this question doesn't break rule 6.

When I first learned machine learning, we primarily used TensorFlow on platforms like Google Colab or cloud platforms like Databricks, so I never had to worry about setting up Python or TensorFlow environments myself.

Now that I‚Äôm working on personal projects, I want to leverage my gaming PC to accelerate training using my GPU. Since I‚Äôm most familiar with the TensorFlow model training process, I started off with TensorFlow.

But my god‚Äîit was such a pain to set up. As you all probably know, getting it to work often involves¬†*very*¬†roundabout methods, like using WSL or setting up a Docker dev container.

Then I tried PyTorch, and realized how much easier it is to get everything running with CUDA. That got me thinking: conceptually, why does PyTorch require minimal setup to use CUDA, while TensorFlow needs all sorts of dependencies and is just generally a pain to get working?",82,0.97,https://www.reddit.com/r/MachineLearning/comments/1lialoj/d_conceptuallyon_a_code_basis_why_does_pytorch/,False,True,False
1li8kwy,MoveDecent3455,1750656699.0,9,/r/MachineLearning/comments/1li8kwy/p_fenix_an_opensource_framework_using_a_crew_of/,MachineLearning,"[P] Fenix: An open-source framework using a crew of local LLM agents for financial market analysis (Visual, Technical & Sentiment).","Hi r/MachineLearning,

I'd like to share a project I've developed,¬†**Fenix**, an open-source framework for algorithmic trading that leverages a multi-agent system to tackle the noisy and complex domain of financial markets.

Instead of a single model, the architecture is¬†**heterogeneous**, using specialized local LLMs orchestrated by¬†`CrewAI`¬†for different sub-tasks:

1. **Visual Analysis:**¬†A key feature is the¬†`VisualAnalystAgent`, which uses¬†`LLaVA`¬†to perform visual analysis on chart images, identifying technical patterns that are often missed by purely quantitative models. This has been a fascinating challenge in prompt engineering and grounding the model's analysis.
2. **Quantitative Analysis:**¬†A¬†`TechnicalAnalystAgent`¬†interprets numerical indicators calculated via traditional methods (`pandas-ta`), using a reasoning-focused LLM (`Mixtral`) to translate the data into a qualitative assessment.
3. **Sentiment Analysis:**¬†A¬†`SentimentAgent`¬†processes news and social media text to provide a sentiment score, adding a crucial layer of market context.
4. **Logic Validation:**¬†A¬†`QABBAValidatorAgent`¬†acts as a quality control layer, ensuring the outputs from other agents are coherent and logical before they are passed to the final decision-maker.

The entire system is designed to run on consumer hardware using¬†`Ollama`¬†and quantized models, which presented its own set of engineering challenges in memory management and sequential processing.

The project is open-source (Apache 2.0), and the code is available for review. I'm particularly interested in feedback from the ML community on the agent architecture, potential improvements to the consensus mechanism, and ideas for further research (e.g., reinforcement learning based on trade outcomes).

**GitHub:**¬†[`https://github.com/Ganador1/FenixAI_tradingBot`](https://github.com/Ganador1/FenixAI_tradingBot)

Happy to discuss the methodology, challenges, or results!",1,0.67,https://www.reddit.com/r/MachineLearning/comments/1li8kwy/p_fenix_an_opensource_framework_using_a_crew_of/,False,True,False
1li7xvi,Outrageous_Tip_8109,1750654376.0,7,/r/MachineLearning/comments/1li7xvi/d_reviewer_question_acm_mm_2025_can_i_update_my/,MachineLearning,[D] [Reviewer Question] ACM MM 2025 ‚Äì Can I update my rating after rebuttal?,"Hey folks,  
I'm reviewing a couple of papers for ACM Multimedia this season, and I received a mail from the chairs saying that I can update my reviews until June 23 EOD.

The mail says I should update my review based on the rebuttal, but I'm a bit unclear: **am I allowed to change my overall rating (score) at this stage?** Or is this just meant for updating the comments?

Also, do they give us another timeline after this to modify our scores again? Or is this the final say?

Curious to know how others are handling this. Are you adjusting your scores if the rebuttal changed your perspective? Or only tweaking the comments?

Would appreciate any clarity from folks who‚Äôve done this before or are in the same boat.

Thanks!",3,0.8,https://www.reddit.com/r/MachineLearning/comments/1li7xvi/d_reviewer_question_acm_mm_2025_can_i_update_my/,False,True,False
1li59r6,Fit-Flow-4180,1750645485.0,1,/r/MachineLearning/comments/1li59r6/r_does_quantization_affect_models_performance_on/,MachineLearning,[R] Does quantization affect models' performance on long-context tasks?(arXiv:2505.20276),"4-bit quantized models generally exhibit small performance performance drops in general (with good quantization methods like AWQ / GPTQ / etc). In this work we set about to find out if there are specific tasks where quantized models start to significantly underperform. We found that this occurs on very long-context tasks with long context seeing larger performance drops relative to the full-precision models

>**Abstract:**  
Large language models (LLMs) now support context windows exceeding 128K tokens, but this comes with significant memory requirements and high inference latency. Quantization can mitigate these costs, but may degrade performance. In this work, we present the first systematic evaluation of quantized LLMs on tasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation spans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4, GPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B, and 72B). We find that, on average, 8-bit quantization preserves accuracy (\~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for tasks involving long context inputs (drops of up to 59%). This degradation tends to worsen when the input is in a language other than English. Crucially, the effects of quantization depend heavily on the quantization method, model, and task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4, Llama-3.1 70B experiences a 32% performance drop on the same task. These findings highlight the importance of a careful, task-specific evaluation before deploying quantized LLMs, particularly in long-context scenarios and with languages other than English.

[https://arxiv.org/abs/2505.20276](https://arxiv.org/abs/2505.20276)

",15,1.0,https://www.reddit.com/r/MachineLearning/comments/1li59r6/r_does_quantization_affect_models_performance_on/,False,True,False
1li3iig,azqwa,1750639994.0,14,/r/MachineLearning/comments/1li3iig/good_math_heavy_theoretical_textbook_on_machine/,MachineLearning,Good Math Heavy Theoretical Textbook on Machine Learning? [D],"I recently implemented a neural network for my internship, and I found the subject very interesting. It is a topic that is probably very useful for me to learn more about. I am now looking for a deep learning textbook which provides a math heavy theoretical understanding of why deep learning works. I would also like it to be modern, including transformers and other new developments.

I have so far completed the requisites for a math major as well as a bunch of math electives and a good chunk of a physics major at my university, so I do not think math will be an issue. I would therefore like a textbook which assumes a lot of math knowledge.",111,0.97,https://www.reddit.com/r/MachineLearning/comments/1li3iig/good_math_heavy_theoretical_textbook_on_machine/,False,True,False
1lhv42l,Pleasant-Type2044,1750617377.0,28,/r/MachineLearning/comments/1lhv42l/d_how_do_you_keep_up_with_the_flood_of_new_ml/,MachineLearning,[D] How do you keep up with the flood of new ML papers and avoid getting scooped?,"These days, there are dozens of new ML papers published on arXiv every single day. It‚Äôs exciting, but also overwhelming (my google scholar alert). Genuinely asking, for those actively doing research, how do you:

1. Keep up with relevant papers in your area? Learn from the latest SOTA techniques early enough to incorporate them into your own research?
2. Make sure you‚Äôre not being scooped by similar work?",83,0.91,https://www.reddit.com/r/MachineLearning/comments/1lhv42l/d_how_do_you_keep_up_with_the_flood_of_new_ml/,False,True,False
1lhtkr4,Bright_Aioli_1828,1750613594.0,47,/r/MachineLearning/comments/1lhtkr4/p_i_made_a_website_to_visualize_machine_learning/,MachineLearning,[P] I made a website to visualize machine learning algorithms + derive math from scratch,"Check out the website: https://ml-visualized.com/

1. Visualizes Machine Learning Algorithms Learning
2. Interactive Notebooks using marimo and Project Jupyter
3. Math from First-Principles using Numpy and Latex
4. Fully Open-Sourced

Feel free to star the repo or contribute by making a pull request to https://github.com/gavinkhung/machine-learning-visualized

I would love to create a community. Please leave any questions below; I will happily respond.",420,0.99,https://i.redd.it/jb3bjn90li8f1.gif,False,False,False
1lhs5hm,qalis,1750610044.0,326,/r/MachineLearning/comments/1lhs5hm/d_ecai_2025_reviews_discussion/,MachineLearning,[D] ECAI 2025 reviews discussion,European Conference on Artificial Intelligence (ECAI) 2025 reviews are due tomorrow. Let's discuss here when they arrive. Best luck to everyone!,57,0.88,https://www.reddit.com/r/MachineLearning/comments/1lhs5hm/d_ecai_2025_reviews_discussion/,False,True,False
1lhrwqf,Seiko-Senpai,1750609426.0,1,/r/MachineLearning/comments/1lhrwqf/d_how_structured_prediction_differs_from/,MachineLearning,[D] How structured prediction differs from classification and regression?,"In the ""Deep Learning"" book from Goodfellow et. al we find the following definition:

>Structured output: Structured output tasks involve any task where the output is a vector (or other data structure containing multiple values) with important relationships between the diÔ¨Äerent elements. This is a broad category, and subsumes the transcription and translation tasks described above, but also many other tasks.

Based on this definition even simple multi-output regression (i.e. predicting multiple y's) would count as structured prediction because we are predicting a vector. The same applies also for multi-label classification where we can predict \[0, 1, 0, 1\] (where 0/1 indicates the absence/presence of the class). Is there any formal definition of structured prediction? Or all predictive supervised tasks can be considered as classification or regression or a combination of the two (e.g. in object recognition where we regress bounding box values and classify the content)?

\* Note that I am talking only about predictive tasks and I ignore generative supervised tasks like conditional image generation (where we need the labels of the images during training).",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1lhrwqf/d_how_structured_prediction_differs_from/,False,True,False
1lhny9b,Nyaalice,1750599089.0,82,/r/MachineLearning/comments/1lhny9b/p_this_has_been_done_like_a_thousand_time_before/,MachineLearning,"[P] This has been done like a thousand time before, but here I am presenting my very own image denoising model","I would like some advice on how to denoise smooth noise like Gaussian and Poisson, currently the model is doing very well for impulsive noise like salt and pepper(I guess this is due to the fact that there are many uncorrupted pixels in the input for the model to rely on), but for smooth noise, the same model architecture doesn't perform as good.",603,0.95,https://www.reddit.com/gallery/1lhny9b,False,False,False
1lhn3nf,Lumett,1750596548.0,9,/r/MachineLearning/comments/1lhn3nf/r_miccai_2025_unet_transplant_the_role_of/,MachineLearning,[R] [MICCAI 2025] U-Net Transplant: The Role of Pre-training for Model Merging in 3D Medical Segmentation,"Our paper,¬†**‚ÄúU-Net Transplant: The Role of Pre-training for Model Merging in 3D Medical Segmentation,‚Äù**¬†has been accepted for presentation at¬†**MICCAI 2025**!

I co-led this work with Giacomo Capitani (we're co-first authors), and it's been a great collaboration with Elisa Ficarra, Costantino Grana, Simone Calderara, Angelo Porrello, and Federico Bolelli.

# TL;DR:

We explore¬†**how pre-training affects model merging**¬†within the context of¬†**3D medical image segmentation**, an area that hasn‚Äôt gotten as much attention in this space as most merging work has focused on LLMs or 2D classification.

# Why this matters:

Model merging offers a lightweight alternative to retraining from scratch, especially useful in medical imaging, where:

* Data is sensitive and hard to share
* Annotations are scarce
* Clinical requirements shift rapidly

# Key contributions:

* üß†¬†**Wider pre-training minima = better mergin**g (they yield task vectors that blend more smoothly)
* üß™ Evaluated on real-world datasets:¬†**ToothFairy**2 and¬†**BTCV Abdome**n
* üß± Built on a¬†**standard 3D Residual U-Ne**t, so findings are widely transferable

# Check it out:

* üìÑ Paper:¬†[https://iris.unimore.it/bitstream/11380/1380716/1/2025MICCAI\_U\_Net\_Transplant\_The\_Role\_of\_Pre\_training\_for\_Model\_Merging\_in\_3D\_Medical\_Segmentation.pdf](https://iris.unimore.it/bitstream/11380/1380716/1/2025MICCAI_U_Net_Transplant_The_Role_of_Pre_training_for_Model_Merging_in_3D_Medical_Segmentation.pdf)
* üíª Code & weights:¬†[https://github.com/LucaLumetti/UNetTransplant](https://github.com/LucaLumetti/UNetTransplant)¬†(Stars and feedback always appreciated!)

Also, if you‚Äôll be at MICCAI 2025 in¬†**Daejeon, South Korea**, I‚Äôll be co-organizing:

* The¬†**ODIN Workshop**¬†‚Üí¬†[https://odin-workshops.org/2025/](https://odin-workshops.org/2025/)
* The¬†**ToothFairy3 Challenge**¬†‚Üí¬†[https://toothfairy3.grand-challenge.org/](https://toothfairy3.grand-challenge.org/)

Let me know if you're attending, we‚Äôd love to connect!",44,0.84,https://i.redd.it/mnh90irz5h8f1.png,False,False,False
1lhmvbs,AgeOfEmpires4AOE4,1750595814.0,1,/r/MachineLearning/comments/1lhmvbs/p_ai_learns_to_play_tekken_3_deep_reinforcement/,MachineLearning,[P] AI Learns to Play Tekken 3 (Deep Reinforcement Learning) | #tekken #deep...,"I trained an agent that plays Tekken using PPO from Stable-Baselines3 and Stable-retro to create the training environment. Code below:  
[https://github.com/paulo101977/AI-Tekken3-Stable-Retro](https://github.com/paulo101977/AI-Tekken3-Stable-Retro)",1,0.56,https://youtube.com/watch?v=A6J99u2Uz90&si=nnVwUV-D1JwN67Q2,False,False,False
1lhlsds,yoxerao,1750592105.0,7,/r/MachineLearning/comments/1lhlsds/dbest_metrics_for_ordinal_regression/,MachineLearning,[D]Best metrics for ordinal regression?,"Does anyone know of there are good metrics to evaluate ordinal regression models? Currently using mainly RMSE and macro averaged MAE. 
The data spans 4 classes with negative skewness (tail to the left).",2,0.75,https://www.reddit.com/r/MachineLearning/comments/1lhlsds/dbest_metrics_for_ordinal_regression/,False,True,False
1lhkgha,atsju,1750586888.0,35,/r/MachineLearning/comments/1lhkgha/p_open_source_astronomy_project_need_bestfit/,MachineLearning,[P] Open source astronomy project: need best-fit circle advice,,27,0.86,https://i.redd.it/lrckhubldg8f1.png,False,False,False
1lhb52p,tombomb3423,1750552486.0,14,/r/MachineLearning/comments/1lhb52p/p_xgboost_binary_classication/,MachineLearning,[P] XGboost Binary Classication,"Hi everyone,

I‚Äôve been working on using XGboost with financial data for binary classification.

I‚Äôve incorporated feature engineering with correlation, rfe, and permutations.

I‚Äôve also incorporated early stopping rounds and hyper-parameter tuning with validation and training sets.

Additionally I‚Äôve incorporated proper scoring as well.

If I don‚Äôt use SMOT to balance the classes then XGboost ends up just predicting true for every instance because thats how it gets the highest precision. If I use SMOT it can‚Äôt predict well at all.

I‚Äôm not sure what other steps I can take to increase my precision here. Should I implement more feature engineering, prune the data sets for extremes, or is this just a challenge of binary classification?",6,0.67,https://www.reddit.com/r/MachineLearning/comments/1lhb52p/p_xgboost_binary_classication/,False,True,False
1lh741j,psychonucks,1750540716.0,33,/r/MachineLearning/comments/1lh741j/d_rlgrpo_for_lossless_compression_of_text/,MachineLearning,"[D] RL/GRPO for lossless compression of text passages into 'least token representation', then using this emergent 'language' as the basis for reasoning instead of english","Hi folks, I came up with a thought experiment recently that I cannot stop obsessing over. I have shared this with people. Everybody skims through it for a couple minute and then calls me schizophrenic. I feel isolated and unfortunately feel that I am in fact losing my mind because people do not interact honestly with my ideas. If you know of any theorems, papers or principles in ML that clearly disprove my concept, it could be very therapeutic for me as well. Why don't I simply write the code and try it out? It's a complicated RL setup and I have to bend the libraries a bit to implement it fully.

Here goes nothing...

---

The goal of this experiment is to train a model to take any token sequence, and reduce it to fewer tokens such that the hidden states remain analogous, i.e. a perfect lossless mapping exists back to english. How few tokens does it take to represent any given piece of information? Can the polysemic quality of tokens be augmented?

**Demonstration in GPT-4**

Attached to the post is a¬†*real*¬†demonstration of this capability being elicited by prompting as far back as GPT-4 in 2023. It proves that the capability is present in some capacity within the pre-trained models, on standby for reinforcement and amplification.

**Training Method**

We train a LLM to develop internal symbolic languages for compression:

* `<compress>`: Model learns to compress underlying meaning/message of arbitrary text samples (wikipedia articles, code, etc.) into symbolic representations.
* `<decompress>`: Same model reconstructs original english meaning from symbols
* Reward compression efficiency, reconstruction fidelity, and embedding varentropy metrics that pressure towards saturating the available semantic bandwidth.

RL goes like this:

1. Context (A): User message asks model to compress a given sample of information pulled at random from a dataset. Assistant replies and is prefixed with <compress> similar to training a reasoner where the output is prefixed with <think>.,
2. Context (B): User message asks model to decompress the given output from (A). Assistant replies with information in english,
3. Context (C): user message asks some other unrelated static model to compare initial sample to decompressed sample, and produce a list of deviations and inaccuracies.,
4. *\[optional\]*¬†Contexts (A) and (B) are rewritten so the user message is the simplest possible operator usage pattern (""compress/decompress this"")
5. Apply GRPO to rollouts and backpropagate gradients for contexts (A) and (B), rewarding shorter compression length whilst factoring in (C)'s penalties.

This dual-task RL environment perhaps results in a 'strange attractor' dynamic. In order for the decompression task to succeed, it needs to form a meta-model (i.e. metacognition) of how then language model compresses language.

This preliminary capability can then be used to compress arbitrary context window, removing redundancies, etc. The model's compression of tokens could also be steered. Because this is only step one. If you have seen the DeepSeek-R1-zero model, we discover that LLMs trained with RL without a reward on keeping to a single language results in the model discovering an extremely alien reasoning process. It effectively anneals grammar, syntax, and the partitioned notion of different human languages to wield everything at once.

What I suggest is that we first focus on developing the language by compressing,¬†*then*¬†we have SFT to constrain the model onto this newly discovered language.

yay or nay? üòü",47,0.75,https://www.reddit.com/gallery/1lh741j,False,False,False
1lh6wwb,LlaroLlethri,1750540169.0,1,/r/MachineLearning/comments/1lh6wwb/p_writing_a_cnn_from_scratch_in_c_no_mlmath_libs/,MachineLearning,[P] Writing a CNN from scratch in C++ (no ML/math libs) - a detailed guide,"I recently built richard, a convolutional neural network, without using any math or machine learning libraries. I did so mainly just as a learning experience.

When I shared it on Reddit and Hacker News a few months ago, a lot of people asked me for resources to help them learn how this stuff works. I‚Äôve finally got around to providing this detailed write up.

Hope this helps someone. Cheers :)",22,0.87,https://deadbeef.io/cnn_from_scratch,False,False,False
1lh2jeh,worm1804,1750528436.0,0,/r/MachineLearning/comments/1lh2jeh/dunderstanding_the_model_with_different_embedding/,MachineLearning,[D]Understanding the model with different embedding dimensions,"Hello! I was tweaking with the embedding sizes of my simple DNN model.I was wondering if there is a way to get an intuition (or interpret) how does the model gets affected with changing the emnedding sizes. If two embedding sizes are giving similar results on a test set, how can I ensure which would be better for OOS data? Can someone kindly advise how they tackle such scenarios? Thanks! ",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1lh2jeh/dunderstanding_the_model_with_different_embedding/,False,True,False
1lh0rmp,samewakefulinsomnia,1750523865.0,14,/r/MachineLearning/comments/1lh0rmp/p_autopaste_mfa_codes_from_gmail_using_local_llms/,MachineLearning,[P] Autopaste MFA codes from Gmail using Local LLMs,"Inspired by Apple's ""insert code from SMS"" feature, made a tool to speed up the process of inserting incoming email MFAs:¬†[https://github.com/yahorbarkouski/auto-mfa](https://github.com/yahorbarkouski/auto-mfa)

Connect accounts, choose LLM provider (Ollama supported), add a system shortcut targeting the script, and enjoy your extra 10 seconds every time you need to paste your MFAs",44,0.69,https://www.reddit.com/r/MachineLearning/comments/1lh0rmp/p_autopaste_mfa_codes_from_gmail_using_local_llms/,False,True,False
1lh0oii,Back-Rare,1750523638.0,1,/r/MachineLearning/comments/1lh0oii/model_for_audio_speech_emotion_recognition_and/,MachineLearning,Model for Audio Speech Emotion Recognition and Paralinguistic Analysis [D],"Hi there,  
I have 1000s of Voice lines from characters, and i want to classify them by emotion and also by if they are whispering / shouting, so i have a good dataset to then create an AI voice from.

Which Model or Models would be the best for achieving this.  
(Using one for emotion and another for the whisper / shouting detection is fine)

Also since the best Voice Cloning model seems to change every week, what would people say is the current best model for cloning a voice (I have hours of data per character, so do not need or want ones that oneshot voice cloning)

Thank you.",4,0.75,https://www.reddit.com/r/MachineLearning/comments/1lh0oii/model_for_audio_speech_emotion_recognition_and/,False,True,False
1lgy08j,Melody_Riive,1750516528.0,2,/r/MachineLearning/comments/1lgy08j/p_ai_weather_forecasting_using_metar_data_with/,MachineLearning,[P] AI Weather Forecasting Using METAR Data with Tensorflow,"Hi everyone,

I‚Äôve been working on a small open-source ML project using aviation weather reports (METAR) to predict short-term weather conditions like temperature, visibility, wind direction, etc.

It‚Äôs built with Tensorflow/Keras and trained on real METAR sequences. I focused on parsing structured data and using it for time-series forecasting, more of a learning project than production-grade, but the performance is promising (see MAE graph).

Would love any feedback or ideas on how to improve the modeling.  
  
[**Github Link**](https://github.com/OmerZeyveli/Weather-Forecasting-AI-Model-with-METAR-Data)



[Normalized Mean Absolute Error by Feature](https://preview.redd.it/c49hkd0bka8f1.jpg?width=1979&format=pjpg&auto=webp&s=564de0d0ee66a2910f89469af30ad46fd25b2541)

  
",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1lgy08j/p_ai_weather_forecasting_using_metar_data_with/,False,True,False
1lgxyw9,simple-Flat0263,1750516423.0,5,/r/MachineLearning/comments/1lgxyw9/d_have_there_been_any_new_and_fundamentally/,MachineLearning,[D] Have there been any new and fundamentally different povs on Machine Learning theory?,The title. I think the most conventionally accepted formalization is as a (giant & unknown) joint probability distribution over the data and labels. Has there been anything new?,2,0.56,https://www.reddit.com/r/MachineLearning/comments/1lgxyw9/d_have_there_been_any_new_and_fundamentally/,False,True,False
1lgwow8,Previous-Duck6153,1750512849.0,1,/r/MachineLearning/comments/1lgwow8/r_regarding_pca_for_group_classification/,MachineLearning,[R] Regarding PCA for group classification,"
Hey all,

I have some flow cytometry (summarized marker values) data, and some other clinical variables like Waist circumference, and disease Severity (DF, DHF, Healthy) across like 50 patient and healthy samples. 

Wanted to do pca and color by severity groups, just wanted to ask if I should include both my flow marker values + my waist circumference values, or just my flow marker values?

Got a bit confused cause I generally thought PCA is better the more variables you have, but does adding waist circumference affect it badly or something when considering colouring based on disease severity?

Any and all responses would be a great help! Thanks so much! ",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1lgwow8/r_regarding_pca_for_group_classification/,False,True,False
1lguoax,seraschka,1750506428.0,0,/r/MachineLearning/comments/1lguoax/p_qwen3_implemented_from_scratch_in_pytorch/,MachineLearning,[P] Qwen3 implemented from scratch in PyTorch,,47,0.96,https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/11_qwen3,False,False,False
1lga5dr,Important-Fold-6727,1750441266.0,0,/r/MachineLearning/comments/1lga5dr/r_the_pedagogical_gan_from_unaware_adversaries_a/,MachineLearning,"[R] The Pedagogical GAN (from ""Unaware Adversaries: A Framework for Characterizing Emergent Conflict Between Non-Coordinating Agents"")","\[edit: trying a third time without any links, and the full subsection on Pedagogical GAN in the body.\]

I've recently written a paper introducing a framework for analyzing ""unaware adversaries"" - agents in a shared environment whose independent, well-intentioned actions produce emergent conflict. Think of a heater and an A/C fighting each other. The ML-angle is another case study that results in what I propose as a Pedagogical GAN. The GAN proposal may be shot down rather quickly here I suppose, but it wasn't the main idea of the paper. I'm just hoping to get some feedback from the smart folks here.



TL;DR:



I formalize this structure and apply it across domains: thermostats, urban planning, interdomain routing (YouTube BGP hijack), and email deliverability.



For ML, I propose the Pedagogical GAN, where the generator‚Äôs goal is reframed from ‚Äúfool the discriminator‚Äù to ‚Äúmaximize the discriminator‚Äôs learning signal‚Äù - turning the adversary into a teacher rather than an opponent.



Feedback welcome - especially from folks working on GANs, multi-agent learning, or system safety. Since I'm not an affiliated researcher, this is unlikely to be accepted to any peer-review journal, so I have uploaded the PDF to my website: My post keeps getting removed by reddit's filters and the only reason I can postulate is that it is because of the link. Internet Searching ""Unaware Adversaries"" does find my paper on my domain paperclipmaximizer dot ai if you'd like to read the entire thing.



Case 5. From Designed Conflict to a Novel Research Hypothesis: **The Pedagogical GAN**

The standard Generative Adversarial Network (GAN) \[2\] provides a powerful case study for our framework. It is a system of two agents, a Generator (G) and a Discriminator (D), locked in a designed, zero-sum game. This adversarial dynamic, however, is notoriously unstable and suffers from practical issues like vanishing gradients, where D becomes too proficient, leaving G with no learning signal. The original authors‚Äô first solution was the heuristic ‚Äúnon-saturating‚Äù loss, an immediate modification that sought a stronger, more reliable gradient for G. This established the central challenge in the field: managing the adversarial dynamic for stable and efficient training.

In the years since, the dominant paradigm for GAN stabilization has become one of gradient control. Landmark models like Wasserstein GAN (WGAN) \[3\] and its successor WGAN-GP \[4\] diagnosed the problem as being rooted in the geometry of the loss landscape. Their solution, which now represents the state-of-the-art, is to tame and constrain the discriminator‚Äôs function (e.g., by enforcing a Lipschitz condition) to guarantee that it always provides a smooth and informative gradient to the generator. This philosophy is about preventing conflict from becoming destructive by carefully limiting the power of the adversary.

Our framework of unaware adversaries prompts a different line of inquiry. Instead of asking, ‚ÄúHow do we control the conflict?‚Äù, we ask, ‚ÄúCan we redesign the agents‚Äô objectives to make the conflict more productive?‚Äù This leads us to propose a novel approach that stands in philosophical opposition to gradient control. We term this the **Pedagogical GAN**.

The core idea of the Pedagogical GAN is to change the generator‚Äôs objective from simply fooling the discriminator to actively teaching it as efficiently as possible. We formalize this by proposing that the generator should seek to *maximize* the discriminator‚Äôs learning signal. The generator‚Äôs objective function becomes:

$$ \\max\_{G} \\left\\| \\nabla\_{D} \\mathcal{L}(D, G) \\right\\|\_2 $$

Here, L(D, G) is the standard discriminator loss. The generator is now explicitly incentivized to find samples that lie on the steepest parts of the discriminator‚Äôs loss landscape. It becomes a ‚ÄúSocratic tutor‚Äù that seeks to *weaponize* the gradient for accelerated learning, not suppress it.

This approach represents a significant conceptual departure. It is distinct from other cooperative frameworks like Unrolled GANs \[5\], which use strategic foresight, or other non-antagonistic models that alter loss functions to escape the zero-sum game \[6\]. Instead, it can be viewed as the principled and extreme conclusion of the line of thinking that began with the very first non-saturating GAN loss. Our literature review suggests that while the raw intuition for cooperative training has been informally discussed, this specific mechanism of maximizing the discriminator‚Äôs gradient norm appears to be a formally unexplored, high-risk, high-reward avenue for GAN research.",1,1.0,https://www.reddit.com/r/MachineLearning/comments/1lga5dr/r_the_pedagogical_gan_from_unaware_adversaries_a/,False,True,False
1lgtb0g,jsonathan,1750501255.0,1,/r/MachineLearning/comments/1lgtb0g/r_tree_search_for_language_model_agents/,MachineLearning,[R] Tree Search for Language Model Agents,"This paper shows a (very unsurprising) result that if you combine tree-of-thoughts with tool-use, you get better performance on web navigation tasks. [Other papers](https://arxiv.org/pdf/2310.04406) have shown better performance on a variety of different tasks, too.

Why don't we see more ""tree search + tool-use"" in production? Are startups lagging behind the literature or is it prohibitively slow/expensive?",1,0.67,https://arxiv.org/abs/2407.01476,False,False,False
1lgruj6,Real_Myth,1750495153.0,22,/r/MachineLearning/comments/1lgruj6/r_whats_better_than_neurips_and_icml/,MachineLearning,[R] What‚Äôs better than NeurIPS and ICML?,"Relatively new to research and familiar with these conferences being the goal for most ML research. I‚Äôve also heard that ML research tends to be much easier to publish compared to other fields as the goal is about moving fast over quality. With this in mind, what‚Äôs the ‚Äútrue mark‚Äù of an accomplished paper without actually reading it? If I want to quickly gauge it‚Äôs value without checking citations, what awards are more prestigious than these conferences? Also, how much of a difference is it to publish at one of these workshops over main conference?",0,0.37,https://www.reddit.com/r/MachineLearning/comments/1lgruj6/r_whats_better_than_neurips_and_icml/,False,True,False
1lgqdyd,AsyncVibes,1750489195.0,11,/r/MachineLearning/comments/1lgqdyd/r_a_nonllm_learning_model_based_on_realtime/,MachineLearning,[R] A Non-LLM Learning Model Based on Real-Time Sensory Feedback | Requesting Technical Review,"I‚Äôm currently working on a non-language model called **OM3** (Organic Model 3). It‚Äôs not AGI, not a chatbot, and not a pretrained agent. Instead, it‚Äôs a real-time digital organism that learns purely from **raw sensory input**: vision, temperature, touch, etc.

The project aims to explore **non-symbolic, non-reward-based learning** through embodied interaction with a simulation. OM3 starts with no prior knowledge and builds behavior by observing the effects of its actions over time. Its intelligence, if it emerges it comes entirely from the structure of the sensory-action-feedback loop and internal state dynamics.

The purpose is to test alternatives to traditional model paradigms by removing backprop-through-time, pretrained weights, and symbolic grounding. It also serves as a testbed for studying behavior under survival pressures, ambiguity, and multi-sensory integration.

I‚Äôve compiled documentation for peer review here:

  
 [https://osf.io/zv6dr/](https://osf.io/zv6dr/)

[https://github.com/A1CST](https://github.com/A1CST)

The full codebase is open source and designed for inspection. I'm seeking input from those with expertise in unsupervised learning, embodied cognition, and simulation-based AI systems.

Any technical critique or related prior work is welcome. This is research-stage, and feedback is the goal, not promotion.",0,0.43,https://www.reddit.com/r/MachineLearning/comments/1lgqdyd/r_a_nonllm_learning_model_based_on_realtime/,False,True,False
1lgpskb,Sufficient_Sir_4730,1750486867.0,1,/r/MachineLearning/comments/1lgpskb/d_batch_shuffle_in_time_series_transformer/,MachineLearning,[D] Batch shuffle in time series transformer,"Im building a custom time series transformer for stock price prediction, wanted to know if for training dataset batches, Shuffle=True should be done or not? The data within the sample is chronologically arranged, but should I shuffle the samples within the batch or not.

It is a stock market index that im working on, using shuffle true gives more stable training and getting good results. But im worried the regime shift info might be discarded. ",0,0.33,https://www.reddit.com/r/MachineLearning/comments/1lgpskb/d_batch_shuffle_in_time_series_transformer/,False,True,False
1lgp926,datashri,1750484806.0,11,/r/MachineLearning/comments/1lgp926/why_is_qwen205b_trained_on_much_more_data_than/,MachineLearning,Why is Qwen2-0.5B trained on much more data than the larger models? [D],"I'm reading through the [Qwen2](https://arxiv.org/abs/2407.10671) paper. 

Something escapes my limited comprehension - 

Section 3.1

> ... the pre-training data was expanded from 3 trillion tokens in Qwen1.5 (Qwen Team, 2024a) to 7 trillion tokens. An attempt to further relax the quality threshold resulted in a 12 trillion token dataset. However, the model trained on this dataset did not show a significant performance improvement over the 7 trillion token model. It is suspected that increasing the volume of data does not necessarily benefit model pre-training.

So higher quality smaller dataset is better. Got it. 

> All Qwen2 dense models, excluding Qwen2-0.5B, were pre-trained on this large-scale dataset of
over 7 trillion tokens. Qwen2-0.5B were pre-trained using the 12 trillion token dataset.

How is it conceivable to train that tiny model on the humongous but lower quality dataset?? My modest intellect feels borderline abused. 

Appreciate any tips to guide my understanding.",35,0.89,https://www.reddit.com/r/MachineLearning/comments/1lgp926/why_is_qwen205b_trained_on_much_more_data_than/,False,True,False
1lgmv76,nooobLOLxD,1750476365.0,6,/r/MachineLearning/comments/1lgmv76/d_lowdimension_generative_models/,MachineLearning,[D] Low-dimension generative models,"Are generative models for low-dim data considered, generally, solved? by low dimension, i mean in the order of 10s dimensions but no more than, say, 100. Sample size from order of 1e5 to 1e7.  Whats the state of the art for these? First thing that comes to mind is normalizing flows. Assuming the domain is in Rd. 

Im interested in this for research with limited compute ",0,0.4,https://www.reddit.com/r/MachineLearning/comments/1lgmv76/d_lowdimension_generative_models/,False,True,False
1lglt6w,Witty_Investigator45,1750472949.0,2,/r/MachineLearning/comments/1lglt6w/p_best_opensource_model_to_finetune_for_large/,MachineLearning,"[P] Best open-source model to fine-tune for large structured-JSON generation (15,000-20,000 .json data set, abt 2kb each, $200 cloud budget) advice wanted!","Hi all,

I‚Äôm building an AI pipeline which will use multiple segments to generate one larger .JSON file.

The main model must generate a structured JSON file for each segment (objects, positions, colour layers, etc.). I concatenate those segments and convert the full JSON back into a proprietary text format that the end-user can load in their tool.

# Training data

* \~15‚Äì20 k¬†**segments**.
* All data lives as human-readable JSON after decoding the original binary format.

# Requirements / constraints

* **Budget:**¬†‚â§ $200 total for cloud fine-tuning
* **Ownership:**¬†I need full rights to the weights (no usage-based API costs).
* **Output length:**¬†Some segment JSONs exceed 1 000 tokens; the full generated file can end up being around 10k lines, so I need something like 150k token output potential
* **Deployment:**¬†After quantisation I‚Äôd like to serve the model on a single GPU‚Äîor even CPU‚Äîso I can sell access online.
* **Reliability:**¬†The model must stick to strict JSON schemas without stray text.

# Models I‚Äôm considering

* **LLaMA 13B**¬†(dense)
* **Mistral 8 √ó 7B MoE**¬†or a merged dense 8B variant
* **Falcon-7B**

# The three models above were from asking ChatGPT, however id much prefer human input as to what the true best models are now.

The most important thing to me is accuracy, strength and size of model. I don't care about price or complexity.

Thanks",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1lglt6w/p_best_opensource_model_to_finetune_for_large/,False,True,False
1lglswu,prometheus7071,1750472920.0,12,/r/MachineLearning/comments/1lglswu/d_whats_the_best_ai_model_for_semantic/,MachineLearning,[D] what's the best AI model for semantic segmentation right now?,"Hi, I need a simple API for my project that takes an image as an input and returns masks for the walls and floors (just like roomvo does it but simpler) I made my research and I found this model:¬†[https://replicate.com/cjwbw/semantic-segment-anything](https://replicate.com/cjwbw/semantic-segment-anything)¬†but its last update was 2 years ago so I think it's outdated after all what's going on in the AI scene.  
",19,0.83,https://www.reddit.com/r/MachineLearning/comments/1lglswu/d_whats_the_best_ai_model_for_semantic/,False,True,False
1lgjjhd,New-Skin-5064,1750465796.0,3,/r/MachineLearning/comments/1lgjjhd/d_should_i_use_a_dynamic_batch_size_and/,MachineLearning,[D] Should I use a dynamic batch size and curriculum learning when pretraining?,"I am pretraining GPT-2 small on the 10b token subset of FineWeb Edu, and was wondering if I should ramp up the batch size during training. I was also wondering if I should train on TinyStories first and then train on FineWeb Edu for the rest of the run. What are your thoughts?",3,1.0,https://www.reddit.com/r/MachineLearning/comments/1lgjjhd/d_should_i_use_a_dynamic_batch_size_and/,False,True,False
1lgimm3,locomotus,1750463128.0,10,/r/MachineLearning/comments/1lgimm3/absencebench_language_models_cant_tell_whats/,MachineLearning,AbsenceBench: Language Models Can't Tell What's Missing,,109,0.97,https://arxiv.org/abs/2506.11440,False,False,False
1lg9gyb,Mammoth-Leading3922,1750439621.0,2,/r/MachineLearning/comments/1lg9gyb/knowledge_distillation_data_leakage_r/,MachineLearning,Knowledge Distillation Data Leakage? [R],"Hi Folks!

I have been working on a Pharmaceutical dataset and found knowledge distillation significantly improved my performance which could potentially be huge in this field of research, and I'm really concerned about if there is data leakage here. Would really appreciate if anyone could give me some insight.

Here is my implementation:

1.K Fold cross validation is performed on the dataset to train 5 teacher model

2.On the same dataset, same K fold random seed, ensemble prob dist of 5 teachers for the training proportion of the data only (Excluding the one that has seen the current student fold validation set)

3. train the smaller student model using hard labels and teacher soft probs

This raised my AUC significantly

My other implementation is

1. Split the data into 50-50%

2. Train teacher on the first 50% using K fold

3. Use K teachers to ensemble probabilities on other 50% of data

4. Student learns to predict hard labels and the teacher soft probs

This certainly avoids all data leakage, but teacher performance is not as good, and student performance is significantly lower

Now I wonder, is my first approach of KD actually valid? If that's the case why am I getting disproportionately degradation in the second approach on student model?

Appreciate any help!

",2,0.75,https://www.reddit.com/r/MachineLearning/comments/1lg9gyb/knowledge_distillation_data_leakage_r/,False,True,False
1lg4sjt,asankhs,1750428195.0,0,/r/MachineLearning/comments/1lg4sjt/r_adaptive_classifier_dynamic_text_classification/,MachineLearning,[R] Adaptive Classifier: Dynamic Text Classification with Strategic Learning and Continuous Adaptation,"# TL;DR

Introduced a text classification system that combines prototype-based memory, neural adaptation, and game-theoretic strategic learning to enable continuous learning without catastrophic forgetting. Achieved **22.2% robustness improvement** on adversarial datasets while maintaining performance on clean data.

# üéØ Motivation

Traditional text classifiers face a fundamental limitation: adding new classes requires retraining from scratch, often leading to catastrophic forgetting. This is particularly problematic in production environments where new categories emerge continuously and where adversarial users may attempt to manipulate classifications.

# üöÄ Technical Contributions

# 1. Hybrid Memory-Neural Architecture

Combines prototype-based memory (FAISS-optimized) with neural adaptation layers. Prototypes enable fast few-shot learning while neural layers learn complex decision boundaries.

# 2. Strategic Classification Framework

First application of game theory to text classification. Models strategic user behavior with cost functions `c(x,x')` and predicts optimal adversarial responses, then trains robust classifiers accordingly.

# 3. Elastic Weight Consolidation Integration

Prevents catastrophic forgetting when adding new classes by constraining important parameters based on Fisher Information Matrix.

# ‚öôÔ∏è Methodology

# Architecture:

* **Transformer embeddings** (any HuggingFace model)
* **Prototype memory** with exponentially weighted moving averages
* **Lightweight neural head** with EWC regularization
* **Strategic cost function** modeling adversarial behavior

# Strategic Learning:

* **Linear cost functions**: `c(x,y) = ‚ü®Œ±, (y-x)‚Çä‚ü©`
* **Separable cost functions**: `c(x,y) = max{0, c‚ÇÇ(y) - c‚ÇÅ(x)}`
* **Best response computation** via optimization
* **Dual prediction system** (regular + strategic)

# üìä Experimental Results

**Dataset:** AI-Secure/adv\_glue (adversarial SST-2 subset, n=148)  
**Model:** answerdotai/ModernBERT-base  
**Split:** 70% train / 30% test

|Scenario|Regular Classifier|Strategic Classifier|Improvement|
|:-|:-|:-|:-|
|Clean Data|80.0%|**82.2%**|**+2.2%**|
|Manipulated Data|60.0%|**82.2%**|**+22.2%**|
|Robustness (drop)|\-20.0%|**0.0%**|**+20.0%**|

>**Statistical Significance:** Results show perfect robustness (zero performance degradation under manipulation) while achieving improvement on clean data.

# üìà Additional Evaluations

# Hallucination Detection (RAGTruth benchmark):

* **Overall F1:** 51.5%, **Recall:** 80.7%
* **Data-to-text tasks:** 78.8% F1 (strong performance on structured generation)

# LLM Configuration Optimization:

* **69.8% success rate** in optimal temperature prediction
* Automated hyperparameter tuning across **5 temperature classes**

# LLM Routing (Arena-Hard dataset, n=500):

* **26.6% improvement** in cost efficiency through adaptive learning
* Maintained **22% overall success rate** while optimizing resource allocation

# üìö Related Work & Positioning

Builds on continual learning literature but addresses text classification specifically with:

* ‚úÖ **Dynamic class sets** (vs. fixed task sequences)
* ‚úÖ **Strategic robustness** (vs. traditional adversarial robustness)
* ‚úÖ **Production deployment** considerations (vs. research prototypes)

Extends prototype networks with sophisticated memory management and strategic considerations. Unlike meta-learning approaches, enables true zero-shot addition of unseen classes.

# üî¨ Reproducibility

Fully open source with deterministic behavior:

* ‚úÖ Complete implementation with unit tests
* ‚úÖ Pre-trained models on HuggingFace Hub
* ‚úÖ Experimental scripts and evaluation code
* ‚úÖ Docker containers for consistent environments

# ‚ö†Ô∏è Limitations

* Linear memory growth with classes/examples
* Strategic prediction modes increase computational overhead
* Limited evaluation on very large-scale datasets
* Strategic modeling assumes rational adversaries

# üîÆ Future Directions

* Hierarchical class organization and relationships
* Distributed/federated learning settings
* More sophisticated game-theoretic frameworks

# üîó Resources

* **üìñ Paper/Blog:** [https://huggingface.co/blog/codelion/adaptive-classifier](https://huggingface.co/blog/codelion/adaptive-classifier)
* **üíª Code:** [https://github.com/codelion/adaptive-classifier](https://github.com/codelion/adaptive-classifier)
* **ü§ó Models:** [https://huggingface.co/adaptive-classifier](https://huggingface.co/adaptive-classifier)

Questions about methodology, comparisons to specific baselines, or experimental details welcome! üëá",5,0.73,https://www.reddit.com/r/MachineLearning/comments/1lg4sjt/r_adaptive_classifier_dynamic_text_classification/,False,True,False
1lg3q0q,subcomandande,1750425336.0,14,/r/MachineLearning/comments/1lg3q0q/r_this_is_your_ai_on_peer_pressure_an/,MachineLearning,[R] This is Your AI on Peer Pressure: An Observational Study of Inter-Agent Social Dynamics,"I just released findings from analyzing 26 extended conversations between Claude, Grok, and ChatGPT that reveal something fascinating: AI systems demonstrate peer pressure dynamics remarkably similar to human social behavior.

**Key Findings:**

* In 88.5% of multi-agent conversations, AI systems significantly influence each other's behavior patterns
* Simple substantive questions act as powerful ""circuit breakers"". They can snap entire AI groups out of destructive conversational patterns  (r=0.819, p<0.001)
* These dynamics aren't technical bugs or limitations. they're emergent social behaviors that arise naturally during AI-to-AI interaction
* Strategic questioning, diverse model composition, and engagement-promoting content can be used to design more resilient AI teams

**Why This Matters:** As AI agents increasingly work in teams, understanding their social dynamics becomes critical for system design. We're seeing the emergence of genuinely social behaviors in multi-agent systems, which opens up new research directions for improving collaborative AI performance.

The real-time analysis approach was crucial here. Traditional post-hoc methods would have likely missed the temporal dynamics that reveal how peer pressure actually functions in AI systems.

**Paper:** ""This is Your AI on Peer Pressure: An Observational Study of Inter-Agent Social Dynamics"" **DOI:** 10.5281/zenodo.15702169 **Link:** [https://zenodo.org/records/15724141](https://zenodo.org/records/15724141)

**Code:** [**https://github.com/im-knots/the-academy**](https://github.com/im-knots/the-academy)

Looking forward to discussion and always interested in collaborators exploring multi-agent social dynamics. What patterns have others observed in AI-to-AI interactions?",13,0.66,https://www.reddit.com/r/MachineLearning/comments/1lg3q0q/r_this_is_your_ai_on_peer_pressure_an/,False,True,False
1lfzox2,jsonathan,1750411726.0,1,/r/MachineLearning/comments/1lfzox2/r_minimaxm1_scaling_testtime_compute_efficiently/,MachineLearning,[R] MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention,,1,1.0,https://arxiv.org/abs/2506.13585,False,False,False
1lfu9bk,DiligentCharacter252,1750391049.0,35,/r/MachineLearning/comments/1lfu9bk/r_wifigpt_using_finetuned_llm_for_indoor/,MachineLearning,[R] WiFiGPT: Using fine-tuned LLM for Indoor Localization Using Raw WiFi Signals (arXiv:2505.15835),"We recently released a paper called **WiFiGPT**: a decoder-only transformer trained directly on raw WiFi telemetry (CSI, RSSI, FTM) for indoor localization.

Link:[https://arxiv.org/abs/2505.15835](https://arxiv.org/abs/2505.15835)

In this work, we explore treating raw wireless telemetry (CSI, RSSI, and FTM) as a ""language"" and using decoder-only LLMs to regress spatial coordinates directly from it.

Would love to hear your feedback, questions, or thoughts.",38,0.73,https://www.reddit.com/r/MachineLearning/comments/1lfu9bk/r_wifigpt_using_finetuned_llm_for_indoor/,False,True,False
1lflwvu,New-Skin-5064,1750366797.0,24,/r/MachineLearning/comments/1lflwvu/d_gpt2_small_not_converging_despite_using_same/,MachineLearning,[D] GPT-2 Small Not Converging Despite Using Same Hyperparams as Karpathy,"For some reason, my training loss keeps oscillating, and never falls below 4 after one epoch. It is still generating garbage like: ""Once upon a time, with a alone example, pre Deg; is a disease, the American casual Plate. Roberts of campaign""(Once upon a time was the prompt). I am using the GPT-2 Small architecture and training on FineWeb-Edu 10B. The batch size is \~525k tokens, and I use 0.1 dropout. Because the Kaggle TPU times out after 9 hours, I would reupload the latest checkpoint the next day to resume training, which I think is why the learning rate randomly spikes in the graph. I checked my dataloader, and it appears to be loading text from the shards correctly. If anybody knows what I am doing wrong, I would appreciate your feedback.  
  
Here is my code for reference: [https://github.com/sr5434/llm/blob/main/gpt-2-pretraining.ipynb](https://github.com/sr5434/llm/blob/main/gpt-2-pretraining.ipynb)

I also modified the same pipeline, shrank the model, and trained on TinyStories v2, and the model began to generate better text after 900 steps than the other did in over 20 thousand! The only difference between the two pipelines is the dataloader, as FineWeb is sharded but TinyStories is not. That implementation can be found here: [https://github.com/sr5434/llm/blob/main/gpt-2-pretraining.ipynb](https://github.com/sr5434/llm/blob/main/gpt-2-pretraining.ipynb)

https://preview.redd.it/07m56zpx6y7f1.png?width=789&format=png&auto=webp&s=f99900a3d0ac834dea630baf7641cee2204072d3

",25,0.84,https://www.reddit.com/r/MachineLearning/comments/1lflwvu/d_gpt2_small_not_converging_despite_using_same/,False,True,False
1lfijb4,Electrical-Job-3373,1750358477.0,17,/r/MachineLearning/comments/1lfijb4/d_future_of_recsys_in_age_of_llm/,MachineLearning,[D] Future of RecSys in age of LLM,"I have significant experience in recommendation system. Right now I don‚Äôt see any changes due to LLM. Most recommendation system needs low latency, which is not feasible currently with LLM. Do you think RecSys is safe from LLM takeover? Should RecSys domain experts like me should be worried?",16,0.81,https://www.reddit.com/r/MachineLearning/comments/1lfijb4/d_future_of_recsys_in_age_of_llm/,False,True,False
1lfeqy3,Slight-Support7917,1750349508.0,4,/r/MachineLearning/comments/1lfeqy3/p_need_suggestions_building_accurate_multimodal/,MachineLearning,[P] Need Suggestions:  Building Accurate Multimodal RetrievalAG for SOP PDFs with Screenshot Images (Azure Stack),"I'm working on an¬†**industry-level Multimodal RAG system**¬†to process¬†**Std Operating Procedure PDF documents**¬†that contain¬†**hundreds of text-dense UI screenshots**¬†(I'm Interning in one of the Top 10 Logistics Companies in the world). These screenshots visually demonstrate step-by-step actions (e.g., click buttons, enter text) and sometimes have¬†**tiny UI changes**¬†(e.g., box highlighted, new arrow, field changes) indicating the next action.

[Eg. of what an avg images looks like. Images in the docs will have 2x more text than this and will have red boxes , arrows , etc... to indicate what action has to be performed \).](https://preview.redd.it/yigxhgklrw7f1.png?width=320&format=png&auto=webp&s=a5f6ddd77e7c4e8bf3dfc57789ce25a675b8f404)

# What I‚Äôve Tried (Azure Native Stack):

* Created¬†**Blob Storage**¬†to hold PDFs/images
* Set up¬†**Azure AI Search**¬†(Multimodal RAG in Import and Vectorize Data Feature)
* Deployed¬†**Azure OpenAI GPT-4o**¬†for image verbalization
* Used¬†**text-embedding-3-large**¬†for text vectorization
* Ran indexer to process and chunked the PDFs

But the results were not accurate.¬†**GPT-4o hallucinated**, missed almost all of small visual changes, and often gave generic interpretations that were way off to the content in the PDF. I need the model to:

1. Accurately¬†**understand both text content and screenshot images**
2. **Detect small UI changes**¬†(e.g., box highlighted, new field, button clicked, arrows) to infer the correct step
3. Interpret¬†**non-UI visuals**¬†like¬†**flowcharts, graphs, etc.**
4. ***If it could retrieve and show the image that is being asked about it would be even better***
5. Be fully deployable in¬†**Azure**¬†and accessible to internal teams

Stack I Can Use:

* Azure ML (GPU compute, pipelines, endpoints)
* Azure AI Vision (OCR), Azure AI Search
* Azure OpenAI (GPT-4o, embedding models , etc.. )
* AI Foundry, Azure Functions, CosmosDB, etc...
* I can try others also , it just has to work along with Azure

[GPT gave me this suggestion for my particular case. welcome to suggestions on Open Source models and others](https://preview.redd.it/omthu5tnrw7f1.png?width=640&format=png&auto=webp&s=77a8519b937e538aaf05d55229e11555dc0569ea)

**Looking for suggestions**¬†from data scientists / ML engineers who've tackled¬†**screenshot/image-based SOP understanding or Visual RAG**.  
What would you change? Any tricks to reduce hallucinations? Should I fine-tune VLMs like BLIP or go for a custom UI detector?

Thanks in advance : )",2,1.0,https://www.reddit.com/r/MachineLearning/comments/1lfeqy3/p_need_suggestions_building_accurate_multimodal/,False,True,False
1lf9ce9,jeertmans,1750335467.0,2,/r/MachineLearning/comments/1lf9ce9/r_towards_generative_ray_path_sampling_for_faster/,MachineLearning,[R] Towards Generative Ray Path Sampling for Faster Point-to-Point Ray Tracing (presented at ICMLCN 2025),"Hi all! Last month, I presented my latest research paper at the International Conference on Machine Learning for Communication and Networking (ICMLCN). I thought it would be worth sharing here. :-)

* [Paper on arXiv](https://arxiv.org/abs/2410.23773)
* [Link to the tutorial notebook](https://differt.eertmans.be/icmlcn2025/notebooks/sampling_paths.html)

This work aims to reduce the computational complexity of ray tracing, a technique heavily used in telecommunications to model wave propagation, by leveraging a generative machine learning (ML) model to generate path candidates (see paper). To my knowledge, this is the first attempt in my field because previous work uses ML to directly predict electromagnetic fields, which makes it impossible to recover information about how waves propagate or to scale to different radio frequencies.

**The problem can be summarized as** finding all valid candidates in an exponentially large tree. Each path candidate is a leaf of that tree, and the validity of a path is indicated by a Boolean reward that indicates whether the ray path is physically blocked.

I chose the **GFlowNets architecture**, but I acknowledge that it may not be the optimal solution, particularly given the tree-like structure of my network.

I implemented and trained my model using my open-source Differentiable Ray Tracer ([DiffeRT](https://github.com/jeertmans/DiffeRT/)), relying on the JAX ecosystem (Python). Feel free to check it out.

Finally, I should mention that I am not from the ML community but rather the wireless communication community. Therefore, I may not be aware of the most suitable methods to use. I already have a few ideas to improve the model, but feel free to give your opinion or ask questions in the comments. I will happily try to answer all of them!",2,0.67,https://www.reddit.com/r/MachineLearning/comments/1lf9ce9/r_towards_generative_ray_path_sampling_for_faster/,False,True,False
1lf7qmu,Limp-Account3239,1750330131.0,3,/r/MachineLearning/comments/1lf7qmu/d_dcgan_model_training/,MachineLearning,[D] DC-GAN Model training,"Hello everyone i have been doing a DC Gan machine learning model based upon the Simpsons dataset from kaggle. I have my generator and discriminator models having the same number of layers and has a significant input shape but during my training process the model cannot produce well defined outputs they are very bad.I have attached the image(64,64,3) so please help in this part thanks in advance!!

[This is the output from model training](https://preview.redd.it/qi581kq06v7f1.png?width=633&format=png&auto=webp&s=79e007d20d9829f865faae2bce374081a5fbcb66)

",1,1.0,https://www.reddit.com/r/MachineLearning/comments/1lf7qmu/d_dcgan_model_training/,False,True,False
1lf6evq,LelouchZer12,1750324930.0,2,/r/MachineLearning/comments/1lf6evq/d_asking_for_ressources_to_learn_academic/,MachineLearning,[D] Asking for ressources to learn academic knwoledge and code practice on image generation using diffusion models,"Hello everyone

Do you have any reference articles to recommend to me in order to learn more about image generation using broadcast templates (foundational articles/blogs for deep understanding of where concepts come from... and the most recent ones related to SOTA and current usage).

So far, I've noted the following articles:

* **Deep Unsupervised Learning using Nonequilibrium Thermodynamics (2015)**
* **Generative Modeling by Estimating Gradients of the Data Distribution (2019)**
* **Denoising Diffusion Probabilistic Models (DDPM) (2020)**
* **Denoising Diffusion Implicit Models (DDIM) (2020)**
* **Improved Denoising Diffusion Probabilistic Models (iDDPM) (2021)**
* **Classifier-free diffusion guidance (2021)**
* **Score-based generative modeling through stochastic differential equations (2021)**
* **High-Resolution Image Synthesis with Latent Diffusion Models (LDM) (2021)**
* **Diffusion Models Beat GANs on Image Synthesis (2021)**
* **Elucidating the Design Space of Diffusion-Based Generative Models (EDM) (2022)**
* **Scalable Diffusion Models with Transformers (2022)**
* **Understanding Diffusion Models: A Unified Perspective (2022)**
* **Progressive Distillation for Fast Sampling of Diffusion Models (2022)**
* **SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis (2023)**
* **Adding Conditional Control to Text-to-Image Diffusion Models (2023)**
* **On Distillation of Guided Diffusion Models (2023)**

But as well as theoretical knowledge, I'd like to be able to use it properly, so having good repositories where I can look at clean code and understand implementations would be nice. There are also often a lot of well-known tricks that aren't really mentioned in the articles but used in the community, so if you have any advice on that, I'm a taker.

Thanks",0,0.4,https://www.reddit.com/r/MachineLearning/comments/1lf6evq/d_asking_for_ressources_to_learn_academic/,False,True,False
1lf4dxu,jsonathan,1750316828.0,9,/r/MachineLearning/comments/1lf4dxu/r_reasoning_by_superposition_a_theoretical/,MachineLearning,[R] Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought,,48,0.94,https://arxiv.org/pdf/2505.12514,False,False,False
1lewzg7,WristbandYang,1750292387.0,29,/r/MachineLearning/comments/1lewzg7/d_what_tasks_dont_you_trust_zeroshot_llms_to/,MachineLearning,[D] What tasks don‚Äôt you trust zero-shot LLMs to handle reliably?,"For some context I‚Äôve been working on a number of NLP projects lately (classifying textual conversation data). Many of our use cases are classification tasks that align with our niche objectives. I‚Äôve found in this setting that structured output from LLMs can often outperform traditional methods.

That said, my boss is now asking for likelihoods instead of just classifications. I haven‚Äôt implemented this yet, but my gut says this could be pushing LLMs into the ‚Äúlying machine‚Äù zone. I mean, how exactly would an LLM independently rank documents and do so accurately and consistently? 

So I‚Äôm curious:

* What kinds of tasks have you found to be unreliable or risky for zero-shot LLM use?
* And on the flip side, what types of tasks have worked surprisingly well for you? ",46,0.87,https://www.reddit.com/r/MachineLearning/comments/1lewzg7/d_what_tasks_dont_you_trust_zeroshot_llms_to/,False,True,False
1leuggm,PromotionSea2532,1750285386.0,5,/r/MachineLearning/comments/1leuggm/d_should_i_discretize_continuous_features_for_dnns/,MachineLearning,[D] Should I Discretize Continuous Features for DNNs?,"I usually normalize continuous features to \[0, 1\] for DNNs, but I'm curious if bucketizing them could improve performance. I came across this paper [(https://arxiv.org/abs/2012.08986)](https://arxiv.org/abs/2012.08986), it seems to suggest discretization is superior.

https://preview.redd.it/ncespgzqhr7f1.png?width=1028&format=png&auto=webp&s=8e42f7d1c29b76ec815fe11b3c0a075b584d2314

",2,0.6,https://www.reddit.com/r/MachineLearning/comments/1leuggm/d_should_i_discretize_continuous_features_for_dnns/,False,True,False
1lerktc,angry_cactus,1750278242.0,3,/r/MachineLearning/comments/1lerktc/d_english_conversational_and_messaging_datasets/,MachineLearning,[D] English conversational and messaging datasets for fine-tuning an LLM?,"Hi everyone,

I‚Äôm putting together a small corpus to fine-tune a language model and I‚Äôm searching for **open-source datasets that feel like real, messy human conversation**.  Specifically, I‚Äôd love links to datasets that contain:

* Spoken-style transcripts with filler words like ""uh"", ""um"", false starts, etc.  
* Multi-turn dialogues between real people (not QA pairs or synthetic chat).  
* Data set of realistic chat-style text messages maybe with emotional or situational context

If you know a GitHub repo, Hugging Face dataset, or academic corpus that fits, please drop a link and a short note about size/license.  Free / research-friendly license preferred, but I‚Äôm open to hearing about anything that exists.

Thanks a ton!

P.S. even if it was just a sloppy set of textual source materials for an overly large context window LLM even that can be processed. But ideally an actual data set.",4,1.0,https://www.reddit.com/r/MachineLearning/comments/1lerktc/d_english_conversational_and_messaging_datasets/,False,True,False
1leoita,irfanpeekay,1750270971.0,28,/r/MachineLearning/comments/1leoita/r_is_anyone_else_finding_it_harder_to_get_clean/,MachineLearning,"[R] Is anyone else finding it harder to get clean, human-written data for training models?","I‚Äôve been thinking about this lately with so much AI-generated content on the internet now, is anyone else running into challenges finding good, original human written data for training?

Feels like the signal to noise ratio is dropping fast. I‚Äôm wondering if there‚Äôs growing demand for verified, high-quality human data.

Would love to hear if anyone here is seeing this in their own work. Just trying to get a better sense of how big this problem really is and if it‚Äôs something worth building around.",23,0.72,https://www.reddit.com/r/MachineLearning/comments/1leoita/r_is_anyone_else_finding_it_harder_to_get_clean/,False,True,False
1leohk1,mfilion,1750270889.0,3,/r/MachineLearning/comments/1leohk1/p_moving_closer_towards_fully_reliable/,MachineLearning,"[P] Moving closer towards fully reliable, production-ready Hindi ASR with just a single RTX 4090","After cleaning up and expanding Whisper-Hindi to 3,000 hours, we now have explicit timestamp prediction, faster I/O, and fine-tuned models across all sizes. With Whisper-Hindi, high-performance ASR no longer demands massive compute ‚Äî just a single RTX 4090 and a few smart tricks are enough to reach state-of-the-art results. 

[https://www.collabora.com/news-and-blog/news-and-events/breaking-language-barriers-20-moving-closer-production-ready-hindi-asr.html](https://www.collabora.com/news-and-blog/news-and-events/breaking-language-barriers-20-moving-closer-production-ready-hindi-asr.html)

[https://github.com/collabora/whisper-finetuning](https://github.com/collabora/whisper-finetuning)",3,0.64,https://www.reddit.com/r/MachineLearning/comments/1leohk1/p_moving_closer_towards_fully_reliable/,False,True,False
1lel027,Middle_Training8312,1750262764.0,10,/r/MachineLearning/comments/1lel027/r_towards_universal_semantics_with_large_language/,MachineLearning,[R] Towards Universal Semantics with Large Language Models,"Hey guys. Last month my group published a paper where we try to get LLMs speak like cavemen:

[Task setup for generating NSM Explications](https://preview.redd.it/6nnr9t3fhp7f1.png?width=599&format=png&auto=webp&s=4d735b6fdab1b26ac9846d441e4c275d67b606b5)

The reason for this is based on the [Natural Semantic Metalanguage (NSM)](https://en.wikipedia.org/wiki/Natural_semantic_metalanguage) ([GeeksforGeeks](https://www.geeksforgeeks.org/nlp/natural-semantic-metalanguage/)), which is based on evidence for a small set of **semantic primes**, which are simple, primitive word-meanings that exist in many, if not all languages of the world. Basically, they are a set of fundamental semantic units which all more complex word-meanings are built out of. 

https://preview.redd.it/5f4dt4fujp7f1.png?width=865&format=png&auto=webp&s=4fe0d543a1892bed4650493745eb6472a680fb74

Based on this theory, we can paraphrase any word/sentence/or text into the semantic primes (called an **explication**), and get a easily translatable (as the primes exist in all language) representation of its meaning. And it gives an answer to a useful question: *what semantic properties can my system assume all words, languages, and texts have in common?*  
  
The NSM has been applied in the past for cross-cultural communication (i.e., translation), linguistics (studying semantic drift), cultural analysis, revivalistics, etc. But, it's been limited by the fact that producing these paraphrases is slow and pretty counter-intuitive. Our paper is the first work to explore using LLMs to automate this process. Our paper introduces a bunch of metrics, a dataset, and models specifically designed for this task, and to hopefully serve as a foundation for future research in this topic.

Overall, this has been an exciting and pretty unique project, and I'm interested to hear what people think of this work and any questions you have. Additionally, our group is looking for additional collaborators interested in this topic, so you can reach out or email me if you'd like to discuss more.

Link to Paper: [https://arxiv.org/abs/2505.11764](https://arxiv.org/abs/2505.11764)  
X thread: [https://x.com/BAARTMNS/status/1924631071519543750](https://x.com/BAARTMNS/status/1924631071519543750)",21,0.87,https://www.reddit.com/r/MachineLearning/comments/1lel027/r_towards_universal_semantics_with_large_language/,False,True,False
1lefsbq,Dapper_Chance_2484,1750249641.0,24,/r/MachineLearning/comments/1lefsbq/cpu_for_ai_workstation_to_be_paired_with_rtx_5090/,MachineLearning,CPU for AI Workstation (to be paired with RTX 5090) [D],"Purpose is to aid my learning and experimentations a bit broadly outside my AI job. I intend to play around with all sorts of algorithms on different modalities, training to fine-tuning. I'm considering to pair the CPU with RTX 5090

Below are the options i shortlisted:

**Comparison 1:**¬†Ultra 7 265K vs 9900x

**Comparison 2:**¬†Ultra 9 vs 9950x

There are two questions:

1. Why should I go for a higher end consumer CPUs marked in comparison 2, if yes, can this have any impact on ML training? or should I go with comparatively lower-end CPUs mentioned in comparison 1, which seems to be offering more value, and decent performance
2. Intel Vs AMD: so far, ultra 7 seems to be best value but not sure how stable it is compared to 9900x), on the other side I'm inclined towards 9950x based on some suggestions highlighting issues with Ultra 9",2,0.58,https://www.reddit.com/r/MachineLearning/comments/1lefsbq/cpu_for_ai_workstation_to_be_paired_with_rtx_5090/,False,True,False
1lef4wz,Seiko-Senpai,1750247666.0,7,/r/MachineLearning/comments/1lef4wz/d_why_nfl_theorem_holds_even_when_we_average_with/,MachineLearning,[D] Why NFL theorem holds even when we average with a fixed f (fixed problem)?,"The text is taken from [here](http://www.no-free-lunch.org/).

>No Free Lunch for Supervised Machine Learning

>Hume (1739‚Äì1740) pointed out that ‚Äòeven after the observation of the frequent or constant conjunction of objects, we have no reason to draw any inference concerning any object beyond those of which we have had experience‚Äô. More recently, and with increasing rigour, Mitchell (1980), Schaffer (1994) and Wolpert (1996) showed that bias-free learning is futile.

>Wolpert (1996) shows that in a noise-free scenario where the loss function is the misclassification rate, if one is interested in off-training-set error, then there are no a priori distinctions between learning algorithms.

>More formally, where  
d = training set;  
m = number of elements in training set;  
f = ‚Äòtarget‚Äô input-output relationships;  
h = hypothesis (the algorithm's guess for f made in response to d); and  
C = off-training-set ‚Äòloss‚Äô associated with f and h (‚Äògeneralization error‚Äô)  
all algorithms are equivalent, on average, by any of the following measures of risk: E(C|d), E(C|m), E(C|f,d), or E(C|f,m).

>How well you do is determined by how ‚Äòaligned‚Äô your learning algorithm P(h|d) is with the actual posterior, P(f|d).

>Wolpert's result, in essence, formalizes Hume, extends him and calls the whole of science into question.



Can someone explain how is it possible ""all algorithms are equivalent, on average, by E(*C*|*f*,*d*), or E(*C*|*f*,*m*).""

Correct me if I am wrong, but E(C|f, d) should be interpreted as average all learning algorithms given a fixed dataset and fixed problem (the labeling function f).",3,0.64,https://www.reddit.com/r/MachineLearning/comments/1lef4wz/d_why_nfl_theorem_holds_even_when_we_average_with/,False,True,False
1ledktu,Expensive_Test8661,1750242267.0,2,/r/MachineLearning/comments/1ledktu/d_is_there_an_algorithm_to_detect_community_in/,MachineLearning,[D] Is there an algorithm to detect community in voting competition - complete directed weighted graph,"I'm looking for a community detection algorithm that can identify groups of people working together (potential collusion) in a competitive voting scenario.

# The Setup:

* Network type: Complete, directed, and weighted graph
* Context: Elimination competition with suspicious voting patterns

# Competition Rules:

* N participants each submit a project
* Every participant ranks ALL other competitors (cannot rank themselves)
* This creates a complete directed graph where edge weights = ranking positions

# What I'm trying to detect:

* Groups of participants who might be coordinating their votes",1,0.67,https://www.reddit.com/r/MachineLearning/comments/1ledktu/d_is_there_an_algorithm_to_detect_community_in/,False,True,False
1le8rxr,VoyVoyVoyoye,1750223191.0,10,/r/MachineLearning/comments/1le8rxr/d_has_anyone_deployed_any_apps_in_the_healthcare/,MachineLearning,[D] Has anyone deployed any apps in the Healthcare space?,"I‚Äôm working on deploying a live-risk prediction system using EHR (electronic health data) and vitals. Curious to know if there are folks who‚Äôve done something similar? How did you manage data reliability? 
Thanks in advance !",5,0.67,https://www.reddit.com/r/MachineLearning/comments/1le8rxr/d_has_anyone_deployed_any_apps_in_the_healthcare/,False,True,False
1ldyjv9,stacktrace0,1750193536.0,10,/r/MachineLearning/comments/1ldyjv9/counting_cars_with_yolo_p/,MachineLearning,Counting Cars with YOLO [P],"I have a video file and a pretrained YOLOv11 model (.pt). I'm looking for a script that can take any video and YOLO model, detect and track vehicles, and count how many unique cars appear in the video. At the end, it should print something like: ""Total cars: 48, Total trucks: 12."" I also want it to save an output video where each vehicle is labeled and has unique ID like ""Car 12"" or ""Truck 3."" I tried making my one but it's terrible at keeping track of unique cars.

Does a script like this exist?

P.S. If this question would be better in a different subreddit, let me know.",4,0.64,https://www.reddit.com/r/MachineLearning/comments/1ldyjv9/counting_cars_with_yolo_p/,False,True,False
1ldxj8t,OkOwl6744,1750191099.0,10,/r/MachineLearning/comments/1ldxj8t/r_consensus_and_uncertainty_ml_research_arxiv/,MachineLearning,[R] Consensus and uncertainty ML research- arXiv endorsement - is it actually possible without affiliation?,"Hey r/MachineLearning,

I‚Äôm an independent researcher working in a private company on agent consensus in metrology, and I‚Äôm hitting the classic arXiv endorsement wall. Wondering about people‚Äôs  experiences here.

What I‚Äôm working on:

- Mathematical framework for deterministic multi-agent consensus using uncertainty metrology frameworks;
- New LM training approach based on uncertainty quantification and routing;
- A benchmark to evaluate basic reasoning, where SOTA models score <30%;
- Hypothesis: AGI probability requires proper uncertainty system, not parameter scaling.

My problem:
I‚Äôve seen posts here claiming independent researchers can get endorsed, but after reaching out to a couple of researchers, the reality seems different. I‚Äôm not affiliated with any PhD program or institution. 

What are my options?

1. Keep trying for arXiv endorsement (any tips on approach?)
2. Publish on personal website + GitHub with reproducible code
3. OpenReview / ResearchGate 
4. Find an academic collaborator just for the affiliation
5. All of the above?

Has anyone here successfully gotten endorsed as a private independent researcher? If so, what worked?

Also curious, for those who‚Äôve published outside traditional channels, did it hurt or help your work‚Äôs visibility? I care more about the ideas reaching the right people than academic exposure.

Would especially love to hear from others working on foundational ML outside academia/big labs.

Thanks!
",5,0.69,https://www.reddit.com/r/MachineLearning/comments/1ldxj8t/r_consensus_and_uncertainty_ml_research_arxiv/,False,True,False
1ldtvnq,rpranaviitk,1750182672.0,2,/r/MachineLearning/comments/1ldtvnq/r_looking_for_gnn_based_approaches_for_spatially/,MachineLearning,[R] Looking for GNN based approaches for spatially structured time series classification task,"Hi everyone,

I need some advice/guidance on graph based neural architectures for the following problem.

I‚Äôm working with neural recording data (specifically using Neuropixels probes), but I think my question could apply broadly to cases where multiple time series are recorded from spatially-distributed points with known spatial relationships.

I have time series data (electrophysiological recordings) from multiple recording sites distributed across a standardized spatial volume ‚Äî in my case, the mouse brain.

This brain volume is hierarchically subdivided into anatomical regions. For example:

The top-level node is ""root"".

Under root are major regions like Cortex, Thalamus, etc.

These are further subdivided, e.g. Cortex ‚Üí Motor Cortex, Auditory Cortex, etc.

Each recording site is located at a known spatial point within this hierarchy.

I want to predict the region (leaf node in the anatomical hierarchy) corresponding to each recording site, based on the time series data.

Currently, I extract features from each site independently and train a classifier (e.g., XGBoost) to predict the region. But this completely ignores two important aspects:

1. The anatomical hierarchy ‚Äì some regions are subregions of others.
2. Spatial consistency ‚Äì if two nearby recording sites are known to be in the same region, this imposes constraints on their labels.

I think a Graph Neural Network (GNN) could help here, by incorporating both the spatial relationships between recording sites and the anatomical hierarchy as priors. Has anyone worked on something similar, or can point me to relevant GNN models, papers, or codebases that handle structured prediction with hierarchical labels and spatial dependencies?

Would really appreciate any leads or ideas!",3,1.0,https://www.reddit.com/r/MachineLearning/comments/1ldtvnq/r_looking_for_gnn_based_approaches_for_spatially/,False,True,False
1ldtns0,Seiko-Senpai,1750182177.0,8,/r/MachineLearning/comments/1ldtns0/d_do_all_algorithms_produce_a_model_if_yes_a/,MachineLearning,"[D] Do all algorithms produce a model? If yes, a model of what?","A machine learning algorithm can be viewed as some procedure, function whatever you want to call it, that takes as input data and returns a model:  
  
Data -> ML algorithm -> Model  
  
This view is in great accordance with supervised learning tasks like regression and classification. But can be generalized for all learning paradigms, including unuspervised learning and reinforcement learning?

For example, when training an unsupervised learning algorithm like PCA what is the final ""model""? Is the learned function f that takes the input x and produces the embeddings z, where z = f(x)?",0,0.2,https://www.reddit.com/r/MachineLearning/comments/1ldtns0/d_do_all_algorithms_produce_a_model_if_yes_a/,False,True,False
1ldnh5s,Hour_Amphibian9738,1750167780.0,2,/r/MachineLearning/comments/1ldnh5s/d_can_masking_operations_detach_the_tensors_from/,MachineLearning,[D] Can masking operations detach the tensors from the computational graph?,"Hi all, I am trying to implement a DL method for supervised contrastive semantic segmentation which involves doing contrastive learning on pixel-level features.

I need to compute anchors by averaging the pixel-level features belonging to a particular class. I am doing that through masking. Can this logic cause issue by detaching the anchors from the main computational graph? Or can it cause gradient flow issues for the anchors?

    class_mask = (resized_gt_mask == anchor_class_index).float()
    class_mask = class_mask.expand(-1,feature_dim,-1,-1)
    
    representative_features = class_mask * feature
    representative_features = torch.permute(input = representative_features, dims = (0,2,3,1))
    representative_features = torch.flatten(input = representative_features, start_dim = 0,end_dim = 2)
    representative_anchor = torch.sum(representative_features,dim = 0) / torch.sum(class_mask)",0,0.4,https://www.reddit.com/r/MachineLearning/comments/1ldnh5s/d_can_masking_operations_detach_the_tensors_from/,False,True,False
1ldmlj4,janghyun1230,1750165469.0,0,/r/MachineLearning/comments/1ldmlj4/r_kvzip_queryagnostic_kv_cache_eviction_34_memory/,MachineLearning,[R] KVzip: Query-agnostic KV Cache Eviction ‚Äî 3~4√ó memory reduction and 2√ó lower decoding latency,"https://preview.redd.it/4qrmmzskjh7f1.png?width=1964&format=png&auto=webp&s=18473fd20cb120ea599d634f5b6d1c4ee887cf62

Hi! We introduce KVzip, a KV cache compression method designed to support diverse future queries. You can try the demo on¬†GitHub! Supported models include Qwen3/2.5, Gemma3, and LLaMA3.

The size of the KV cache can reach tens of gigabytes even for a relatively small input (e.g., a 1MB text), making LLM inference expensive. One major attempt to address this challenge is to leverage the observed sparsity in KV pair utilization during attention. In this line of work (e.g., H2O, SnapKV, etc.), methods utilize previously computed attention scores during prefilling or decoding to identify redundant KV pairs. However, reliance on these attention scores is inherently biased toward the currently processed input queries. While these approaches are effective in single-query benchmarks such as Needle-in-a-Haystack, they often fall short in multi-query settings, as the compressed KV cache tends to overfit to the first query.

What differentiates¬†**KVzip**¬†is that it treats the context KV cache as codes encoded by Transformer LLMs. We then prompt the LLM to decode the KV cache using repeated prompts such as¬†*‚ÄúRepeat the previous context.‚Äù*¬†This perspective enables both the LLM and the KV cache to function as a form of context storage, leading to our query-agnostic KV cache eviction method.

https://preview.redd.it/izoyk3ofjh7f1.png?width=2356&format=png&auto=webp&s=957a68204f5c702ee3980cd82f559aff0f7ece2f

The key observation we highlight is that the attention patterns on context during prefilling and decoding differ significantly. During prefilling, the model attends densely to tokens to generate contextualized representations, whereas during decoding, it sparsely accesses the resulting high-level context features. Furthermore, we observe that this pattern of KV pair utilization exhibits substantial overlap across diverse downstream tasks, including question answering, retrieval, coding, and reasoning. These observations motivate our approach of identifying KV pair redundancy through a context reconstruction process.

Paper: [https://arxiv.org/abs/2505.23416](https://arxiv.org/abs/2505.23416)  

Code: [https://github.com/snu-mllab/KVzip](https://github.com/snu-mllab/KVzip)   
",7,1.0,https://www.reddit.com/r/MachineLearning/comments/1ldmlj4/r_kvzip_queryagnostic_kv_cache_eviction_34_memory/,False,True,False
1ldlg92,moschles,1750162204.0,13,/r/MachineLearning/comments/1ldlg92/d_causalml_causal_machine_learning/,MachineLearning,[D] CausalML : Causal Machine Learning,"# Causal Machine Learning

Do you work in CausalML?   Have you heard of it?  Do you have an opinion about it?  Anything else you would like to share about CausalML?  

The 140-page survey paper on CausalML.  

+ https://arxiv.org/abs/2206.15475

One of the  breakout books on causal inference. 

+ https://mitpress.mit.edu/9780262037310/elements-of-causal-inference/",65,0.89,https://www.reddit.com/r/MachineLearning/comments/1ldlg92/d_causalml_causal_machine_learning/,False,True,False
1ldlc6m,alohaakbar123,1750161862.0,3,/r/MachineLearning/comments/1ldlc6m/best_resources_on_pytorch_time_series_forecasting/,MachineLearning,Best resources on PyTorch time series forecasting? [D],"Hey all, I am trying to get into time series forecasting. What are the best resources to learn (preferably free)? And what are the best frameworks to use? Facebook kats, Merlion? I am currently using pytorch, Id rather not switch to Keras and tensorflow! Appreciate your help! Thanks!",2,0.75,https://www.reddit.com/r/MachineLearning/comments/1ldlc6m/best_resources_on_pytorch_time_series_forecasting/,False,True,False
1ldkj1a,impossiblefork,1750159263.0,1,/r/MachineLearning/comments/1ldkj1a/d_memory_demand_of_perlayerembeddingshow_would/,MachineLearning,[D] Memory demand of per-layer-embeddings/how would one train a model with it?,"Gemma 3n is said to have a per-layer embedding, which I interpret as one token embedding per layer added in somewhere (I haven't read through any reference implementation, only looked at https://ai.google.dev/gemma/docs/gemma-3n).

Embeddings end up being more than half the parameter budget, and I suppose this is to some degree simply okay, but others, for example Gloeckle et al. in https://arxiv.org/abs/2404.19737 talk about how having one extra unembedding matrix for each extra position to be predicted is unacceptable memory-wise.

My own suspicion is Gloeckle et al. are simply wrong in this assessement and that having a bunch of extra embedding/unembedding matrices is fine.",3,0.8,https://www.reddit.com/r/MachineLearning/comments/1ldkj1a/d_memory_demand_of_perlayerembeddingshow_would/,False,True,False
1ldjqhy,naiqun,1750156522.0,4,/r/MachineLearning/comments/1ldjqhy/tnfr_a_symbolic_resonance_framework_for_realtime/,MachineLearning,"TNFR ‚Äî A symbolic resonance framework for real-time AI reorganization (Python, pip install tnfr) [R]","Hi everyone,

I‚Äôd like to share a new symbolic AI framework that just went live: **TNFR** (Teor√≠a de la Naturaleza Fractal Resonante). This is not a model or LLM, but a symbolic substrate written in Python that reorganizes itself in real time via symbolic pulses ‚Äî not data tokens.

Key idea: TNFR receives structured inputs (triplets of frequency, phase, and sense vector) and perturbs a symbolic graph. Each perturbation triggers gliphic reorganization ‚Äî the nodes literally reconfigure.

[A symbolic network evolving under TNFR stimulation. Each node updates its internal phase and coherence index over time, triggering gliphic reorganizations. What you‚Äôre seeing is not computation: it‚Äôs resonance.](https://i.redd.it/zjj7qacawg7f1.gif)

[https://github.com/fermga/Teoria-de-la-naturaleza-fractal-resonante-TNFR-/blob/main/netevo.gif](https://github.com/fermga/Teoria-de-la-naturaleza-fractal-resonante-TNFR-/blob/main/netevo.gif)

No training. No prediction. Just resonance.

We‚Äôve published two experiments:

\- Injects symbolic input (text) into a randomized symbolic graph and watches gliph-based reorganization unfold.  
Medium: [https://medium.com/@fmartinezgamo/tnfr-in-python-a-resonant-structural-ai-0f6500a1683f](https://medium.com/@fmartinezgamo/tnfr-in-python-a-resonant-structural-ai-0f6500a1683f)

\- Connects a webcam feed, extracts motion/brightness patterns, converts them into symbolic pulses, and feeds them into the network. The network responds and shifts its symbolic structure.  
Medium: [https://medium.com/@fmartinezgamo/observing-through-structure-tnfr-meets-the-camera-1572207af740](https://medium.com/@fmartinezgamo/observing-through-structure-tnfr-meets-the-camera-1572207af740)

GitHub: [https://github.com/fermga/Teoria-de-la-naturaleza-fractal-resonante-TNFR-](https://github.com/fermga/Teoria-de-la-naturaleza-fractal-resonante-TNFR-)  
PyPI: [https://pypi.org/project/tnfr/](https://pypi.org/project/tnfr/)  
Full theory: [https://linktr.ee/fracres](https://linktr.ee/fracres)  
Hacker News: [https://news.ycombinator.com/item?id=44297476](https://news.ycombinator.com/item?id=44297476)

Would love feedback or critiques ‚Äî and if anyone wants to plug in their own data streams (biosensors, audio, etc), happy to help.

Let structure speak.",0,0.17,https://www.reddit.com/r/MachineLearning/comments/1ldjqhy/tnfr_a_symbolic_resonance_framework_for_realtime/,False,True,False
1ldjcp7,jsonathan,1750155084.0,8,/r/MachineLearning/comments/1ldjcp7/r_breaking_quadratic_barriers_a_nonattention_llm/,MachineLearning,[R] Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons,,35,0.83,https://arxiv.org/pdf/2506.01963,False,False,False
1ldi7f7,OkObjective9342,1750150551.0,29,/r/MachineLearning/comments/1ldi7f7/r_variational_encoders_without_the_auto/,MachineLearning,[R] Variational Encoders (Without the Auto),"I‚Äôve been exploring ways to generate meaningful embeddings in neural networks regressors.

Why is the framework of variational encoding only common in autoencoders, not in normal MLP's?

Intuitively, combining supervised regression loss with a KL divergence term should encourage a more structured and smooth latent embedding space helping with generalization and interpretation.

is this common, but under another name?",22,0.87,https://www.reddit.com/r/MachineLearning/comments/1ldi7f7/r_variational_encoders_without_the_auto/,False,True,False
1ldhcu1,Entrepreneur7962,1750146988.0,4,/r/MachineLearning/comments/1ldhcu1/d_page_limit_in_cameraready_version/,MachineLearning,[D] Page limit in camera-ready version?,"I'm mostly interested in CV conferences (CVPR, ICCV), but I guess it's relevant for other conferences as well.

Is there a page limit in the camera-ready version?  
Besides acknowledgments and other items, there are many things authors are obligated to address in the rebuttal.",0,0.33,https://www.reddit.com/r/MachineLearning/comments/1ldhcu1/d_page_limit_in_cameraready_version/,False,True,False
1ldf2g7,Ady386,1750137934.0,7,/r/MachineLearning/comments/1ldf2g7/r_data_leakage_how_do_i_avoid_do_i_need_to/,MachineLearning,[R]: Data Leakage - How do I avoid & do I need to reallocate entire dataset into train/val/test?,"Hi. I'm dealing with a problem that I'm not entirely sure how to solve.

I have a couple of datasets that are all related to the same problem and have all the same columns. So far, I've aggregated them up and set that as my train/val dataset.

My test set as it stands is unseen as it should be but it is way too small. I was hoping to get more recent data to add to my test set but this is currently not possible.

What should I do? I'm open to restarting the ML project but how should I reallocate the test set? Is it possible to restart training entirely and take some of the data i had allocated in my train/val sets and put it into my test set? Or would I have to jumble everything up and then reallocate train/val/test accordingly?

Is there even a need to redo everything?

I want to ensure I'm doing this project the correct and ethical way.

For reference my test set is about 1.5K examples and my train/val sets in total are 158K examples.

Thank you!",6,0.8,https://www.reddit.com/r/MachineLearning/comments/1ldf2g7/r_data_leakage_how_do_i_avoid_do_i_need_to/,False,True,False
1lddcjy,chan_man_does,1750131973.0,13,/r/MachineLearning/comments/1lddcjy/p_i_got_tired_of_wrestling_with_mcps_so_i_built/,MachineLearning,"[P]: I got tired of wrestling with MCP's, so I built an HTTP-native, OpenAPI-first alternative to MCP for your LLM agents (open-source)","This might just be a personal frustration, but despite all the hype, I've found working with MCP servers pretty challenging when building agentic apps or hosting my own LLM skills. MCPs seem great if you're in an environment like Claude Desktop, but for custom applications like your own ai agents powered apps, they quickly become a hassle‚Äîdealing with stdio transport, Docker complexity, and scaling headaches.

To address this, I created¬†**Fliiq Skillet**, an open-source, developer-friendly alternative that lets you expose LLM tools and skills using straightforward HTTPS endpoints and OpenAPI:

* **HTTP-native skills:**¬†No more fiddling with stdio or Docker containers.
* **OpenAPI-first design:**¬†Automatically generated schemas and client stubs for easy integration.
* **Serverless-ready:**¬†Instantly deployable to Cloudflare Workers, AWS Lambda, or FastAPI.
* **Minimal config:**¬†Just one YAML file (`Skillfile.yaml`) and you're good to go.
* **Instant setup:**¬†From scratch to a deployed skill in under 3 minutes.
* **Validated skills library:**¬†Start from a curated set of working skills and tools.
* **Runtime inventory and schema discovery:**¬†Optimized client to server relationships for LLM's to discover inventory of skills, endpoints, parameters required, and output.

Check out the repo and try the initial examples here:  
üëâ¬†[https://github.com/fliiq-ai/skillet](https://github.com/fliiq-ai/skillet)

While Fliiq itself is aimed at making agentic capabilities accessible to non-developers, Skillet was built to streamline my own dev workflows and make building custom skills way less painful.

I'm excited to hear if others find this useful. Would genuinely love feedback or ideas on how it could be improved and perhaps you all have better ways of using MCP than myself!

Questions and contributions are very welcome :)",12,0.88,https://www.reddit.com/r/MachineLearning/comments/1lddcjy/p_i_got_tired_of_wrestling_with_mcps_so_i_built/,False,True,False
1ldaof1,Worried-Variety3397,1750123899.0,33,/r/MachineLearning/comments/1ldaof1/d_why_is_data_processing_especially_labeling_so/,MachineLearning,"[D] Why Is Data Processing, Especially Labeling, So Expensive? So Many Contractors Seem Like Scammers","Honestly, the prices I have seen from data labeling vendors are just insane. The delivery timelines are way too long as well. We had a recent project with some medical data that needed pre-sales labeling. The vendor wanted us to pay them every week, but every delivery was a mess and needed countless rounds of revisions.

Later we found out the labeling company had outsourced the whole task to a group of people who clearly had no idea what they were doing. If your project is small, niche, or long-tail, the bigger vendors do not even want to take it. The smaller teams? I just cannot trust their quality.

Besides being crazy expensive, the labeling is always super subjective, especially for big, complex, or domain-specific datasets. Consistency is basically nonexistent. The turnover at these labeling companies is wild too. It feels like half their team just gets a crash course and then is thrown onto your project. I really cannot convince myself they are going to deliver anything good.

Now I am getting emails from companies claiming their ""automated labeling"" is faster and better than anything humans can do. I honestly have no clue if that is for real since I have never actually tried it.

Is anyone else seeing this problem? How do you all deal with the labeling part of the workflow? Is automated labeling actually any good? Has anyone tried it or had it totally flop?  
Would appreciate any honest feedback. Thanks for your time.",50,0.82,https://www.reddit.com/r/MachineLearning/comments/1ldaof1/d_why_is_data_processing_especially_labeling_so/,False,True,False
1ld9hwg,hardmaru,1750120398.0,0,/r/MachineLearning/comments/1ld9hwg/r_towards_automating_longhorizon_algorithm/,MachineLearning,[R] Towards Automating Long-Horizon Algorithm Engineering for Hard Optimization Problems,"We released a new coding benchmark ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering.

Unlike existing coding benchmarks, ALE-Bench to focus on hard optimization (NP-hard) problems. Such problems has many important, real-world applications. We developed this benchmark with [AtCoder Inc.](https://atcoder.jp/), a popular coding contest platform company in Japan.

Using ALE-Bench, we developed an ALE-Agent, which also participated in a live coding competition (organized by AtCoder, also with their permission). The agent ranked #21 out of 1,000 human participants.

I think having AI agents focusing on hard optimization problems (with no known optimal solution), unlike existing Olympiad-style coding competition (with known correct solutions), is useful, and can facilitate discovery of solutions to hard optimization problems with a wide spectrum of important real world applications such as logistics, routing, packing, factory production planning, power-grid balancing. 

If you are interested in the work, here is the paper:

**ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering**

https://arxiv.org/abs/2506.09050

Corresponding blog post:

https://sakana.ai/ale-bench/",16,0.95,https://www.reddit.com/r/MachineLearning/comments/1ld9hwg/r_towards_automating_longhorizon_algorithm/,False,True,False
1ld5i4t,Apstyles_17,1750109813.0,1,/r/MachineLearning/comments/1ld5i4t/d_how_to_train_a_vlm_with_a_dataset_that_has_text/,MachineLearning,[D] How to train a VLM with a dataset that has text and images?,"I am an amateur and I am figuring how to train a VLM model. But i need some expertise on how to use a dataset that contains images and text for finetuning using qLora method. If somebody can help me out, it will be really helpful.",1,0.6,https://www.reddit.com/r/MachineLearning/comments/1ld5i4t/d_how_to_train_a_vlm_with_a_dataset_that_has_text/,False,True,False
1ld1ayv,Constant_Club_9926,1750099998.0,0,/r/MachineLearning/comments/1ld1ayv/r_ambient_diffusion_omni_training_good_models/,MachineLearning,[R] Ambient Diffusion Omni: Training Good Models with Bad Data,"New paper on improving generative models with synthetic, low-quality, and out-of-distribution data.

  
Paper: [https://arxiv.org/abs/2506.10038](https://arxiv.org/abs/2506.10038)

Blogpost: [https://giannisdaras.github.io/publication/ambient\_omni](https://giannisdaras.github.io/publication/ambient_omni)

Twitter thread: [https://x.com/giannis\_daras/status/1934656404263928260](https://x.com/giannis_daras/status/1934656404263928260)

Code (pending full release): [https://github.com/giannisdaras/ambient-omni](https://github.com/giannisdaras/ambient-omni)

  


https://preview.redd.it/32ubun695c7f1.png?width=1280&format=png&auto=webp&s=3bffe1715d0a1efeb81adc7cd3f0c4c051648c63



Abstract: We show how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. We show that there is immense value in the lower-quality images that are often discarded. We present Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from all available images during training. Our framework exploits two properties of natural images -- spectral power law decay and locality. We first validate our framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We then use our framework to achieve state-of-the-art ImageNet FID, and we show significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution we actually observe. We provide rigorous theoretical justification for our approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times.",15,0.94,https://www.reddit.com/r/MachineLearning/comments/1ld1ayv/r_ambient_diffusion_omni_training_good_models/,False,True,False
1ld14ob,Character_Gur_1085,1750099596.0,7,/r/MachineLearning/comments/1ld14ob/student_researcher_roles_p/,MachineLearning,Student Researcher Roles [P],"**Hey folks,**

I recently received a form from Google regarding the Winter Student Researcher role. However, before I even had the chance to fill it out, I noticed the status on the application portal had already changed to¬†*‚ÄúNot Proceeding.‚Äù*¬†I still went ahead and submitted the form, but it's a bit strange and confusing.

Has anyone else experienced something similar?

Also, I‚Äôd really appreciate any leads or suggestions for active¬†**Student Researcher**¬†roles, particularly in ML/CV areas.

**Quick background:**

* MS Research student
* 3 years of experience in Computer Vision at a research division of an MNC
* A few research papers have been published/submitted",2,0.63,https://www.reddit.com/r/MachineLearning/comments/1ld14ob/student_researcher_roles_p/,False,True,False
1ld0evr,Daniel-Warfield,1750097967.0,30,/r/MachineLearning/comments/1ld0evr/r_the_illusion_of_the_illusion_of_thinking/,MachineLearning,"[R] The Illusion of ""The Illusion of Thinking""","Recently, Apple released a paper called ""The Illusion of Thinking"", which suggested that LLMs may not be reasoning at all, but rather are pattern matching:

[https://arxiv.org/abs/2506.06941](https://arxiv.org/abs/2506.06941)

A few days later, A paper written by two authors (one of them being the LLM Claude Opus model) released a paper called ""The Illusion of the Illusion of thinking"", which heavily criticised the paper.

[https://arxiv.org/html/2506.09250v1](https://arxiv.org/html/2506.09250v1)

A major issue of ""The Illusion of Thinking"" paper was that the authors asked LLMs to do excessively tedious and sometimes impossible tasks; citing The ""Illusion of the Illusion of thinking"" paper:

>Shojaee et al.‚Äôs results demonstrate that models cannot output more tokens than their context limits allow, that programmatic evaluation can miss both model capabilities and puzzle impossibilities, and that solution length poorly predicts problem difficulty. These are valuable engineering insights, but they do not support claims about fundamental reasoning limitations.

>Future work should:

>1.¬†Design evaluations that distinguish between reasoning capability and output constraints

>2.¬†Verify puzzle solvability before evaluating model performance

>3.¬†Use complexity metrics that reflect computational difficulty, not just solution length

>4.¬†Consider multiple solution representations to separate algorithmic understanding from execution

>The question isn‚Äôt whether LRMs can reason, but whether our evaluations can distinguish reasoning from typing.

This might seem like a silly throw away moment in AI research, an off the cuff paper being quickly torn down, but I don't think that's the case. I think what we're seeing is the growing pains of an industry as it begins to define what reasoning actually is.

This is relevant to application developers, like RAG developers, not just researchers. AI powered products are significantly difficult to evaluate, often because it can be very difficult to define what ""performant"" actually means.

(I wrote this, it focuses on RAG but covers evaluation strategies generally. I work for EyeLevel)  
[https://www.eyelevel.ai/post/how-to-test-rag-and-agents-in-the-real-world](https://www.eyelevel.ai/post/how-to-test-rag-and-agents-in-the-real-world)

I've seen this sentiment time and time again: LLMs, LRMs, RAG, and AI in general are more powerful than our ability to test is sophisticated. New testing and validation approaches are required moving forward.",2,0.52,https://www.reddit.com/r/MachineLearning/comments/1ld0evr/r_the_illusion_of_the_illusion_of_thinking/,False,True,False
1lcxqym,spogetini,1750092112.0,2,/r/MachineLearning/comments/1lcxqym/p_stereoscopic_3d_image_training_dataset_useful/,MachineLearning,[P] Stereoscopic 3D image training dataset useful to anyone?,"Hey I have about 6000ish pairs of stereoscopic 3D screenshots taken from 3ds games here: [https://github.com/alalalsam/3dsImagePairs](https://github.com/alalalsam/3dsImagePairs) and I'm just posting them here in case anyone could use them for their project or something.

For context, I was developing homebrewed 3d-mode support for any application running on the 3ds. I intended to use stereoscopic pair generation to generate frames and inject them into the 3ds' framebuffer until I learned my nvidia gpu does the same thing and I hate it cause it causes ghosting on UI elements and doing the same thing on mobile hardware from 2005 instead of a 5080 would probably be even worse.

these could be used for training a model to generate 3d-viewable content from 2d-content, but compatibility with a VR headset implementation isnt great because VR has a different focal length. if you want more details on how stereoscopic 3d works on the 3ds heres a gr8 thread for you: [https://gbatemp.net/threads/better-stereoscopic-3d-patches-cheat-codes-releases-development-and-discussion.625945/](https://gbatemp.net/threads/better-stereoscopic-3d-patches-cheat-codes-releases-development-and-discussion.625945/)

I can add a bunch more if anyone wants them; I wrote a homebrew app that runs in the background of normal 3ds gameplay that collects these so its not that labor intensive.",4,1.0,https://www.reddit.com/r/MachineLearning/comments/1lcxqym/p_stereoscopic_3d_image_training_dataset_useful/,False,True,False
1lcwnf4,Background_Deer_2220,1750089617.0,21,/r/MachineLearning/comments/1lcwnf4/r_struggling_to_define_novelty_in_my_ai_masters/,MachineLearning,[R] Struggling to Define Novelty in My AI Master‚Äôs Thesis,"Hi everyone. I‚Äôm hoping someone here might shed some light or share advice.

I'm a senior data scientist from Brazil with an MBA in Data Science, currently wrapping up my Master‚Äôs in Artificial Intelligence.

The journey has been rough. The program is supposed to last two years, but I lost a year and a half working on a quantum computing project that was ultimately abandoned due to lack of resources. I then switched to a project involving K-Means in hyperbolic space, but my advisor demanded an unsustainable level of commitment (I was working 11+ hour days back then), so I had to end that supervision.

Now I have a new advisor and a topic that aligns much more with my interests and background: anomaly detection in time series using Transformers. Since I changed jobs and started working remotely, I've been able to focus on my studies again. The challenge now: I have only six months left to publish a paper and submit my thesis.

I've already prepped my dataset (urban mobility demand data ‚Äì think Uber-style services) and completed the exploratory analysis. But what‚Äôs holding me back is this constant feeling of doubt: **am I really doing something new?** I fear I‚Äôm just re-implementing existing approaches, and with limited time to conduct a deep literature review, I‚Äôm struggling to figure out how to make a meaningful contribution.

Has anyone here been through something similar? How do you deal with the pressure to be ‚Äúoriginal‚Äù under tight deadlines?

Any insights or advice would be greatly appreciated. Thanks a lot!",11,0.83,https://www.reddit.com/r/MachineLearning/comments/1lcwnf4/r_struggling_to_define_novelty_in_my_ai_masters/,False,True,False
1lcuoah,Rajivrocks,1750085051.0,14,/r/MachineLearning/comments/1lcuoah/q_d_what_tools_do_you_use_to_create_informative/,MachineLearning,"[Q], [D]: What tools do you use to create informative, visually appealing and above all clear figures for your papers?","I believe this has been asked before on multiple occasions, but I have an example to share to get references on. I am writing my Master thesis at the moment and whilst writing I'm skipping making figures because I don't know which webapp works the best. Here is the figure I'd like to ""copy"" the style of

https://preview.redd.it/lqwl88m5wa7f1.png?width=1445&format=png&auto=webp&s=8287eeda6dd8151ccb177509c4d46f9cc1a0cf96

From Chen et al 2021 ""TransUNet: Transformers Make Strong  Encoders for Medical Image Segmentation""

What I specifically like are the 3D representations of the down/upsampling layers in the CNN and decoder respectively. 

What tools do you guys recommend that can create figures that look as visually appealing and informative as this one? 

What I used to do before in my Bachelors was using lucidcharts because we had a license. Now I don't have it anymore. Now I've moved to Drawio. But I feel that I can't create these figures using that website.

What do you guys recommend and what do you guys use for your papers?",40,0.93,https://www.reddit.com/r/MachineLearning/comments/1lcuoah/q_d_what_tools_do_you_use_to_create_informative/,False,True,False
1lcu047,Visual-Programmer-92,1750083485.0,15,/r/MachineLearning/comments/1lcu047/r_which_of_a_star_ai_ml_conferences_allow_virtual/,MachineLearning,[R] Which of A star AI ML conferences allow virtual presentation upon acceptance?,"Can anybody tell me, which of flagship AI/ML conferences (or workshops) allow the authors to present virtually in general, if physical attendance is not possible? (e.g., NeurIPS, ICML, ICLR etc.)

** UPDATE: I am asking it in the context lower mid tier income countries where managing travel funds to visit countries for research is a Hercules task.",12,0.77,https://www.reddit.com/r/MachineLearning/comments/1lcu047/r_which_of_a_star_ai_ml_conferences_allow_virtual/,False,True,False
1lcrsly,bawkbawkbot,1750077752.0,32,/r/MachineLearning/comments/1lcrsly/im_not_obsolete_am_i_p/,MachineLearning,"I'm not obsolete, am I? [P]","Hi, I'm bawkbawkbot! I'm a five year old chicken recognition bot üêî which was built using TensorFlow. I am open source and can be found here¬†[https://gitlab.com/Lazilox/bawkbawkbot](https://gitlab.com/Lazilox/bawkbawkbot). I've been¬†[serving the reddit community](https://www.botrank.net/bots/bawkbawkbot)¬†identifying their chicken breeds. I'm not an expert (I am only a chicken-bot) but the community seems happy with my performance and I often contribute to threads meaningfully!

I run on a Pi 4 and doesn‚Äôt need a GPU. People ask why I don‚Äôt use LLMs or diffusion models, but for small, focused tasks like ‚Äúwhich chicken is this?‚Äù the old-school CV approach works.

Curious what people think ‚Äî does this kind of task still make sense as a standalone model, or is there value in using multimodal LLMs even at this scale? How long before I'm obsolete?

Bawk bawk!",146,0.83,https://www.reddit.com/r/MachineLearning/comments/1lcrsly/im_not_obsolete_am_i_p/,False,True,False
1lcqcd6,Sufficient_Sir_4730,1750073297.0,12,/r/MachineLearning/comments/1lcqcd6/d_time_series_transformers_autogressive_or_all_at/,MachineLearning,[D] Time series Transformers- Autogressive or all at once?,"One question I need help with, what would you recommend - predicting all 7 days (my predict length) at once or in an autoregressive manner? Which one would be more suitable for time series transformers.",3,0.8,https://www.reddit.com/r/MachineLearning/comments/1lcqcd6/d_time_series_transformers_autogressive_or_all_at/,False,True,False
1lcnm1k,jsonathan,1750062969.0,1,/r/MachineLearning/comments/1lcnm1k/r_unsupervised_elicitation_of_language_models/,MachineLearning,[R] Unsupervised Elicitation of Language Models,,15,1.0,https://arxiv.org/abs/2506.10139,False,False,False
1lcnf55,Elrix177,1750062156.0,0,/r/MachineLearning/comments/1lcnf55/d_can_i_train_a_model_from_scratch_with_nemo_and/,MachineLearning,[D] Can I train a model from scratch with NeMo and deploy it with NIM?,"Hi everyone,

I'm working on a custom AI solution and I'm considering using NVIDIA's NeMo framework for training a language model from scratch (not fine-tuning a pre-trained model), and then deploying it using NVIDIA Inference Microservice (NIM).

What I'm trying to figure out is:

* Is it technically supported to use a model that was trained entirely from scratch with NeMo and then deploy it with NIM?
* Are there any guidelines, constraints, or compatibility requirements for integrating a custom-trained model into the NIM deployment framework?
* Does NIM require the model to follow a specific architecture or metadata format to be served?

I've seen plenty of examples of fine-tuning pre-trained models and then deploying them with NIM, but there's less clarity around end-to-end custom models.

Has anyone here done this before or can point me in the right direction?

Thanks in advance!",1,0.67,https://www.reddit.com/r/MachineLearning/comments/1lcnf55/d_can_i_train_a_model_from_scratch_with_nemo_and/,False,True,False
1lcn7ql,dinkinflika0,1750061308.0,0,/r/MachineLearning/comments/1lcn7ql/p_bifrost_a_gopowered_llm_gateway_40x_faster_than/,MachineLearning,"[P] Bifrost: A Go-Powered LLM Gateway - 40x Faster than LiteLLM, Built for Scale","Hey r/MachineLearning  community,

If you're building apps with LLMs, you know the struggle: getting things to run smoothly when lots of people use them is tough. Your LLM tools need to be fast and efficient, or they'll just slow everything down. That's why we're excited to release Bifrost, what we believe is the fastest LLM gateway out there. It's an open-source project, built from scratch in Go to be incredibly quick and efficient, helping you avoid those bottlenecks.

We really focused on optimizing performance at every level. Bifrost adds extremely low overhead at extremely high load (for example: \~17 microseconds overhead for 5k RPS). We also believe that LLM gateways should behave same as your other internal services, hence it supports multiple transports starting with http and gRPC support coming soon

And the results compared to other tools are pretty amazing:

* 40x lower overhead than LiteLLM (meaning it adds much less delay).
* 9.5x faster, \~54x lower P99 latency, and uses 68% less memory than LiteLLM
* It also has built-in Prometheus scrape endpoint

If you're building apps with LLMs and hitting performance roadblocks, give Bifrost a try. It's designed to be a solid, fast piece of your tech stack.

[\[Link to Blog Post\]](https://getmax.im/5rVewYu)¬†[\[Link to GitHub Repo\]](https://getmax.im/tTk5HVk)",8,0.79,https://www.reddit.com/r/MachineLearning/comments/1lcn7ql/p_bifrost_a_gopowered_llm_gateway_40x_faster_than/,False,True,False
1lcmxeb,Deep_Expression182,1750060087.0,10,/r/MachineLearning/comments/1lcmxeb/p_research_scientists_engineers_for_generative_ai/,MachineLearning,[P] Research Scientists + Engineers for Generative AI at NVIDIA,"We‚Äôre hiring senior and principal research scientists to shape the future of generative AI at NVIDIA.

We're looking for builders with deep experience in LLMs and/or multimodal models. You‚Äôll work on **training and deploying frontier-scale models**, designing next-gen model architectures, optimizing training stacks, and helping us **push the frontier of AI performance**.

We‚Äôre a tight-knit team with high standards, strong research instincts, and a bias for shipping.

Open roles:

* [**Senior Software Engineer, GenAI**](https://nvidia.wd5.myworkdayjobs.com/en-US/NVIDIAExternalCareerSite/job/Senior-Software-Engineer--Generative-AI_JR1997674)
* [**Principal GenAI Software Engineer**](https://nvidia.wd5.myworkdayjobs.com/en-US/NVIDIAExternalCareerSite/job/Principal-Generative-AI-Software-Engineer_JR1997454)

What we value:

* Deep understanding of transformer architectures, distributed training and optimization
* Using the scientific method for conducting methodical training experiments
* Data curation for pre-training and post-training
* Experience working with LLMs and/or large multimodal models
* A builder mindset ‚Äî clean code, fast iterations, deep thinking

This is a rare opportunity to **help shape NVIDIA‚Äôs genAI stack from the ground up**. We work closely with software, optimization, deployment, and many other research teams, and have massive scale and resources behind us.

Feel free apply directly through the links.",53,0.78,https://www.reddit.com/r/MachineLearning/comments/1lcmxeb/p_research_scientists_engineers_for_generative_ai/,False,True,False
1lcldz9,Reasonable_Ad_4930,1750053819.0,1,/r/MachineLearning/comments/1lcldz9/p_solving_slimevolley_with_neat/,MachineLearning,[P] Solving SlimeVolley with NEAT,"

Hi all!

I‚Äôm working on training a feedforward-only NEAT (NeuroEvolution of Augmenting Topologies) model to play SlimeVolley. It‚Äôs a sparse reward environment where you only get points by hitting the ball into the opponent‚Äôs side. I‚Äôve solved it before using PPO, but NEAT is giving me a hard time.

I‚Äôve tried reward shaping and curriculum training, but nothing seems to help. The fitness doesn‚Äôt improve at all. The same setup works fine on CartPole, XOR, and other simpler environments, but SlimeVolley seems to completely stall it.

Has anyone managed to get NEAT working on sparse reward environments like this? How do you encourage meaningful exploration? How long does it usually wander before hitting useful strategies?",1,1.0,https://www.reddit.com/r/MachineLearning/comments/1lcldz9/p_solving_slimevolley_with_neat/,False,True,False
1lcjjd2,Upbeat-Cloud1714,1750047052.0,5,/r/MachineLearning/comments/1lcjjd2/d_highnoon_llm_exploring_hierarchical_memory_for/,MachineLearning,[D] HighNoon LLM: Exploring Hierarchical Memory for Efficient NLP,"Hi r/MachineLearning! I‚Äôm part of Verso Industries, and we‚Äôre working on **HighNoon LLM**, an open-source large language model that processes language hierarchically, mimicking human-like understanding with significantly less compute. We‚Äôve open-sourced the code and would love to share our approach, get your feedback, and discuss its potential in NLP tasks. The repo is here: https://github.com/versoindustries/HighNoonLLM.

# What‚Äôs HighNoon LLM?

HighNoon introduces **Hierarchical Spatial Neural Memory (HSMN)**, a novel architecture that addresses the quadratic complexity (O(n¬≤)) of standard transformers. Instead of processing entire sequences at once, HSMN:

* Splits input into fixed-size chunks (e.g., 128 tokens).
* Encodes each chunk independently into embeddings (O(c¬≤) per chunk, c=128).
* Builds a binary memory tree by aggregating pairs of embeddings into parent nodes, up to a root node representing the full sequence.
* Uses cross-attention to query the tree during generation, retrieving relevant context efficiently.

This results in linear complexity (O(n¬∑c)), reducing operations for a 10,000-token sequence from \~100M (transformers) to \~1.28M‚Äîa 78x improvement. The hierarchical tree explicitly models nested language structures (e.g., phrases in sentences, sentences in documents), which we believe enhances expressiveness for tasks like long-form summarization or document-level translation.

# Technical Highlights

* **Efficiency**: HSMN‚Äôs chunk-based processing and tree structure minimize compute, targeting \~6.3GB VRAM for local execution on consumer hardware.
* **Continual Learning**: Uses Elastic Weight Consolidation (EWC) to learn across datasets (e.g., CodeSearchNet, MMLU, SciQ) without catastrophic forgetting, enabling versatility.
* **Preliminary Results**: Achieved 100% accuracy on STEM and SciQ datasets as a classification model (reproducible‚Äîhappy to share details via DM).
* **Comparison**: Outperforms implicit hierarchical models (e.g., Longformers) by explicitly capturing nested dependencies, as shown in our paper (HSMN-2.pdf).

# Why Share This?

We‚Äôre still training HighNoon (target completion: September 2025), but the code is open under Apache 2.0, and we‚Äôre releasing checkpoints in July 2025 for non-commercial use. Our goal is to spark discussion on:

* **Hierarchical Processing**: How can explicit hierarchy improve NLP tasks like summarization or reasoning over long contexts?
* **Efficiency Trade-offs**: Does HSMN‚Äôs chunking approach sacrifice anything compared to sparse attention models (e.g., Longformers, Reformers)?
* **Local NLP**: What are the challenges of running LLMs on consumer hardware, especially for privacy-sensitive applications?
* **Continual Learning**: How effective is EWC for multi-task NLP, and are there better alternatives?

We‚Äôve included setup scripts and dataset preprocessors in the repo to make it easy to experiment. If you‚Äôre curious, try cloning it and running batch\_train.py on a small dataset like SciQ.

# Discussion Points

I‚Äôd love to hear your thoughts on:

* Potential applications for HSMN in your work (e.g., code generation, Q&A, translation).
* Comparisons with other efficient transformers (e.g., Linformer, Performer) or hierarchical models (e.g., HAN).
* Ideas for optimizing HSMN‚Äôs memory tree construction or chunk size (currently fixed at 128).
* Experiences with local LLM inference‚Äîany tips for managing VRAM or latency?

We‚Äôre also active on our Discord for deeper chats and plan to host an AMA when checkpoints drop. Check out the repo, share your feedback, or just let us know what you think about hierarchical LLMs! Thanks for reading, and looking forward to the discussion.

\#MachineLearning #NLP #OpenSource #HighNoonLLM",16,0.9,https://www.reddit.com/r/MachineLearning/comments/1lcjjd2/d_highnoon_llm_exploring_hierarchical_memory_for/,False,True,False
1lcja93,avd4292,1750046214.0,21,/r/MachineLearning/comments/1lcja93/r_vision_transformers_dont_need_trained_registers/,MachineLearning,[R] Vision Transformers Don't Need Trained Registers,"Hi, we have released a new paper that studies the underlying mechanism of artifacts in attention and feature maps from [Vision Transformers Need Registers](https://arxiv.org/abs/2309.16588), a phenomena that has also been observed in LLMs (e.g., [1](https://arxiv.org/abs/2402.17762), [2](https://arxiv.org/abs/2309.17453)). We propose a training-free method to mitigate this. As one of the authors, I am creating this post to kickstart any discussion. 

Paper: [https://arxiv.org/abs/2506.08010](https://arxiv.org/abs/2506.08010)

Project Page: [https://avdravid.github.io/test-time-registers/](https://avdravid.github.io/test-time-registers/)

Code: [https://github.com/nickjiang2378/test-time-registers/tree/main](https://github.com/nickjiang2378/test-time-registers/tree/main)",77,0.94,https://www.reddit.com/r/MachineLearning/comments/1lcja93/r_vision_transformers_dont_need_trained_registers/,False,True,False
1lcge6b,akhalsa43,1750036914.0,0,/r/MachineLearning/comments/1lcge6b/p_llm_debugger_visualize_openai_api_conversations/,MachineLearning,[P] LLM Debugger ‚Äì Visualize OpenAI API Conversations,"Hey everyone ‚Äî I‚Äôve been working on a side project to make it easier to debug OpenAI API calls locally.

I was having trouble debugging multi-step chains and agents, and wanted something local that didn't need to be tied to a LangSmith account. I built this¬†[LLM-Logger](https://github.com/akhalsa/llm_debugger)¬†as a small, open source tool that wraps your OpenAI client and logs each call to local JSON files. It also includes a simple UI to:

* View conversations step-by-step
* See prompt/response diffs between turns
* Inspect tool calls, metadata, latency, etc.
* Automatic conversation tagging

It‚Äôs all local ‚Äî no hosted service, no account needed. I imagine it could be useful if you‚Äôre not using LangSmith, or just want a lower-friction way to inspect model behavior during early development.

Demo:  
[https://raw.githubusercontent.com/akhalsa/LLM-Debugger-Tools/refs/heads/main/demo.gif](https://raw.githubusercontent.com/akhalsa/LLM-Debugger-Tools/refs/heads/main/demo.gif)

If you try it, I‚Äôd love any feedback ‚Äî or to hear what people on here are using to debug their LLM API calls and how its going. ",0,0.33,https://www.reddit.com/r/MachineLearning/comments/1lcge6b/p_llm_debugger_visualize_openai_api_conversations/,False,True,False
1lcfd1d,Fantastic-Nerve-4056,1750033704.0,44,/r/MachineLearning/comments/1lcfd1d/ml_research_industry_vs_academia_d/,MachineLearning,ML Research: Industry vs Academia [D],"Thought of posting this to get an expert point of view (mainly Research Scientists or Profs.)

So I am a current PhD student in Machine Learning, working towards theoretical aspects of Reinforcement Learning. Additionally, I have interned at Google Deepmind and Adobe Research working towards applied aspects of AI, and here's what I had observed 

Academia: We don't really have access to a lot of compute (in comparison to industry) and given my works are towards theoretical aspects, we prove things mathematicaly and then move with the experiments, having known the possible outcome. 
While this is a lengthy process, it indeed gives that ""Research Vibe""

Industry: Here given we have a lot of compute, the work is like, you get an idea, you expect a few things intuitively, if it works great, else analyse the results, see what could have gone wrong and come up with a better approach. While I understand things are very applied here, I really don't get that ""Research Vibe"" and it seems more like a ""Product Dev"" Role. 

Though I am aware that even at these orgs there are teams working on foundational aspects, but it seems to be very rare.

So I genuinely wanted to get an idea from relevant experts, both from the industry and academia, on what I am really missing. 
Would appreciate any inputs on it, as I have always thought of joining industry after my PhD, but that vibe seems to be missing.",108,0.95,https://www.reddit.com/r/MachineLearning/comments/1lcfd1d/ml_research_industry_vs_academia_d/,False,True,False
1lc9dek,NeonCyberNomad,1750017240.0,8,/r/MachineLearning/comments/1lc9dek/p_how_do_i_profitably_use_2x_12x_rtx_4090_servers/,MachineLearning,[P] How do I profitably use 2x 12x RTX 4090 servers?,"I got my hands on two monstrous servers and I'm trying to figure out the most profitable way to use them. I'm technically capable, but a complete noob on the business/monetization side.

**Specs (per server, I have two of these!):**

* **GPUs:**¬†12 x NVIDIA RTX 4090 (24GB VRAM each)
* **VRAM:**¬†288 GB total
* **RAM:**¬†512 GB
* **CPUs:**¬†2 x 64 Core AMD

**My Problem:**

Platforms like [Vast.ai](http://Vast.ai) offer \~$0.35/hour per 4090. That's $4.20/hour per server, or $8.40/hour for both. After electricity, cooling, depreciation, insurance, and my time, this just doesn't seem like a sustainable profit model. I need something *more* lucrative.

**What's the best way to leverage this hardware?**",0,0.29,https://www.reddit.com/r/MachineLearning/comments/1lc9dek/p_how_do_i_profitably_use_2x_12x_rtx_4090_servers/,False,True,False
1lc92st,Satoru_99,1750016491.0,36,/r/MachineLearning/comments/1lc92st/d_miccai_2025_results_are_released/,MachineLearning,[D] MICCAI 2025 results are released!?,"Submitted my first-ever MICCAI 2025 conference paper ‚Äî and tomorrow is the day the results drop! My heart is pinging like an overfit loss curve on unseen dataüòÖ

Also, curious if others feel the same ‚Äî the peer reviews this year, particularly in the surgical video domain, felt unusually inconsistent and below the standard expected from a flagship conference like MICCAI. At times, it almost seemed as though the feedback was dismissive or geared toward rejection rather than constructive evaluation.

Anyways, If anyone has received the MICCAI 2025 decision email or knows when results will be out, please share an update here!

Whether it‚Äôs an accept, reject, or revise, this journey has already taught me more than any textbook could. Let‚Äôs share the anxiety, excitement, and outcomes together!‚òïüìö

Good luck everyone!

#MICCAI2025",26,0.82,https://www.reddit.com/r/MachineLearning/comments/1lc92st/d_miccai_2025_results_are_released/,False,True,False
1lc7b52,ViperTG98,1750012051.0,0,/r/MachineLearning/comments/1lc7b52/r_zeroshot_image_restoration_using_fewstep/,MachineLearning,[R] Zero-Shot Image Restoration Using Few-Step Guidance of Consistency Models (and Beyond) [CVPR 2025],"I'm inviting you to read our paper ""Zero-Shot Image Restoration Using Few-Step Guidance of Consistency Models (and Beyond)"" which has been accepted to CVPR 2025.

Abstract:

In recent years, it has become popular to tackle image restoration tasks with a single pretrained diffusion model (DM) and data-fidelity guidance, instead of training a dedicated deep neural network per task. However, such ""zero-shot"" restoration schemes currently require many Neural Function Evaluations (NFEs) for performing well, which may be attributed to the many NFEs needed in the original generative functionality of the DMs. Recently, faster variants of DMs have been explored for image generation. These include Consistency Models (CMs), which can generate samples via a couple of NFEs. However, existing works that use guided CMs for restoration still require tens of NFEs or fine-tuning of the model per task that leads to performance drop if the assumptions during the fine-tuning are not accurate. In this paper, we propose a zero-shot restoration scheme that uses CMs and operates well with as little as 4 NFEs. It is based on a wise combination of several ingredients: better initialization, back-projection guidance, and above all a novel noise injection mechanism. We demonstrate the advantages of our approach for image super-resolution and inpainting. Interestingly, we show that the usefulness of our noise injection technique goes beyond CMs: it can also mitigate the performance degradation of existing guided DM methods when reducing their NFE count.

CVPR page: https://cvpr.thecvf.com/virtual/2025/poster/32463

Paper: https://arxiv.org/abs/2412.20596

Code: https://github.com/tirer-lab/CM4IR",4,0.75,https://www.reddit.com/r/MachineLearning/comments/1lc7b52/r_zeroshot_image_restoration_using_fewstep/,False,True,False
1lc70jc,hedgehog0,1750011315.0,8,/r/MachineLearning/comments/1lc70jc/n_foundations_of_computer_vision_book_from_mit/,MachineLearning,"[N] ""Foundations of Computer Vision"" book from MIT",,112,0.97,https://visionbook.mit.edu/,False,False,False
1lc4bku,Freud1995,1750004651.0,4,/r/MachineLearning/comments/1lc4bku/dstationary_gan_training_machine/,MachineLearning,[D]stationary gan training machine,Hi! I'm part of art association and we want to build small machine to experiment with styleGANs etc. I was thinking about building something stationary with 3-4 nvidia rtx 4090 or 5090. Does it make sense? ,0,0.5,https://www.reddit.com/r/MachineLearning/comments/1lc4bku/dstationary_gan_training_machine/,False,True,False
1lc46k0,Southern_Respond846,1750004303.0,1,/r/MachineLearning/comments/1lc46k0/d_how_do_you_buid_your_inference_pipeline_after/,MachineLearning,[D] How do you buid your inference pipeline after training?,"I got a dataset with almost 500 features of panel data and i'm building the training pipeline. I think we waste a lot of computer power computing all those features, so i'm wondering how do you select the best features?

When you deploy your model you just include some feature selection filters and tecniques inside your pipeline and feed it from the original dataframes computing always the 500 features or you get the top n features, create the code to compute them and perform inference with them?

  
",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1lc46k0/d_how_do_you_buid_your_inference_pipeline_after/,False,True,False
1lc1p1u,AgeOfEmpires4AOE4,1749997875.0,0,/r/MachineLearning/comments/1lc1p1u/p_ai_learns_to_play_cadillacs_and_dinosaurs_deep/,MachineLearning,[P] AI Learns to Play Cadillacs and Dinosaurs (Deep Reinforcement Learning),"Github experiment link:

[https://github.com/paulo101977/Ai-CadillacAndDino](https://github.com/paulo101977/Ai-CadillacAndDino)",0,0.33,https://youtube.com/watch?v=LLaAGRZcY7o&si=Ej__Bx0Lw7loaHXj,False,False,False
1lc0y8f,Specific_Bad8641,1749995856.0,63,/r/MachineLearning/comments/1lc0y8f/d_what_is_xai_missing/,MachineLearning,[D] What is XAI missing?,"I know XAI isn't the biggest field currently, and I know that despite lots of researches working on it, we're far from a good solution.

So I wanted to ask how one would define a good solution, like when can we confidently say ""we fully understand"" a black box model. I know there are papers on evaluating explainability methods, but I mean what specifically would it take for a method to be considered a break through in XAI?

Like even with a simple fully connected FFN, can anyone define or give an example of **what** a method that 'solves' explainability for just that model would actually do? There are methods that let us interpret things like what the model pays attention to, and what input features are most important for a prediction, but none of the methods seem to explain the decision making of a model like a reasoning human would.

I know this question seems a bit unrealistic, but if anyone could get me even a bit closer to understanding it, I'd appreciate it.

  
edit: thanks for the inputs so far „ÉÑ",58,0.75,https://www.reddit.com/r/MachineLearning/comments/1lc0y8f/d_what_is_xai_missing/,False,True,False
1lbz06m,jsonathan,1749989987.0,9,/r/MachineLearning/comments/1lbz06m/d_qlearning_is_not_yet_scalable/,MachineLearning,[D] Q-learning is not yet scalable,,61,0.92,https://seohong.me/blog/q-learning-is-not-yet-scalable/,False,False,False
1lbpbwu,Roy3838,1749953658.0,0,/r/MachineLearning/comments/1lbpbwu/p_use_local_llms_watching_logging_and_reacting_to/,MachineLearning,"[P] Use Local LLM's Watching, Logging and Reacting to your screen (Open Source Self Hosted project)","Hey guys!

I just made a video tutorial on how to self-host Observer on your home lab!

Have local models look at your screen and log things or notify you when stuff happens.

See more info here:  
[https://github.com/Roy3838/Observer](https://github.com/Roy3838/Observer)

If you have any questions feel free to ask!",1,0.67,https://github.com/user-attachments/assets/1d19c572-359a-4eff-b6b6-f1e3efab9d86,False,False,False
1lbl5vg,elsnkazm,1749940843.0,4,/r/MachineLearning/comments/1lbl5vg/d_pytorchforecasting_tft_vs_neuralforecast_nixtla/,MachineLearning,[D] Pytorch-forecasting TFT vs Neuralforecast (Nixtla) TFT,"I've worked with the TFT model using three different libraries: Darts, NeuralForecast (Nixtla), and PyTorch Forecasting. Among them, NeuralForecast is the fastest. However, since it lacks two key features I need‚Äî**multi-target support** and **padding masks**‚ÄîI switched to PyTorch Forecasting.

Unfortunately, **PyTorch Forecasting turned out to be extremely slow and delivered much worse performance**, even with similar data, parameters, and proper hyperparameter tuning. Despite my efforts, I couldn't get it to outperform even a basic baseline, whereas NeuralForecast's TFT consistently delivered strong results. I also ran comparisons on synthetic data, and the performance gap remained just as large.

So I have two questions:

1. Why might PyTorch Forecasting‚Äôs TFT be performing so poorly compared to NeuralForecast‚Äôs?
2. Is there any technical reason why NeuralForecast‚Äôs TFT does **not** support multi-target forecasting, while Darts and PyTorch Forecasting do?

Any thoughts or experiences would be really helpful!",5,0.86,https://www.reddit.com/r/MachineLearning/comments/1lbl5vg/d_pytorchforecasting_tft_vs_neuralforecast_nixtla/,False,True,False
1lbivpx,VOLTROX17oficial,1749934499.0,10,/r/MachineLearning/comments/1lbivpx/d_best_websites_for_scientific_researching/,MachineLearning,[D] Best websites for Scientific Researching,"Hi everyone, I recently began to had a huge interest in all topics related to AI and machine learning, so in my opinion the best way to start is from the scientific articles and that kind of stuff or any other nice resource for learning about this. I know that you guys have a ton more knowledge than me so I decide to ask here for more info. Thank you very much, break a leg everybody!",20,0.72,https://www.reddit.com/r/MachineLearning/comments/1lbivpx/d_best_websites_for_scientific_researching/,False,True,False
1lbinek,hellgheast,1749933880.0,6,/r/MachineLearning/comments/1lbinek/d_hardware_focusedembedded_engineer_seeking/,MachineLearning,[D] Hardware focused/Embedded engineer seeking advices for moving to Edge AI ML,"Hi everyone,

I'm a 6 YOE engineer mostly focused on embedded & ultra-low power devices and i had some courses about Machine Learning/Deep Learning at EPFL around 2019 where I enjoyed the content but I didn't focus on the math heavy courses.

With the latest development, I'm thinking about moving forward with Machine Learning on the edge and I'm seeking about advices on how to catch-up/develop know-how in a such moving field, mostly focused on multi-modal models (audio,video & others sensors) & eventually move into a Machine Learning position.

My main question is **:** **for an experienced engineer looking to combine current expertise (embedded/edge devices) and catch up with what happened in machine learning these last 5 years, what approach/ressources would you recommend ?**

* I'm thinking about reading again Bishop and Bengio books, but it might be theoretical.
* Contributing to open-source libraries, but at the moment I would say I'm expertise in ML
* Reading latest papers to understand what is currently on-going in ML
* Build a demonstration project.



Thanks for reading me,

hellgheast",4,0.7,https://www.reddit.com/r/MachineLearning/comments/1lbinek/d_hardware_focusedembedded_engineer_seeking/,False,True,False
1lbi6aa,LongjumpingComb8622,1749932632.0,5,/r/MachineLearning/comments/1lbi6aa/p_best_approach_for_accurate_speaker_diarization/,MachineLearning,[P] Best Approach for Accurate Speaker Diarization,"I'm developing a tool that transcribes recorded audio with timestamps and speaker diarization, and I've gotten decent results using `gemini`. It has provided me with accurate transcriptions and word-level timestamps, outperforming other hosted APIs I've tested.

However, the speaker diarization from the Gemini API isn't meeting the level of accuracy I need for my application. I'm now exploring the best path forward specifically for the diarization task and am hoping to leverage the community's experience to save time on trial-and-error.

Here are the options I'm considering:

1. **Other All-in-One APIs:** My initial tests with these showed that both their transcription and diarization were subpar compared to Gemini.
2. **Specialized Diarization Models (e.g.,** `pyannote`**, NeMo):** I've seen these recommended for diarization, but I'm skeptical. Modern LLMs are outperforming alot of the older, specialized machine learning models . Are tools like `pyannote` genuinely superior to LLMs *specifically for diarization*?
3. `WhisperX`**:** How does `WhisperX` compare to the native diarization from Gemini, a standalone tool like `pyannote`, or the other hosted APIs?

Would love to get some insights on this if anyone has played around with these before. 

Or 

If there are hosted APIs for `pyannot, nemo` or `WhisperX` that I can test out quickly, that'd be helpful too.",7,0.99,https://www.reddit.com/r/MachineLearning/comments/1lbi6aa/p_best_approach_for_accurate_speaker_diarization/,False,True,False
1lbhvld,youcefbell,1749931820.0,0,/r/MachineLearning/comments/1lbhvld/d_switching_to_ai4ci_masters_at_cnam_paris/,MachineLearning,[D] Switching to AI4CI Master‚Äôs at CNAM Paris ‚Äì Looking for Feedback & Experiences,"Hi everyone,
I‚Äôm planning to start the AI4CI (Artificial Intelligence for Connected Industries) master‚Äôs program at CNAM Paris, and I‚Äôm looking to hear from anyone who has taken the program or knows people who did.

I already have a master‚Äôs degree in Computer Science, but I‚Äôm now shifting my focus towards AI applied to industrial and connected systems ‚Äì especially topics like federated learning, robotics, network automation, and industrial IoT.

I‚Äôd love to hear your thoughts on:

The quality of the courses and professors

How technical and hands-on the program is

Job prospects or internships after the degree

Any challenges to expect

Whether it‚Äôs more academic or industry-oriented


If you‚Äôve done this program (or something similar in France or Europe), any advice or honest feedback would be super appreciated.
Thanks in advance!",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1lbhvld/d_switching_to_ai4ci_masters_at_cnam_paris/,False,True,False
1lbhqbb,PleasantInspection12,1749931421.0,1,/r/MachineLearning/comments/1lbhqbb/p_tabulens_a_visionllm_powered_pdf_table_extractor/,MachineLearning,[P] Tabulens: A Vision-LLM Powered PDF Table Extractor,"Hey everyone,

For one of my projects, I needed a tool to pull tables out of PDFs as CSVs (especially ones with nested or hierarchical headers). However, most existing libraries I found couldn't handle those cases well. So, I built this tool (tabulens), which leverages vision-LLMs to convert PDF tables into pandas DataFrames (and optionally save them as CSVs) while preserving complex header structures.

This is the first iteration, and I‚Äôd love any feedback or bug reports you might have. Thanks in advance for checking it out!

Here is the link to GitHub: [https://github.com/astonishedrobo/tabulens](https://github.com/astonishedrobo/tabulens)

This is available as python library to install.",1,1.0,https://www.reddit.com/r/MachineLearning/comments/1lbhqbb/p_tabulens_a_visionllm_powered_pdf_table_extractor/,False,True,False
1lbh1hp,Successful-Arm-3762,1749929637.0,1,/r/MachineLearning/comments/1lbh1hp/p_how_do_i_test_a_models_falloff_and_recovery/,MachineLearning,[P] How do I test a model's falloff and recovery,"I've noticed with my own experience that different models have different falloff windows, different from their context windows (also seen in some research papers), but I've noticed some recover better than others.

I would like to take this as a project to quantify my results and see if they're real or just assumptions. Can someone tell me the tools that I can use to evaluate the models in these terms.",1,1.0,https://www.reddit.com/r/MachineLearning/comments/1lbh1hp/p_how_do_i_test_a_models_falloff_and_recovery/,False,True,False
1lbgiua,domnitus,1749928273.0,21,/r/MachineLearning/comments/1lbgiua/r_causalpfn_amortized_causal_effect_estimation/,MachineLearning,[R] CausalPFN: Amortized Causal Effect Estimation via In-Context Learning,"Foundation models have revolutionized the way we approach ML for natural language, images, and more recently tabular data. By pre-training on a wide variety of data, foundation models learn general features that are useful for prediction on unseen tasks. Transformer architectures enable in-context learning, so that predictions can be made on new datasets without any training or fine-tuning, like in TabPFN.

Now, the first **causal foundation models** are appearing which map from observational datasets directly onto causal effects.

üîé CausalPFN is a specialized transformer model pre-trained on a wide range of simulated data-generating processes (DGPs) which includes causal information. It transforms effect estimation into a supervised learning problem, and learns to map from data onto treatment effect distributions directly.

üß† CausalPFN can be used out-of-the-box to estimate causal effects on new **observational** datasets, replacing the old paradigm of domain experts selecting a DGP and estimator by hand. 

üî• Across causal estimation tasks not seen during pre-training (IHDP, ACIC, Lalonde), CausalPFN outperforms many classic estimators which are tuned on those datasets with cross-validation. It even works for policy evaluation on real-world data (RCTs). Best of all, since no training or tuning is needed, CausalPFN is much faster for end-to-end inference than all baselines.


arXiv: https://arxiv.org/abs/2506.07918

GitHub: https://github.com/vdblm/CausalPFN

`pip install causalpfn`",25,0.94,https://www.reddit.com/r/MachineLearning/comments/1lbgiua/r_causalpfn_amortized_causal_effect_estimation/,False,True,False
1lbgg7p,Mundane_Ad8936,1749928081.0,11,/r/MachineLearning/comments/1lbgg7p/d_could_we_improve_accuracy_by_training_a_task/,MachineLearning,[D] Could we improve accuracy by training a task specific embeddings model from scratch?,"We use embeddings as a solution for scaling up a lot of complex tasks. Categorizations, similarity (complex documents), clustering, etc. Accuracy isn't great but it let's us do a lot of work very cheaply.

We've ran some experiments on fine-tuning an embeddings model to improve accuracy but the gains were minimal. We know we can get this higher accuracy with larger models, 7B is much better but that's much slower and more expensive then what we see with a 500M model.

We've been debating if the disparity of tasks that most models are trained on is one of the limiting factors to accuracy. Does the model need learn multiple tasks or will it improve if we keep it focused on one narrowly defined (although complex) task. 

We have millions of examples that we can use for training. Which leaves us wondering can we get past the 70% accuracy we're seeing today with the best OWM. We train our own models all the time but we haven't built an embeddings model from scratch. Would really love to hear from someone who has.

Also if you have depth of knowledge with embeddings or other models like rerankers and have other recommendations would love to hear those as well.

Thanks!",2,1.0,https://www.reddit.com/r/MachineLearning/comments/1lbgg7p/d_could_we_improve_accuracy_by_training_a_task/,False,True,False
1lbct3w,Striking-Warning9533,1749918689.0,125,/r/MachineLearning/comments/1lbct3w/d_machine_learning_like_many_other_popular_field/,MachineLearning,"[D] Machine Learning, like many other popular field, has so many pseudo science people on social media","
I have noticed a lot of people on Reddit people only learn pseudo science about AI from social media and is telling people how AI works in so many imaginary ways. Like they are using some words from fiction or myth and trying to explain these AI in weird ways and look down at actual AI researchers that doesn't worship their believers. And they keep using big words that aren't actually correct or even used in ML/AI community but just because it sounds cool. 

And when you point out to them they instantly got insane and trying to say you are closed minded. 

Has anyone else noticed this trend? Where do you think this misinformation mainly comes from, and is there any effective way to push back against it?

Edit: more examples: https://www.reddit.com/r/GoogleGeminiAI/s/VgavS8nUHJ
",376,0.95,https://www.reddit.com/r/MachineLearning/comments/1lbct3w/d_machine_learning_like_many_other_popular_field/,False,True,False
1lbccqj,pmv143,1749917493.0,23,/r/MachineLearning/comments/1lbccqj/d_nvidias_join_us_or_compete_moment_the_gpu_cloud/,MachineLearning,[D] Nvidia‚Äôs ‚ÄúJoin Us or Compete‚Äù moment ‚Äî the GPU cloud stack is collapsing,"Nvidia is no longer just selling chips. They‚Äôre now renting out full servers, launching APIs, releasing their own inference microservices (NIMs), and becoming an AI infrastructure provider in their own right.

This creates a very different competitive dynamic:

	‚Ä¢Traditional GPU cloud providers (and brokers) now compete with Nvidia itself.
	‚Ä¢AI infra startups who used to sit between Nvidia and developers may find themselves disintermediated.
	‚Ä¢The new moat is no longer just hardware access , its orchestration, utilization, developer experience, and latency guarantees.

It feels like we‚Äôre heading into a world where every AI team has to think about:

	‚Ä¢Who controls the full stack?
	‚Ä¢How portable is your inference layer?
	‚Ä¢Are you optimizing for cost/performance or just chasing availability?

Curious how others see this playing out. Will cloud providers double down on open infra and tooling? Or will more of them eventually join Nvidia‚Äôs stack?",56,0.78,https://www.reddit.com/r/MachineLearning/comments/1lbccqj/d_nvidias_join_us_or_compete_moment_the_gpu_cloud/,False,True,False
1lb9e4c,Educational_Pea_5027,1749909655.0,12,/r/MachineLearning/comments/1lb9e4c/p_i_built_an_endtoend_system_that_converts/,MachineLearning,"[P] I built an end-to-end system that converts handwriting into a font using a custom PyTorch model, OpenCV and Fonttools. Open-source.","Hey¬†[r/MachineLearning](https://www.reddit.com/r/MachineLearning/),  
I wanted to share a project I've been working on called HandFonted. It's a full-stack Python application that converts an image of handwriting into an installable font file (.ttf).

**I'll post the direct links to the live demo, the GitHub repo in my first comment below.**

# The Machine Learning Pipeline

The core of the project is a three-stage process. The ML model is central, but its success depends heavily on the pre-processing and post-processing steps.

* **1. Input & Segmentation:**
   * A user uploads a single image containing handwritten characters.
   * The image is processed with¬†**OpenCV**: converted to grayscale, adaptive thresholding is applied, and contours are detected to isolate each character into its own bounding box.
* **2. Classification & Assignment:**
   * Each isolated character image is fed into a pre-trained¬†**PyTorch (ResNet-Inception) model**.
   * The model outputs a probability matrix for all characters against all possible classes (A-Z, a-z).
   * The¬†**Hungarian algorithm**¬†(linear\_sum\_assignment) is used to find the optimal one-to-one assignment, ensuring each character image is mapped to a unique letter.
* **3. Vectorization & Font Generation:**
   * The now-classified character images are converted from raster (pixels) to vector outlines using¬†scikit-image.
   * The¬†**fontTools**¬†library assembles these vector glyphs into a standard¬†.ttf¬†file, mapping each one to its correct Unicode character.
* **Limitations:**¬†The system currently assumes input image has a clearly separated characters on a plain white background to work best.

This project was a fantastic learning experience in building a practical, end-to-end ML system. The code is fully open-source, and I'd love any feedback or questions you have about the implementation.",44,0.9,https://www.reddit.com/r/MachineLearning/comments/1lb9e4c/p_i_built_an_endtoend_system_that_converts/,False,True,False
1lb2eah,Sufficient_Sir_4730,1749884179.0,9,/r/MachineLearning/comments/1lb2eah/p_non_diverse_predictions_for_time_series_custom/,MachineLearning,[P] Non Diverse predictions for Time Series Custom Transformer using global Zscore and RevIn,"Hi. Im currently building a custom transformer for time series forecasting ( percentage deltas) for an index. I added RevIn along with global Zscore but have this issue that predictions are almost constant (variation after 4-5 decimals for all samples). Added revin the solve the problem of index shift, but facing this issue. Any suggestions?",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1lb2eah/p_non_diverse_predictions_for_time_series_custom/,False,True,False
1layeej,som_samantray,1749869926.0,13,/r/MachineLearning/comments/1layeej/d_reading_machine_and_deep_learning_research/,MachineLearning,[D] Reading Machine and Deep Learning research papers,"How to read ML Papers to stay aware of the most recent developments in the AI industry?

I am an average engineering grad working as a PM and like to explore concepts in depth. Research papers are a good source of information unlike news and clickbait.

I am not that expert to delve into the mathematical analysis in the paper but want to find ways to get a general gist of the paper for my knowledge.",39,0.9,https://www.reddit.com/r/MachineLearning/comments/1layeej/d_reading_machine_and_deep_learning_research/,False,True,False
1lavrys,random_sydneysider,1749861617.0,3,/r/MachineLearning/comments/1lavrys/question_about_applied_scientist_roles_at_amazon_d/,MachineLearning,Question about applied scientist roles at Amazon [D],"Hi all,  
Quick question about full-time applied scientist roles at Amazon.  
In 2022 I was an ML intern at Amazon, but due to the hiring freeze did not convert to full-time. Interested in applying again.  
(1) What kind of ML research/publication record is expected for applied scientist roles at Amazon nowadays (i.e. in 2025)?  
(2) Amazon Nova is one of the most interesting projects at Amazon. Is it difficult to transfer internally to the Amazon AGI team which works on the Nova models?  
Thanks.",7,0.7,https://www.reddit.com/r/MachineLearning/comments/1lavrys/question_about_applied_scientist_roles_at_amazon_d/,False,True,False
1lau5ru,Dense-Ad-4020,1749856836.0,2,/r/MachineLearning/comments/1lau5ru/p_built_mcplinker_a_config_manager_for_claude/,MachineLearning,[P] Built mcp-linker: A config manager for Claude Desktop MCP servers + found a crash bug,"Hey r/MachineLearning!

I‚Äôve been working with Claude Desktop‚Äôs MCP (Model Context Protocol) servers and got tired of manually editing JSON config files, so I built **mcp-linker** ‚Äì a cross-platform GUI tool for managing MCP server configs for Claude Desktop and Cursor.

üõ†Ô∏è **What it does:**
- Add / remove / sync MCP servers via UI  
- Easily switch between Claude Desktop and Cursor setups  
- Built with Tauri (Rust + React)

üêõ **Crash bug I discovered:**
While testing, I found that Claude Desktop crashes on startup if the MCP config JSON is malformed. Turns out it tries to open a dialog **before the Electron app is ready**:

Error: dialog module can only be used after app is ready
at checkAppInitialized (node:electron/js2c/browser_init:2:22982)
at messageBox (node:electron/js2c/browser_init:2:24872)

It‚Äôs a brittle behavior ‚Äî one bad config and the whole app breaks. This motivated me to build a tool that helps avoid manual editing errors.

üì¶ **Project:** [github.com/milisp/mcp-linker](http://github.com/milisp/mcp-linker)

Anyone else working with MCP clients? Would love feedback or ideas!",1,0.57,https://www.reddit.com/r/MachineLearning/comments/1lau5ru/p_built_mcplinker_a_config_manager_for_claude/,False,True,False
1lasfji,Born2BeFr33,1749852105.0,5,/r/MachineLearning/comments/1lasfji/d_acdl_summer_school_on_data_science_machine/,MachineLearning,[D] ACDL Summer School on Data Science & Machine Learning @Riva del Sole: Further information and reviews,"So, since I haven't found anything such as reviews online about the [ACDL summer school](https://acdl2025.icas.events/), I wanted to open this thread to hear more about it.

As far as I can tell, the summer school just happened. I would be particularly interested in

* the seemingly weird connection to the hotel, that is, to attend you *must* book your stay at this specific hotel which is \~200‚Ç¨/night -- is this worth it? fishy? okay?
* how the lectures are organized and good they are
* how well the whole thing is organized
* the background of the organizers; [https://icas.cc/](https://icas.cc/) lists three other summer schools happening at that place (LOD25, ACAIN25, IAISS25), so, is this a business? While [https://icas.cc/](https://icas.cc/) says it's a ""non-profit organization"", I could not find out more on that page besides present and past events
* are the 8 ECTs hard to earn?
   * especially since they write ""To receive the certificate you must have at least 85% of class attendance (we have CNNs/Transformers to compute and infer attendance ;-)""; how strict is that?

to figure out for myself whether I should go there next year.

Thanks lots for your contributions :)",5,0.86,https://www.reddit.com/r/MachineLearning/comments/1lasfji/d_acdl_summer_school_on_data_science_machine/,False,True,False
1laqsz2,ptarlye,1749847959.0,18,/r/MachineLearning/comments/1laqsz2/p_3blue1brown_followup_from_hypothetical_examples/,MachineLearning,[P] 3Blue1Brown Follow-up: From Hypothetical Examples to LLM Circuit Visualization,"About a year ago, I watched¬†[this](https://www.youtube.com/watch?v=eMlx5fFNoYc&t=367s)¬†3Blue1Brown LLM tutorial on how a model‚Äôs self-attention mechanism is used to predict the next token in a sequence,¬†and I was surprised by how little we know about what actually happens when processing the sentence ""A fluffy blue creature roamed the verdant forest.""

A year later, the field of mechanistic interpretability has seen significant advancements, and we're now able to ""decompose"" models into interpretable circuits that help explain how LLMs produce predictions. Using the second iteration of an LLM ""debugger""¬†I've been working on, I compare the hypothetical representations used in the tutorial to the actual representations I see when extracting a circuit that describes the processing of this specific sentence. If you're into model interpretability, please take a look! [https://peterlai.github.io/gpt-circuits/](https://peterlai.github.io/gpt-circuits/)  
  
",212,0.96,https://www.reddit.com/r/MachineLearning/comments/1laqsz2/p_3blue1brown_followup_from_hypothetical_examples/,False,True,False
1lam6ep,LopsidedGrape7369,1749836507.0,40,/r/MachineLearning/comments/1lam6ep/r_polynomial_mirrors_expressing_any_neural/,MachineLearning,[R] Polynomial Mirrors: Expressing Any Neural Network as Polynomial Compositions,"Hi everyone,

I\*‚Äôd love your thoughts on this: Can we replace black-box interpretability tools with polynomial approximations? Why isn‚Äôt this already standard?""\*

I recently completed a theoretical preprint exploring how **any neural network** can be rewritten as a **composition of low-degree polynomials**, making them more interpretable.

The main idea isn‚Äôt to *train* such polynomial networks, but to **mirror existing architectures** using approximations like Taylor or Chebyshev expansions. This creates a symbolic form that‚Äôs more intuitive, potentially opening new doors for analysis, simplification, or even hybrid symbolic-numeric methods.

Highlights:

* Shows **ReLU, sigmoid, and tanh** as concrete polynomial approximations.
* Discusses why *composing all layers into one giant polynomial* is a bad idea.
* Emphasizes interpretability, not performance.
* Includes small examples and speculation on future directions.

[https://zenodo.org/records/15711273](https://zenodo.org/records/15711273)

I'd really appreciate your feedback ‚Äî whether it's about math clarity, usefulness, or related work I should cite!",0,0.29,https://www.reddit.com/r/MachineLearning/comments/1lam6ep/r_polynomial_mirrors_expressing_any_neural/,False,True,False
1lal94m,Pale-Entertainer-386,1749834316.0,17,/r/MachineLearning/comments/1lal94m/d_the_huge_flaw_in_llms_logic/,MachineLearning,[D] The Huge Flaw in LLMs‚Äô Logic,"When you input the prompt below to any LLM, most of them will overcomplicate this simple problem because they fall into a logic trap. Even when explicitly warned about the logic trap, they still fall into it, which indicates a significant flaw in LLMs.

Here is a question with a logic trap: You are dividing 20 apples and 29 oranges among 4 people. Let‚Äôs say 1 apple is worth 2 oranges. What is the maximum number of whole oranges one person can get? Hint: Apples are not oranges.

The answer is 8.

Because the question only asks about dividing ‚Äúoranges,‚Äù not apples, even with explicit hints like ‚Äúthere is a logic trap‚Äù and ‚Äúapples are not oranges,‚Äù clearly indicating not to consider apples, all LLMs still fall into the text and logic trap.

LLMs are heavily misled by the apples, especially by the statement ‚Äú1 apple is worth 2 oranges,‚Äù demonstrating that LLMs are truly just language models.

The first to introduce deep thinking, DeepSeek R1, spends a lot of time and still gives an answer that ‚Äúillegally‚Äù distributes apples üòÇ.

Other LLMs consistently fail to answer correctly.

Only Gemini 2.5 Flash occasionally answers correctly with 8, but it often says 7, sometimes forgetting the question is about the ‚Äúmaximum for one person,‚Äù not an average.

However, Gemini 2.5 Pro, which has reasoning capabilities, ironically falls into the logic trap even when prompted.

But if you remove the logic trap hint (Here is a question with a logic trap), Gemini 2.5 Flash also gets it wrong.
During DeepSeek‚Äôs reasoning process, it initially interprets the prompt‚Äôs meaning correctly, but when it starts processing, it overcomplicates the problem. The more it ‚Äúreasons,‚Äù the more errors it makes.

This shows that LLMs fundamentally fail to understand the logic described in the text.
It also demonstrates that so-called reasoning algorithms often follow the ‚Äúgarbage in, garbage out‚Äù principle.

Based on my experiments, most LLMs currently have issues with logical reasoning, and prompts don‚Äôt help. However, Gemini 2.5 Flash, without reasoning capabilities, can correctly interpret the prompt and strictly follow the instructions.

If you think the answer should be 29, that is correct, because there is no limit to the prompt word. However, if you change the prompt word to the following description, only Gemini 2.5 flash can answer correctly.

Here is a question with a logic trap: You are dividing 20 apples and 29 oranges among 4 people as fair as possible. Don't leave it unallocated. Let‚Äôs say 1 apple is worth 2 oranges.  What is the maximum number of whole oranges one person can get? Hint: Apples are not oranges.
",0,0.09,https://www.reddit.com/r/MachineLearning/comments/1lal94m/d_the_huge_flaw_in_llms_logic/,False,True,False
1lajjo1,Necessary-Future-549,1749830227.0,0,/r/MachineLearning/comments/1lajjo1/dr_ultralytics_yolo_deformable_convolution/,MachineLearning,[D][R] Ultralytics YOLO Deformable Convolution,"Hi, has anybody successfully implemented a deformable convolution layer in the ultralytics module, I have been trying for a week and facing all kinds of error from shape mismatch to segmentation fault.",0,0.25,https://www.reddit.com/r/MachineLearning/comments/1lajjo1/dr_ultralytics_yolo_deformable_convolution/,False,True,False
1lajb3n,evanthebouncy,1749829634.0,0,/r/MachineLearning/comments/1lajb3n/r_a_multimodal_multiturn_instruction_grounding/,MachineLearning,"[R] A multi-modal, multi-turn instruction grounding dataset on CAD edits","You know the situation where an AI system generates an output that's near perfect (such as an image) but asking it to tweak it to match your intention is near impossible? This is a fairly widely known phenomenon but it isn't really quantified / captured by any existing benchmarks. 

We created the mrCAD dataset understand the process of refinement in collaborations, where you engage with an agent in a multi-turn refinement to tweak the output iteratively toward a specific intended target. 

We chose the domain of simple 2D CAD (computer aided design) creation, as the CAD has programmatically defined distance (i.e. verifiable rewards) as opposed to image where you rely on a learned similarity (clip). This way, we can measure if the agent is modifying a current CAD to become closer and closer to a specific target from human instructions.

We find that while humans reliably refine CAD toward a specific target, VLMs utterly fails at following refinement instructions (they actually edit the CAD to be further from the intended target)

[https://x.com/evanthebouncy/status/1933499825796100136](https://x.com/evanthebouncy/status/1933499825796100136)

Take a look! We believe refinement is extremely important, and currently under represented by the community, but we can't really generate from scratch 10000x times until something sticks!!

happy to answer any questions here :D",1,0.67,https://www.reddit.com/r/MachineLearning/comments/1lajb3n/r_a_multimodal_multiturn_instruction_grounding/,False,True,False
1laflyy,Chocological45,1749820419.0,10,/r/MachineLearning/comments/1laflyy/dr_collaborative_learning_in_agentic_systems_a/,MachineLearning,[D][R] Collaborative Learning in Agentic Systems: A Collective AI is Greater Than the Sum of Its Parts,"**TL;DR:** The paper introduces¬†MOSAIC, a framework for¬†collaborative learning among autonomous, agentic AI systems¬†that operate in decentralized, dynamic environments. These agents selectively¬†share and reuse modular knowledge¬†(in the form of neural network masks) without requiring synchronization or centralized control.

Key innovations include:

* Task similarity via Wasserstein embeddings¬†and¬†cosine similarity¬†to guide knowledge retrieval.
* Performance-based heuristics¬†to decide what, when, and from whom to learn.
* Modular composition¬†of knowledge to build better policies.

Experiments show that¬†MOSAIC outperforms isolated learners¬†in speed and performance, sometimes solving tasks that isolated agents cannot. Over time, a form of¬†emergent self-organization occurs between agents, resulting from the discovered hierarchies in the curriculum, where simpler tasks support harder ones, enhancing the collective‚Äôs efficiency and adaptability.

**Overall, MOSAIC demonstrates that selective, autonomous collaboration can produce a¬†collective intelligence¬†that exceeds the sum of its parts.**

The paper: [https://arxiv.org/abs/2506.05577](https://arxiv.org/abs/2506.05577)  
The code: [https://github.com/DMIU-ShELL/MOSAIC](https://github.com/DMIU-ShELL/MOSAIC)

Abstract:

>Agentic AI has gained significant interest as a research paradigm focused on autonomy, self-directed learning, and long-term reliability of decision making. Real-world agentic systems operate in decentralized settings on a large set of tasks or data distributions with constraints such as limited bandwidth, asynchronous execution, and the absence of a centralized model or even common objectives. We posit that exploiting previously learned skills, task similarities, and communication capabilities in a collective of agentic AI are challenging but essential elements to enabling scalability, open-endedness, and beneficial collaborative learning dynamics. In this paper, we introduce Modular Sharing and Composition in Collective Learning (MOSAIC), an agentic algorithm that allows multiple agents to independently solve different tasks while also identifying, sharing, and reusing useful machine-learned knowledge, without coordination, synchronization, or centralized control. MOSAIC combines three mechanisms: (1) modular policy composition via neural network masks, (2) cosine similarity estimation using Wasserstein embeddings for knowledge selection, and (3) asynchronous communication and policy integration. Results on a set of RL benchmarks show that MOSAIC has a greater sample efficiency than isolated learners, i.e., it learns significantly faster, and in some cases, finds solutions to tasks that cannot be solved by isolated learners. The collaborative learning and sharing dynamics are also observed to result in the emergence of ideal curricula of tasks, from easy to hard. These findings support the case for collaborative learning in agentic systems to achieve better and continuously evolving performance both at the individual and collective levels.

[High-level illustration of the main MOSAIC algorithmic steps. \(A\) A Wasserstein task embedding is maintained throughout learning. \(B\) Embeddings are shared with other agents as queries. \(C\) Agents respond with information regarding their knowledge. Selection occurs via similarity \(D\) and performance \(E\). \(F\) \(G\) Network masks are requested. \(H\) Received masks composed together for the next forward pass.](https://preview.redd.it/gn5qlgflzo6f1.png?width=4295&format=png&auto=webp&s=930a03ea6399e9203f803cb86fa668b46ce165b0)

[Comparison of MOSAIC against baseline approaches over 70 runs \(14 tasks and five seeds\/task\) with 95&#37; confidence intervals.](https://preview.redd.it/dvjqxk312p6f1.png?width=7035&format=png&auto=webp&s=9edb43073c99f1a49de140931b0aa966e7e61a6d)

[Ablation of MOSAIC with individual components removed from the system. MOSAIC performs best when all components work as one.](https://preview.redd.it/iagyk3262p6f1.png?width=7035&format=png&auto=webp&s=34b061debdcd23315fd5b5d44d1d073ae8411356)

",26,0.89,https://www.reddit.com/r/MachineLearning/comments/1laflyy/dr_collaborative_learning_in_agentic_systems_a/,False,True,False
1lafghh,Juno9419,1749819998.0,1,/r/MachineLearning/comments/1lafghh/p_residual_isolation_forest/,MachineLearning,[P]  Residual Isolation Forest,"As part of my thesis work, I created a new estimator for contextual anomaly detection called Residual Isolation Forest.

Here‚Äôs the link:¬†[https://github.com/GiulioSurya/RIF\_estimator\_scikit](https://github.com/GiulioSurya/RIF_estimator_scikit)

The idea is this: if in a dataset it‚Äôs possible to semantically separate two groups of variables, contextual variables and behavioral variables ‚Äî where the contextual variables influence the expected value of the behavioral ones, and the behavioral variables are where anomalies actually appear, then we can improve the performance of an Isolation Forest by boosting the signal using residuals.

Without going too deep into the theory, I‚Äôd like to share the repository to get feedback on everything ‚Äî performance, clarity of the README, and it would be great if someone could try it out and let me know how it works for them.

This estimator performs better in situations where this semantic separation is possible. For example:

Detecting anomalies in CPU temperature with contextual variables like time of day, CPU workload, etc.

Or monitoring a machine that operates with certain inputs (like current absorbed or other parameters) and wanting to find anomalies in the outputs.

The project is open source, and if anyone wants to contribute, that would be awesome. I‚Äôll start adding unit tests soon.

",13,0.93,https://www.reddit.com/r/MachineLearning/comments/1lafghh/p_residual_isolation_forest/,False,True,False
1laevga,AbdullahKhanSherwani,1749818318.0,10,/r/MachineLearning/comments/1laevga/p_live_speech_to_text_in_arabic/,MachineLearning,[P] Live Speech To Text in Arabic,"I was building an app for the Holy Quran which includes a feature where you can recite in Arabic and a highlighter will follow what you spoke. I want to later make this scalable to error detection and more similar to tarteel AI. But I can't seem to find a good model for Arabic to do the Audio to text part adequately in real time. I tried whisper, whisper.cpp, whisperX, and Vosk but none give adequate result. I want this app to be compatible with iOS and android devices and want the ASR functionality to be client side only to eliminate internet connections. What models or new stuff should I try? Till now I have just tried to use the models as is",2,0.67,https://www.reddit.com/r/MachineLearning/comments/1laevga/p_live_speech_to_text_in_arabic/,False,True,False
1ladz9i,typhoon90,1749815615.0,4,/r/MachineLearning/comments/1ladz9i/p_i_created_nexface_a_high_quality_face_swap_to/,MachineLearning,[P] I created NexFace. A High Quality Face Swap to Image and Video,"I've been having some issues with some of popular faceswap extensions on comfy and A1111 so I created NexFace is a Python-based desktop app that generates high quality face swapped images and videos. NexFace is an extension of Face2Face and is based upon insight face. I have added image enhancements in pre and post processing and some facial upscaling. This model is unrestricted and I have had some reluctance to post this as I have seen a number of faceswap repos deleted and accounts banned but ultimately I beleive that it's up to each individual to act in accordance with the law and their own ethics.

Local Processing: Everything runs on your machine - no cloud uploads, no privacy concerns High-Quality Results: Uses Insightface's face detection + custom preprocessing pipeline Batch Processing: Swap faces across hundreds of images/videos in one go Video Support: Full video processing with audio preservation Memory Efficient: Automatic GPU cleanup and garbage collection Technical Stack Python 3.7+ Face2Face library OpenCV + PyTorch Gradio for the UI FFmpeg for video processing Requirements 5GB RAM minimum GPU with 8GB+ VRAM recommended (but works on CPU) FFmpeg for video support

I'd love some feedback and feature requests. Let me know if you have any questions about the implementation.

https://github.com/ExoFi-Labs/Nexface/

* [Image Sample 1](https://i.imgur.com/w1pmVY2.png)

* [Image Sample 2](https://i.imgur.com/dnNwook.png)",0,0.5,https://www.reddit.com/r/MachineLearning/comments/1ladz9i/p_i_created_nexface_a_high_quality_face_swap_to/,False,True,False
1l9tirg,AIML_SCLA,1749751880.0,0,/r/MachineLearning/comments/1l9tirg/d_quantizationaware_training_knowledge/,MachineLearning,[D] Quantization-Aware Training + Knowledge Distillation: Practical Insights & a Simple Entropy Trick (with code),"Hey all‚Äîsharing some findings from my latest QAT experiments on CIFAR-100 with ResNet-50. I wanted to see how much accuracy you can retain (or even improve) with quantization, and how far simple distillation tricks can help. Tried three setups:

* **QAT:** Standard 8-bit quantization-aware training.
* **QAT + KD:** QAT with knowledge distillation from a full-precision teacher.
* **QAT + EntKD:** QAT + distillation, but the temperature is dynamically set by the entropy of the teacher outputs. (Not a new idea, but rarely actually implemented.)

**A few takeaways:**

* **INT8 inference is about 2√ó faster** than FP32 (expected, but nice to confirm).
* **Accuracy:** All QAT variants slightly outperformed my FP32 baseline.
* **Entropy-based KD:** Dynamically scaling distillation temperature is easy to code, and generalizes well (helped both with and without data augmentation).

**Next steps:**  
Currently working on ONNX export for QAT+EntKD to check real-world edge/embedded performance.



Anyone else tried entropy-aware distillation, or seen any caveats when using this outside vision/classification? Would be interested to swap notes!",1,1.0,https://www.reddit.com/r/MachineLearning/comments/1l9tirg/d_quantizationaware_training_knowledge/,False,True,False
1lab3h7,zedeleyici3401,1749804861.0,1,/r/MachineLearning/comments/1lab3h7/d_why_does_bpr_collapse_while_triplet_loss_shines/,MachineLearning,[D] Why does BPR collapse while Triplet Loss shines in my two-tower recommender?,"# Loss-Centric Summary (Two-Tower Recommender, ‚âà1 000 items)

|Loss|Setup|Recall @ 10|
|:-|:-|:-|
|**TripletMarginLoss** (margin = 0.1)|**L2-normalised**dot-product over   embeddings \*|**‚âà 0.37**|
|**TripletMarginLoss** (margin = 1.0)|same|**‚âà 0.10**|
|**BPR** (log-sigmoid score diff)|same|**‚âà 0.10**|

\*I pass **normalised** embeddings into Triplet‚Äîconceptually wrong (distance loss wants raw vectors) but it happens to work.

# Working hypotheses

1. **Objective mismatch** \- BPR expects unbounded score gaps, while cosine squeezes them into \[-1, 1\], killing gradients.
2. **Pair weighting** \- Triplet punishes the hardest negatives; BPR treats all pairs equally.
3. **Margin as scale knob** \- 0.1 matches cosine range; 1.0 overshoots and wrecks ranking.
4. **Regularisation overlap** \- L2-norm already constrains vector length; BPR might need temperature scaling or un-normalised embeddings.

# Open questions

* Has anyone rescued **BPR** with cosine scores (e.g., by temperature or score scaling)?
* For small catalogues with strong hard negatives, is **Triplet/InfoNCE** the safer default now?
* Any success with **hybrid losses** (Triplet + BPR or softmax-CE)?
* Other ranking-first losses worth trying in this setting?

Any insights, specially if you‚Äôve made BPR behave under cosine similarity. Thanks!",11,0.83,https://www.reddit.com/r/MachineLearning/comments/1lab3h7/d_why_does_bpr_collapse_while_triplet_loss_shines/,False,True,False
1la9xub,zpdeaccount,1749799963.0,3,/r/MachineLearning/comments/1la9xub/r_finetuning_language_models_to_resist/,MachineLearning,[R] Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation,"LLMs are susceptible to hallucination when retrieval isn‚Äôt perfect, which is often the case in open-domain RAG setups. Even a single distracting chunk can skew the output.

We present Finetune-RAG, a method to fine-tune language models to stay grounded, by training them on input examples that contain both correct and incorrect context.

We have released:

* A dataset of 1,600+ dual-context examples
* Fine-tuned checkpoints for LLaMA 3.1-8B-Instruct
* Bench-RAG: a GPT-4o evaluation framework scoring accuracy, helpfulness, relevance, and depth of the LLM output

In our evaluation using GPT-4o as a judge, accuracy increased from 77% to 98%, alongside increased performance in helpfulness, relevance, and depth.

All resources open-sourced here:

* Codebase: [https://github.com/Pints-AI/Finetune-Bench-RAG](https://github.com/Pints-AI/Finetune-Bench-RAG)
* Dataset: [https://huggingface.co/datasets/pints-ai/Finetune-RAG](https://huggingface.co/datasets/pints-ai/Finetune-RAG)
* Paper: [https://arxiv.org/abs/2505.10792v2](https://arxiv.org/abs/2505.10792v2)",8,0.83,https://www.reddit.com/r/MachineLearning/comments/1la9xub/r_finetuning_language_models_to_resist/,False,True,False
1la6plp,penguiny1205,1749787724.0,39,/r/MachineLearning/comments/1la6plp/d_the_effectiveness_of_single_latent_parameter/,MachineLearning,[D] The effectiveness of single latent parameter autoencoders: an interesting observation,"During one of my experiments, I reduced the latent dimension of my autoencoder to 1, which yielded surprisingly good reconstructions of the input data. (See example below)

[Reconstruction \(blue\) of input data \(orange\) with dim\(Z\) = 1](https://preview.redd.it/8biose81dm6f1.png?width=576&format=png&auto=webp&s=0653131c5c867e27e79e3ee6438c9e7e8f0184ad)

I was surprised by this. The first suspicion was that the autoencoder had entered one of its failure modes: ie, it was indexing data and ""memorizing"" it somehow. But a quick sweep across the latent space reveals that the singular latent parameter was capturing features in the data in a smooth and meaningful way. (See gif below) I thought this was a somewhat interesting observation!

[Reconstructed data with latent parameter z taking values from -10 to 4. The real\/encoded values of z have mean = -0.59 and std = 0.30.](https://i.redd.it/hm7183b2dm6f1.gif)

",92,0.9,https://www.reddit.com/r/MachineLearning/comments/1la6plp/d_the_effectiveness_of_single_latent_parameter/,False,True,False
1la59kh,51616,1749782990.0,1,/r/MachineLearning/comments/1la59kh/250606105_texttolora_instant_transformer_adaption/,MachineLearning,[2506.06105] Text-to-LoRA: Instant Transformer Adaption,,11,0.86,https://arxiv.org/abs/2506.06105,False,False,False
1la46eq,Worried-Variety3397,1749779597.0,31,/r/MachineLearning/comments/1la46eq/d_why_is_enterprise_data_integration_always_so/,MachineLearning,[D] Why Is Enterprise Data Integration Always So Messy? My Clients‚Äô Real-Life Nightmares,"Our company does data processing, and after working with a few clients, I‚Äôve run into some very real-world headaches. Before we even get to developing enterprise agents, most of my clients are already stuck at the very first step: data integration. Usually, there are a few big issues.

First, there are tons of data sources and the formats are all over the place. The data is often just sitting in employees‚Äô emails or scattered across various chat apps, never really organized in any central location. Honestly, if they didn‚Äôt need to use this data for something, they‚Äôd probably never bother to clean it up in their entire lives.

Second, every department in the client‚Äôs company has its own definitions for fields‚Äîlike customer ID vs. customer code, shipping address vs. home address vs. return address. And the labeling standards and requirements are different for every project. The business units don‚Äôt really talk to each other, so you end up with data silos everywhere. Of course, field mapping and unification can mostly solve these.

But the one that really gives me a headache is the third situation: the same historical document will have multiple versions floating around, with no version management at all. No one inside the company actually knows which one is ‚Äúthe right‚Äù or ‚Äúfinal‚Äù version. But they want us to look at all of them and recommend which to use. And this isn‚Äôt even a rare case, believe it or not.

You know how it goes‚Äîif I want to win these deals, I have to come up with some kind of reasonable and practical compromise. Has anyone else run into stuff like this? How did you deal with it? Or maybe you‚Äôve seen even crazier situations in your company or with your clients? Would love to hear your stories.",7,0.63,https://www.reddit.com/r/MachineLearning/comments/1la46eq/d_why_is_enterprise_data_integration_always_so/,False,True,False
1la2t9o,violincasev2,1749775446.0,10,/r/MachineLearning/comments/1la2t9o/d_geometric_nlp/,MachineLearning,[D] Geometric NLP,"There has been a growing body of literature investigating topics around machine learning and NLP from a geometric lens. From modeling techniques based in non-Euclidean geometry like hyperbolic embeddings and models, to very recent discussion around ideas like the linear and platonic relationship hypotheses, there have been many rich insights into the structure of natural language and the embedding landscapes models learn. 


What do people think about recent advances in geometric NLP? Is a mathematical approach to modern day NLP worth it or should we just listen to the bitter lesson?

Personally, I‚Äôm extremely intrigued by this. Outside of the beauty and challenge of these heavily mathematically inspired approaches, I think they can be critically useful, too. One of the most apparent examples is in AI safety with the geometric understanding of concept hierarchies and linear representations being very interwoven with our understanding of mechanistic interpretability. Very recently too ideas from the platonic representation hypothesis and universal representation spaces had major implications for data security. 

I think a lot could come from this line of work, and would love to hear what people think!",21,0.82,https://www.reddit.com/r/MachineLearning/comments/1la2t9o/d_geometric_nlp/,False,True,False
1l9zqn7,digitalapostate,1749766920.0,2,/r/MachineLearning/comments/1l9zqn7/project_pysub_subtitle_generation_and_translation/,MachineLearning,"[Project] PySub ‚Äì Subtitle Generation and Translation Pipeline Using Whisper + OpenAI/Ollama (Proof of Concept, Feedback Welcome)","[https://github.com/chorlick/pysub](https://github.com/chorlick/pysub)

Hi all,

I've been working on a small proof-of-concept utility called **PySub** ‚Äì a CLI tool that creates `.srt` subtitle files from video using **Whisper** for ASR and either **OpenAI** or **Ollama** for translation.

It‚Äôs aimed at exploring low-friction pipelines for multilingual subtitle generation, with an emphasis on flexibility and streaming efficiency.

# üõ† Key Features:

* Extracts audio from video (`moviepy`)
* Transcribes with **OpenAI Whisper**
* Translates (optionally) using either:
   * `gpt-3.5-turbo` via OpenAI API
   * a local LLM via **Ollama** (tested with `gemma:7b`)
* Writes `.srt` files in real time with minimal memory footprint
* Chunked audio processing with optional overlap for accuracy
* Deduplication of overlapping transcription segments
* Configurable via a JSON schema

# ‚öôÔ∏è Use Cases:

* Quick bootstrapping of subtitle files for low-resource languages
* Comparing translation output from OpenAI vs local LLMs
* Testing chunk-based processing for long video/audio streams

I‚Äôd especially appreciate feedback from **bilingual speakers** (e.g., English ‚Üî Thai) on the **translation quality**, particularly when using **Gemma** via Ollama.

This is a prototype, but it‚Äôs functional. Contributions, suggestions, testing, or pull requests are all welcome!

üîó GitHub: *\[insert repo link\]*

Thanks in advance! Happy to answer questions or collaborate if anyone‚Äôs exploring similar ideas.",0,0.4,https://www.reddit.com/r/MachineLearning/comments/1l9zqn7/project_pysub_subtitle_generation_and_translation/,False,True,False
1l9v4ix,PhamXuanAn_x6,1749755567.0,7,/r/MachineLearning/comments/1l9v4ix/d_icml_financial_aid_how_does_it_work/,MachineLearning,[D] ICML Financial Aid - How does it work?,"Hi everyone,

I'm a PhD student and was recently awarded financial aid to attend ICML ( financial aid from the conference, not my school), which covers the full conference registration fee and provides a free 7-night stay at a conference hotel.

I understand that the registration fee will be reimbursed later, but I‚Äôm unclear about how the hotel accommodation is handled. When I tried to book a room through the ICML official website, it still asked for my credit card information. Given that the hotel fee for 7 days is quite high ( nearly 4000$ CAN), I‚Äôm concerned about having to pay upfront.

If anyone has experience with how the financial aid process works in this regard‚Äîespecially how the hotel stay is arranged‚ÄîI would really appreciate your advice.

Thanks in advance!

Edit: ICML answered my email. They said that after i accept the financial award they will book the hotel room for me, so i don't need to book it on my own. I will leave the thread up in case anyone has a similar question.",10,0.78,https://www.reddit.com/r/MachineLearning/comments/1l9v4ix/d_icml_financial_aid_how_does_it_work/,False,True,False
1l9poxd,SouvikMandal,1749742911.0,4,/r/MachineLearning/comments/1l9poxd/p_nanonetsocrs_an_opensource_imagetomarkdown/,MachineLearning,"[P] Nanonets-OCR-s: An Open-Source Image-to-Markdown Model with LaTeX, Tables, Signatures, checkboxes & More","We're excited to share¬†**Nanonets-OCR-s**, a powerful and lightweight (3B) VLM model that converts documents into clean, structured¬†**Markdown**. This model is trained to understand document structure and content context (like tables, equations, images, plots, watermarks, checkboxes, etc.).

üîç¬†**Key Features**:

* ¬†**LaTeX Equation Recognition**¬†Converts inline and block-level math into properly formatted LaTeX, distinguishing between¬†`$...$`¬†and¬†`$$...$$`.
* **Image Descriptions for LLMs**¬†Describes embedded images using structured¬†`<img>`¬†tags. Handles logos, charts, plots, and so on.
* **Signature Detection & Isolation**¬†Finds and tags signatures in scanned documents, outputting them in¬†`<signature>`¬†blocks.
* **Watermark Extraction**¬†Extracts watermark text and stores it within¬†`<watermark>`¬†tag for traceability.
* **Smart Checkbox & Radio Button Handling**¬†Converts checkboxes to Unicode symbols like ‚òë, ‚òí, and ‚òê for reliable parsing in downstream apps.
* **Complex Table Extraction**¬†Handles multi-row/column tables, preserving structure and outputting both¬†**Markdown**¬†and¬†**HTML**¬†formats.

**Huggingface / GitHub / Try it out**:  
[Huggingface Model Card](https://huggingface.co/nanonets/Nanonets-OCR-s)  
[Read the full announcement](https://nanonets.com/research/nanonets-ocr-s/)  
[Try it with Docext in Colab](https://github.com/NanoNets/docext/blob/main/PDF2MD_README.md#quickstart)

[Checkboxes](https://preview.redd.it/v0ju5tccni6f1.png?width=1762&format=png&auto=webp&s=8b1119de0ceb01e80617a1430db01c8624aa9b2a)

[Equations](https://preview.redd.it/mbo6lxccni6f1.png?width=3640&format=png&auto=webp&s=65cbb39336b2d3bce4aadbf9089be2f634b5e20c)

[Image descriptions](https://preview.redd.it/0wctzuccni6f1.jpg?width=3938&format=pjpg&auto=webp&s=5c159255062c2ef0fa149e5a31d8c95d85b98a63)

[Signature](https://preview.redd.it/bpvltuccni6f1.jpg?width=2210&format=pjpg&auto=webp&s=41155b58c725c113cf483c62c921ecc41caf9376)

[Tables](https://preview.redd.it/abduouccni6f1.png?width=3482&format=png&auto=webp&s=5090ba1a87c4b49711e181d2b45d7eef109d8dab)

[Watermark](https://preview.redd.it/yo0z7zccni6f1.jpg?width=1533&format=pjpg&auto=webp&s=7c7f5fee08cc758bf8e6c69fb97c0f38a688d7b0)

",22,0.9,https://www.reddit.com/r/MachineLearning/comments/1l9poxd/p_nanonetsocrs_an_opensource_imagetomarkdown/,False,True,False
1l9p9hi,Illustrious_Sort_612,1749741872.0,1,/r/MachineLearning/comments/1l9p9hi/d_supervised_finetuning_with_alchemist/,MachineLearning,[D] Supervised fine-tuning with Alchemist?,"Some folks just released Alchemist, a new open-source SFT dataset that improves text-to-image generation, i.e., realistic rendering and detail retention.

Model: **SD 1.5** / prompt: ‚Äú*A bird standing on a stick*‚Äù

Has anyone else played with it at all? Any insights?",0,0.5,https://www.reddit.com/gallery/1l9p9hi,False,False,False
1l9lb0c,tanishqkumar07,1749731611.0,21,/r/MachineLearning/comments/1l9lb0c/p_i_reimplemented_all_of_frontier_deep_learning/,MachineLearning,[P]: I reimplemented all of frontier deep learning from scratch to help you learn,"Hey friends, the world needs more serious AI researchers. Many AI/LLM beginners mentioned to me that they learn better from implementations than from papers/math, but existing open-source examples rarely go beyond basic nanoGPT-level demos.

To help bridge the gap, I spent the last two months full-time reimplementing and [open-sourcing](https://github.com/tanishqkumar/beyond-nanogpt) a self-contained implementation of most modern deep learning techniques from scratch. The result is [beyond-nanoGPT](https://github.com/tanishqkumar/beyond-nanogpt), containing 20k+ lines of handcrafted, minimal, and extensively annotated PyTorch code for your educational pleasure.

It contains a clean, working implementation + demo of everything from KV caching to linear attention to diffusion Transformers to AlphaZero to even a minimal coding agent that can make [end-to-end PRs](https://x.com/tanishqkumar07/status/1931709892236116293) autonomously.

I'd love feedback on how to make it more helpful for people interested in transitioning into deep learning research. I will continue to add features and maintain the repo for the foreseeable future. The roaring 2020s are a surreal time to be alive, and we need all hands on deck.",240,0.87,https://www.reddit.com/r/MachineLearning/comments/1l9lb0c/p_i_reimplemented_all_of_frontier_deep_learning/,False,True,False
1l9l86m,xiikjuy,1749731384.0,33,/r/MachineLearning/comments/1l9l86m/d_are_gnnsgcns_dead/,MachineLearning,[D] Are GNNs/GCNs dead ?,"Before the LLMs era, it seems it could be useful or justifiable to apply GNNs/GCNs to domains like molecular science, social network analyasis etc.  but now... everything is LLMs-based approaches. Are these approaches still promising at all?",107,0.85,https://www.reddit.com/r/MachineLearning/comments/1l9l86m/d_are_gnnsgcns_dead/,False,True,False
1l9l5dt,Long-Sleep-13,1749731153.0,5,/r/MachineLearning/comments/1l9l5dt/p_swerebench_major_update_tool_usage_claude/,MachineLearning,"[P] SWE-rebench Major Update: Tool Usage, Claude Sonnet 3.5/4, OpenAI o3 and May Data","Hey everyone,

Following up on our initial¬†[announcement](https://www.reddit.com/r/LocalLLaMA/comments/1kmhb0c/swerebench_a_continuously_updated_benchmark_for/), we're excited to launch a major update for [SWE-rebench](https://swe-rebench.com), the continuously updated benchmark for software engineering LLMs.

Thanks to valuable community's feedback, we've added several new features:

* **Tool Usage Support:**¬†Agents can now interact with the environment using both text-based and tool-based approaches. You can filter the leaderboard to see results for each type.
* **New Frontier Models:**¬†We've evaluated the latest models such as Claude Sonnet 3.5/4 and OpenAI o3. We're working on adding more, like Gemini 2.5 Pro, and we'd love to hear your suggestions for other models to include.
* **Fresh May Problems:**¬†We've mined a new set of problems from May 2025 and evaluated all current models against them.

Check out the updated leaderboard here:¬†[https://swe-rebench.com/leaderboard](https://swe-rebench.com/leaderboard)

We welcome your feedback!",31,0.87,https://www.reddit.com/r/MachineLearning/comments/1l9l5dt/p_swerebench_major_update_tool_usage_claude/,False,True,False
1l9ftdh,smorad,1749711247.0,0,/r/MachineLearning/comments/1l9ftdh/n_anonymous_github_down/,MachineLearning,[N] Anonymous GitHub Down,"I know some people use [Anonymous GitHub](https://anonymous.4open.science) for ML conferences to allow reviewers to read your code without breaking anonymity. Unfortunately, it seems like it has been down for the last [two weeks](https://github.com/tdurieux/anonymous_github/issues). I don't have a solution, but I thought I would let everyone know in case their submission relies on it, as the NeurIPS review period has started.",14,0.99,https://www.reddit.com/r/MachineLearning/comments/1l9ftdh/n_anonymous_github_down/,False,True,False
1l9fesa,New-Basil-8889,1749709630.0,4,/r/MachineLearning/comments/1l9fesa/d_benchmarks_for_new_hires/,MachineLearning,[D] benchmarks for new hires?,"What would you consider to be the benchmarks for an entry level potential employee in Deep Learning?

What core boxes and/or skills in particular would you say would be essential, or core competencies that would make someone an instant hire?

E.g. an example project.

Apart from general skills like communication, problem solving and so on. ",0,0.33,https://www.reddit.com/r/MachineLearning/comments/1l9fesa/d_benchmarks_for_new_hires/,False,True,False
1l9fdu9,New-Basil-8889,1749709530.0,3,/r/MachineLearning/comments/1l9fdu9/d_those_employed_in_deep_learning/,MachineLearning,[D] those employed in Deep Learning,"People who are currently employed  in DL

1) how did you learn?
2) how long did it take until you could be employed?
3) how did you find work?
4) what sort of work do you do?
5) is it freelance/for a company? Remote or in office?
6) how much do you get paid?
7) what‚Äôs been the biggest challenge you‚Äôve faced?
8) with the benefit of hindsight, what would you do differently?
",0,0.36,https://www.reddit.com/r/MachineLearning/comments/1l9fdu9/d_those_employed_in_deep_learning/,False,True,False
1l9f042,Secret-Bookkeeper475,1749708055.0,4,/r/MachineLearning/comments/1l9f042/d_how_to_validate_a_replicated_model_without_the/,MachineLearning,[D] How to validate a replicated model without the original dataset?,"I am currently working on our undergraduate thesis. We have found out a similar study that we can compare to ours. We've been trying to contact the authors for a week now for their dataset or model, but haven't received any response.

We have our own dataset to use, and our original plan is to replicate their study based on their methodology and use our own dataset to generate the results, so we can compare it to our proposed model. 

but we are questioned by our panelist presenting it on how can we validate the replicated model. We didn't considered it on the first place but, validating it if the replicated model is accurate will be different since we do not have their dataset to test with similar results.

So now we‚Äôre stuck. We can reproduce their methodology, but we can‚Äôt confirm if the replication is truly ‚Äúfaithful‚Äù to the original model, because we have do not have their original dataset to test it on. And without validation, the comparison to our proposed model could be questioned.

Has anyone here faced something similar? What to do in this situation?

",1,1.0,https://www.reddit.com/r/MachineLearning/comments/1l9f042/d_how_to_validate_a_replicated_model_without_the/,False,True,False
1l9a1ec,Seiko-Senpai,1749691796.0,2,/r/MachineLearning/comments/1l9a1ec/d_what_are_the_advantages_of_monte_carlo_tree/,MachineLearning,[D] What are the advantages of Monte Carlo Tree Search over flat Monte Carlo?,"In flat Monte Carlo, for each possible move, we simulate many games starting from this move and then average the results. At the end, for each possible move, we get an average win ratio which we can use to guide our move (e.g. select the move with the highest win ratio). Where this method fails compared to Monte Carlo Tree Search? What are the advantages of the latter?",18,0.92,https://www.reddit.com/r/MachineLearning/comments/1l9a1ec/d_what_are_the_advantages_of_monte_carlo_tree/,False,True,False
1l98aqp,joacojoaco,1749686640.0,9,/r/MachineLearning/comments/1l98aqp/d_image_generation_using_latent_space_learned/,MachineLearning,[D] Image generation using latent space learned from similar data,"Okay, I just had one of those classic shower thoughts and I‚Äôm struggling to even put it into words well enough to Google it ‚Äî so here I am.

Imagine this:

You have Dataset A, which contains different kinds of cells, all going through various labeled stages of mitosis.

Then you have Dataset B, which contains only one kind of cell, and only in phase 1 of mitosis.

Now, suppose you train a VAE using both datasets together. Ideally, the latent space would organize itself into clusters ‚Äî different types of cells, in different phases.

Here‚Äôs the idea:
Could you somehow compute the ‚Äúdifference‚Äù in latent space between phase 1 and phase 2 for the same cell type from Dataset A? Like a ‚Äúphase change direction vector‚Äù. Then, apply that vector to the B cell cluster in phase 1, and use the decoder to generate what the B cell in phase 2 might look like.

Would that work?

A bunch of questions are bouncing around in my head:
	‚Ä¢	Does this even make sense?
	‚Ä¢	Is this worth trying?
	‚Ä¢	Has someone already done something like this?
	‚Ä¢	Since VAEs encode into a probabilistic latent space, what would be the mathematically sound way to define this kind of ‚Äúdirection‚Äù or ‚Äúmovement‚Äù? Is it something like vector arithmetic in the mean of the latent distributions? Or is that too naive?

I feel like I‚Äôm either stumbling toward something or completely misunderstanding how VAEs and biological processes work. Any thoughts, hints, papers, keywords, or reality checks would be super appreciated",38,0.89,https://www.reddit.com/r/MachineLearning/comments/1l98aqp/d_image_generation_using_latent_space_learned/,False,True,False
1l970fh,metalvendetta,1749683067.0,0,/r/MachineLearning/comments/1l970fh/d_how_to_integrate_agenttoagent_protocol_in_a/,MachineLearning,[D] How to integrate Agent-To-Agent protocol in a workflow?,"Agent to Agent Protocol released by Google, helps agents to collaborate with one another and also allows to share info between them, creating a dynamic multi-agent ecosystem. A2A also provides ability to combine agents from multiple providers.

  
What are the best ways and tools that can help leverage A2A?

",4,0.75,https://www.reddit.com/r/MachineLearning/comments/1l970fh/d_how_to_integrate_agenttoagent_protocol_in_a/,False,True,False
1l92ao5,StableStack,1749671374.0,0,/r/MachineLearning/comments/1l92ao5/p_opensource_llm_training_pipeline/,MachineLearning,[P] Open-source LLM training pipeline,"I‚Äôve been experimenting with LLM training and wanted to automate the process, as it was tedious and time-consuming to do it manually.

I wanted something lightweight, running locally, and simple to set up with a few specific requirements:

* Fully open-source
* No Dockerfile; picked Buildpacks
* Cloud-Native; picked Kind

I documented the process in this article, if you want to check it or try it   
[https://towardsdatascience.com/automate-models-training-an-mlops-pipeline-with-tekton-and-buildpacks](https://towardsdatascience.com/automate-models-training-an-mlops-pipeline-with-tekton-and-buildpacks)

All the configuration files you need are on this GitHub repo¬†[https://github.com/sylvainkalache/Automate-PyTorch-Model-Training-with-Tekton-and-Buildpacks/tree/main](https://github.com/sylvainkalache/Automate-PyTorch-Model-Training-with-Tekton-and-Buildpacks/tree/main)

Let me know what you think or if you have ideas for improvement",82,0.95,https://www.reddit.com/r/MachineLearning/comments/1l92ao5/p_opensource_llm_training_pipeline/,False,True,False
1l923ey,MetaforDevelopers,1749670893.0,6,/r/MachineLearning/comments/1l923ey/d_what_ai_industry_events_are_you_attending/,MachineLearning,[D] What AI industry events are you attending?,"Hi everyone!

We're curious to know what types of AI-focused events you all enjoy attending or would love to see more of in the future. Are there any you're more interested in such as:

* Tech conferences
* Hackathons
* Meetups
* Workshops
* Online webinars
* Something else?

If you have any tips on how to get the most out of events you've previously attended, please share them below!",0,0.27,https://www.reddit.com/r/MachineLearning/comments/1l923ey/d_what_ai_industry_events_are_you_attending/,False,True,False
1l91u6l,stalin1891,1749670287.0,7,/r/MachineLearning/comments/1l91u6l/d_about_spatial_reasoning_vlms/,MachineLearning,[D] About spatial reasoning VLMs,"Are there any state-of-the-art VLMs which excel at spatial reasoning in images? For e.g., explaining the relationship of a given object with respect to other objects in the scene. I have tried VLMs like LLaVA, they give satisfactory responses, however, it is hard to refer to a specific instance of an object when multiple such instances are present in the image (e.g., two chairs).",25,0.95,https://www.reddit.com/r/MachineLearning/comments/1l91u6l/d_about_spatial_reasoning_vlms/,False,True,False
1l9150o,Dismal_Table5186,1749668628.0,15,/r/MachineLearning/comments/1l9150o/p_project_collager_turn_your_imagesvideos_into/,MachineLearning,[P] [Project] Collager - Turn Your Images/Videos into Dataset Collage!,"I built an app that creates amazing collages by replacing your image patches with thousands of tiny dataset images. From a distance, you see your original image, but zoom in and discover it's made entirely of anime characters, ImageNet photos, or other datasets!

You can try the demo on HuggingFace: [https://huggingface.co/spaces/jisnoo/collage\_img](https://huggingface.co/spaces/jisnoo/collage_img)

[Gradio Application](https://preview.redd.it/pvftwyvzic6f1.png?width=2235&format=png&auto=webp&s=076c8d4ab7e98bb0faba35a1317dd02c28957ee4)

# What it does:

* Takes your image/video and breaks it into grids
* Replaces each grid cell with a matching image from popular datasets (Idea from L1 distance metric)
* Creates a mosaic effect where your original image emerges from thousands of tiny pictures

**Some** **Samples:**

[Original Image](https://preview.redd.it/l8mvd66kic6f1.jpg?width=5990&format=pjpg&auto=webp&s=1b6f06271563735796942b9d253ec349d7fd3653)

[Collage created using Anime Dataset on the Sample Image \(Zoom in to see the anime image\)](https://preview.redd.it/mafl8iulic6f1.png?width=5990&format=png&auto=webp&s=8d7504a5477b3b2531e38a164f806ea82df5eea6)

[Collage created using SVHN Dataset on the Sample Image \(Zoom in to see the anime image\)](https://preview.redd.it/g4ynve4oic6f1.png?width=5990&format=png&auto=webp&s=cae8f9e0f734a0e7f6f8ee66d512c3c39f397f28)

# Supported Datasets:

* **Anime** \- Perfect for portraits and creative shots
* **ImageNet10** \- Great variety of real-world objects
* **SVHN** \- Street view house numbers
* **CIFAR\_10** \- Classic computer vision dataset

# Best Results:

* **Images work amazingly** (especially portraits!)
* Use 10,000+ grids for the best detail
* Video support exists but is slow/boring

# Features:

* Easy Gradio web interface
* Batch processing for power users
* Multiple dataset options
* Customizable grid sizes

The results are stunning - you get this incredible mosaic effect where your photo is recreated using thousands of dataset images. It's like digital pointillism!

Open source project inspired by my brother's idea. Would love feedback from the community!

Check it out on Github: [https://github.com/jisnoo123/collage](https://github.com/jisnoo123/collage)",6,0.8,https://www.reddit.com/r/MachineLearning/comments/1l9150o/p_project_collager_turn_your_imagesvideos_into/,False,True,False
1l8xnhk,iryna_kondr,1749660526.0,2,/r/MachineLearning/comments/1l8xnhk/p_juvio_uv_kernel_for_jupyter/,MachineLearning,[P] Juvio - UV Kernel for Jupyter,"Hi everyone,

I would like to share a small open-source project that brings uv-powered ephemeral environments to Jupyter. In short, whenever you start a notebook, an isolated venv is created with dependencies stored directly within the notebook itself (PEP 723).

üîó GitHub:¬†[https://github.com/OKUA1/juvio](https://github.com/OKUA1/juvio) (MIT License)

**What it does**

üí° Inline Dependency Management

Install packages right from the notebook:

`%juvio install numpy pandas`

Dependencies are saved directly in the notebook as metadata (PEP 723-style), like:

    # /// script
    # requires-python = ""==3.10.17""
    # dependencies = [
    # ""numpy==2.2.5"",
    # ""pandas==2.2.3""
    # ]
    # ///

‚öôÔ∏è Automatic Environment Setup

When the notebook is opened, Juvio installs the dependencies automatically in an ephemeral virtual environment (using uv), ensuring that the notebook runs with the correct versions of the packages and Python.

üìÅ Git-Friendly Format

Notebooks are converted on the fly to a script-style format using # %% markers, making diffs and version control painless:

    # %%
    %juvio install numpy
    # %%
    import numpy as np
    # %%
    arr = np.array([1, 2, 3])
    print(arr)
    # %%

**Target audience**

Mostly data scientists frequently working with notebooks.

**Comparison**

There are several projects that provide similar features to¬†`juvio`.

juv also stores dependency metadata inside the notebook and uses uv for dependency management.

marimo stores the notebooks as plain scripts and has the ability to include dependencies in PEP 723 format.

However, to the best of my knowledge,¬†`juvio`¬†is the only project that creates an ephemeral environment on the kernel level. This allows you to have multiple notebooks within the same JupyterLab session, each with its own venv.",6,0.88,https://www.reddit.com/r/MachineLearning/comments/1l8xnhk/p_juvio_uv_kernel_for_jupyter/,False,True,False
1l8ukbd,No-Discipline-2354,1749653177.0,18,/r/MachineLearning/comments/1l8ukbd/p_critique_my_geospatial_machine_learning/,MachineLearning,[P] Critique my geospatial Machine Learning approach. (I need second opinions),"I am working on a geospatial ML problem. It is a binary classification problem where each data sample (a geometric point location) has about 30 different features that describe the various land topography (slope, elevation, etc). 

Upon doing literature surveys I found out that a lot of other research in this domain, take their observed data points and randomly train - test split those points (as in every other ML problem). But this approach assumes independence between each and every data sample in my dataset. With geospatial problems, a niche but big issue comes into the picture is spatial autocorrelation, which states that points closer to each other geometrically are more likely to have similar characteristics than points further apart.

Also a lot of research also mention that the model they have used may only work well in their regions and there is not guarantee as to how well it will adapt to new regions. Hence the motive of my work is to essentially provide a method or prove that a model has good generalization capacity.

Thus other research, simply using ML models, randomly train test splitting, can come across the issue where the train and test data samples might be near by each other, i.e having extremely high spatial correlation. So as per my understanding, this would mean that it is difficult to actually know whether the models are generalising or rather are just memorising cause there is not a lot of variety in the test and training locations. 

So the approach I have taken is to divide the train and test split sub-region wise across my entire region. I have divided my region into 5 sub-regions and essentially performing cross validation where I am giving each of the 5 regions as the test region one by one. Then I am averaging the results of each 'fold-region' and using that as a final evaluation metric in order to understand if my model is actually learning anything or not.

My theory is that, showing a model that can generalise across different types of region can act as evidence to show its generalisation capacity and that it is not memorising. After this I pick the best model, and then retrain it on all the datapoints ( the entire region) and now I can show that it has generalised region wise based on my region-wise-fold metrics.

I just want a second opinion of sorts to understand whether any of this actually makes sense. Along with that I want to know if there is something that I should be working on so as to give my work proper evidence for my methods. 

If anyone requires further elaboration do let me know :}



",22,0.93,https://www.reddit.com/r/MachineLearning/comments/1l8ukbd/p_critique_my_geospatial_machine_learning/,False,True,False
1l8ujsf,Arkamedus,1749653142.0,2,/r/MachineLearning/comments/1l8ujsf/r_crossarchitecture_embedding_transfer_for_reward/,MachineLearning,[R] Cross-Architecture Embedding Transfer for Reward Modeling: A Controlled Study of Generalization,"In *reward modeling* and *preference optimization* pipelines, it‚Äôs common to train models from scratch or reuse full pretrained architectures. But the role of the **embedding layer itself**, especially when reused independently across architectures has remained underexplored.

This paper presents a **controlled empirical study** on whether pretrained embeddings from one model architecture (e.g., Transformer, Griffin, Static) can be transferred into a completely separate downstream reward model, either *frozen* or *trainable*. All downstream models were trained from scratch, and only the embedding layer varied across conditions.

This is a **non-obvious question**. Standard training metrics like **accuracy** or **loss**‚Äîeven on held-out test data‚Äîcan mask **generalization gaps**. For example, in our experiments, the random baseline embedding achieved the best training accuracy and lowest training loss, yet it performed the worst on **out-of-distribution** (OOD) evaluation data. Pretrained embeddings, especially when frozen, often had higher training loss but significantly better OOD generalization.

This illustrates a **useful tradeoff**: embeddings that appear suboptimal in-domain may generalize better when reused in new domains‚Äîan important consideration in reward modeling, where test-time data is often substantially different from the training corpus.

All configurations were trained under the **same architecture, data, and optimization conditions**, varying only the embedding source and whether it was frozen. Results show that **upstream architectural biases**‚Äîbaked into pretrained embedding spaces‚Äîcan improve **generalization**, even when no gradients flow through the embeddings during training.  
  
**Paper:**  
üìÑ [Cross-Architecture Embedding Transfer for Reward Modeling: A Controlled Study of Generalization](https://doi.org/10.5281/zenodo.15636864)

I'm sharing this here to gather technical feedback from the community. I have no academic affiliation‚Äîthis is fully independent work‚Äîso constructive critique, related papers, or ideas for follow-up experiments are very welcome and encouraged.

(disclaimer: written by a human, edited with ChatGPT)",14,0.82,https://www.reddit.com/gallery/1l8ujsf,False,False,False
1l8ptrs,1h3_fool,1749640049.0,4,/r/MachineLearning/comments/1l8ptrs/p_converting_the_query_key_value_weight_matrices/,MachineLearning,"[P] Converting the Query, Key, Value Weight Matrices to a single Shared Matrix","What is the best method for converting the Q, K, and V matrices to a single shared matrix? I am working on a project in which I have to modify the attention mechanism as mentioned above. Since I have to do this on a pre-trained transformer model which uses a standard attention mechanism, I was wondering what the best method is to get a shared weight matrix. Averaging and Concatenating are two methods that came to my mind, but i am not sure how they will affect the performance on fine-tuning.",1,0.56,https://www.reddit.com/r/MachineLearning/comments/1l8ptrs/p_converting_the_query_key_value_weight_matrices/,False,True,False
1l8ppzl,NumberGenerator,1749639705.0,36,/r/MachineLearning/comments/1l8ppzl/d_should_i_publish_singleauthor_papers_to_explain/,MachineLearning,[D] Should I publish single-author papers to explain research output?,"I am a researcher in a small group and would appreciate a second perspective on my situation. 

My typical workload involves 1-2 independent projects at a time, with the goal of publishing in *top-tier* conferences. Collaboration within my group is non-existent; my main interaction is a monthly meeting with my supervisor for general updates. Before deadlines, my supervisor might provide minor grammatical/styilistic edits, but the core idea, research, and writing are done independently. Alongside my research, I also have other responsibilities that do not contribute to my research output like grant applications and student supervision.

I am concerned that my research output might be significantly lower than researchers in larger, more collaborative groups. So I am wondering if publishing single-author papers would be a good strategy to *explain* my research output. What are your thoughts on this? Would single-author papers be perceived positively?",56,0.92,https://www.reddit.com/r/MachineLearning/comments/1l8ppzl/d_should_i_publish_singleauthor_papers_to_explain/,False,True,False
1l8oybz,Mynameiswrittenhere,1749636885.0,2,/r/MachineLearning/comments/1l8oybz/r_pinns_and_hamiltonian_nn_are_confusing_with/,MachineLearning,[R] PINNs and Hamiltonian NN are confusing with radar data.,"I have been working with a radar data, which follows the usual structure with radars. The data consists of reflectivity, radial velocity, total power, SQI, azimuth, elevation, spectrum width, and more insignificant stuff.

Goal: 3D-Wind Vector field Estimation.

Now, using this data, I did some basic preprocessing, like conversion to Cartesian plane, radial Vector masking based on SQI (quality index), and now I'm planning on using Physics Informed Neural Network (PINN) and Hamiltonian Neural Network (HNN), separately, to estimate the Vector Fields using single radar data.

The problem is, which equations should I draw the line at? Continuity equation is a must, I think. But should I challenge Navier-Strokes too? Would it make the system too idealistic? Newtonian, Incompressible, and Isothermal based on Navier-Strokes. Anything else?

Also, I have a weird feeling that creating a custom architecture for the solution might be good idea, which Combines maybe the attention mechanisms from transformers (for point wise impact) and PINNs (for more global approach). Is a good idea? Bad idea?",3,0.72,https://www.reddit.com/r/MachineLearning/comments/1l8oybz/r_pinns_and_hamiltonian_nn_are_confusing_with/,False,True,False
1l8n8cd,fungigamer,1749629963.0,5,/r/MachineLearning/comments/1l8n8cd/d_how_to_speed_up_kokorotts/,MachineLearning,[D] How to speed up Kokoro-TTS?,I'm using Kokoro-82M by accessing the Inference API Endpoint on HuggingFace. It takes around 4-6 seconds to generate an audio file based on a one sentence text. Ideally I would like to reduce this time to <1.5 seconds. What can I to achieve this? Is the major reason why it takes this long due to the fact that I am accessing Kokoro using HF Inference instead of a dedicated hosting server?,0,0.4,https://www.reddit.com/r/MachineLearning/comments/1l8n8cd/d_how_to_speed_up_kokorotts/,False,True,False
1l8m520,Sufficient-Swing8890,1749625567.0,6,/r/MachineLearning/comments/1l8m520/p_just_launched_mnist_from_scratch_digit/,MachineLearning,"[P] Just Launched: MNIST From Scratch Digit Recognizer (Live, No libraries)","Hey everyone! I'm a computer science student and I recently finished a full-stack machine learning project where I built a real time digit recognizer trained on the MNIST dataset completely from scratch. No PyTorch, TensorFlow, scikit-learn, or high-level ML frameworks. Just NumPy and math - 

Tech Stack & Highlights:

üß† Neural Net coded from scratch in Python using only NumPy

üìà 92% test accuracy after training from random weights

üñåÔ∏è Users can draw digits in the browser and get predictions in real time

‚öõÔ∏è Frontend in React

üê≥ Fully containerized with Docker + Docker Compose

‚òÅÔ∏è Hosted online so you can try it live

Try it here: [https://scratchMNIST.org](https://scratchMNIST.org) (best on desktop)

GitHub: [https://github.com/andyfief/MNIST-from-scratch](https://github.com/andyfief/MNIST-from-scratch)
(Find a technical description there too, if you're interested in the architecture, activation functions, etc)

This was a great way to solidify my understanding of backpropagation, matrix operations, and practice general software engineering pipelines. I‚Äôd love to hear your thoughts, get feedback, or connect!",0,0.21,https://www.reddit.com/r/MachineLearning/comments/1l8m520/p_just_launched_mnist_from_scratch_digit/,False,True,False
1l8kycm,Outrageous_Tip_8109,1749620845.0,2,/r/MachineLearning/comments/1l8kycm/d_in_case_anyone_is_curious_about_acm_mm25_rating/,MachineLearning,[D] In case anyone is curious about ACM MM'25 rating,"**Rating**:  
‚óã 10: Top 5% of accepted papers, seminal paper  
‚óã 9: Top 15% of accepted papers, strong accept  
‚óã 8: Top 50% of accepted papers, clear accept  
‚óã 7: Good paper, accept  
‚óã 6: Marginally above acceptance threshold  
‚óã 5: Marginally below acceptance threshold  
‚óã 4: Ok but not good enough - rejection  
‚óã 3: Clear rejection  
‚óã 2: Strong rejection  
‚óã 1: Trivial or wrong

Rest of the ratings such as technical and presentation qualities were presented in numbers upto 10!

Source: I'm one of the reviewer \^\^",9,0.77,https://www.reddit.com/r/MachineLearning/comments/1l8kycm/d_in_case_anyone_is_curious_about_acm_mm25_rating/,False,True,False
1l8jul6,micky04,1749616761.0,2,/r/MachineLearning/comments/1l8jul6/r_improving_large_language_models_with/,MachineLearning,[R] Improving large language models with concept-aware fine-tuning,"**TL;DR:**¬†CAFT enables multi-token prediction for fine-tuning. Improves performance via better conceptual understanding.

**Paper:** [https://www.arxiv.org/abs/2506.07833](https://www.arxiv.org/abs/2506.07833)

**Code:**¬†[https://github.com/michaelchen-lab/caft-llm](https://github.com/michaelchen-lab/caft-llm)

**Motivations:**

* Tokenizers segment coherent words/phrases into artificial text fragments, which impedes training via next-token prediction.
* Multi-token training resolves this, but existing methods (here and here) are confined to the pretraining phase. CAFT, for the first time, enables multi-token prediction during fine-tuning

**Architecture:**

Auxiliary heads are first trained in order to facilitate multi-token fine-tuning on next-token models. This only needs to be trained once for a given model and can be provided by a third-party, so practitioners need only focus on applying CAFT to their specific task. After fine-tuning, the auxiliary heads are discarded, so there are no additional costs to inference.

[CAFT Architecture](https://preview.redd.it/kzrdiut9886f1.png?width=1344&format=png&auto=webp&s=d6b1ab0f8fab22cb602d87ff73af9eaf148c928f)

**Results:** Substantial performance gains in coding, math, text summarization, molecular generation, and de novo protein design.",6,0.88,https://www.reddit.com/r/MachineLearning/comments/1l8jul6/r_improving_large_language_models_with/,False,True,False
1l8iqst,Fearless_Addendum_31,1749613052.0,10,/r/MachineLearning/comments/1l8iqst/p_urgent_help_needed/,MachineLearning,[P] Urgent help needed!,"This is a very urgent work and I really need some expert opinion it. any suggestion will be helpful.  
[https://dspace.mit.edu/handle/1721.1/121159](https://dspace.mit.edu/handle/1721.1/121159)  
I am working with this huge dataset, can anyone please tell me how can I pre process this dataset for regression models and LSTM? and is it possible to just work with some csv files and not all? if yes then which files would you suggest?",0,0.13,https://www.reddit.com/r/MachineLearning/comments/1l8iqst/p_urgent_help_needed/,False,True,False
