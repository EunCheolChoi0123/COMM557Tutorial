{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ae80b1c4253c45be869eb4bde5e541b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2eff2edb78714e189cf42beb5aed0a27",
              "IPY_MODEL_80778e1b3bbf49a59dc15f7d2ab2aecd",
              "IPY_MODEL_c44571dfbaa84b5ab706ae6ed7126aaa"
            ],
            "layout": "IPY_MODEL_a34742bbbc804ba39a9eaf3c49edd581"
          }
        },
        "2eff2edb78714e189cf42beb5aed0a27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5940ce79da024ea19e784dfb6b9e356c",
            "placeholder": "​",
            "style": "IPY_MODEL_287efd1e9cc241cd8e48cd4754481450",
            "value": "generation_config.json: 100%"
          }
        },
        "80778e1b3bbf49a59dc15f7d2ab2aecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c86b8d56d617462ab6fe0f436be52956",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62934e529707469cb98581d83139c2e1",
            "value": 124
          }
        },
        "c44571dfbaa84b5ab706ae6ed7126aaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7bd274d38d549da84cea851954d6fe9",
            "placeholder": "​",
            "style": "IPY_MODEL_35ed1fcec1194e82ba3081f90befdab2",
            "value": " 124/124 [00:00&lt;00:00, 5.16kB/s]"
          }
        },
        "a34742bbbc804ba39a9eaf3c49edd581": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5940ce79da024ea19e784dfb6b9e356c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "287efd1e9cc241cd8e48cd4754481450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c86b8d56d617462ab6fe0f436be52956": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62934e529707469cb98581d83139c2e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7bd274d38d549da84cea851954d6fe9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35ed1fcec1194e82ba3081f90befdab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58c9421be5d147ddaedac3c98e1f7747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e435cb0ebb784a1da1c1fda68b6a67c7",
              "IPY_MODEL_a82b1aa347764c66a9df8ab7b4df468a",
              "IPY_MODEL_c0fda04d14da4204b8f544b85f8980c3"
            ],
            "layout": "IPY_MODEL_ff77da88d84e44239d866b3af9176bdd"
          }
        },
        "e435cb0ebb784a1da1c1fda68b6a67c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f48914aa60e24060b10940c9301758b9",
            "placeholder": "​",
            "style": "IPY_MODEL_21b8f5bafe3d498f803940838342d45f",
            "value": "generation_config.json: 100%"
          }
        },
        "a82b1aa347764c66a9df8ab7b4df468a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4f14370736442bbaad56a37072fd5c6",
            "max": 242,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b7db5a5cce046dcb9ae294833af09f9",
            "value": 242
          }
        },
        "c0fda04d14da4204b8f544b85f8980c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f70f9f599754bb2be9029d92605714b",
            "placeholder": "​",
            "style": "IPY_MODEL_f4b5a3888ad14fd591f8dd82d15020f6",
            "value": " 242/242 [00:00&lt;00:00, 17.2kB/s]"
          }
        },
        "ff77da88d84e44239d866b3af9176bdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f48914aa60e24060b10940c9301758b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21b8f5bafe3d498f803940838342d45f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4f14370736442bbaad56a37072fd5c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b7db5a5cce046dcb9ae294833af09f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f70f9f599754bb2be9029d92605714b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4b5a3888ad14fd591f8dd82d15020f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Let's see how GPT models work, by looking at GPT-2\n",
        "- https://huggingface.co/gpt2\n",
        "- Smallest version, with 124M parameters\n",
        "- Released in 2019, https://openai.com/research/better-language-models"
      ],
      "metadata": {
        "id": "6I8a0SzIlyEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First, let's initialize GPT-2 models and tokenizers"
      ],
      "metadata": {
        "id": "asoTFMEjb8py"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znjEoA6G-qAq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ae80b1c4253c45be869eb4bde5e541b3",
            "2eff2edb78714e189cf42beb5aed0a27",
            "80778e1b3bbf49a59dc15f7d2ab2aecd",
            "c44571dfbaa84b5ab706ae6ed7126aaa",
            "a34742bbbc804ba39a9eaf3c49edd581",
            "5940ce79da024ea19e784dfb6b9e356c",
            "287efd1e9cc241cd8e48cd4754481450",
            "c86b8d56d617462ab6fe0f436be52956",
            "62934e529707469cb98581d83139c2e1",
            "c7bd274d38d549da84cea851954d6fe9",
            "35ed1fcec1194e82ba3081f90befdab2"
          ]
        },
        "outputId": "3adeadc2-7d03-4a77-9b55-d3fea978227f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae80b1c4253c45be869eb4bde5e541b3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextStreamer, GenerationConfig\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's see how our GPT-2 tokenizer tokenizes."
      ],
      "metadata": {
        "id": "XgZ0a0PtcG6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode a text prompt\n",
        "input_text = \"I am hungry. Let's go to McDonald's and eat some\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ6A2fRllQUV",
        "outputId": "51ed6a5f-07fc-4e0c-9c5d-794bf6ba8a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   40,   716, 14720,    13,  3914,   338,   467,   284, 14115,   338,\n",
              "           290,  4483,   617]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtPy5EBXcNkU",
        "outputId": "0173c821-29e6-434f-8b51-2b08d507c0e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'Ġam',\n",
              " 'Ġhungry',\n",
              " '.',\n",
              " 'ĠLet',\n",
              " \"'s\",\n",
              " 'Ġgo',\n",
              " 'Ġto',\n",
              " 'ĠMcDonald',\n",
              " \"'s\",\n",
              " 'Ġand',\n",
              " 'Ġeat',\n",
              " 'Ġsome']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now let's throw the phrase in and let GPT-2 calculate the distribution for the next token."
      ],
      "metadata": {
        "id": "pqy1hEkodEVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass through the model\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    logits = outputs.logits[:, -1, :]"
      ],
      "metadata": {
        "id": "tZvYMQcHlO3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply temperature scaling\n",
        "temperature = 0.5\n",
        "scaled_logits = logits / temperature\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probabilities = F.softmax(scaled_logits, dim=-1)"
      ],
      "metadata": {
        "id": "egi9eQyAdZmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort probabilities to get top values and indices\n",
        "top_k = 5  # Number of top tokens you want to consider\n",
        "top_values, top_indices = torch.topk(probabilities, top_k)\n",
        "\n",
        "# Map top indices to tokens and print them alongside their probabilities\n",
        "for value, index in zip(top_values[0], top_indices[0]):\n",
        "    token = tokenizer.decode([index.item()])\n",
        "    print(f\"Token: {token}, Index: {index}, Probability: {value.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWQkazID_pkH",
        "outputId": "ea00bcd8-8f2c-4367-f0b9-7a9a63cd3634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token:  chicken, Index: 9015, Probability: 0.31880518794059753\n",
            "Token:  food, Index: 2057, Probability: 0.2663107216358185\n",
            "Token:  of, Index: 286, Probability: 0.07248274236917496\n",
            "Token:  fries, Index: 31757, Probability: 0.0721219852566719\n",
            "Token:  McDonald, Index: 14115, Probability: 0.03124968893826008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notice how different temperature settings change the distribution.\n",
        "\n",
        "The lower the temperature is, the more deterministic the output is."
      ],
      "metadata": {
        "id": "vBUBAy6OdfDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply temperature scaling\n",
        "temperature = 0.1\n",
        "scaled_logits = logits / temperature\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probabilities = F.softmax(scaled_logits, dim=-1)\n",
        "\n",
        "# Sort probabilities to get top values and indices\n",
        "top_k = 5  # Number of top tokens you want to consider\n",
        "top_values, top_indices = torch.topk(probabilities, top_k)\n",
        "\n",
        "# Map top indices to tokens and print them alongside their probabilities\n",
        "for value, index in zip(top_values[0], top_indices[0]):\n",
        "    token = tokenizer.decode([index.item()])\n",
        "    print(f\"Token: {token}, Index: {index}, Probability: {value.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NAF0Go2do5d",
        "outputId": "5049ce37-fce0-4372-987c-d20cf47bce5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token:  chicken, Index: 9015, Probability: 0.7102358937263489\n",
            "Token:  food, Index: 2057, Probability: 0.2888943552970886\n",
            "Token:  of, Index: 286, Probability: 0.00043148142867721617\n",
            "Token:  fries, Index: 31757, Probability: 0.00042086924077011645\n",
            "Token:  McDonald, Index: 14115, Probability: 6.427331754821353e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply temperature scaling\n",
        "temperature = 1\n",
        "scaled_logits = logits / temperature\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probabilities = F.softmax(scaled_logits, dim=-1)\n",
        "\n",
        "# Sort probabilities to get top values and indices\n",
        "top_k = 5  # Number of top tokens you want to consider\n",
        "top_values, top_indices = torch.topk(probabilities, top_k)\n",
        "\n",
        "# Map top indices to tokens and print them alongside their probabilities\n",
        "for value, index in zip(top_values[0], top_indices[0]):\n",
        "    token = tokenizer.decode([index.item()])\n",
        "    print(f\"Token: {token}, Index: {index}, Probability: {value.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Yf2Qougdo2u",
        "outputId": "c24a8b15-98ea-474b-afb6-6db74ccf4aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token:  chicken, Index: 9015, Probability: 0.07896645367145538\n",
            "Token:  food, Index: 2057, Probability: 0.07217292487621307\n",
            "Token:  of, Index: 286, Probability: 0.03765279799699783\n",
            "Token:  fries, Index: 31757, Probability: 0.03755897656083107\n",
            "Token:  McDonald, Index: 14115, Probability: 0.02472309209406376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lastly, let GPT-2 complete the sentence."
      ],
      "metadata": {
        "id": "yWWdnZNNexXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"I am hungry. Let's go to McDonald's and eat some\", return_tensors=\"pt\", return_attention_mask=True)\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    max_length=150, temperature=0.01, top_p=0.95, repetition_penalty=1.1,\n",
        "    do_sample=True, use_cache=True,\n",
        "    eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id,\n",
        "    transformers_version=\"4.33.1\"\n",
        "    )\n",
        "\n",
        "outputs = model.generate(**inputs, generation_config=generation_config)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04THGsefgV71",
        "outputId": "c230eeda-7ae4-4b0a-9874-4fcac6be7e4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am hungry. Let's go to McDonald's and eat some chicken.\"\n",
            "\"What?\" I asked, looking at the menu on my phone as if it were a question that had been raised in front of me for years now by people who have never heard about this restaurant before or even know what they are talking here about. \"Why don't you just take your kids out there with us? We're going home tonight!\" The waitress said excitedly while she was eating her lunch; we all knew exactly how much better off our children would be when their parents got back from school today! It wasn' so bad because everyone else is still working hard but not really doing anything except watching TV (which isn`t very good either). She then\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now let's try a better model, Qwen2.5-1.5B-Instruct.\n",
        "\n",
        "- Released on Sep 2024 by Qwen team in Alibaba Cloud\n",
        "- 1.5 Billion parameters, around 3 GB\n",
        "- Also the model weights are publicly accessible.\n",
        "- https://huggingface.co/Qwen/Qwen2.5-3B-Instruct\n",
        "- https://github.com/QwenLM/Qwen2.5\n",
        "- It is an instruction-tuned model, which tailors an LLM to Q&A tasks, behave more like a chatbot. (https://www.ibm.com/topics/instruction-tuning)\n",
        "- Also preference-tuned with RLHF (Reinforcement Learning with Human Feedback, https://www.assemblyai.com/blog/how-rlhf-preference-model-tuning-works-and-how-things-may-go-wrong/).\n",
        "- Of course, there are much bigger and better models with publicly accessible model weights. However, these models are too large to fit in Free Google Colab environment without some smart tricks like quantization."
      ],
      "metadata": {
        "id": "03HrxYmx_j0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, GenerationConfig\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "\n",
        "# Move the model to the desired device (e.g., CUDA)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Get the appropriate device\n",
        "model.to(device)  # Move the model to the device"
      ],
      "metadata": {
        "id": "O8ZtSr0nzHod",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513,
          "referenced_widgets": [
            "58c9421be5d147ddaedac3c98e1f7747",
            "e435cb0ebb784a1da1c1fda68b6a67c7",
            "a82b1aa347764c66a9df8ab7b4df468a",
            "c0fda04d14da4204b8f544b85f8980c3",
            "ff77da88d84e44239d866b3af9176bdd",
            "f48914aa60e24060b10940c9301758b9",
            "21b8f5bafe3d498f803940838342d45f",
            "c4f14370736442bbaad56a37072fd5c6",
            "8b7db5a5cce046dcb9ae294833af09f9",
            "2f70f9f599754bb2be9029d92605714b",
            "f4b5a3888ad14fd591f8dd82d15020f6"
          ]
        },
        "outputId": "879b11ae-7194-4af9-998c-ee45b03e9c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58c9421be5d147ddaedac3c98e1f7747"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 1536)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
              "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
              "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
              "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
              "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
              "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode a text prompt\n",
        "input_text = \"I am hungry. Let's go to McDonald's and eat some\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device) # Move input_ids to the device"
      ],
      "metadata": {
        "id": "YU1lMOq0zWMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass through the model\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    logits = outputs.logits[:, -1, :]\n",
        "\n",
        "# Apply temperature scaling\n",
        "temperature = 0.5\n",
        "scaled_logits = logits / temperature\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probabilities = F.softmax(scaled_logits, dim=-1)"
      ],
      "metadata": {
        "id": "wHOG1Wcjz4PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort probabilities to get top values and indices\n",
        "top_k = 5  # Number of top tokens you want to consider\n",
        "top_values, top_indices = torch.topk(probabilities, top_k)\n",
        "\n",
        "# Map top indices to tokens and print them alongside their probabilities\n",
        "for value, index in zip(top_values[0], top_indices[0]):\n",
        "    token = tokenizer.decode([index.item()])\n",
        "    print(f\"Token: {token}, Index: {index}, Probability: {value.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2X4vwh478hc",
        "outputId": "3e19b79c-3af6-4013-f52a-79f855a47eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token:  hamburg, Index: 56847, Probability: 0.8182252645492554\n",
            "Token:  burgers, Index: 62352, Probability: 0.08032900840044022\n",
            "Token:  food, Index: 3607, Probability: 0.04636388272047043\n",
            "Token:  fast, Index: 4937, Probability: 0.020614776760339737\n",
            "Token:  chicken, Index: 16158, Probability: 0.010842937976121902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Along with model capabilities, the context size also got bigger over time.\n",
        "- This means we can cram in more information in our prompts about how we want the models to behave, previous chat logs, long questions, and so on. It follows the instruction we give it quite well.\n",
        "- Yeah, basically what we all do with ChatGPT nowadays."
      ],
      "metadata": {
        "id": "mjFY8D8nneJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "?GenerationConfig"
      ],
      "metadata": {
        "id": "srs6_X80SfKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys_prompt = \"First choose the right option, and then provide your explanation.\"\n",
        "\n",
        "prompt = \"\"\"What is McDonald's famous burger called?\n",
        "A. Whopper\n",
        "B. Big Mac\n",
        "C. Baconator\n",
        "D. Double-double\"\"\"\n",
        "\n",
        "input_text = f\"\"\"<|im_start|>system\n",
        "{sys_prompt}<|im_end|>\n",
        "<|im_start|>user\n",
        "{prompt}<|im_end|>\n",
        "<|im_start|>assistant\"\"\"\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    max_length=1024, temperature=0.01, top_p=0.95, repetition_penalty=1.1,\n",
        "    do_sample=True, use_cache=True, max_new_tokens=1000,\n",
        "    eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id,\n",
        "    transformers_version=\"4.33.1\"\n",
        "    )\n",
        "\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=False)\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", return_attention_mask=False).to('cuda')\n",
        "outputs = model.generate(**inputs, streamer=streamer, generation_config=generation_config)"
      ],
      "metadata": {
        "id": "SCKojhIDLDj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjDmL-5nM_sM",
        "outputId": "f00c555e-e929-4c8a-f49b-8b407e9d68a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|im_start|>system\n",
            "First choose the right option, and then provide your explanation.<|im_end|>\n",
            "<|im_start|>user\n",
            "What is McDonald's famous burger called?\n",
            "A. Whopper\n",
            "B. Big Mac\n",
            "C. Baconator\n",
            "D. Double-double<|im_end|>\n",
            "<|im_start|>assistant\n",
            "The correct answer is B. Big Mac.\n",
            "\n",
            "Explanation:\n",
            "- The \"Big Mac\" is a popular fast food hamburger sandwich that originated in 1967 at the White Castle restaurant chain.\n",
            "- It consists of two all-beef patties, special sauce, lettuce, cheese, pickles, and American cheese on a sesame seed bun.\n",
            "- The name \"Big Mac\" was chosen because it was intended to be twice as big as the original White Castle hamburgers.\n",
            "- While other options like the Whopper (option A) are also very popular, they were not named after this specific sandwich but rather after their respective ingredients or namesake brands.\n",
            "- Option C, the Baconator, is actually a different type of sandwich with bacon instead of beef.\n",
            "- Option D, the Double-double, refers to a dessert item consisting of two scoops of ice cream on top of a waffle cone.<|im_end|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main takeaways:\n",
        "- While ChatGPT may seem miraculous, it is not magic nor did it emerge from a vacuum.\n",
        "- The open-source community has contributed significant efforts to the development of LLMs.\n",
        "- Although open-weight models do not match the performance of GPT-4~5 models, they are smaller, more efficient, and open.\n",
        "- These open-weight models allow researchers to look deeper into the inner workings of large language models (LLMs).\n",
        "\n"
      ],
      "metadata": {
        "id": "TH_ca7yDoex-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# On Your Own\n",
        "- Try to Prompt-Engineer the task you want to try out!"
      ],
      "metadata": {
        "id": "tY9sRI8TC1cf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b2vm7LxnC3o5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}